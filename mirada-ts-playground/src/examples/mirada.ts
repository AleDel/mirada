export interface Node_modules_mirada_dist_src_browser_cameraHelper_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_browser_canvasRender_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_browser_imageCreation_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_browser_index_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_file_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_format_canvasCodec_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_format_format_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_format_index_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_format_jimpCodec_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_index_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_opencvReady_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_tool_grabCut_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_tool_index_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types__cv_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_mirada_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_emscripten_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv__hacks_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv__types_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_Affine3_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_Algorithm_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_AutoBuffer_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_BFMatcher_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_BOWTrainer_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_CascadeClassifier_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_calib3d_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_core_cluster_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_core_hal_interface_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_DescriptorMatcher_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_core_utils_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_core_array_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_dnn_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_DynamicBitset_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_Exception_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_features2d_draw_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_HOGDescriptor_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_FlannBasedMatcher_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_imgproc_color_conversions_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_imgproc_draw_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_imgproc_feature_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_imgproc_filter_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_imgproc_hist_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_imgproc_misc_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_imgproc_object_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_imgproc_shape_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_index_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_imgproc_transform_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_Logger_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_LshTable_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_MatExpr_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_MatOp_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_Matx_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_Node_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_objdetect_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_Mat_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_PCA_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_photo_inpaint_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_RotatedRect_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_softdouble_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_softfloat_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_video_track_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_util_base64_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_util_fileUtil_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_util_imageUtil_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_util_index_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_util_misc_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface mirada_definition {
	node_modules_mirada_dist_src_browser_cameraHelper_d_ts: Node_modules_mirada_dist_src_browser_cameraHelper_d_t;
	node_modules_mirada_dist_src_browser_canvasRender_d_ts: Node_modules_mirada_dist_src_browser_canvasRender_d_t;
	node_modules_mirada_dist_src_browser_imageCreation_d_ts: Node_modules_mirada_dist_src_browser_imageCreation_d_t;
	node_modules_mirada_dist_src_browser_index_d_ts: Node_modules_mirada_dist_src_browser_index_d_t;
	node_modules_mirada_dist_src_file_d_ts: Node_modules_mirada_dist_src_file_d_t;
	node_modules_mirada_dist_src_format_canvasCodec_d_ts: Node_modules_mirada_dist_src_format_canvasCodec_d_t;
	node_modules_mirada_dist_src_format_format_d_ts: Node_modules_mirada_dist_src_format_format_d_t;
	node_modules_mirada_dist_src_format_index_d_ts: Node_modules_mirada_dist_src_format_index_d_t;
	node_modules_mirada_dist_src_format_jimpCodec_d_ts: Node_modules_mirada_dist_src_format_jimpCodec_d_t;
	node_modules_mirada_dist_src_index_d_ts: Node_modules_mirada_dist_src_index_d_t;
	node_modules_mirada_dist_src_opencvReady_d_ts: Node_modules_mirada_dist_src_opencvReady_d_t;
	node_modules_mirada_dist_src_tool_grabCut_d_ts: Node_modules_mirada_dist_src_tool_grabCut_d_t;
	node_modules_mirada_dist_src_tool_index_d_ts: Node_modules_mirada_dist_src_tool_index_d_t;
	node_modules_mirada_dist_src_types__cv_d_ts: Node_modules_mirada_dist_src_types__cv_d_t;
	node_modules_mirada_dist_src_types_mirada_d_ts: Node_modules_mirada_dist_src_types_mirada_d_t;
	node_modules_mirada_dist_src_types_emscripten_d_ts: Node_modules_mirada_dist_src_types_emscripten_d_t;
	node_modules_mirada_dist_src_types_opencv__hacks_d_ts: Node_modules_mirada_dist_src_types_opencv__hacks_d_t;
	node_modules_mirada_dist_src_types_opencv__types_d_ts: Node_modules_mirada_dist_src_types_opencv__types_d_t;
	node_modules_mirada_dist_src_types_opencv_Affine3_d_ts: Node_modules_mirada_dist_src_types_opencv_Affine3_d_t;
	node_modules_mirada_dist_src_types_opencv_Algorithm_d_ts: Node_modules_mirada_dist_src_types_opencv_Algorithm_d_t;
	node_modules_mirada_dist_src_types_opencv_AutoBuffer_d_ts: Node_modules_mirada_dist_src_types_opencv_AutoBuffer_d_t;
	node_modules_mirada_dist_src_types_opencv_BFMatcher_d_ts: Node_modules_mirada_dist_src_types_opencv_BFMatcher_d_t;
	node_modules_mirada_dist_src_types_opencv_BOWTrainer_d_ts: Node_modules_mirada_dist_src_types_opencv_BOWTrainer_d_t;
	node_modules_mirada_dist_src_types_opencv_CascadeClassifier_d_ts: Node_modules_mirada_dist_src_types_opencv_CascadeClassifier_d_t;
	node_modules_mirada_dist_src_types_opencv_calib3d_d_ts: Node_modules_mirada_dist_src_types_opencv_calib3d_d_t;
	node_modules_mirada_dist_src_types_opencv_core_cluster_d_ts: Node_modules_mirada_dist_src_types_opencv_core_cluster_d_t;
	node_modules_mirada_dist_src_types_opencv_core_hal_interface_d_ts: Node_modules_mirada_dist_src_types_opencv_core_hal_interface_d_t;
	node_modules_mirada_dist_src_types_opencv_DescriptorMatcher_d_ts: Node_modules_mirada_dist_src_types_opencv_DescriptorMatcher_d_t;
	node_modules_mirada_dist_src_types_opencv_core_utils_d_ts: Node_modules_mirada_dist_src_types_opencv_core_utils_d_t;
	node_modules_mirada_dist_src_types_opencv_core_array_d_ts: Node_modules_mirada_dist_src_types_opencv_core_array_d_t;
	node_modules_mirada_dist_src_types_opencv_dnn_d_ts: Node_modules_mirada_dist_src_types_opencv_dnn_d_t;
	node_modules_mirada_dist_src_types_opencv_DynamicBitset_d_ts: Node_modules_mirada_dist_src_types_opencv_DynamicBitset_d_t;
	node_modules_mirada_dist_src_types_opencv_Exception_d_ts: Node_modules_mirada_dist_src_types_opencv_Exception_d_t;
	node_modules_mirada_dist_src_types_opencv_features2d_draw_d_ts: Node_modules_mirada_dist_src_types_opencv_features2d_draw_d_t;
	node_modules_mirada_dist_src_types_opencv_HOGDescriptor_d_ts: Node_modules_mirada_dist_src_types_opencv_HOGDescriptor_d_t;
	node_modules_mirada_dist_src_types_opencv_FlannBasedMatcher_d_ts: Node_modules_mirada_dist_src_types_opencv_FlannBasedMatcher_d_t;
	node_modules_mirada_dist_src_types_opencv_imgproc_color_conversions_d_ts: Node_modules_mirada_dist_src_types_opencv_imgproc_color_conversions_d_t;
	node_modules_mirada_dist_src_types_opencv_imgproc_draw_d_ts: Node_modules_mirada_dist_src_types_opencv_imgproc_draw_d_t;
	node_modules_mirada_dist_src_types_opencv_imgproc_feature_d_ts: Node_modules_mirada_dist_src_types_opencv_imgproc_feature_d_t;
	node_modules_mirada_dist_src_types_opencv_imgproc_filter_d_ts: Node_modules_mirada_dist_src_types_opencv_imgproc_filter_d_t;
	node_modules_mirada_dist_src_types_opencv_imgproc_hist_d_ts: Node_modules_mirada_dist_src_types_opencv_imgproc_hist_d_t;
	node_modules_mirada_dist_src_types_opencv_imgproc_misc_d_ts: Node_modules_mirada_dist_src_types_opencv_imgproc_misc_d_t;
	node_modules_mirada_dist_src_types_opencv_imgproc_object_d_ts: Node_modules_mirada_dist_src_types_opencv_imgproc_object_d_t;
	node_modules_mirada_dist_src_types_opencv_imgproc_shape_d_ts: Node_modules_mirada_dist_src_types_opencv_imgproc_shape_d_t;
	node_modules_mirada_dist_src_types_opencv_index_d_ts: Node_modules_mirada_dist_src_types_opencv_index_d_t;
	node_modules_mirada_dist_src_types_opencv_imgproc_transform_d_ts: Node_modules_mirada_dist_src_types_opencv_imgproc_transform_d_t;
	node_modules_mirada_dist_src_types_opencv_Logger_d_ts: Node_modules_mirada_dist_src_types_opencv_Logger_d_t;
	node_modules_mirada_dist_src_types_opencv_LshTable_d_ts: Node_modules_mirada_dist_src_types_opencv_LshTable_d_t;
	node_modules_mirada_dist_src_types_opencv_MatExpr_d_ts: Node_modules_mirada_dist_src_types_opencv_MatExpr_d_t;
	node_modules_mirada_dist_src_types_opencv_MatOp_d_ts: Node_modules_mirada_dist_src_types_opencv_MatOp_d_t;
	node_modules_mirada_dist_src_types_opencv_Matx_d_ts: Node_modules_mirada_dist_src_types_opencv_Matx_d_t;
	node_modules_mirada_dist_src_types_opencv_Node_d_ts: Node_modules_mirada_dist_src_types_opencv_Node_d_t;
	node_modules_mirada_dist_src_types_opencv_objdetect_d_ts: Node_modules_mirada_dist_src_types_opencv_objdetect_d_t;
	node_modules_mirada_dist_src_types_opencv_Mat_d_ts: Node_modules_mirada_dist_src_types_opencv_Mat_d_t;
	node_modules_mirada_dist_src_types_opencv_PCA_d_ts: Node_modules_mirada_dist_src_types_opencv_PCA_d_t;
	node_modules_mirada_dist_src_types_opencv_photo_inpaint_d_ts: Node_modules_mirada_dist_src_types_opencv_photo_inpaint_d_t;
	node_modules_mirada_dist_src_types_opencv_RotatedRect_d_ts: Node_modules_mirada_dist_src_types_opencv_RotatedRect_d_t;
	node_modules_mirada_dist_src_types_opencv_softdouble_d_ts: Node_modules_mirada_dist_src_types_opencv_softdouble_d_t;
	node_modules_mirada_dist_src_types_opencv_softfloat_d_ts: Node_modules_mirada_dist_src_types_opencv_softfloat_d_t;
	node_modules_mirada_dist_src_types_opencv_video_track_d_ts: Node_modules_mirada_dist_src_types_opencv_video_track_d_t;
	node_modules_mirada_dist_src_util_base64_d_ts: Node_modules_mirada_dist_src_util_base64_d_t;
	node_modules_mirada_dist_src_util_fileUtil_d_ts: Node_modules_mirada_dist_src_util_fileUtil_d_t;
	node_modules_mirada_dist_src_util_imageUtil_d_ts: Node_modules_mirada_dist_src_util_imageUtil_d_t;
	node_modules_mirada_dist_src_util_index_d_ts: Node_modules_mirada_dist_src_util_index_d_t;
	node_modules_mirada_dist_src_util_misc_d_ts: Node_modules_mirada_dist_src_util_misc_d_t;
}
export const mirada: mirada_definition = {"node_modules_mirada_dist_src_browser_cameraHelper_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_browser_cameraHelper_d_ts","originalFileName":"node_modules/mirada/dist/src/browser/cameraHelper.d.ts","content":"export declare class CameraHelper {\n    videoInput: HTMLVideoElement;\n    outputCanvas: HTMLCanvasElement;\n    callback: () => void;\n    streaming: boolean;\n    protected stream: MediaStream | undefined;\n    protected onCameraStartedCallback: ((stream?: MediaStream, videoInput?: HTMLVideoElement) => any) | undefined;\n    constructor(videoInput: HTMLVideoElement, outputCanvas: HTMLCanvasElement, callback: () => void);\n    start(): void;\n    stop(): void;\n    startCamera(resolution: 'qvga' | 'vga', callback: (...args: any[]) => any, video: HTMLVideoElement): void;\n    stopCamera(): void;\n    protected onVideoStarted(): void;\n    protected onVideoStopped(): void;\n    protected onVideoCanPlay(): void;\n}\n"},"node_modules_mirada_dist_src_browser_canvasRender_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_browser_canvasRender_d_ts","originalFileName":"node_modules/mirada/dist/src/browser/canvasRender.d.ts","content":"import { Mat, Rect } from '../types/opencv';\nexport interface ABOptions {\n    name?: string;\n    canvas?: HTMLCanvasElement;\n    appendToBody?: boolean;\n}\n/**\n * A sub optimal method to load a image array buffer (encoded in jpg, png) whiteouts knowing its format or size.\n  * 1) creates a blob and a url object\n  * * loads the url in a HTML Image (to know its dimensions )\n  * * draw the image in a canvas ().\n  *\n  * This method is useful as a decoder for the browser without libraries\n */\nexport declare function renderArrayBufferInCanvas(a: ArrayBuffer, mime: string, options?: ABOptions): Promise<{\n    canvas: HTMLCanvasElement;\n    width: number;\n    height: number;\n}>;\nexport interface Options {\n    appendToBody?: boolean;\n    rgba?: boolean;\n    canvas?: HTMLCanvasElement;\n    region?: Rect;\n    clear?: boolean;\n    forceSameSize?: boolean;\n}\nexport declare function renderInCanvas(mat: Mat, options?: Options): HTMLCanvasElement;\n"},"node_modules_mirada_dist_src_browser_imageCreation_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_browser_imageCreation_d_ts","originalFileName":"node_modules/mirada/dist/src/browser/imageCreation.d.ts","content":"import { Mat } from '../types/opencv';\nexport declare function fromInputFileElement(a: HTMLInputElement): Promise<Mat[]>;\nexport declare function fetchImageData(url: string): Promise<ImageData>;\nexport declare function getHtmlImageData(img: Mat): ImageData;\n"},"node_modules_mirada_dist_src_browser_index_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_browser_index_d_ts","originalFileName":"node_modules/mirada/dist/src/browser/index.d.ts","content":"import * as cameraHelper from './cameraHelper';\nimport * as canvasRender from './canvasRender';\nexport { CameraHelper } from './cameraHelper';\nexport { renderArrayBufferInCanvas, renderInCanvas } from './canvasRender';\nexport { fetchImageData, fromInputFileElement, getHtmlImageData } from './imageCreation';\nexport declare const browser: {\n    fromInputFileElement(a: HTMLInputElement): Promise<import(\"..\").Mat[]>;\n    fetchImageData(url: string): Promise<ImageData>;\n    getHtmlImageData(img: import(\"..\").Mat): ImageData;\n    CameraHelper: typeof cameraHelper.CameraHelper;\n    renderArrayBufferInCanvas(a: ArrayBuffer, mime: string, options?: canvasRender.ABOptions): Promise<{\n        canvas: HTMLCanvasElement;\n        width: number;\n        height: number;\n    }>;\n    renderInCanvas(mat: import(\"..\").Mat, options?: canvasRender.Options | undefined): HTMLCanvasElement;\n};\n"},"node_modules_mirada_dist_src_file_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_file_d_ts","originalFileName":"node_modules/mirada/dist/src/file.d.ts","content":"import { ImageData, Mat } from './types/opencv';\nimport fileType = require('file-type');\n/**\n * A thin layer on top of cv.Mat with lots of utilities to load, write, encode, etc.\n */\nexport declare class File {\n    readonly name: string;\n    protected _mat: Mat;\n    constructor(name: string, _mat: Mat);\n    size(): {\n        width: any;\n        height: any;\n    };\n    getMimeType(): string | undefined;\n    getExtension(): string;\n    asMat(): Mat;\n    asImageData(): ImageData;\n    asDataUrl(): string;\n    readonly width: any;\n    readonly height: any;\n    readonly mat: Mat;\n    /**\n     * Returns an array buffer containing the image encoded in given format or inferring format from its name.\n     */\n    asArrayBuffer(format?: string): Promise<ArrayBuffer>;\n    /**\n     * Writes this image on given file path, encoded in given format (or inferred form current name).\n     */\n    write(path?: string, format?: string): Promise<void>;\n    /**\n     * Shows this image in given HTML canvas or image element.\n     */\n    show(el: HTMLElement): void;\n    asBase64(format?: string): Promise<string>;\n    delete(): any;\n    /**\n     * Loads file from given base64 string containing an encoded image.\n    */\n    static fromBase64(base64: string, name?: string): Promise<File>;\n    /**\n     * Loads file from given array buffer containing an encoded image.\n     */\n    static fromArrayBuffer(buffer: ArrayBuffer, name?: string): Promise<File>;\n    /**\n     * Loads file from given array buffer view containing an encoded image.\n     */\n    static fromArrayBufferView(a: ArrayBufferView, name?: string): Promise<File>;\n    static getBufferFileType(a: ArrayBuffer): fileType.FileTypeResult;\n    static getBufferFileName(a: ArrayBuffer): string;\n    /**\n     * Loads file from given data url string containing an encoded image.\n    */\n    static fromDataUrl(dataUrl: string, name?: string): Promise<File>;\n    /**\n     * Loads files from files in html input element of type \"file\".\n     */\n    static fromHtmlFileInputElement(el: HTMLInputElement): Promise<File[]>;\n    /**\n     * Loads file form existing HTMLElement or HTMLImageElement\n     */\n    static fromCanvas(el: HTMLElement | string): File;\n    /**\n     * Shortcut for [resolve] that returns the first result.\n     */\n    static resolveOne(files: string | File | undefined | (string | File | undefined)[]): Promise<File | undefined>;\n    /**\n     * Given paths, urls or files it will try to load them all and return a list of File for those succeed.\n     */\n    static resolve(files: string | File | undefined | (string | File | undefined)[]): Promise<File[]>;\n    static isFile(f: any): f is File;\n    static asPath(f: string | File): string;\n    static fromData(data: ImageData, name?: string): File;\n    private static _buildName;\n    static fromMat(mat: Mat, name?: string): File;\n    static fromUrl(url: string, o?: RequestInit & FileOptions): Promise<File>;\n    static fromFile(path: string, o?: FileOptions): Promise<File>;\n}\ninterface FileOptions {\n    name?: string;\n}\nexport {};\n"},"node_modules_mirada_dist_src_format_canvasCodec_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_format_canvasCodec_d_ts","originalFileName":"node_modules/mirada/dist/src/format/canvasCodec.d.ts","content":"import { FormatCodec } from '../types/mirada';\n/**\n  Example of declaring a format codec that uses DOM canvas instance which must be provided by the user.\n  \n```ts\nimport * as Jimp from 'jimp'\nclass JimpProxy implements FormatProxyClass {\n  async create() {\n   return new JimpFormatCodec(Jimp)\n  }\n}\n```\n */\nexport declare class CanvasCodec implements FormatCodec {\n    constructor();\n    decode(buffer: ArrayBuffer, format?: string): Promise<ImageData | undefined>;\n    encode(data: ImageData, format: string, quality?: number): Promise<ArrayBuffer | undefined>;\n}\n"},"node_modules_mirada_dist_src_format_format_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_format_format_d_ts","originalFileName":"node_modules/mirada/dist/src/format/format.d.ts","content":"import { FormatCodec, FormatProxy } from '../types/mirada';\nimport { ImageData } from '../types/opencv';\n/**\n * Nor or opencv.js or this library implement any image format so users are\n * responsible of providing a FormatProxy using some library.\n *\n */\nexport declare function installFormatProxy(proxy: FormatProxy): Promise<void>;\nexport declare function unInstallFormatProxies(): Promise<void>;\n/**\n * @internal\n */\nexport declare function loadFormatProxies(): Promise<void>;\nexport declare function unloadFormatProxies(): void;\nexport declare function getDefaultCodec(): FormatCodec;\nexport declare function decodeOrThrow(buffer: ArrayBuffer, format?: string): Promise<ImageData>;\nexport declare function encodeOrThrow(data: ImageData, format: string, quality?: number): Promise<ArrayBuffer>;\n"},"node_modules_mirada_dist_src_format_index_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_format_index_d_ts","originalFileName":"node_modules/mirada/dist/src/format/index.d.ts","content":"export * from './canvasCodec';\nexport * from './format';\nexport * from './jimpCodec';\nimport * as canvasCodec from './canvasCodec';\nimport * as jimpCodec from './jimpCodec';\nexport declare const format: {\n    JimpCodec: typeof jimpCodec.JimpCodec;\n    installFormatProxy(proxy: import(\"..\").FormatProxy): Promise<void>;\n    unInstallFormatProxies(): Promise<void>;\n    loadFormatProxies(): Promise<void>;\n    unloadFormatProxies(): void;\n    getDefaultCodec(): import(\"..\").FormatCodec;\n    decodeOrThrow(buffer: ArrayBuffer, format?: string | undefined): Promise<import(\"..\").ImageData>;\n    encodeOrThrow(data: import(\"..\").ImageData, format: string, quality?: number | undefined): Promise<ArrayBuffer>;\n    CanvasCodec: typeof canvasCodec.CanvasCodec;\n};\n"},"node_modules_mirada_dist_src_format_jimpCodec_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_format_jimpCodec_d_ts","originalFileName":"node_modules/mirada/dist/src/format/jimpCodec.d.ts","content":"import { FormatCodec } from '../types/mirada';\ndeclare type AnyConstructor = {\n    [a: string]: any;\n    new (...args: any[]): any;\n};\ndeclare type Jimp = AnyConstructor;\n/**\n  Example of declaring a Jimp proxy as a class\n  \n```ts\nimport * as Jimp from 'jimp'\nclass JimpProxy implements FormatProxyClass {\n  async create() {\n   return new JimpFormatCodec(Jimp)\n  }\n}\n```\n */\nexport declare class JimpCodec implements FormatCodec {\n    protected jimp: Jimp;\n    constructor(jimp: Jimp);\n    decode(buffer: ArrayBuffer): Promise<ImageData | undefined>;\n    encode(data: ImageData, format: string, quality?: number): Promise<ArrayBuffer | undefined>;\n}\nexport {};\n"},"node_modules_mirada_dist_src_index_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_index_d_ts","originalFileName":"node_modules/mirada/dist/src/index.d.ts","content":"export * from './browser';\nexport * from './file';\nexport * from './format/';\nexport * from './opencvReady';\nexport * from './tool/';\nexport * from './types/mirada';\nexport * from './types/opencv';\nexport * from './util';\nimport './types/_cv';\n"},"node_modules_mirada_dist_src_opencvReady_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_opencvReady_d_ts","originalFileName":"node_modules/mirada/dist/src/opencvReady.d.ts","content":"import { FS } from './types/emscripten';\nexport declare const FS_ROOT = \"/work\";\n/**\n * gets the emscripten FS API\n */\nexport declare function getFS(): FS;\nexport interface LoadOptions {\n    onloadCallback?: () => void;\n    opencvUrl?: string;\n    /**\n     * node.js : current working dir. By default is '.'\n     */\n    cwd?: string;\n}\n/**\n * Loads opencv.js file. It will do it only once no matter if called multiple times.\n * In the browser a new script element is created to load the file while in Node.js\n * the file is loaded using a require() call.\n *\n * Returns a promise resolved when the library is ready or rejected if there's a problem.\n *\n * Notice that among the options users can define the location of opencv.js file, which\n * in the case of the browser it could be in an external server.\n */\nexport declare function loadOpencv(options?: LoadOptions): Promise<void> | Promise<FS>;\n"},"node_modules_mirada_dist_src_tool_grabCut_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_tool_grabCut_d_ts","originalFileName":"node_modules/mirada/dist/src/tool/grabCut.d.ts","content":"import { File } from '..';\nimport { ImageData, Rect, Scalar } from '../types/opencv';\nexport interface GrabCutOptions extends Rect {\n    image: File;\n    /**\n     * If given a rectangle frame will be drawn on given coordinates with that color.\n     */\n    frameColor?: Scalar;\n}\nexport interface GrabCutResult {\n    image: ImageData;\n}\nexport declare function grabCut(o: GrabCutOptions): Promise<GrabCutResult>;\n"},"node_modules_mirada_dist_src_tool_index_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_tool_index_d_ts","originalFileName":"node_modules/mirada/dist/src/tool/index.d.ts","content":"import * as grabCut from './grabCut';\nexport declare const tool: {\n    grabCut(o: grabCut.GrabCutOptions): Promise<grabCut.GrabCutResult>;\n};\n"},"node_modules_mirada_dist_src_types__cv_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types__cv_d_ts","originalFileName":"node_modules/mirada/dist/src/types/_cv.d.ts","content":"import { FS } from './emscripten';\nimport { CV } from './opencv';\ndeclare global {\n    var cv: CV & {\n        FS: FS;\n    };\n}\n"},"node_modules_mirada_dist_src_types_mirada_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_mirada_d_ts","originalFileName":"node_modules/mirada/dist/src/types/mirada.d.ts","content":"import { ImageData } from './opencv';\nexport { FS } from './emscripten';\n/**\n * User provided image formats encode/decode implementation. The proxy is responsible of creating codec\n * instances\n *\n *  This is particularly useful in this library so it can actually contain the implementation of concrete\n *  Codecs (see [JimpCodec]) without actually being responsible of loading / instantiating the library which\n *  will have to be handled by a JimpProxy provided by a third party (test, playground/user) . In other words,\n *  mirada provides codecs implementations for several libraries and at the while keeping agnostic/independent\n *\n *  This is probably called only once and after obtaining a codec the same instance is used by the manager.\n *\n */\nexport declare type FormatProxy = FormatProxyClass | (() => FormatCodec) | (() => Promise<FormatCodec>);\n/**\n * a class-like representation for format proxy instead functions\n * */\nexport interface FormatProxyClass {\n    /**\n     * This is probably called only once and after obtaining a codec the same instance is used by the manager.\n     */\n    create(): Promise<FormatCodec>;\n}\n/**\n * Codec instances are created by format proxies and are responsible of encode and decode certain set of image\n * formats. See IMPORTANT: formats are lowercase and in general the common extension of files\n */\nexport interface FormatCodec {\n    /**\n   * Given an array buffer that contains the content of an encoded image it will return a decoded ImageData\n   * object. The format parameter could be needed by some poor decoders that don't support file type sniffing.\n   * For example, magica or jimp libraries don't need this.\n   */\n    decode(buffer: ArrayBuffer, format?: string): Promise<ImageData | undefined>;\n    /**\n     * given an image data representing an unencoded raw image it will return an array buffer containing the\n     * enconcoded image content in given format.\n     */\n    encode(data: ImageData, format: string, quality?: number): Promise<ArrayBuffer | undefined>;\n    /**\n     * if provided an error will be thrown in case users request to decode to a format not included in this\n     * list.\n     */\n    getSupportedDecodeFormats?(): string[];\n    /**\n     * if provided an error will be thrown in case users request to encode to a format not included in this\n     * list.\n     */\n    getSupportedEncodeFormats?(): string[];\n}\n"},"node_modules_mirada_dist_src_types_emscripten_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_emscripten_d_ts","originalFileName":"node_modules/mirada/dist/src/types/emscripten.d.ts","content":"interface Lookup {\n    path: string;\n    node: FSNode;\n}\ninterface FSStream {\n}\ninterface FSNode {\n}\nexport interface FS {\n    lookupPath(path: string, opts: any): Lookup;\n    getPath(node: FSNode): string;\n    isFile(mode: number): boolean;\n    isDir(mode: number): boolean;\n    isLink(mode: number): boolean;\n    isChrdev(mode: number): boolean;\n    isBlkdev(mode: number): boolean;\n    isFIFO(mode: number): boolean;\n    isSocket(mode: number): boolean;\n    major(dev: number): number;\n    minor(dev: number): number;\n    makedev(ma: number, mi: number): number;\n    registerDevice(dev: number, ops: any): void;\n    syncfs(populate: boolean, callback: (e: any) => any): void;\n    syncfs(callback: (e: any) => any, populate?: boolean): void;\n    mount(type: any, opts: any, mountpoint: string): any;\n    unmount(mountpoint: string): void;\n    mkdir(path: string, mode?: number): any;\n    mkdev(path: string, mode?: number, dev?: number): any;\n    symlink(oldpath: string, newpath: string): any;\n    rename(old_path: string, new_path: string): void;\n    rmdir(path: string): void;\n    readdir(path: string): string[];\n    unlink(path: string): void;\n    readlink(path: string): string;\n    stat(path: string, dontFollow?: boolean): any;\n    lstat(path: string): any;\n    chmod(path: string, mode: number, dontFollow?: boolean): void;\n    lchmod(path: string, mode: number): void;\n    fchmod(fd: number, mode: number): void;\n    chown(path: string, uid: number, gid: number, dontFollow?: boolean): void;\n    lchown(path: string, uid: number, gid: number): void;\n    fchown(fd: number, uid: number, gid: number): void;\n    truncate(path: string, len: number): void;\n    ftruncate(fd: number, len: number): void;\n    utime(path: string, atime: number, mtime: number): void;\n    open(path: string, flags: string, mode?: number, fd_start?: number, fd_end?: number): FSStream;\n    close(stream: FSStream): void;\n    llseek(stream: FSStream, offset: number, whence: number): any;\n    read(stream: FSStream, buffer: ArrayBufferView, offset: number, length: number, position?: number): number;\n    write(stream: FSStream, buffer: ArrayBufferView, offset: number, length: number, position?: number, canOwn?: boolean): number;\n    allocate(stream: FSStream, offset: number, length: number): void;\n    mmap(stream: FSStream, buffer: ArrayBufferView, offset: number, length: number, position: number, prot: number, flags: number): any;\n    ioctl(stream: FSStream, cmd: any, arg: any): any;\n    readFile(path: string, opts?: {\n        encoding: string;\n        flags: string;\n    }): ArrayBufferView;\n    writeFile(path: string, data: ArrayBufferView, opts?: {\n        encoding: string;\n        flags: string;\n    }): void;\n    writeFile(path: string, data: string, opts?: {\n        encoding: string;\n        flags: string;\n    }): void;\n    analyzePath(p: string): any;\n    cwd(): string;\n    chdir(path: string): void;\n    init(input: () => number, output: (c: number) => any, error: (c: number) => any): void;\n    createLazyFile(parent: string, name: string, url: string, canRead: boolean, canWrite: boolean): FSNode;\n    createLazyFile(parent: FSNode, name: string, url: string, canRead: boolean, canWrite: boolean): FSNode;\n    createPreloadedFile(parent: string, name: string, url: string, canRead: boolean, canWrite: boolean, onload?: () => void, onerror?: () => void, dontCreateFile?: boolean, canOwn?: boolean): void;\n    createPreloadedFile(parent: FSNode, name: string, url: string, canRead: boolean, canWrite: boolean, onload?: () => void, onerror?: () => void, dontCreateFile?: boolean, canOwn?: boolean): void;\n    createDataFile(parent: string, name: string, data: ArrayBufferView, canRead: boolean, canWrite: boolean, canOwn: boolean): void;\n}\nexport interface EmscriptenModule {\n    print(str: string): void;\n    printErr(str: string): void;\n    arguments: string[];\n    environment: EnvironmentType;\n    preInit: Array<{\n        (): void;\n    }>;\n    preRun: Array<{\n        (): void;\n    }>;\n    postRun: Array<{\n        (): void;\n    }>;\n    onAbort: {\n        (what: any): void;\n    };\n    onRuntimeInitialized: {\n        (): void;\n    };\n    preinitializedWebGLContext: WebGLRenderingContext;\n    noInitialRun: boolean;\n    noExitRuntime: boolean;\n    logReadFiles: boolean;\n    filePackagePrefixURL: string;\n    wasmBinary: ArrayBuffer;\n    destroy(object: object): void;\n    getPreloadedPackage(remotePackageName: string, remotePackageSize: number): ArrayBuffer;\n    instantiateWasm(imports: WebAssemblyImports, successCallback: (module: WebAssemblyModule) => void): WebAssemblyExports;\n    locateFile(url: string): string;\n    onCustomMessage(event: MessageEvent): void;\n    Runtime: any;\n    ccall(ident: string, returnType: ValueType | null, argTypes: ValueType[], args: TypeCompatibleWithC[], opts?: CCallOpts): any;\n    cwrap(ident: string, returnType: ValueType | null, argTypes: ValueType[], opts?: CCallOpts): (...args: any[]) => any;\n    setValue(ptr: number, value: any, type: string, noSafe?: boolean): void;\n    getValue(ptr: number, type: string, noSafe?: boolean): number;\n    ALLOC_NORMAL: number;\n    ALLOC_STACK: number;\n    ALLOC_STATIC: number;\n    ALLOC_DYNAMIC: number;\n    ALLOC_NONE: number;\n    allocate(slab: any, types: string | string[], allocator: number, ptr: number): number;\n    HEAP: Int32Array;\n    IHEAP: Int32Array;\n    FHEAP: Float64Array;\n    HEAP8: Int8Array;\n    HEAP16: Int16Array;\n    HEAP32: Int32Array;\n    HEAPU8: Uint8Array;\n    HEAPU16: Uint16Array;\n    HEAPU32: Uint32Array;\n    HEAPF32: Float32Array;\n    HEAPF64: Float64Array;\n    TOTAL_STACK: number;\n    TOTAL_MEMORY: number;\n    FAST_MEMORY: number;\n    addOnPreRun(cb: () => any): void;\n    addOnInit(cb: () => any): void;\n    addOnPreMain(cb: () => any): void;\n    addOnExit(cb: () => any): void;\n    addOnPostRun(cb: () => any): void;\n    intArrayFromString(stringy: string, dontAddNull?: boolean, length?: number): number[];\n    intArrayToString(array: number[]): string;\n    writeStringToMemory(str: string, buffer: number, dontAddNull: boolean): void;\n    writeArrayToMemory(array: number[], buffer: number): void;\n    writeAsciiToMemory(str: string, buffer: number, dontAddNull: boolean): void;\n    addRunDependency(id: any): void;\n    removeRunDependency(id: any): void;\n    preloadedImages: any;\n    preloadedAudios: any;\n    _malloc(size: number): number;\n    _free(ptr: number): void;\n}\ndeclare type EnvironmentType = \"WEB\" | \"NODE\" | \"SHELL\" | \"WORKER\";\ndeclare type ValueType = \"number\" | \"string\" | \"array\" | \"boolean\";\ndeclare type TypeCompatibleWithC = number | string | any[] | boolean;\ndeclare type WebAssemblyImports = Array<{\n    name: string;\n    kind: string;\n}>;\ndeclare type WebAssemblyExports = Array<{\n    module: string;\n    name: string;\n    kind: string;\n}>;\ninterface CCallOpts {\n    async?: boolean;\n}\ninterface WebAssemblyModule {\n}\nexport {};\n"},"node_modules_mirada_dist_src_types_opencv__hacks_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv__hacks_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/_hacks.d.ts","content":"export declare class Range {\n    start: number;\n    end: number;\n    constructor(start: number, end: number);\n}\nexport declare class Scalar extends Array<number> {\n    static all(...v: number[]): Scalar;\n}\nexport { Mat as InputArray, Mat as InputArrayOfArrays, Mat as InputOutputArray, Mat as InputOutputArrayOfArrays, Mat as MatVector, Mat as OutputArray, Mat as OutputArrayOfArrays } from './Mat';\nexport { Scalar as GScalar };\nexport { Point as Point2f };\nexport { Point as KeyPoint };\nexport { Point as Point2l };\nexport { Size as Point2d };\nexport { Size as Size2d };\nexport { Size as Size2f };\nexport { Size as Size2l };\nexport { Rect as Rect_ };\nexport declare class Point {\n    constructor(x: number, y: number);\n    x: number;\n    y: number;\n}\nexport declare class Size {\n    constructor(width: number, height: number);\n    width: number;\n    height: number;\n}\nexport declare class Rect {\n    constructor();\n    constructor(point: Point, size: Size);\n    constructor(x: number, y: number, width: number, height: number);\n    x: number;\n    y: number;\n    width: number;\n    height: number;\n}\nexport declare class TermCriteria {\n    type: number;\n    maxCount: number;\n    epsilon: number;\n    constructor();\n    constructor(type: number, maxCount: number, epsilon: number);\n}\nexport declare const TermCriteria_EPS: any;\nexport declare const TermCriteria_COUNT: any;\nexport declare const TermCriteria_MAX_ITER: any;\nexport declare class MinMaxLoc {\n    minVal: number;\n    maxVal: number;\n    minLoc: Point;\n    maxLoc: Point;\n    constructor();\n    constructor(minVal: number, maxVal: number, minLoc: Point, maxLoc: Point);\n}\nexport declare function exceptionFromPtr(err: number): any;\nexport declare function onRuntimeInitialized(): any;\nexport declare function FS_createDataFile(arg0: string, path: string, data: Uint8Array, arg3: boolean, arg4: boolean, arg5: boolean): any;\ndeclare class Vector<T> {\n    get(i: number): T;\n    get(i: number, j: number, data: any): T;\n    set(i: number, t: T): void;\n    put(i: number, j: number, data: any): any;\n    size(): number;\n    push_back(n: T): any;\n    resize(count: number, value?: T): void;\n    delete(): void;\n}\nexport declare class Vec3d extends Vector<any> {\n}\nexport declare class IntVector extends Vector<number> {\n}\nexport declare class FloatVector extends Vector<number> {\n}\nexport declare class DoubleVector extends Vector<number> {\n}\nexport declare class PointVector extends Vector<Point> {\n}\nexport declare class KeyPointVector extends Vector<any> {\n}\nexport declare class DMatchVector extends Vector<any> {\n}\nexport declare class DMatchVectorVector extends Vector<Vector<any>> {\n}\nexport declare class RectVector extends Rect implements Vector<Rect> {\n    get(i: number): Rect;\n    set(i: number, t: Rect): void;\n    put(i: number, j: number, data: any): any;\n    size(): number;\n    push_back(n: Rect): void;\n    resize(count: number, value?: Rect | undefined): void;\n    delete(): void;\n}\nexport declare class VideoCapture {\n    constructor(videoSource: HTMLVideoElement | string);\n    read(m: Mat): any;\n    video: HTMLVideoElement;\n}\nimport { LineTypes, Mat, RotatedRect } from '.';\nimport '../_cv';\nexport declare function matFromImageData(imageData: ImageData): Mat;\n/** since we don't support inheritance yet we force Mat to extend Mat_ which type defined here: */\nexport declare class Mat_ extends Vector<Mat> {\n    delete(): void;\n    data: ImageData;\n    data32F: any;\n    ucharPtr(i: any, j: any): any;\n    charPtr(i: any, j: any): any;\n    shortPtr(i: any, j: any): any;\n    ushortPtr(i: any, j: any): any;\n    intPtr(i: any, j: any): any;\n    floatPtr(i: any, j: any): any;\n    doublePtr(i: any, j: any): any;\n    intPtr(i: any, j: any): any;\n    roi(rect: Rect): Mat;\n}\nexport declare class ImageData {\n    data: ArrayBufferView;\n    width: number;\n    height: number;\n}\nexport declare const CV_8U: any;\nexport declare const CV_8UC1: any;\nexport declare const CV_8UC2: any;\nexport declare const CV_8UC3: any;\nexport declare const CV_8UC4: any;\nexport declare const CV_8S: any;\nexport declare const CV_8SC1: any;\nexport declare const CV_8SC2: any;\nexport declare const CV_8SC3: any;\nexport declare const CV_8SC4: any;\nexport declare const CV_16U: any;\nexport declare const CV_16UC1: any;\nexport declare const CV_16UC2: any;\nexport declare const CV_16UC3: any;\nexport declare const CV_16UC4: any;\nexport declare const CV_16S: any;\nexport declare const CV_16SC1: any;\nexport declare const CV_16SC2: any;\nexport declare const CV_16SC3: any;\nexport declare const CV_16SC4: any;\nexport declare const CV_32S: any;\nexport declare const CV_32SC1: any;\nexport declare const CV_32SC2: any;\nexport declare const CV_32SC3: any;\nexport declare const CV_32SC4: any;\nexport declare const CV_32F: any;\nexport declare const CV_32FC1: any;\nexport declare const CV_32FC2: any;\nexport declare const CV_32FC3: any;\nexport declare const CV_32FC4: any;\nexport declare const CV_64F: any;\nexport declare const CV_64FC1: any;\nexport declare const CV_64FC2: any;\nexport declare const CV_64FC3: any;\nexport declare const CV_64FC4: any;\nexport declare function ellipse1(dst: Mat, rotatedRect: RotatedRect, ellipseColor: Scalar, arg0: number, line: LineTypes): void;\nexport declare function imread(canvasOrImageHtmlElement: HTMLElement | string): Mat;\nexport declare function imshow(canvasSource: HTMLElement | string, mat: Mat): void;\nexport declare type Mat4 = any;\nexport declare type Mat3 = any;\nexport declare type Vec3 = any;\nexport declare type float_type = any;\nexport declare type int = any;\nexport declare type bool = any;\nexport declare type FileNode = any;\nexport declare type FileStorage = any;\nexport declare type Ptr = any;\nexport declare type size_t = any;\nexport declare type double = any;\nexport declare type DMatch = any;\nexport declare type float = any;\nexport declare type UMat = any;\nexport declare type DetectionROI = any;\nexport declare type Matrix = any;\nexport declare type BucketKey = any;\nexport declare type Bucket = any;\nexport declare type LshStats = any;\nexport declare type MatAllocator = any;\nexport declare type uchar = any;\nexport declare type MatSize = any;\nexport declare type MatStep = any;\nexport declare type UMatData = any;\nexport declare type typename = any;\nexport declare type Vec = any;\nexport declare type Point_ = any;\nexport declare type Point3_ = any;\nexport declare type MatCommaInitializer_ = any;\nexport declare type MatIterator_ = any;\nexport declare type MatConstIterator_ = any;\nexport declare type AccessFlag = any;\nexport declare type UMatUsageFlags = any;\nexport declare type _Tp = any;\nexport declare type Matx_AddOp = any;\nexport declare type Matx_SubOp = any;\nexport declare type _T2 = any;\nexport declare type Matx_ScaleOp = any;\nexport declare type Matx_MulOp = any;\nexport declare type Matx_DivOp = any;\nexport declare type Matx_MatMulOp = any;\nexport declare type Matx_TOp = any;\nexport declare type diag_type = any;\nexport declare type _EqPredicate = any;\nexport declare type cvhalDFT = any;\nexport declare type schar = any;\nexport declare type ushort = any;\nexport declare type short = any;\nexport declare type int64 = any;\nexport declare type ErrorCallback = any;\nexport declare type unsigned = any;\nexport declare type uint64 = any;\nexport declare type float16_t = any;\nexport declare type AsyncArray = any;\nexport declare type Net = any;\nexport declare type Moments = any;\nexport declare type uint64_t = any;\nexport declare type uint32_t = any;\nexport declare type int32_t = any;\nexport declare type int64_t = any;\n"},"node_modules_mirada_dist_src_types_opencv__types_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv__types_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/_types.d.ts","content":"export * from './Affine3';\nexport * from './Algorithm';\nexport * from './AutoBuffer';\nexport * from './BFMatcher';\nexport * from './BOWTrainer';\nexport * from './calib3d';\nexport * from './CascadeClassifier';\nexport * from './core_array';\nexport * from './core_cluster';\nexport * from './core_hal_interface';\nexport * from './core_utils';\nexport * from './DescriptorMatcher';\nexport * from './dnn';\nexport * from './DynamicBitset';\nexport * from './Exception';\nexport * from './features2d_draw';\nexport * from './FlannBasedMatcher';\nexport * from './HOGDescriptor';\nexport * from './imgproc_color_conversions';\nexport * from './imgproc_draw';\nexport * from './imgproc_feature';\nexport * from './imgproc_filter';\nexport * from './imgproc_hist';\nexport * from './imgproc_misc';\nexport * from './imgproc_object';\nexport * from './imgproc_shape';\nexport * from './imgproc_transform';\nexport * from './Logger';\nexport * from './LshTable';\nexport * from './Mat';\nexport * from './MatExpr';\nexport * from './MatOp';\nexport * from './Matx';\nexport * from './Node';\nexport * from './objdetect';\nexport * from './PCA';\nexport * from './photo_inpaint';\nexport * from './RotatedRect';\nexport * from './softdouble';\nexport * from './softfloat';\nexport * from './video_track';\nexport * from './_hacks';\n"},"node_modules_mirada_dist_src_types_opencv_Affine3_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_Affine3_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/Affine3.d.ts","content":"import { float_type, int, Mat, Mat3, Mat4, Vec3 } from './_types';\n/**\n * It represents a 4x4 homogeneous transformation matrix `$T$`\n *\n * `\\\\[T = \\\\begin{bmatrix} R & t\\\\\\\\ 0 & 1\\\\\\\\ \\\\end{bmatrix} \\\\]`\n *\n * where `$R$` is a 3x3 rotation matrix and `$t$` is a 3x1 translation vector.\n *\n * You can specify `$R$` either by a 3x3 rotation matrix or by a 3x1 rotation vector, which is\n * converted to a 3x3 rotation matrix by the Rodrigues formula.\n *\n * To construct a matrix `$T$` representing first rotation around the axis `$r$` with rotation angle\n * `$|r|$` in radian (right hand rule) and then translation by the vector `$t$`, you can use\n *\n * ```cpp\n * cv::Vec3f r, t;\n * cv::Affine3f T(r, t);\n * ```\n *\n * If you already have the rotation matrix `$R$`, then you can use\n *\n * ```cpp\n * cv::Matx33f R;\n * cv::Affine3f T(R, t);\n * ```\n *\n * To extract the rotation matrix `$R$` from `$T$`, use\n *\n * ```cpp\n * cv::Matx33f R = T.rotation();\n * ```\n *\n * To extract the translation vector `$t$` from `$T$`, use\n *\n * ```cpp\n * cv::Vec3f t = T.translation();\n * ```\n *\n * To extract the rotation vector `$r$` from `$T$`, use\n *\n * ```cpp\n * cv::Vec3f r = T.rvec();\n * ```\n *\n * Note that since the mapping from rotation vectors to rotation matrices is many to one. The returned\n * rotation vector is not necessarily the one you used before to set the matrix.\n *\n * If you have two transformations `$T = T_1 * T_2$`, use\n *\n * ```cpp\n * cv::Affine3f T, T1, T2;\n * T = T2.concatenate(T1);\n * ```\n *\n * To get the inverse transform of `$T$`, use\n *\n * ```cpp\n * cv::Affine3f T, T_inv;\n * T_inv = T.inv();\n * ```\n *\n * Source:\n * [opencv2/core/affine.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/core/affine.hpp#L129).\n *\n */\nexport declare class Affine3 {\n    matrix: Mat4;\n    constructor();\n    constructor(affine: Mat4);\n    /**\n     *   The resulting 4x4 matrix is\n     *\n     *   `\\\\[ \\\\begin{bmatrix} R & t\\\\\\\\ 0 & 1\\\\\\\\ \\\\end{bmatrix} \\\\]`\n     *\n     * @param R 3x3 rotation matrix.\n     *\n     * @param t 3x1 translation vector.\n     */\n    constructor(R: Mat3, t?: Vec3);\n    /**\n     *   Rodrigues vector.\n     *\n     *   The last row of the current matrix is set to [0,0,0,1].\n     *\n     * @param rvec 3x1 rotation vector. Its direction indicates the rotation axis and its length\n     * indicates the rotation angle in radian (using right hand rule).\n     *\n     * @param t 3x1 translation vector.\n     */\n    constructor(rvec: Vec3, t?: Vec3);\n    /**\n     *   Combines all constructors above. Supports 4x4, 3x4, 3x3, 1x3, 3x1 sizes of data matrix.\n     *\n     *   The last row of the current matrix is set to [0,0,0,1] when data is not 4x4.\n     *\n     * @param data 1-channel matrix. when it is 4x4, it is copied to the current matrix and t is not\n     * used. When it is 3x4, it is copied to the upper part 3x4 of the current matrix and t is not used.\n     * When it is 3x3, it is copied to the upper left 3x3 part of the current matrix. When it is 3x1 or\n     * 1x3, it is treated as a rotation vector and the Rodrigues formula is used to compute a 3x3 rotation\n     * matrix.\n     *\n     * @param t 3x1 translation vector. It is used only when data is neither 4x4 nor 3x4.\n     */\n    constructor(data: Mat, t?: Vec3);\n    constructor(vals: float_type);\n    cast(arg401: any): Affine3;\n    concatenate(affine: Affine3): Affine3;\n    /**\n     *   the inverse of the current matrix.\n     */\n    inv(method?: int): Affine3;\n    /**\n     *   Copy the 3x3 matrix L to the upper left part of the current matrix\n     *\n     *   It sets the upper left 3x3 part of the matrix. The remaining part is unaffected.\n     *\n     * @param L 3x3 matrix.\n     */\n    linear(L: Mat3): Mat3;\n    /**\n     *   the upper left 3x3 part\n     */\n    linear(): Mat3;\n    rotate(R: Mat3): Affine3;\n    rotate(rvec: Vec3): Affine3;\n    /**\n     *   Rotation matrix.\n     *\n     *   Copy the rotation matrix to the upper left 3x3 part of the current matrix. The remaining elements\n     * of the current matrix are not changed.\n     *\n     * @param R 3x3 rotation matrix.\n     */\n    rotation(R: Mat3): Mat3;\n    /**\n     *   Rodrigues vector.\n     *\n     *   It sets the upper left 3x3 part of the matrix. The remaining part is unaffected.\n     *\n     * @param rvec 3x1 rotation vector. The direction indicates the rotation axis and its length\n     * indicates the rotation angle in radian (using the right thumb convention).\n     */\n    rotation(rvec: Vec3): Vec3;\n    /**\n     *   Combines rotation methods above. Supports 3x3, 1x3, 3x1 sizes of data matrix.\n     *\n     *   It sets the upper left 3x3 part of the matrix. The remaining part is unaffected.\n     *\n     * @param data 1-channel matrix. When it is a 3x3 matrix, it sets the upper left 3x3 part of the\n     * current matrix. When it is a 1x3 or 3x1 matrix, it is used as a rotation vector. The Rodrigues\n     * formula is used to compute the rotation matrix and sets the upper left 3x3 part of the current\n     * matrix.\n     */\n    rotation(data: Mat): Mat;\n    /**\n     *   the upper left 3x3 part\n     */\n    rotation(): Mat3;\n    /**\n     *   Rodrigues vector.\n     *\n     *   a vector representing the upper left 3x3 rotation matrix of the current matrix.\n     *\n     *   Since the mapping between rotation vectors and rotation matrices is many to one, this function\n     * returns only one rotation vector that represents the current rotation matrix, which is not\n     * necessarily the same one set by `[rotation(const Vec3& rvec)]`.\n     */\n    rvec(): Vec3;\n    translate(t: Vec3): Affine3;\n    /**\n     *   Copy t to the first three elements of the last column of the current matrix\n     *\n     *   It sets the upper right 3x1 part of the matrix. The remaining part is unaffected.\n     *\n     * @param t 3x1 translation vector.\n     */\n    translation(t: Vec3): Vec3;\n    /**\n     *   the upper right 3x1 part\n     */\n    translation(): Vec3;\n    static Identity(): Affine3;\n}\n"},"node_modules_mirada_dist_src_types_opencv_Algorithm_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_Algorithm_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/Algorithm.d.ts","content":"import { bool, FileNode, FileStorage, Ptr } from './_types';\n/**\n * especially for classes of algorithms, for which there can be multiple implementations. The examples\n * are stereo correspondence (for which there are algorithms like block matching, semi-global block\n * matching, graph-cut etc.), background subtraction (which can be done using mixture-of-gaussians\n * models, codebook-based algorithm etc.), optical flow (block matching, Lucas-Kanade, Horn-Schunck\n * etc.).\n *\n * Here is example of [SimpleBlobDetector](#d0/d7a/classcv_1_1SimpleBlobDetector}) use in your\n * application via [Algorithm](#d3/d46/classcv_1_1Algorithm}) interface:\n *\n * ```cpp\n *     Ptr<Feature2D> sbd = SimpleBlobDetector::create();\n *     FileStorage fs_read(\"SimpleBlobDetector_params.xml\", FileStorage::READ);\n *\n *     if (fs_read.isOpened()) // if we have file with parameters, read them\n *     {\n *         sbd->read(fs_read.root());\n *         fs_read.release();\n *     }\n *     else // else modify the parameters and store them; user can later edit the file to use different\n * parameters\n *     {\n *         fs_read.release();\n *         FileStorage fs_write(\"SimpleBlobDetector_params.xml\", FileStorage::WRITE);\n *         sbd->write(fs_write);\n *         fs_write.release();\n *     }\n *\n *     Mat result, image = imread(\"../data/detect_blob.png\", IMREAD_COLOR);\n *     vector<KeyPoint> keypoints;\n *     sbd->detect(image, keypoints, Mat());\n *\n *     drawKeypoints(image, keypoints, result);\n *     for (vector<KeyPoint>::iterator k = keypoints.begin(); k != keypoints.end(); ++k)\n *         circle(result, k->pt, (int)k->size, Scalar(0, 0, 255), 2);\n *\n *     imshow(\"result\", result);\n *     waitKey(0);\n * ```\n *\n * Source:\n * [opencv2/core.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/core.hpp#L3077).\n *\n */\nexport declare class Algorithm {\n    constructor();\n    clear(): void;\n    empty(): bool;\n    /**\n     *   Returns the algorithm string identifier. This string is used as top level xml/yml node tag when\n     * the object is saved to a file or string.\n     */\n    getDefaultName(): String;\n    read(fn: FileNode): FileNode;\n    /**\n     *   Saves the algorithm to a file. In order to make this method work, the derived class must implement\n     * Algorithm::write(FileStorage& fs).\n     */\n    save(filename: String): String;\n    write(fs: FileStorage): FileStorage;\n    write(fs: Ptr, name?: String): Ptr;\n    /**\n     *   This is static template method of [Algorithm]. It's usage is following (in the case of SVM):\n     *\n     *   ```cpp\n     *   Ptr<SVM> svm = Algorithm::load<SVM>(\"my_svm_model.xml\");\n     *   ```\n     *\n     *    In order to make this method work, the derived class must overwrite [Algorithm::read](const\n     * [FileNode]& fn).\n     *\n     * @param filename Name of the file to read.\n     *\n     * @param objname The optional name of the node to read (if empty, the first top-level node will be\n     * used)\n     */\n    static load(arg0: any, filename: String, objname?: String): Ptr;\n    /**\n     *   This is static template method of [Algorithm]. It's usage is following (in the case of SVM):\n     *\n     *   ```cpp\n     *   Ptr<SVM> svm = Algorithm::loadFromString<SVM>(myStringModel);\n     *   ```\n     *\n     * @param strModel The string variable containing the model you want to load.\n     *\n     * @param objname The optional name of the node to read (if empty, the first top-level node will be\n     * used)\n     */\n    static loadFromString(arg1: any, strModel: String, objname?: String): Ptr;\n    /**\n     *   This is static template method of [Algorithm]. It's usage is following (in the case of SVM):\n     *\n     *   ```cpp\n     *   cv::FileStorage fsRead(\"example.xml\", FileStorage::READ);\n     *   Ptr<SVM> svm = Algorithm::read<SVM>(fsRead.root());\n     *   ```\n     *\n     *    In order to make this method work, the derived class must overwrite [Algorithm::read](const\n     * [FileNode]& fn) and also have static create() method without parameters (or with all the optional\n     * parameters)\n     */\n    static read(arg2: any, fn: FileNode): Ptr;\n}\n"},"node_modules_mirada_dist_src_types_opencv_AutoBuffer_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_AutoBuffer_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/AutoBuffer.d.ts","content":"import { size_t } from './_types';\n/**\n * The class is used for temporary buffers in functions and methods. If a temporary buffer is usually\n * small (a few K's of memory), but its size depends on the parameters, it makes sense to create a\n * small fixed-size array on stack and use it if it's large enough. If the required buffer size is\n * larger than the fixed size, another buffer of sufficient size is allocated dynamically and released\n * after the processing. Therefore, in typical cases, when the buffer size is small, there is no\n * overhead associated with malloc()/free(). At the same time, there is no limit on the size of\n * processed data.\n *\n * This is what [AutoBuffer](#d8/dd0/classcv_1_1AutoBuffer}) does. The template takes 2 parameters -\n * type of the buffer elements and the number of stack-allocated elements. Here is how the class is\n * used:\n *\n * ```cpp\n * void my_func(const cv::Mat& m)\n * {\n *    cv::AutoBuffer<float> buf(1000); // create automatic buffer containing 1000 floats\n *\n *    buf.allocate(m.rows); // if m.rows <= 1000, the pre-allocated buffer is used,\n *                          // otherwise the buffer of \"m.rows\" floats will be allocated\n *                          // dynamically and deallocated in cv::AutoBuffer destructor\n *    ...\n * }\n * ```\n *\n * Source:\n * [opencv2/core/utility.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/core/utility.hpp#L128).\n *\n */\nexport declare class AutoBuffer {\n    constructor();\n    constructor(_size: size_t);\n    constructor(buf: AutoBuffer);\n    allocate(_size: size_t): void;\n    data(): any;\n    data(): any;\n    deallocate(): void;\n    resize(_size: size_t): void;\n    size(): size_t;\n}\n"},"node_modules_mirada_dist_src_types_opencv_BFMatcher_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_BFMatcher_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/BFMatcher.d.ts","content":"import { bool, int, Ptr } from './_types';\n/**\n * For each descriptor in the first set, this matcher finds the closest descriptor in the second set by\n * trying each one. This descriptor matcher supports masking permissible matches of descriptor sets.\n *\n * Source:\n * [opencv2/features2d.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/features2d.hpp#L1140).\n *\n */\nexport declare class BFMatcher {\n    constructor(normType?: int, crossCheck?: bool);\n    /**\n     * @param emptyTrainData If emptyTrainData is false, the method creates a deep copy of the object,\n     * that is, copies both parameters and train data. If emptyTrainData is true, the method creates an\n     * object copy with the current parameters but with empty train data.\n     */\n    clone(emptyTrainData?: bool): Ptr;\n    isMaskSupported(): bool;\n    /**\n     * @param normType One of NORM_L1, NORM_L2, NORM_HAMMING, NORM_HAMMING2. L1 and L2 norms are\n     * preferable choices for SIFT and SURF descriptors, NORM_HAMMING should be used with ORB, BRISK and\n     * BRIEF, NORM_HAMMING2 should be used with ORB when WTA_K==3 or 4 (see ORB::ORB constructor\n     * description).\n     *\n     * @param crossCheck If it is false, this is will be default BFMatcher behaviour when it finds the k\n     * nearest neighbors for each query descriptor. If crossCheck==true, then the knnMatch() method with\n     * k=1 will only return pairs (i,j) such that for i-th query descriptor the j-th descriptor in the\n     * matcher's collection is the nearest and vice versa, i.e. the BFMatcher will only return consistent\n     * pairs. Such technique usually produces best results with minimal number of outliers when there are\n     * enough matches. This is alternative to the ratio test, used by D. Lowe in SIFT paper.\n     */\n    static create(normType?: int, crossCheck?: bool): Ptr;\n}\n"},"node_modules_mirada_dist_src_types_opencv_BOWTrainer_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_BOWTrainer_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/BOWTrainer.d.ts","content":"import { int, Mat } from './_types';\n/**\n * For details, see, for example, *Visual Categorization with Bags of Keypoints* by Gabriella Csurka,\n * Christopher R. Dance, Lixin Fan, Jutta Willamowski, Cedric Bray, 2004. :\n *\n * Source:\n * [opencv2/features2d.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/features2d.hpp#L1339).\n *\n */\nexport declare class BOWTrainer {\n    constructor();\n    /**\n     *   The training set is clustered using clustermethod to construct the vocabulary.\n     *\n     * @param descriptors Descriptors to add to a training set. Each row of the descriptors matrix is a\n     * descriptor.\n     */\n    add(descriptors: Mat): Mat;\n    clear(): void;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     */\n    cluster(): Mat;\n    /**\n     *   The vocabulary consists of cluster centers. So, this method returns the vocabulary. In the first\n     * variant of the method, train descriptors stored in the object are clustered. In the second variant,\n     * input descriptors are clustered.\n     *\n     * @param descriptors Descriptors to cluster. Each row of the descriptors matrix is a descriptor.\n     * Descriptors are not added to the inner train descriptor set.\n     */\n    cluster(descriptors: Mat): Mat;\n    descriptorsCount(): int;\n    getDescriptors(): Mat;\n}\n"},"node_modules_mirada_dist_src_types_opencv_CascadeClassifier_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_CascadeClassifier_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/CascadeClassifier.d.ts","content":"import { bool, double, FileNode, InputArray, int, Mat, Ptr, Rect, Size } from './_types';\nexport declare class CascadeClassifier extends Mat {\n    cc: Ptr;\n    constructor();\n    /**\n     * @param filename Name of the file from which the classifier is loaded.\n     */\n    constructor(filename: String);\n    /**\n     *   The function is parallelized with the TBB library.\n     *\n     * (Python) A face detection example using cascade classifiers can be found at\n     * opencv_source_code/samples/python/facedetect.py\n     *\n     * @param image Matrix of the type CV_8U containing an image where objects are detected.\n     *\n     * @param objects Vector of rectangles where each rectangle contains the detected object, the\n     * rectangles may be partially outside the original image.\n     *\n     * @param scaleFactor Parameter specifying how much the image size is reduced at each image scale.\n     *\n     * @param minNeighbors Parameter specifying how many neighbors each candidate rectangle should have\n     * to retain it.\n     *\n     * @param flags Parameter with the same meaning for an old cascade as in the function\n     * cvHaarDetectObjects. It is not used for a new cascade.\n     *\n     * @param minSize Minimum possible object size. Objects smaller than that are ignored.\n     *\n     * @param maxSize Maximum possible object size. Objects larger than that are ignored. If maxSize ==\n     * minSize model is evaluated on single scale.\n     */\n    detectMultiScale(image: InputArray, objects: Rect, scaleFactor?: double, minNeighbors?: int, flags?: int, minSize?: Size, maxSize?: Size): InputArray;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param image Matrix of the type CV_8U containing an image where objects are detected.\n     *\n     * @param objects Vector of rectangles where each rectangle contains the detected object, the\n     * rectangles may be partially outside the original image.\n     *\n     * @param numDetections Vector of detection numbers for the corresponding objects. An object's number\n     * of detections is the number of neighboring positively classified rectangles that were joined\n     * together to form the object.\n     *\n     * @param scaleFactor Parameter specifying how much the image size is reduced at each image scale.\n     *\n     * @param minNeighbors Parameter specifying how many neighbors each candidate rectangle should have\n     * to retain it.\n     *\n     * @param flags Parameter with the same meaning for an old cascade as in the function\n     * cvHaarDetectObjects. It is not used for a new cascade.\n     *\n     * @param minSize Minimum possible object size. Objects smaller than that are ignored.\n     *\n     * @param maxSize Maximum possible object size. Objects larger than that are ignored. If maxSize ==\n     * minSize model is evaluated on single scale.\n     */\n    detectMultiScale(image: InputArray, objects: Rect, numDetections: any, scaleFactor?: double, minNeighbors?: int, flags?: int, minSize?: Size, maxSize?: Size): InputArray;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts. This function allows you to retrieve the final stage\n     * decision certainty of classification. For this, one needs to set `outputRejectLevels` on true and\n     * provide the `rejectLevels` and `levelWeights` parameter. For each resulting detection,\n     * `levelWeights` will then contain the certainty of classification at the final stage. This value can\n     * then be used to separate strong from weaker classifications.\n     *\n     *   A code sample on how to use it efficiently can be found below:\n     *\n     *   ```cpp\n     *   Mat img;\n     *   vector<double> weights;\n     *   vector<int> levels;\n     *   vector<Rect> detections;\n     *   CascadeClassifier model(\"/path/to/your/model.xml\");\n     *   model.detectMultiScale(img, detections, levels, weights, 1.1, 3, 0, Size(), Size(), true);\n     *   cerr << \"Detection \" << detections[0] << \" with weight \" << weights[0] << endl;\n     *   ```\n     */\n    detectMultiScale(image: InputArray, objects: Rect, rejectLevels: any, levelWeights: any, scaleFactor?: double, minNeighbors?: int, flags?: int, minSize?: Size, maxSize?: Size, outputRejectLevels?: bool): InputArray;\n    empty(): bool;\n    getFeatureType(): int;\n    getMaskGenerator(): Ptr;\n    getOldCascade(): any;\n    getOriginalWindowSize(): Size;\n    isOldFormatCascade(): bool;\n    /**\n     * @param filename Name of the file from which the classifier is loaded. The file may contain an old\n     * HAAR classifier trained by the haartraining application or a new cascade classifier trained by the\n     * traincascade application.\n     */\n    load(filename: String): String;\n    /**\n     *   The file may contain a new cascade classifier (trained traincascade application) only.\n     */\n    read(node: FileNode): FileNode;\n    setMaskGenerator(maskGenerator: Ptr): Ptr;\n    static convert(oldcascade: String, newcascade: String): String;\n}\n"},"node_modules_mirada_dist_src_types_opencv_calib3d_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_calib3d_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/calib3d.d.ts","content":"import { bool, double, float, InputArray, InputArrayOfArrays, InputOutputArray, int, Mat, OutputArray, OutputArrayOfArrays, Point2d, Rect, Size, size_t, TermCriteria, Vec3d } from './_types';\n/**\n * the overall RMS re-projection error.\n * The function estimates the intrinsic camera parameters and extrinsic parameters for each of the\n * views. The algorithm is based on Zhang2000 and BouguetMCT . The coordinates of 3D object points and\n * their corresponding 2D projections in each view must be specified. That may be achieved by using an\n * object with a known geometry and easily detectable feature points. Such an object is called a\n * calibration rig or calibration pattern, and OpenCV has built-in support for a chessboard as a\n * calibration rig (see findChessboardCorners ). Currently, initialization of intrinsic parameters\n * (when CALIB_USE_INTRINSIC_GUESS is not set) is only implemented for planar calibration patterns\n * (where Z-coordinates of the object points must be all zeros). 3D calibration rigs can also be used\n * as long as initial cameraMatrix is provided.\n *\n * The algorithm performs the following steps:\n *\n * Compute the initial intrinsic parameters (the option only available for planar calibration patterns)\n * or read them from the input parameters. The distortion coefficients are all set to zeros initially\n * unless some of CALIB_FIX_K? are specified.\n * Estimate the initial camera pose as if the intrinsic parameters have been already known. This is\n * done using solvePnP .\n * Run the global Levenberg-Marquardt optimization algorithm to minimize the reprojection error, that\n * is, the total sum of squared distances between the observed feature points imagePoints and the\n * projected (using the current estimates for camera parameters and the poses) object points\n * objectPoints. See projectPoints for details.\n *\n * If you use a non-square (=non-NxN) grid and findChessboardCorners for calibration, and\n * calibrateCamera returns bad values (zero distortion coefficients, an image center very far from\n * (w/2-0.5,h/2-0.5), and/or large differences between `$f_x$` and `$f_y$` (ratios of 10:1 or more)),\n * then you have probably used patternSize=cvSize(rows,cols) instead of using\n * patternSize=cvSize(cols,rows) in findChessboardCorners .\n *\n * [calibrateCameraRO], [findChessboardCorners], [solvePnP], [initCameraMatrix2D], [stereoCalibrate],\n * [undistort]\n *\n * @param objectPoints In the new interface it is a vector of vectors of calibration pattern points in\n * the calibration pattern coordinate space (e.g. std::vector<std::vector<cv::Vec3f>>). The outer\n * vector contains as many elements as the number of the pattern views. If the same calibration pattern\n * is shown in each view and it is fully visible, all the vectors will be the same. Although, it is\n * possible to use partially occluded patterns, or even different patterns in different views. Then,\n * the vectors will be different. The points are 3D, but since they are in a pattern coordinate system,\n * then, if the rig is planar, it may make sense to put the model to a XY coordinate plane so that\n * Z-coordinate of each input object point is 0. In the old interface all the vectors of object points\n * from different views are concatenated together.\n *\n * @param imagePoints In the new interface it is a vector of vectors of the projections of calibration\n * pattern points (e.g. std::vector<std::vector<cv::Vec2f>>). imagePoints.size() and\n * objectPoints.size() and imagePoints[i].size() must be equal to objectPoints[i].size() for each i. In\n * the old interface all the vectors of object points from different views are concatenated together.\n *\n * @param imageSize Size of the image used only to initialize the intrinsic camera matrix.\n *\n * @param cameraMatrix Output 3x3 floating-point camera matrix $A =\n * \\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}$ . If CV_CALIB_USE_INTRINSIC_GUESS and/or\n * CALIB_FIX_ASPECT_RATIO are specified, some or all of fx, fy, cx, cy must be initialized before\n * calling the function.\n *\n * @param distCoeffs Output vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5,\n * k_6 [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])$ of 4, 5, 8, 12 or 14 elements.\n *\n * @param rvecs Output vector of rotation vectors (see Rodrigues ) estimated for each pattern view\n * (e.g. std::vector<cv::Mat>>). That is, each k-th rotation vector together with the corresponding\n * k-th translation vector (see the next output parameter description) brings the calibration pattern\n * from the model coordinate space (in which object points are specified) to the world coordinate\n * space, that is, a real position of the calibration pattern in the k-th pattern view (k=0.. M -1).\n *\n * @param tvecs Output vector of translation vectors estimated for each pattern view.\n *\n * @param stdDeviationsIntrinsics Output vector of standard deviations estimated for intrinsic\n * parameters. Order of deviations values: $(f_x, f_y, c_x, c_y, k_1, k_2, p_1, p_2, k_3, k_4, k_5, k_6\n * , s_1, s_2, s_3, s_4, \\tau_x, \\tau_y)$ If one of parameters is not estimated, it's deviation is\n * equals to zero.\n *\n * @param stdDeviationsExtrinsics Output vector of standard deviations estimated for extrinsic\n * parameters. Order of deviations values: $(R_1, T_1, \\dotsc , R_M, T_M)$ where M is number of pattern\n * views, $R_i, T_i$ are concatenated 1x3 vectors.\n *\n * @param perViewErrors Output vector of the RMS re-projection error estimated for each pattern view.\n *\n * @param flags Different flags that may be zero or a combination of the following values:\n * CALIB_USE_INTRINSIC_GUESS cameraMatrix contains valid initial values of fx, fy, cx, cy that are\n * optimized further. Otherwise, (cx, cy) is initially set to the image center ( imageSize is used),\n * and focal distances are computed in a least-squares fashion. Note, that if intrinsic parameters are\n * known, there is no need to use this function just to estimate extrinsic parameters. Use solvePnP\n * instead.CALIB_FIX_PRINCIPAL_POINT The principal point is not changed during the global optimization.\n * It stays at the center or at a different location specified when CALIB_USE_INTRINSIC_GUESS is set\n * too.CALIB_FIX_ASPECT_RATIO The functions considers only fy as a free parameter. The ratio fx/fy\n * stays the same as in the input cameraMatrix . When CALIB_USE_INTRINSIC_GUESS is not set, the actual\n * input values of fx and fy are ignored, only their ratio is computed and used\n * further.CALIB_ZERO_TANGENT_DIST Tangential distortion coefficients $(p_1, p_2)$ are set to zeros and\n * stay zero.CALIB_FIX_K1,...,CALIB_FIX_K6 The corresponding radial distortion coefficient is not\n * changed during the optimization. If CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the\n * supplied distCoeffs matrix is used. Otherwise, it is set to 0.CALIB_RATIONAL_MODEL Coefficients k4,\n * k5, and k6 are enabled. To provide the backward compatibility, this extra flag should be explicitly\n * specified to make the calibration function use the rational model and return 8 coefficients. If the\n * flag is not set, the function computes and returns only 5 distortion\n * coefficients.CALIB_THIN_PRISM_MODEL Coefficients s1, s2, s3 and s4 are enabled. To provide the\n * backward compatibility, this extra flag should be explicitly specified to make the calibration\n * function use the thin prism model and return 12 coefficients. If the flag is not set, the function\n * computes and returns only 5 distortion coefficients.CALIB_FIX_S1_S2_S3_S4 The thin prism distortion\n * coefficients are not changed during the optimization. If CALIB_USE_INTRINSIC_GUESS is set, the\n * coefficient from the supplied distCoeffs matrix is used. Otherwise, it is set to\n * 0.CALIB_TILTED_MODEL Coefficients tauX and tauY are enabled. To provide the backward compatibility,\n * this extra flag should be explicitly specified to make the calibration function use the tilted\n * sensor model and return 14 coefficients. If the flag is not set, the function computes and returns\n * only 5 distortion coefficients.CALIB_FIX_TAUX_TAUY The coefficients of the tilted sensor model are\n * not changed during the optimization. If CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the\n * supplied distCoeffs matrix is used. Otherwise, it is set to 0.\n *\n * @param criteria Termination criteria for the iterative optimization algorithm.\n */\nexport declare function calibrateCamera(objectPoints: InputArrayOfArrays, imagePoints: InputArrayOfArrays, imageSize: Size, cameraMatrix: InputOutputArray, distCoeffs: InputOutputArray, rvecs: OutputArrayOfArrays, tvecs: OutputArrayOfArrays, stdDeviationsIntrinsics: OutputArray, stdDeviationsExtrinsics: OutputArray, perViewErrors: OutputArray, flags?: int, criteria?: TermCriteria): double;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function calibrateCamera(objectPoints: InputArrayOfArrays, imagePoints: InputArrayOfArrays, imageSize: Size, cameraMatrix: InputOutputArray, distCoeffs: InputOutputArray, rvecs: OutputArrayOfArrays, tvecs: OutputArrayOfArrays, flags?: int, criteria?: TermCriteria): double;\n/**\n * This function is an extension of [calibrateCamera()] with the method of releasing object which was\n * proposed in strobl2011iccv. In many common cases with inaccurate, unmeasured, roughly planar targets\n * (calibration plates), this method can dramatically improve the precision of the estimated camera\n * parameters. Both the object-releasing method and standard method are supported by this function. Use\n * the parameter **iFixedPoint** for method selection. In the internal implementation,\n * [calibrateCamera()] is a wrapper for this function.\n *\n * the overall RMS re-projection error.\n * The function estimates the intrinsic camera parameters and extrinsic parameters for each of the\n * views. The algorithm is based on Zhang2000, BouguetMCT and strobl2011iccv. See [calibrateCamera()]\n * for other detailed explanations.\n *\n * [calibrateCamera], [findChessboardCorners], [solvePnP], [initCameraMatrix2D], [stereoCalibrate],\n * [undistort]\n *\n * @param objectPoints Vector of vectors of calibration pattern points in the calibration pattern\n * coordinate space. See calibrateCamera() for details. If the method of releasing object to be used,\n * the identical calibration board must be used in each view and it must be fully visible, and all\n * objectPoints[i] must be the same and all points should be roughly close to a plane. The calibration\n * target has to be rigid, or at least static if the camera (rather than the calibration target) is\n * shifted for grabbing images.\n *\n * @param imagePoints Vector of vectors of the projections of calibration pattern points. See\n * calibrateCamera() for details.\n *\n * @param imageSize Size of the image used only to initialize the intrinsic camera matrix.\n *\n * @param iFixedPoint The index of the 3D object point in objectPoints[0] to be fixed. It also acts as\n * a switch for calibration method selection. If object-releasing method to be used, pass in the\n * parameter in the range of [1, objectPoints[0].size()-2], otherwise a value out of this range will\n * make standard calibration method selected. Usually the top-right corner point of the calibration\n * board grid is recommended to be fixed when object-releasing method being utilized. According to\n * strobl2011iccv, two other points are also fixed. In this implementation, objectPoints[0].front and\n * objectPoints[0].back.z are used. With object-releasing method, accurate rvecs, tvecs and\n * newObjPoints are only possible if coordinates of these three fixed points are accurate enough.\n *\n * @param cameraMatrix Output 3x3 floating-point camera matrix. See calibrateCamera() for details.\n *\n * @param distCoeffs Output vector of distortion coefficients. See calibrateCamera() for details.\n *\n * @param rvecs Output vector of rotation vectors estimated for each pattern view. See\n * calibrateCamera() for details.\n *\n * @param tvecs Output vector of translation vectors estimated for each pattern view.\n *\n * @param newObjPoints The updated output vector of calibration pattern points. The coordinates might\n * be scaled based on three fixed points. The returned coordinates are accurate only if the above\n * mentioned three fixed points are accurate. If not needed, noArray() can be passed in. This parameter\n * is ignored with standard calibration method.\n *\n * @param stdDeviationsIntrinsics Output vector of standard deviations estimated for intrinsic\n * parameters. See calibrateCamera() for details.\n *\n * @param stdDeviationsExtrinsics Output vector of standard deviations estimated for extrinsic\n * parameters. See calibrateCamera() for details.\n *\n * @param stdDeviationsObjPoints Output vector of standard deviations estimated for refined coordinates\n * of calibration pattern points. It has the same size and order as objectPoints[0] vector. This\n * parameter is ignored with standard calibration method.\n *\n * @param perViewErrors Output vector of the RMS re-projection error estimated for each pattern view.\n *\n * @param flags Different flags that may be zero or a combination of some predefined values. See\n * calibrateCamera() for details. If the method of releasing object is used, the calibration time may\n * be much longer. CALIB_USE_QR or CALIB_USE_LU could be used for faster calibration with potentially\n * less precise and less stable in some rare cases.\n *\n * @param criteria Termination criteria for the iterative optimization algorithm.\n */\nexport declare function calibrateCameraRO(objectPoints: InputArrayOfArrays, imagePoints: InputArrayOfArrays, imageSize: Size, iFixedPoint: int, cameraMatrix: InputOutputArray, distCoeffs: InputOutputArray, rvecs: OutputArrayOfArrays, tvecs: OutputArrayOfArrays, newObjPoints: OutputArray, stdDeviationsIntrinsics: OutputArray, stdDeviationsExtrinsics: OutputArray, stdDeviationsObjPoints: OutputArray, perViewErrors: OutputArray, flags?: int, criteria?: TermCriteria): double;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function calibrateCameraRO(objectPoints: InputArrayOfArrays, imagePoints: InputArrayOfArrays, imageSize: Size, iFixedPoint: int, cameraMatrix: InputOutputArray, distCoeffs: InputOutputArray, rvecs: OutputArrayOfArrays, tvecs: OutputArrayOfArrays, newObjPoints: OutputArray, flags?: int, criteria?: TermCriteria): double;\n/**\n * The function performs the Hand-Eye calibration using various methods. One approach consists in\n * estimating the rotation then the translation (separable solutions) and the following methods are\n * implemented:\n *\n * R. Tsai, R. Lenz A New Technique for Fully Autonomous and Efficient 3D Robotics Hand/EyeCalibration\n * Tsai89\n * F. Park, B. Martin Robot Sensor Calibration: Solving AX = XB on the Euclidean Group Park94\n * R. Horaud, F. Dornaika Hand-Eye Calibration Horaud95\n *\n * Another approach consists in estimating simultaneously the rotation and the translation\n * (simultaneous solutions), with the following implemented method:\n *\n * N. Andreff, R. Horaud, B. Espiau On-line Hand-Eye Calibration Andreff99\n * K. Daniilidis Hand-Eye Calibration Using Dual Quaternions Daniilidis98\n *\n * The following picture describes the Hand-Eye calibration problem where the transformation between a\n * camera (\"eye\") mounted on a robot gripper (\"hand\") has to be estimated.\n *\n * The calibration procedure is the following:\n *\n * a static calibration pattern is used to estimate the transformation between the target frame and the\n * camera frame\n * the robot gripper is moved in order to acquire several poses\n * for each pose, the homogeneous transformation between the gripper frame and the robot base frame is\n * recorded using for instance the robot kinematics `\\\\[ \\\\begin{bmatrix} X_b\\\\\\\\ Y_b\\\\\\\\ Z_b\\\\\\\\ 1\n * \\\\end{bmatrix} = \\\\begin{bmatrix} _{}^{b}\\\\textrm{R}_g & _{}^{b}\\\\textrm{t}_g \\\\\\\\ 0_{1 \\\\times 3} &\n * 1 \\\\end{bmatrix} \\\\begin{bmatrix} X_g\\\\\\\\ Y_g\\\\\\\\ Z_g\\\\\\\\ 1 \\\\end{bmatrix} \\\\]`\n * for each pose, the homogeneous transformation between the calibration target frame and the camera\n * frame is recorded using for instance a pose estimation method (PnP) from 2D-3D point correspondences\n * `\\\\[ \\\\begin{bmatrix} X_c\\\\\\\\ Y_c\\\\\\\\ Z_c\\\\\\\\ 1 \\\\end{bmatrix} = \\\\begin{bmatrix}\n * _{}^{c}\\\\textrm{R}_t & _{}^{c}\\\\textrm{t}_t \\\\\\\\ 0_{1 \\\\times 3} & 1 \\\\end{bmatrix} \\\\begin{bmatrix}\n * X_t\\\\\\\\ Y_t\\\\\\\\ Z_t\\\\\\\\ 1 \\\\end{bmatrix} \\\\]`\n *\n * The Hand-Eye calibration procedure returns the following homogeneous transformation `\\\\[\n * \\\\begin{bmatrix} X_g\\\\\\\\ Y_g\\\\\\\\ Z_g\\\\\\\\ 1 \\\\end{bmatrix} = \\\\begin{bmatrix} _{}^{g}\\\\textrm{R}_c &\n * _{}^{g}\\\\textrm{t}_c \\\\\\\\ 0_{1 \\\\times 3} & 1 \\\\end{bmatrix} \\\\begin{bmatrix} X_c\\\\\\\\ Y_c\\\\\\\\\n * Z_c\\\\\\\\ 1 \\\\end{bmatrix} \\\\]`\n *\n * This problem is also known as solving the `$\\\\mathbf{A}\\\\mathbf{X}=\\\\mathbf{X}\\\\mathbf{B}$`\n * equation: `\\\\[ \\\\begin{align*} ^{b}{\\\\textrm{T}_g}^{(1)} \\\\hspace{0.2em} ^{g}\\\\textrm{T}_c\n * \\\\hspace{0.2em} ^{c}{\\\\textrm{T}_t}^{(1)} &= \\\\hspace{0.1em} ^{b}{\\\\textrm{T}_g}^{(2)}\n * \\\\hspace{0.2em} ^{g}\\\\textrm{T}_c \\\\hspace{0.2em} ^{c}{\\\\textrm{T}_t}^{(2)} \\\\\\\\\n * (^{b}{\\\\textrm{T}_g}^{(2)})^{-1} \\\\hspace{0.2em} ^{b}{\\\\textrm{T}_g}^{(1)} \\\\hspace{0.2em}\n * ^{g}\\\\textrm{T}_c &= \\\\hspace{0.1em} ^{g}\\\\textrm{T}_c \\\\hspace{0.2em} ^{c}{\\\\textrm{T}_t}^{(2)}\n * (^{c}{\\\\textrm{T}_t}^{(1)})^{-1} \\\\\\\\ \\\\textrm{A}_i \\\\textrm{X} &= \\\\textrm{X} \\\\textrm{B}_i \\\\\\\\\n * \\\\end{align*} \\\\]`\n *\n * Additional information can be found on this .\n *\n * A minimum of 2 motions with non parallel rotation axes are necessary to determine the hand-eye\n * transformation. So at least 3 different poses are required, but it is strongly recommended to use\n * many more poses.\n *\n * @param R_gripper2base Rotation part extracted from the homogeneous matrix that transforms a point\n * expressed in the gripper frame to the robot base frame ( $_{}^{b}\\textrm{T}_g$). This is a vector\n * (vector<Mat>) that contains the rotation matrices for all the transformations from gripper frame to\n * robot base frame.\n *\n * @param t_gripper2base Translation part extracted from the homogeneous matrix that transforms a point\n * expressed in the gripper frame to the robot base frame ( $_{}^{b}\\textrm{T}_g$). This is a vector\n * (vector<Mat>) that contains the translation vectors for all the transformations from gripper frame\n * to robot base frame.\n *\n * @param R_target2cam Rotation part extracted from the homogeneous matrix that transforms a point\n * expressed in the target frame to the camera frame ( $_{}^{c}\\textrm{T}_t$). This is a vector\n * (vector<Mat>) that contains the rotation matrices for all the transformations from calibration\n * target frame to camera frame.\n *\n * @param t_target2cam Rotation part extracted from the homogeneous matrix that transforms a point\n * expressed in the target frame to the camera frame ( $_{}^{c}\\textrm{T}_t$). This is a vector\n * (vector<Mat>) that contains the translation vectors for all the transformations from calibration\n * target frame to camera frame.\n *\n * @param R_cam2gripper Estimated rotation part extracted from the homogeneous matrix that transforms a\n * point expressed in the camera frame to the gripper frame ( $_{}^{g}\\textrm{T}_c$).\n *\n * @param t_cam2gripper Estimated translation part extracted from the homogeneous matrix that\n * transforms a point expressed in the camera frame to the gripper frame ( $_{}^{g}\\textrm{T}_c$).\n *\n * @param method One of the implemented Hand-Eye calibration method, see cv::HandEyeCalibrationMethod\n */\nexport declare function calibrateHandEye(R_gripper2base: InputArrayOfArrays, t_gripper2base: InputArrayOfArrays, R_target2cam: InputArrayOfArrays, t_target2cam: InputArrayOfArrays, R_cam2gripper: OutputArray, t_cam2gripper: OutputArray, method?: HandEyeCalibrationMethod): void;\n/**\n * The function computes various useful camera characteristics from the previously estimated camera\n * matrix.\n *\n * Do keep in mind that the unity measure 'mm' stands for whatever unit of measure one chooses for the\n * chessboard pitch (it can thus be any value).\n *\n * @param cameraMatrix Input camera matrix that can be estimated by calibrateCamera or stereoCalibrate\n * .\n *\n * @param imageSize Input image size in pixels.\n *\n * @param apertureWidth Physical width in mm of the sensor.\n *\n * @param apertureHeight Physical height in mm of the sensor.\n *\n * @param fovx Output field of view in degrees along the horizontal sensor axis.\n *\n * @param fovy Output field of view in degrees along the vertical sensor axis.\n *\n * @param focalLength Focal length of the lens in mm.\n *\n * @param principalPoint Principal point in mm.\n *\n * @param aspectRatio $f_y/f_x$\n */\nexport declare function calibrationMatrixValues(cameraMatrix: InputArray, imageSize: Size, apertureWidth: double, apertureHeight: double, fovx: any, fovy: any, focalLength: any, principalPoint: any, aspectRatio: any): void;\nexport declare function checkChessboard(img: InputArray, size: Size): bool;\n/**\n * The functions compute:\n *\n * `\\\\[\\\\begin{array}{l} \\\\texttt{rvec3} = \\\\mathrm{rodrigues} ^{-1} \\\\left ( \\\\mathrm{rodrigues} (\n * \\\\texttt{rvec2} ) \\\\cdot \\\\mathrm{rodrigues} ( \\\\texttt{rvec1} ) \\\\right ) \\\\\\\\ \\\\texttt{tvec3} =\n * \\\\mathrm{rodrigues} ( \\\\texttt{rvec2} ) \\\\cdot \\\\texttt{tvec1} + \\\\texttt{tvec2} \\\\end{array} ,\\\\]`\n *\n * where `$\\\\mathrm{rodrigues}$` denotes a rotation vector to a rotation matrix transformation, and\n * `$\\\\mathrm{rodrigues}^{-1}$` denotes the inverse transformation. See Rodrigues for details.\n *\n * Also, the functions can compute the derivatives of the output vectors with regards to the input\n * vectors (see matMulDeriv ). The functions are used inside stereoCalibrate but can also be used in\n * your own code where Levenberg-Marquardt or another gradient-based solver is used to optimize a\n * function that contains a matrix multiplication.\n *\n * @param rvec1 First rotation vector.\n *\n * @param tvec1 First translation vector.\n *\n * @param rvec2 Second rotation vector.\n *\n * @param tvec2 Second translation vector.\n *\n * @param rvec3 Output rotation vector of the superposition.\n *\n * @param tvec3 Output translation vector of the superposition.\n *\n * @param dr3dr1 Optional output derivative of rvec3 with regard to rvec1\n *\n * @param dr3dt1 Optional output derivative of rvec3 with regard to tvec1\n *\n * @param dr3dr2 Optional output derivative of rvec3 with regard to rvec2\n *\n * @param dr3dt2 Optional output derivative of rvec3 with regard to tvec2\n *\n * @param dt3dr1 Optional output derivative of tvec3 with regard to rvec1\n *\n * @param dt3dt1 Optional output derivative of tvec3 with regard to tvec1\n *\n * @param dt3dr2 Optional output derivative of tvec3 with regard to rvec2\n *\n * @param dt3dt2 Optional output derivative of tvec3 with regard to tvec2\n */\nexport declare function composeRT(rvec1: InputArray, tvec1: InputArray, rvec2: InputArray, tvec2: InputArray, rvec3: OutputArray, tvec3: OutputArray, dr3dr1?: OutputArray, dr3dt1?: OutputArray, dr3dr2?: OutputArray, dr3dt2?: OutputArray, dt3dr1?: OutputArray, dt3dt1?: OutputArray, dt3dr2?: OutputArray, dt3dt2?: OutputArray): void;\n/**\n * For every point in one of the two images of a stereo pair, the function finds the equation of the\n * corresponding epipolar line in the other image.\n *\n * From the fundamental matrix definition (see findFundamentalMat ), line `$l^{(2)}_i$` in the second\n * image for the point `$p^{(1)}_i$` in the first image (when whichImage=1 ) is computed as:\n *\n * `\\\\[l^{(2)}_i = F p^{(1)}_i\\\\]`\n *\n * And vice versa, when whichImage=2, `$l^{(1)}_i$` is computed from `$p^{(2)}_i$` as:\n *\n * `\\\\[l^{(1)}_i = F^T p^{(2)}_i\\\\]`\n *\n * Line coefficients are defined up to a scale. They are normalized so that `$a_i^2+b_i^2=1$` .\n *\n * @param points Input points. $N \\times 1$ or $1 \\times N$ matrix of type CV_32FC2 or vector<Point2f>\n * .\n *\n * @param whichImage Index of the image (1 or 2) that contains the points .\n *\n * @param F Fundamental matrix that can be estimated using findFundamentalMat or stereoRectify .\n *\n * @param lines Output vector of the epipolar lines corresponding to the points in the other image.\n * Each line $ax + by + c=0$ is encoded by 3 numbers $(a, b, c)$ .\n */\nexport declare function computeCorrespondEpilines(points: InputArray, whichImage: int, F: InputArray, lines: OutputArray): void;\n/**\n * The function converts points homogeneous to Euclidean space using perspective projection. That is,\n * each point (x1, x2, ... x(n-1), xn) is converted to (x1/xn, x2/xn, ..., x(n-1)/xn). When xn=0, the\n * output point coordinates will be (0,0,0,...).\n *\n * @param src Input vector of N-dimensional points.\n *\n * @param dst Output vector of N-1-dimensional points.\n */\nexport declare function convertPointsFromHomogeneous(src: InputArray, dst: OutputArray): void;\n/**\n * The function converts 2D or 3D points from/to homogeneous coordinates by calling either\n * convertPointsToHomogeneous or convertPointsFromHomogeneous.\n *\n * The function is obsolete. Use one of the previous two functions instead.\n *\n * @param src Input array or vector of 2D, 3D, or 4D points.\n *\n * @param dst Output vector of 2D, 3D, or 4D points.\n */\nexport declare function convertPointsHomogeneous(src: InputArray, dst: OutputArray): void;\n/**\n * The function converts points from Euclidean to homogeneous space by appending 1's to the tuple of\n * point coordinates. That is, each point (x1, x2, ..., xn) is converted to (x1, x2, ..., xn, 1).\n *\n * @param src Input vector of N-dimensional points.\n *\n * @param dst Output vector of N+1-dimensional points.\n */\nexport declare function convertPointsToHomogeneous(src: InputArray, dst: OutputArray): void;\n/**\n * The function implements the Optimal Triangulation Method (see Multiple View Geometry for details).\n * For each given point correspondence points1[i] <-> points2[i], and a fundamental matrix F, it\n * computes the corrected correspondences newPoints1[i] <-> newPoints2[i] that minimize the geometric\n * error `$d(points1[i], newPoints1[i])^2 + d(points2[i],newPoints2[i])^2$` (where `$d(a,b)$` is the\n * geometric distance between points `$a$` and `$b$` ) subject to the epipolar constraint\n * `$newPoints2^T * F * newPoints1 = 0$` .\n *\n * @param F 3x3 fundamental matrix.\n *\n * @param points1 1xN array containing the first set of points.\n *\n * @param points2 1xN array containing the second set of points.\n *\n * @param newPoints1 The optimized points1.\n *\n * @param newPoints2 The optimized points2.\n */\nexport declare function correctMatches(F: InputArray, points1: InputArray, points2: InputArray, newPoints1: OutputArray, newPoints2: OutputArray): void;\n/**\n * This function decompose an essential matrix E using svd decomposition HartleyZ00 . Generally 4\n * possible poses exists for a given E. They are `$[R_1, t]$`, `$[R_1, -t]$`, `$[R_2, t]$`, `$[R_2,\n * -t]$`. By decomposing E, you can only get the direction of the translation, so the function returns\n * unit t.\n *\n * @param E The input essential matrix.\n *\n * @param R1 One possible rotation matrix.\n *\n * @param R2 Another possible rotation matrix.\n *\n * @param t One possible translation.\n */\nexport declare function decomposeEssentialMat(E: InputArray, R1: OutputArray, R2: OutputArray, t: OutputArray): void;\n/**\n * This function extracts relative camera motion between two views observing a planar object from the\n * homography H induced by the plane. The intrinsic camera matrix K must also be provided. The function\n * may return up to four mathematical solution sets. At least two of the solutions may further be\n * invalidated if point correspondences are available by applying positive depth constraint (all points\n * must be in front of the camera). The decomposition method is described in detail in Malis .\n *\n * @param H The input homography matrix between two images.\n *\n * @param K The input intrinsic camera calibration matrix.\n *\n * @param rotations Array of rotation matrices.\n *\n * @param translations Array of translation matrices.\n *\n * @param normals Array of plane normal matrices.\n */\nexport declare function decomposeHomographyMat(H: InputArray, K: InputArray, rotations: OutputArrayOfArrays, translations: OutputArrayOfArrays, normals: OutputArrayOfArrays): int;\n/**\n * The function computes a decomposition of a projection matrix into a calibration and a rotation\n * matrix and the position of a camera.\n *\n * It optionally returns three rotation matrices, one for each axis, and three Euler angles that could\n * be used in OpenGL. Note, there is always more than one sequence of rotations about the three\n * principal axes that results in the same orientation of an object, e.g. see Slabaugh . Returned tree\n * rotation matrices and corresponding three Euler angles are only one of the possible solutions.\n *\n * The function is based on RQDecomp3x3 .\n *\n * @param projMatrix 3x4 input projection matrix P.\n *\n * @param cameraMatrix Output 3x3 camera matrix K.\n *\n * @param rotMatrix Output 3x3 external rotation matrix R.\n *\n * @param transVect Output 4x1 translation vector T.\n *\n * @param rotMatrixX Optional 3x3 rotation matrix around x-axis.\n *\n * @param rotMatrixY Optional 3x3 rotation matrix around y-axis.\n *\n * @param rotMatrixZ Optional 3x3 rotation matrix around z-axis.\n *\n * @param eulerAngles Optional three-element vector containing three Euler angles of rotation in\n * degrees.\n */\nexport declare function decomposeProjectionMatrix(projMatrix: InputArray, cameraMatrix: OutputArray, rotMatrix: OutputArray, transVect: OutputArray, rotMatrixX?: OutputArray, rotMatrixY?: OutputArray, rotMatrixZ?: OutputArray, eulerAngles?: OutputArray): void;\n/**\n * The function draws individual chessboard corners detected either as red circles if the board was not\n * found, or as colored corners connected with lines if the board was found.\n *\n * @param image Destination image. It must be an 8-bit color image.\n *\n * @param patternSize Number of inner corners per a chessboard row and column (patternSize =\n * cv::Size(points_per_row,points_per_column)).\n *\n * @param corners Array of detected corners, the output of findChessboardCorners.\n *\n * @param patternWasFound Parameter indicating whether the complete board was found or not. The return\n * value of findChessboardCorners should be passed here.\n */\nexport declare function drawChessboardCorners(image: InputOutputArray, patternSize: Size, corners: InputArray, patternWasFound: bool): void;\n/**\n * [solvePnP]\n *\n * This function draws the axes of the world/object coordinate system w.r.t. to the camera frame. OX is\n * drawn in red, OY in green and OZ in blue.\n *\n * @param image Input/output image. It must have 1 or 3 channels. The number of channels is not\n * altered.\n *\n * @param cameraMatrix Input 3x3 floating-point matrix of camera intrinsic parameters. $A =\n * \\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}$\n *\n * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6\n * [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is empty,\n * the zero distortion coefficients are assumed.\n *\n * @param rvec Rotation vector (see Rodrigues ) that, together with tvec, brings points from the model\n * coordinate system to the camera coordinate system.\n *\n * @param tvec Translation vector.\n *\n * @param length Length of the painted axes in the same unit than tvec (usually in meters).\n *\n * @param thickness Line thickness of the painted axes.\n */\nexport declare function drawFrameAxes(image: InputOutputArray, cameraMatrix: InputArray, distCoeffs: InputArray, rvec: InputArray, tvec: InputArray, length: float, thickness?: int): void;\n/**\n * It computes `\\\\[ \\\\begin{bmatrix} x\\\\\\\\ y\\\\\\\\ \\\\end{bmatrix} = \\\\begin{bmatrix} a_{11} & a_{12}\\\\\\\\\n * a_{21} & a_{22}\\\\\\\\ \\\\end{bmatrix} \\\\begin{bmatrix} X\\\\\\\\ Y\\\\\\\\ \\\\end{bmatrix} + \\\\begin{bmatrix}\n * b_1\\\\\\\\ b_2\\\\\\\\ \\\\end{bmatrix} \\\\]`\n *\n * Output 2D affine transformation matrix `$2 \\\\times 3$` or empty matrix if transformation could not\n * be estimated. The returned matrix has the following form: `\\\\[ \\\\begin{bmatrix} a_{11} & a_{12} &\n * b_1\\\\\\\\ a_{21} & a_{22} & b_2\\\\\\\\ \\\\end{bmatrix} \\\\]`\n * The function estimates an optimal 2D affine transformation between two 2D point sets using the\n * selected robust algorithm.\n *\n * The computed transformation is then refined further (using only inliers) with the\n * Levenberg-Marquardt method to reduce the re-projection error even more.\n *\n * The RANSAC method can handle practically any ratio of outliers but needs a threshold to distinguish\n * inliers from outliers. The method LMeDS does not need any threshold but it works correctly only when\n * there are more than 50% of inliers.\n *\n * [estimateAffinePartial2D], [getAffineTransform]\n *\n * @param from First input 2D point set containing $(X,Y)$.\n *\n * @param to Second input 2D point set containing $(x,y)$.\n *\n * @param inliers Output vector indicating which points are inliers (1-inlier, 0-outlier).\n *\n * @param method Robust method used to compute transformation. The following methods are possible:\n * cv::RANSAC - RANSAC-based robust methodcv::LMEDS - Least-Median robust method RANSAC is the default\n * method.\n *\n * @param ransacReprojThreshold Maximum reprojection error in the RANSAC algorithm to consider a point\n * as an inlier. Applies only to RANSAC.\n *\n * @param maxIters The maximum number of robust method iterations.\n *\n * @param confidence Confidence level, between 0 and 1, for the estimated transformation. Anything\n * between 0.95 and 0.99 is usually good enough. Values too close to 1 can slow down the estimation\n * significantly. Values lower than 0.8-0.9 can result in an incorrectly estimated transformation.\n *\n * @param refineIters Maximum number of iterations of refining algorithm (Levenberg-Marquardt). Passing\n * 0 will disable refining, so the output matrix will be output of robust method.\n */\nexport declare function estimateAffine2D(from: InputArray, to: InputArray, inliers?: OutputArray, method?: int, ransacReprojThreshold?: double, maxIters?: size_t, confidence?: double, refineIters?: size_t): any;\n/**\n * It computes `\\\\[ \\\\begin{bmatrix} x\\\\\\\\ y\\\\\\\\ z\\\\\\\\ \\\\end{bmatrix} = \\\\begin{bmatrix} a_{11} &\n * a_{12} & a_{13}\\\\\\\\ a_{21} & a_{22} & a_{23}\\\\\\\\ a_{31} & a_{32} & a_{33}\\\\\\\\ \\\\end{bmatrix}\n * \\\\begin{bmatrix} X\\\\\\\\ Y\\\\\\\\ Z\\\\\\\\ \\\\end{bmatrix} + \\\\begin{bmatrix} b_1\\\\\\\\ b_2\\\\\\\\ b_3\\\\\\\\\n * \\\\end{bmatrix} \\\\]`\n *\n * The function estimates an optimal 3D affine transformation between two 3D point sets using the\n * RANSAC algorithm.\n *\n * @param src First input 3D point set containing $(X,Y,Z)$.\n *\n * @param dst Second input 3D point set containing $(x,y,z)$.\n *\n * @param out Output 3D affine transformation matrix $3 \\times 4$ of the form \\[ \\begin{bmatrix} a_{11}\n * & a_{12} & a_{13} & b_1\\\\ a_{21} & a_{22} & a_{23} & b_2\\\\ a_{31} & a_{32} & a_{33} & b_3\\\\\n * \\end{bmatrix} \\]\n *\n * @param inliers Output vector indicating which points are inliers (1-inlier, 0-outlier).\n *\n * @param ransacThreshold Maximum reprojection error in the RANSAC algorithm to consider a point as an\n * inlier.\n *\n * @param confidence Confidence level, between 0 and 1, for the estimated transformation. Anything\n * between 0.95 and 0.99 is usually good enough. Values too close to 1 can slow down the estimation\n * significantly. Values lower than 0.8-0.9 can result in an incorrectly estimated transformation.\n */\nexport declare function estimateAffine3D(src: InputArray, dst: InputArray, out: OutputArray, inliers: OutputArray, ransacThreshold?: double, confidence?: double): int;\n/**\n * Output 2D affine transformation (4 degrees of freedom) matrix `$2 \\\\times 3$` or empty matrix if\n * transformation could not be estimated.\n * The function estimates an optimal 2D affine transformation with 4 degrees of freedom limited to\n * combinations of translation, rotation, and uniform scaling. Uses the selected algorithm for robust\n * estimation.\n *\n * The computed transformation is then refined further (using only inliers) with the\n * Levenberg-Marquardt method to reduce the re-projection error even more.\n *\n * Estimated transformation matrix is: `\\\\[ \\\\begin{bmatrix} \\\\cos(\\\\theta) \\\\cdot s & -\\\\sin(\\\\theta)\n * \\\\cdot s & t_x \\\\\\\\ \\\\sin(\\\\theta) \\\\cdot s & \\\\cos(\\\\theta) \\\\cdot s & t_y \\\\end{bmatrix} \\\\]`\n * Where `$ \\\\theta $` is the rotation angle, `$ s $` the scaling factor and `$ t_x, t_y $` are\n * translations in `$ x, y $` axes respectively.\n *\n * The RANSAC method can handle practically any ratio of outliers but need a threshold to distinguish\n * inliers from outliers. The method LMeDS does not need any threshold but it works correctly only when\n * there are more than 50% of inliers.\n *\n * [estimateAffine2D], [getAffineTransform]\n *\n * @param from First input 2D point set.\n *\n * @param to Second input 2D point set.\n *\n * @param inliers Output vector indicating which points are inliers.\n *\n * @param method Robust method used to compute transformation. The following methods are possible:\n * cv::RANSAC - RANSAC-based robust methodcv::LMEDS - Least-Median robust method RANSAC is the default\n * method.\n *\n * @param ransacReprojThreshold Maximum reprojection error in the RANSAC algorithm to consider a point\n * as an inlier. Applies only to RANSAC.\n *\n * @param maxIters The maximum number of robust method iterations.\n *\n * @param confidence Confidence level, between 0 and 1, for the estimated transformation. Anything\n * between 0.95 and 0.99 is usually good enough. Values too close to 1 can slow down the estimation\n * significantly. Values lower than 0.8-0.9 can result in an incorrectly estimated transformation.\n *\n * @param refineIters Maximum number of iterations of refining algorithm (Levenberg-Marquardt). Passing\n * 0 will disable refining, so the output matrix will be output of robust method.\n */\nexport declare function estimateAffinePartial2D(from: InputArray, to: InputArray, inliers?: OutputArray, method?: int, ransacReprojThreshold?: double, maxIters?: size_t, confidence?: double, refineIters?: size_t): any;\n/**\n * This function is intended to filter the output of the decomposeHomographyMat based on additional\n * information as described in Malis . The summary of the method: the decomposeHomographyMat function\n * returns 2 unique solutions and their \"opposites\" for a total of 4 solutions. If we have access to\n * the sets of points visible in the camera frame before and after the homography transformation is\n * applied, we can determine which are the true potential solutions and which are the opposites by\n * verifying which homographies are consistent with all visible reference points being in front of the\n * camera. The inputs are left unchanged; the filtered solution set is returned as indices into the\n * existing one.\n *\n * @param rotations Vector of rotation matrices.\n *\n * @param normals Vector of plane normal matrices.\n *\n * @param beforePoints Vector of (rectified) visible reference points before the homography is applied\n *\n * @param afterPoints Vector of (rectified) visible reference points after the homography is applied\n *\n * @param possibleSolutions Vector of int indices representing the viable solution set after filtering\n *\n * @param pointsMask optional Mat/Vector of 8u type representing the mask for the inliers as given by\n * the findHomography function\n */\nexport declare function filterHomographyDecompByVisibleRefpoints(rotations: InputArrayOfArrays, normals: InputArrayOfArrays, beforePoints: InputArray, afterPoints: InputArray, possibleSolutions: OutputArray, pointsMask?: InputArray): void;\n/**\n * @param img The input 16-bit signed disparity image\n *\n * @param newVal The disparity value used to paint-off the speckles\n *\n * @param maxSpeckleSize The maximum speckle size to consider it a speckle. Larger blobs are not\n * affected by the algorithm\n *\n * @param maxDiff Maximum difference between neighbor disparity pixels to put them into the same blob.\n * Note that since StereoBM, StereoSGBM and may be other algorithms return a fixed-point disparity map,\n * where disparity values are multiplied by 16, this scale factor should be taken into account when\n * specifying this parameter value.\n *\n * @param buf The optional temporary buffer to avoid memory allocation within the function.\n */\nexport declare function filterSpeckles(img: InputOutputArray, newVal: double, maxSpeckleSize: int, maxDiff: double, buf?: InputOutputArray): void;\nexport declare function find4QuadCornerSubpix(img: InputArray, corners: InputOutputArray, region_size: Size): bool;\n/**\n * The function attempts to determine whether the input image is a view of the chessboard pattern and\n * locate the internal chessboard corners. The function returns a non-zero value if all of the corners\n * are found and they are placed in a certain order (row by row, left to right in every row).\n * Otherwise, if the function fails to find all the corners or reorder them, it returns 0. For example,\n * a regular chessboard has 8 x 8 squares and 7 x 7 internal corners, that is, points where the black\n * squares touch each other. The detected coordinates are approximate, and to determine their positions\n * more accurately, the function calls cornerSubPix. You also may use the function cornerSubPix with\n * different parameters if returned coordinates are not accurate enough.\n *\n * Sample usage of detecting and drawing chessboard corners: :\n *\n * ```cpp\n * Size patternsize(8,6); //interior number of corners\n * Mat gray = ....; //source image\n * vector<Point2f> corners; //this will be filled by the detected corners\n *\n * //CALIB_CB_FAST_CHECK saves a lot of time on images\n * //that do not contain any chessboard corners\n * bool patternfound = findChessboardCorners(gray, patternsize, corners,\n *         CALIB_CB_ADAPTIVE_THRESH + CALIB_CB_NORMALIZE_IMAGE\n *         + CALIB_CB_FAST_CHECK);\n *\n * if(patternfound)\n *   cornerSubPix(gray, corners, Size(11, 11), Size(-1, -1),\n *     TermCriteria(CV_TERMCRIT_EPS + CV_TERMCRIT_ITER, 30, 0.1));\n *\n * drawChessboardCorners(img, patternsize, Mat(corners), patternfound);\n * ```\n *\n * The function requires white space (like a square-thick border, the wider the better) around the\n * board to make the detection more robust in various environments. Otherwise, if there is no border\n * and the background is dark, the outer black squares cannot be segmented properly and so the square\n * grouping and ordering algorithm fails.\n *\n * @param image Source chessboard view. It must be an 8-bit grayscale or color image.\n *\n * @param patternSize Number of inner corners per a chessboard row and column ( patternSize =\n * cv::Size(points_per_row,points_per_colum) = cv::Size(columns,rows) ).\n *\n * @param corners Output array of detected corners.\n *\n * @param flags Various operation flags that can be zero or a combination of the following values:\n * CALIB_CB_ADAPTIVE_THRESH Use adaptive thresholding to convert the image to black and white, rather\n * than a fixed threshold level (computed from the average image brightness).CALIB_CB_NORMALIZE_IMAGE\n * Normalize the image gamma with equalizeHist before applying fixed or adaptive\n * thresholding.CALIB_CB_FILTER_QUADS Use additional criteria (like contour area, perimeter,\n * square-like shape) to filter out false quads extracted at the contour retrieval\n * stage.CALIB_CB_FAST_CHECK Run a fast check on the image that looks for chessboard corners, and\n * shortcut the call if none is found. This can drastically speed up the call in the degenerate\n * condition when no chessboard is observed.\n */\nexport declare function findChessboardCorners(image: InputArray, patternSize: Size, corners: OutputArray, flags?: int): bool;\n/**\n * The function is analog to findchessboardCorners but uses a localized radon transformation\n * approximated by box filters being more robust to all sort of noise, faster on larger images and is\n * able to directly return the sub-pixel position of the internal chessboard corners. The Method is\n * based on the paper duda2018 \"Accurate Detection and Localization of Checkerboard Corners for\n * Calibration\" demonstrating that the returned sub-pixel positions are more accurate than the one\n * returned by cornerSubPix allowing a precise camera calibration for demanding applications.\n *\n * The function requires a white boarder with roughly the same width as one of the checkerboard fields\n * around the whole board to improve the detection in various environments. In addition, because of the\n * localized radon transformation it is beneficial to use round corners for the field corners which are\n * located on the outside of the board. The following figure illustrates a sample checkerboard\n * optimized for the detection. However, any other checkerboard can be used as well.\n *\n * @param image Source chessboard view. It must be an 8-bit grayscale or color image.\n *\n * @param patternSize Number of inner corners per a chessboard row and column ( patternSize =\n * cv::Size(points_per_row,points_per_colum) = cv::Size(columns,rows) ).\n *\n * @param corners Output array of detected corners.\n *\n * @param flags Various operation flags that can be zero or a combination of the following values:\n * CALIB_CB_NORMALIZE_IMAGE Normalize the image gamma with equalizeHist before\n * detection.CALIB_CB_EXHAUSTIVE Run an exhaustive search to improve detection rate.CALIB_CB_ACCURACY\n * Up sample input image to improve sub-pixel accuracy due to aliasing effects. This should be used if\n * an accurate camera calibration is required.\n */\nexport declare function findChessboardCornersSB(image: InputArray, patternSize: Size, corners: OutputArray, flags?: int): bool;\n/**\n * The function attempts to determine whether the input image contains a grid of circles. If it is, the\n * function locates centers of the circles. The function returns a non-zero value if all of the centers\n * have been found and they have been placed in a certain order (row by row, left to right in every\n * row). Otherwise, if the function fails to find all the corners or reorder them, it returns 0.\n *\n * Sample usage of detecting and drawing the centers of circles: :\n *\n * ```cpp\n * Size patternsize(7,7); //number of centers\n * Mat gray = ....; //source image\n * vector<Point2f> centers; //this will be filled by the detected centers\n *\n * bool patternfound = findCirclesGrid(gray, patternsize, centers);\n *\n * drawChessboardCorners(img, patternsize, Mat(centers), patternfound);\n * ```\n *\n * The function requires white space (like a square-thick border, the wider the better) around the\n * board to make the detection more robust in various environments.\n *\n * @param image grid view of input circles; it must be an 8-bit grayscale or color image.\n *\n * @param patternSize number of circles per row and column ( patternSize = Size(points_per_row,\n * points_per_colum) ).\n *\n * @param centers output array of detected centers.\n *\n * @param flags various operation flags that can be one of the following values:\n * CALIB_CB_SYMMETRIC_GRID uses symmetric pattern of circles.CALIB_CB_ASYMMETRIC_GRID uses asymmetric\n * pattern of circles.CALIB_CB_CLUSTERING uses a special algorithm for grid detection. It is more\n * robust to perspective distortions but much more sensitive to background clutter.\n *\n * @param blobDetector feature detector that finds blobs like dark circles on light background.\n *\n * @param parameters struct for finding circles in a grid pattern.\n */\nexport declare function findCirclesGrid(image: InputArray, patternSize: Size, centers: OutputArray, flags: int, blobDetector: any, parameters: any): bool;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function findCirclesGrid(image: InputArray, patternSize: Size, centers: OutputArray, flags?: int, blobDetector?: any): bool;\n/**\n * This function estimates essential matrix based on the five-point algorithm solver in Nister03 .\n * SteweniusCFS is also a related. The epipolar geometry is described by the following equation:\n *\n * `\\\\[[p_2; 1]^T K^{-T} E K^{-1} [p_1; 1] = 0\\\\]`\n *\n * where `$E$` is an essential matrix, `$p_1$` and `$p_2$` are corresponding points in the first and\n * the second images, respectively. The result of this function may be passed further to\n * decomposeEssentialMat or recoverPose to recover the relative pose between cameras.\n *\n * @param points1 Array of N (N >= 5) 2D points from the first image. The point coordinates should be\n * floating-point (single or double precision).\n *\n * @param points2 Array of the second image points of the same size and format as points1 .\n *\n * @param cameraMatrix Camera matrix $K = \\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}$ . Note\n * that this function assumes that points1 and points2 are feature points from cameras with the same\n * camera matrix.\n *\n * @param method Method for computing an essential matrix.\n * RANSAC for the RANSAC algorithm.LMEDS for the LMedS algorithm.\n *\n * @param prob Parameter used for the RANSAC or LMedS methods only. It specifies a desirable level of\n * confidence (probability) that the estimated matrix is correct.\n *\n * @param threshold Parameter used for RANSAC. It is the maximum distance from a point to an epipolar\n * line in pixels, beyond which the point is considered an outlier and is not used for computing the\n * final fundamental matrix. It can be set to something like 1-3, depending on the accuracy of the\n * point localization, image resolution, and the image noise.\n *\n * @param mask Output array of N elements, every element of which is set to 0 for outliers and to 1 for\n * the other points. The array is computed only in the RANSAC and LMedS methods.\n */\nexport declare function findEssentialMat(points1: InputArray, points2: InputArray, cameraMatrix: InputArray, method?: int, prob?: double, threshold?: double, mask?: OutputArray): Mat;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n * This function differs from the one above that it computes camera matrix from focal length and\n * principal point:\n *\n * `\\\\[K = \\\\begin{bmatrix} f & 0 & x_{pp} \\\\\\\\ 0 & f & y_{pp} \\\\\\\\ 0 & 0 & 1 \\\\end{bmatrix}\\\\]`\n *\n * @param points1 Array of N (N >= 5) 2D points from the first image. The point coordinates should be\n * floating-point (single or double precision).\n *\n * @param points2 Array of the second image points of the same size and format as points1 .\n *\n * @param focal focal length of the camera. Note that this function assumes that points1 and points2\n * are feature points from cameras with same focal length and principal point.\n *\n * @param pp principal point of the camera.\n *\n * @param method Method for computing a fundamental matrix.\n * RANSAC for the RANSAC algorithm.LMEDS for the LMedS algorithm.\n *\n * @param prob Parameter used for the RANSAC or LMedS methods only. It specifies a desirable level of\n * confidence (probability) that the estimated matrix is correct.\n *\n * @param threshold Parameter used for RANSAC. It is the maximum distance from a point to an epipolar\n * line in pixels, beyond which the point is considered an outlier and is not used for computing the\n * final fundamental matrix. It can be set to something like 1-3, depending on the accuracy of the\n * point localization, image resolution, and the image noise.\n *\n * @param mask Output array of N elements, every element of which is set to 0 for outliers and to 1 for\n * the other points. The array is computed only in the RANSAC and LMedS methods.\n */\nexport declare function findEssentialMat(points1: InputArray, points2: InputArray, focal?: double, pp?: Point2d, method?: int, prob?: double, threshold?: double, mask?: OutputArray): Mat;\n/**\n * `\\\\[[p_2; 1]^T F [p_1; 1] = 0\\\\]`\n *\n * where `$F$` is a fundamental matrix, `$p_1$` and `$p_2$` are corresponding points in the first and\n * the second images, respectively.\n *\n * The function calculates the fundamental matrix using one of four methods listed above and returns\n * the found fundamental matrix. Normally just one matrix is found. But in case of the 7-point\n * algorithm, the function may return up to 3 solutions ( `$9 \\\\times 3$` matrix that stores all 3\n * matrices sequentially).\n *\n * The calculated fundamental matrix may be passed further to computeCorrespondEpilines that finds the\n * epipolar lines corresponding to the specified points. It can also be passed to\n * stereoRectifyUncalibrated to compute the rectification transformation. :\n *\n * ```cpp\n * // Example. Estimation of fundamental matrix using the RANSAC algorithm\n * int point_count = 100;\n * vector<Point2f> points1(point_count);\n * vector<Point2f> points2(point_count);\n *\n * // initialize the points here ...\n * for( int i = 0; i < point_count; i++ )\n * {\n *     points1[i] = ...;\n *     points2[i] = ...;\n * }\n *\n * Mat fundamental_matrix =\n *  findFundamentalMat(points1, points2, FM_RANSAC, 3, 0.99);\n * ```\n *\n * @param points1 Array of N points from the first image. The point coordinates should be\n * floating-point (single or double precision).\n *\n * @param points2 Array of the second image points of the same size and format as points1 .\n *\n * @param method Method for computing a fundamental matrix.\n * CV_FM_7POINT for a 7-point algorithm. $N = 7$CV_FM_8POINT for an 8-point algorithm. $N \\ge\n * 8$CV_FM_RANSAC for the RANSAC algorithm. $N \\ge 8$CV_FM_LMEDS for the LMedS algorithm. $N \\ge 8$\n *\n * @param ransacReprojThreshold Parameter used only for RANSAC. It is the maximum distance from a point\n * to an epipolar line in pixels, beyond which the point is considered an outlier and is not used for\n * computing the final fundamental matrix. It can be set to something like 1-3, depending on the\n * accuracy of the point localization, image resolution, and the image noise.\n *\n * @param confidence Parameter used for the RANSAC and LMedS methods only. It specifies a desirable\n * level of confidence (probability) that the estimated matrix is correct.\n *\n * @param mask The epipolar geometry is described by the following equation:\n */\nexport declare function findFundamentalMat(points1: InputArray, points2: InputArray, method?: int, ransacReprojThreshold?: double, confidence?: double, mask?: OutputArray): Mat;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function findFundamentalMat(points1: InputArray, points2: InputArray, mask: OutputArray, method?: int, ransacReprojThreshold?: double, confidence?: double): Mat;\n/**\n * The function finds and returns the perspective transformation `$H$` between the source and the\n * destination planes:\n *\n * `\\\\[s_i \\\\vecthree{x'_i}{y'_i}{1} \\\\sim H \\\\vecthree{x_i}{y_i}{1}\\\\]`\n *\n * so that the back-projection error\n *\n * `\\\\[\\\\sum _i \\\\left ( x'_i- \\\\frac{h_{11} x_i + h_{12} y_i + h_{13}}{h_{31} x_i + h_{32} y_i +\n * h_{33}} \\\\right )^2+ \\\\left ( y'_i- \\\\frac{h_{21} x_i + h_{22} y_i + h_{23}}{h_{31} x_i + h_{32} y_i\n * + h_{33}} \\\\right )^2\\\\]`\n *\n * is minimized. If the parameter method is set to the default value 0, the function uses all the point\n * pairs to compute an initial homography estimate with a simple least-squares scheme.\n *\n * However, if not all of the point pairs ( `$srcPoints_i$`, `$dstPoints_i$` ) fit the rigid\n * perspective transformation (that is, there are some outliers), this initial estimate will be poor.\n * In this case, you can use one of the three robust methods. The methods RANSAC, LMeDS and RHO try\n * many different random subsets of the corresponding point pairs (of four pairs each, collinear pairs\n * are discarded), estimate the homography matrix using this subset and a simple least-squares\n * algorithm, and then compute the quality/goodness of the computed homography (which is the number of\n * inliers for RANSAC or the least median re-projection error for LMeDS). The best subset is then used\n * to produce the initial estimate of the homography matrix and the mask of inliers/outliers.\n *\n * Regardless of the method, robust or not, the computed homography matrix is refined further (using\n * inliers only in case of a robust method) with the Levenberg-Marquardt method to reduce the\n * re-projection error even more.\n *\n * The methods RANSAC and RHO can handle practically any ratio of outliers but need a threshold to\n * distinguish inliers from outliers. The method LMeDS does not need any threshold but it works\n * correctly only when there are more than 50% of inliers. Finally, if there are no outliers and the\n * noise is rather small, use the default method (method=0).\n *\n * The function is used to find initial intrinsic and extrinsic matrices. Homography matrix is\n * determined up to a scale. Thus, it is normalized so that `$h_{33}=1$`. Note that whenever an `$H$`\n * matrix cannot be estimated, an empty one will be returned.\n *\n * [getAffineTransform], [estimateAffine2D], [estimateAffinePartial2D], [getPerspectiveTransform],\n * [warpPerspective], [perspectiveTransform]\n *\n * @param srcPoints Coordinates of the points in the original plane, a matrix of the type CV_32FC2 or\n * vector<Point2f> .\n *\n * @param dstPoints Coordinates of the points in the target plane, a matrix of the type CV_32FC2 or a\n * vector<Point2f> .\n *\n * @param method Method used to compute a homography matrix. The following methods are possible:\n * 0 - a regular method using all the points, i.e., the least squares methodRANSAC - RANSAC-based\n * robust methodLMEDS - Least-Median robust methodRHO - PROSAC-based robust method\n *\n * @param ransacReprojThreshold Maximum allowed reprojection error to treat a point pair as an inlier\n * (used in the RANSAC and RHO methods only). That is, if \\[\\| \\texttt{dstPoints} _i -\n * \\texttt{convertPointsHomogeneous} ( \\texttt{H} * \\texttt{srcPoints} _i) \\|_2 >\n * \\texttt{ransacReprojThreshold}\\] then the point $i$ is considered as an outlier. If srcPoints and\n * dstPoints are measured in pixels, it usually makes sense to set this parameter somewhere in the\n * range of 1 to 10.\n *\n * @param mask Optional output mask set by a robust method ( RANSAC or LMEDS ). Note that the input\n * mask values are ignored.\n *\n * @param maxIters The maximum number of RANSAC iterations.\n *\n * @param confidence Confidence level, between 0 and 1.\n */\nexport declare function findHomography(srcPoints: InputArray, dstPoints: InputArray, method?: int, ransacReprojThreshold?: double, mask?: OutputArray, maxIters?: any, confidence?: any): Mat;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function findHomography(srcPoints: InputArray, dstPoints: InputArray, mask: OutputArray, method?: int, ransacReprojThreshold?: double): Mat;\n/**\n * The function returns the camera matrix that is either an exact copy of the input cameraMatrix (when\n * centerPrinicipalPoint=false ), or the modified one (when centerPrincipalPoint=true).\n *\n * In the latter case, the new camera matrix will be:\n *\n * `\\\\[\\\\begin{bmatrix} f_x && 0 && ( \\\\texttt{imgSize.width} -1)*0.5 \\\\\\\\ 0 && f_y && (\n * \\\\texttt{imgSize.height} -1)*0.5 \\\\\\\\ 0 && 0 && 1 \\\\end{bmatrix} ,\\\\]`\n *\n * where `$f_x$` and `$f_y$` are `$(0,0)$` and `$(1,1)$` elements of cameraMatrix, respectively.\n *\n * By default, the undistortion functions in OpenCV (see [initUndistortRectifyMap], [undistort]) do not\n * move the principal point. However, when you work with stereo, it is important to move the principal\n * points in both views to the same y-coordinate (which is required by most of stereo correspondence\n * algorithms), and may be to the same x-coordinate too. So, you can form the new camera matrix for\n * each view where the principal points are located at the center.\n *\n * @param cameraMatrix Input camera matrix.\n *\n * @param imgsize Camera view image size in pixels.\n *\n * @param centerPrincipalPoint Location of the principal point in the new camera matrix. The parameter\n * indicates whether this location should be at the image center or not.\n */\nexport declare function getDefaultNewCameraMatrix(cameraMatrix: InputArray, imgsize?: Size, centerPrincipalPoint?: bool): Mat;\n/**\n * new_camera_matrix Output new camera matrix.\n * The function computes and returns the optimal new camera matrix based on the free scaling parameter.\n * By varying this parameter, you may retrieve only sensible pixels alpha=0 , keep all the original\n * image pixels if there is valuable information in the corners alpha=1 , or get something in between.\n * When alpha>0 , the undistorted result is likely to have some black pixels corresponding to \"virtual\"\n * pixels outside of the captured distorted image. The original camera matrix, distortion coefficients,\n * the computed new camera matrix, and newImageSize should be passed to initUndistortRectifyMap to\n * produce the maps for remap .\n *\n * @param cameraMatrix Input camera matrix.\n *\n * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6\n * [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is\n * NULL/empty, the zero distortion coefficients are assumed.\n *\n * @param imageSize Original image size.\n *\n * @param alpha Free scaling parameter between 0 (when all the pixels in the undistorted image are\n * valid) and 1 (when all the source image pixels are retained in the undistorted image). See\n * stereoRectify for details.\n *\n * @param newImgSize Image size after rectification. By default, it is set to imageSize .\n *\n * @param validPixROI Optional output rectangle that outlines all-good-pixels region in the undistorted\n * image. See roi1, roi2 description in stereoRectify .\n *\n * @param centerPrincipalPoint Optional flag that indicates whether in the new camera matrix the\n * principal point should be at the image center or not. By default, the principal point is chosen to\n * best fit a subset of the source image (determined by alpha) to the corrected image.\n */\nexport declare function getOptimalNewCameraMatrix(cameraMatrix: InputArray, distCoeffs: InputArray, imageSize: Size, alpha: double, newImgSize?: Size, validPixROI?: any, centerPrincipalPoint?: bool): Mat;\nexport declare function getValidDisparityROI(roi1: Rect, roi2: Rect, minDisparity: int, numberOfDisparities: int, SADWindowSize: int): Rect;\n/**\n * The function estimates and returns an initial camera matrix for the camera calibration process.\n * Currently, the function only supports planar calibration patterns, which are patterns where each\n * object point has z-coordinate =0.\n *\n * @param objectPoints Vector of vectors of the calibration pattern points in the calibration pattern\n * coordinate space. In the old interface all the per-view vectors are concatenated. See\n * calibrateCamera for details.\n *\n * @param imagePoints Vector of vectors of the projections of the calibration pattern points. In the\n * old interface all the per-view vectors are concatenated.\n *\n * @param imageSize Image size in pixels used to initialize the principal point.\n *\n * @param aspectRatio If it is zero or negative, both $f_x$ and $f_y$ are estimated independently.\n * Otherwise, $f_x = f_y * \\texttt{aspectRatio}$ .\n */\nexport declare function initCameraMatrix2D(objectPoints: InputArrayOfArrays, imagePoints: InputArrayOfArrays, imageSize: Size, aspectRatio?: double): Mat;\n/**\n * The function computes the joint undistortion and rectification transformation and represents the\n * result in the form of maps for remap. The undistorted image looks like original, as if it is\n * captured with a camera using the camera matrix =newCameraMatrix and zero distortion. In case of a\n * monocular camera, newCameraMatrix is usually equal to cameraMatrix, or it can be computed by\n * [getOptimalNewCameraMatrix] for a better control over scaling. In case of a stereo camera,\n * newCameraMatrix is normally set to P1 or P2 computed by [stereoRectify] .\n *\n * Also, this new camera is oriented differently in the coordinate space, according to R. That, for\n * example, helps to align two heads of a stereo camera so that the epipolar lines on both images\n * become horizontal and have the same y- coordinate (in case of a horizontally aligned stereo camera).\n *\n * The function actually builds the maps for the inverse mapping algorithm that is used by remap. That\n * is, for each pixel `$(u, v)$` in the destination (corrected and rectified) image, the function\n * computes the corresponding coordinates in the source image (that is, in the original image from\n * camera). The following process is applied: `\\\\[ \\\\begin{array}{l} x \\\\leftarrow (u - {c'}_x)/{f'}_x\n * \\\\\\\\ y \\\\leftarrow (v - {c'}_y)/{f'}_y \\\\\\\\ {[X\\\\,Y\\\\,W]} ^T \\\\leftarrow R^{-1}*[x \\\\, y \\\\, 1]^T\n * \\\\\\\\ x' \\\\leftarrow X/W \\\\\\\\ y' \\\\leftarrow Y/W \\\\\\\\ r^2 \\\\leftarrow x'^2 + y'^2 \\\\\\\\ x''\n * \\\\leftarrow x' \\\\frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + 2p_1 x' y'\n * + p_2(r^2 + 2 x'^2) + s_1 r^2 + s_2 r^4\\\\\\\\ y'' \\\\leftarrow y' \\\\frac{1 + k_1 r^2 + k_2 r^4 + k_3\n * r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + p_1 (r^2 + 2 y'^2) + 2 p_2 x' y' + s_3 r^2 + s_4 r^4 \\\\\\\\\n * s\\\\vecthree{x'''}{y'''}{1} = \\\\vecthreethree{R_{33}(\\\\tau_x, \\\\tau_y)}{0}{-R_{13}((\\\\tau_x,\n * \\\\tau_y)} {0}{R_{33}(\\\\tau_x, \\\\tau_y)}{-R_{23}(\\\\tau_x, \\\\tau_y)} {0}{0}{1} R(\\\\tau_x, \\\\tau_y)\n * \\\\vecthree{x''}{y''}{1}\\\\\\\\ map_x(u,v) \\\\leftarrow x''' f_x + c_x \\\\\\\\ map_y(u,v) \\\\leftarrow y'''\n * f_y + c_y \\\\end{array} \\\\]` where `$(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6[, s_1, s_2, s_3, s_4[,\n * \\\\tau_x, \\\\tau_y]]]])$` are the distortion coefficients.\n *\n * In case of a stereo camera, this function is called twice: once for each camera head, after\n * stereoRectify, which in its turn is called after [stereoCalibrate]. But if the stereo camera was not\n * calibrated, it is still possible to compute the rectification transformations directly from the\n * fundamental matrix using [stereoRectifyUncalibrated]. For each camera, the function computes\n * homography H as the rectification transformation in a pixel domain, not a rotation matrix R in 3D\n * space. R can be computed from H as `\\\\[\\\\texttt{R} = \\\\texttt{cameraMatrix} ^{-1} \\\\cdot \\\\texttt{H}\n * \\\\cdot \\\\texttt{cameraMatrix}\\\\]` where cameraMatrix can be chosen arbitrarily.\n *\n * @param cameraMatrix Input camera matrix $A=\\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}$ .\n *\n * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5,\n * k_6[, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is\n * NULL/empty, the zero distortion coefficients are assumed.\n *\n * @param R Optional rectification transformation in the object space (3x3 matrix). R1 or R2 , computed\n * by stereoRectify can be passed here. If the matrix is empty, the identity transformation is assumed.\n * In cvInitUndistortMap R assumed to be an identity matrix.\n *\n * @param newCameraMatrix New camera matrix $A'=\\vecthreethree{f_x'}{0}{c_x'}{0}{f_y'}{c_y'}{0}{0}{1}$.\n *\n * @param size Undistorted image size.\n *\n * @param m1type Type of the first output map that can be CV_32FC1, CV_32FC2 or CV_16SC2, see\n * convertMaps\n *\n * @param map1 The first output map.\n *\n * @param map2 The second output map.\n */\nexport declare function initUndistortRectifyMap(cameraMatrix: InputArray, distCoeffs: InputArray, R: InputArray, newCameraMatrix: InputArray, size: Size, m1type: int, map1: OutputArray, map2: OutputArray): void;\nexport declare function initWideAngleProjMap(cameraMatrix: InputArray, distCoeffs: InputArray, imageSize: Size, destImageWidth: int, m1type: int, map1: OutputArray, map2: OutputArray, projType?: any, alpha?: double): float;\nexport declare function initWideAngleProjMap(cameraMatrix: InputArray, distCoeffs: InputArray, imageSize: Size, destImageWidth: int, m1type: int, map1: OutputArray, map2: OutputArray, projType: int, alpha?: double): float;\n/**\n * The function computes partial derivatives of the elements of the matrix product `$A*B$` with regard\n * to the elements of each of the two input matrices. The function is used to compute the Jacobian\n * matrices in stereoCalibrate but can also be used in any other similar optimization function.\n *\n * @param A First multiplied matrix.\n *\n * @param B Second multiplied matrix.\n *\n * @param dABdA First output derivative matrix d(A*B)/dA of size $\\texttt{A.rows*B.cols} \\times\n * {A.rows*A.cols}$ .\n *\n * @param dABdB Second output derivative matrix d(A*B)/dB of size $\\texttt{A.rows*B.cols} \\times\n * {B.rows*B.cols}$ .\n */\nexport declare function matMulDeriv(A: InputArray, B: InputArray, dABdA: OutputArray, dABdB: OutputArray): void;\n/**\n * The function computes projections of 3D points to the image plane given intrinsic and extrinsic\n * camera parameters. Optionally, the function computes Jacobians - matrices of partial derivatives of\n * image points coordinates (as functions of all the input parameters) with respect to the particular\n * parameters, intrinsic and/or extrinsic. The Jacobians are used during the global optimization in\n * calibrateCamera, solvePnP, and stereoCalibrate . The function itself can also be used to compute a\n * re-projection error given the current intrinsic and extrinsic parameters.\n *\n * By setting rvec=tvec=(0,0,0) or by setting cameraMatrix to a 3x3 identity matrix, or by passing zero\n * distortion coefficients, you can get various useful partial cases of the function. This means that\n * you can compute the distorted coordinates for a sparse set of points or apply a perspective\n * transformation (and also compute the derivatives) in the ideal zero-distortion setup.\n *\n * @param objectPoints Array of object points, 3xN/Nx3 1-channel or 1xN/Nx1 3-channel (or\n * vector<Point3f> ), where N is the number of points in the view.\n *\n * @param rvec Rotation vector. See Rodrigues for details.\n *\n * @param tvec Translation vector.\n *\n * @param cameraMatrix Camera matrix $A = \\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{_1}$ .\n *\n * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6\n * [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is empty,\n * the zero distortion coefficients are assumed.\n *\n * @param imagePoints Output array of image points, 1xN/Nx1 2-channel, or vector<Point2f> .\n *\n * @param jacobian Optional output 2Nx(10+<numDistCoeffs>) jacobian matrix of derivatives of image\n * points with respect to components of the rotation vector, translation vector, focal lengths,\n * coordinates of the principal point and the distortion coefficients. In the old interface different\n * components of the jacobian are returned via different output parameters.\n *\n * @param aspectRatio Optional \"fixed aspect ratio\" parameter. If the parameter is not 0, the function\n * assumes that the aspect ratio (fx/fy) is fixed and correspondingly adjusts the jacobian matrix.\n */\nexport declare function projectPoints(objectPoints: InputArray, rvec: InputArray, tvec: InputArray, cameraMatrix: InputArray, distCoeffs: InputArray, imagePoints: OutputArray, jacobian?: OutputArray, aspectRatio?: double): void;\n/**\n * This function can be used to process output E and mask from findEssentialMat. In this scenario,\n * points1 and points2 are the same input for findEssentialMat. :\n *\n * ```cpp\n * // Example. Estimation of fundamental matrix using the RANSAC algorithm\n * int point_count = 100;\n * vector<Point2f> points1(point_count);\n * vector<Point2f> points2(point_count);\n *\n * // initialize the points here ...\n * for( int i = 0; i < point_count; i++ )\n * {\n *     points1[i] = ...;\n *     points2[i] = ...;\n * }\n *\n * // cametra matrix with both focal lengths = 1, and principal point = (0, 0)\n * Mat cameraMatrix = Mat::eye(3, 3, CV_64F);\n *\n * Mat E, R, t, mask;\n *\n * E = findEssentialMat(points1, points2, cameraMatrix, RANSAC, 0.999, 1.0, mask);\n * recoverPose(E, points1, points2, cameraMatrix, R, t, mask);\n * ```\n *\n * @param E The input essential matrix.\n *\n * @param points1 Array of N 2D points from the first image. The point coordinates should be\n * floating-point (single or double precision).\n *\n * @param points2 Array of the second image points of the same size and format as points1 .\n *\n * @param cameraMatrix Camera matrix $K = \\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}$ . Note\n * that this function assumes that points1 and points2 are feature points from cameras with the same\n * camera matrix.\n *\n * @param R Recovered relative rotation.\n *\n * @param t Recovered relative translation.\n *\n * @param mask Input/output mask for inliers in points1 and points2. : If it is not empty, then it\n * marks inliers in points1 and points2 for then given essential matrix E. Only these inliers will be\n * used to recover pose. In the output mask only inliers which pass the cheirality check. This function\n * decomposes an essential matrix using decomposeEssentialMat and then verifies possible pose\n * hypotheses by doing cheirality check. The cheirality check basically means that the triangulated 3D\n * points should have positive depth. Some details can be found in Nister03 .\n */\nexport declare function recoverPose(E: InputArray, points1: InputArray, points2: InputArray, cameraMatrix: InputArray, R: OutputArray, t: OutputArray, mask?: InputOutputArray): int;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n * This function differs from the one above that it computes camera matrix from focal length and\n * principal point:\n *\n * `\\\\[K = \\\\begin{bmatrix} f & 0 & x_{pp} \\\\\\\\ 0 & f & y_{pp} \\\\\\\\ 0 & 0 & 1 \\\\end{bmatrix}\\\\]`\n *\n * @param E The input essential matrix.\n *\n * @param points1 Array of N 2D points from the first image. The point coordinates should be\n * floating-point (single or double precision).\n *\n * @param points2 Array of the second image points of the same size and format as points1 .\n *\n * @param R Recovered relative rotation.\n *\n * @param t Recovered relative translation.\n *\n * @param focal Focal length of the camera. Note that this function assumes that points1 and points2\n * are feature points from cameras with same focal length and principal point.\n *\n * @param pp principal point of the camera.\n *\n * @param mask Input/output mask for inliers in points1 and points2. : If it is not empty, then it\n * marks inliers in points1 and points2 for then given essential matrix E. Only these inliers will be\n * used to recover pose. In the output mask only inliers which pass the cheirality check.\n */\nexport declare function recoverPose(E: InputArray, points1: InputArray, points2: InputArray, R: OutputArray, t: OutputArray, focal?: double, pp?: Point2d, mask?: InputOutputArray): int;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n *\n * @param E The input essential matrix.\n *\n * @param points1 Array of N 2D points from the first image. The point coordinates should be\n * floating-point (single or double precision).\n *\n * @param points2 Array of the second image points of the same size and format as points1.\n *\n * @param cameraMatrix Camera matrix $K = \\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}$ . Note\n * that this function assumes that points1 and points2 are feature points from cameras with the same\n * camera matrix.\n *\n * @param R Recovered relative rotation.\n *\n * @param t Recovered relative translation.\n *\n * @param distanceThresh threshold distance which is used to filter out far away points (i.e. infinite\n * points).\n *\n * @param mask Input/output mask for inliers in points1 and points2. : If it is not empty, then it\n * marks inliers in points1 and points2 for then given essential matrix E. Only these inliers will be\n * used to recover pose. In the output mask only inliers which pass the cheirality check.\n *\n * @param triangulatedPoints 3d points which were reconstructed by triangulation.\n */\nexport declare function recoverPose(E: InputArray, points1: InputArray, points2: InputArray, cameraMatrix: InputArray, R: OutputArray, t: OutputArray, distanceThresh: double, mask?: InputOutputArray, triangulatedPoints?: OutputArray): int;\nexport declare function rectify3Collinear(cameraMatrix1: InputArray, distCoeffs1: InputArray, cameraMatrix2: InputArray, distCoeffs2: InputArray, cameraMatrix3: InputArray, distCoeffs3: InputArray, imgpt1: InputArrayOfArrays, imgpt3: InputArrayOfArrays, imageSize: Size, R12: InputArray, T12: InputArray, R13: InputArray, T13: InputArray, R1: OutputArray, R2: OutputArray, R3: OutputArray, P1: OutputArray, P2: OutputArray, P3: OutputArray, Q: OutputArray, alpha: double, newImgSize: Size, roi1: any, roi2: any, flags: int): float;\n/**\n * The function transforms a single-channel disparity map to a 3-channel image representing a 3D\n * surface. That is, for each pixel (x,y) and the corresponding disparity d=disparity(x,y) , it\n * computes:\n *\n * `\\\\[\\\\begin{array}{l} [X \\\\; Y \\\\; Z \\\\; W]^T = \\\\texttt{Q} *[x \\\\; y \\\\; \\\\texttt{disparity} (x,y)\n * \\\\; 1]^T \\\\\\\\ \\\\texttt{\\\\_3dImage} (x,y) = (X/W, \\\\; Y/W, \\\\; Z/W) \\\\end{array}\\\\]`\n *\n * The matrix Q can be an arbitrary `$4 \\\\times 4$` matrix (for example, the one computed by\n * stereoRectify). To reproject a sparse set of points {(x,y,d),...} to 3D space, use\n * perspectiveTransform .\n *\n * @param disparity Input single-channel 8-bit unsigned, 16-bit signed, 32-bit signed or 32-bit\n * floating-point disparity image. If 16-bit signed format is used, the values are assumed to have no\n * fractional bits.\n *\n * @param _3dImage Output 3-channel floating-point image of the same size as disparity . Each element\n * of _3dImage(x,y) contains 3D coordinates of the point (x,y) computed from the disparity map.\n *\n * @param Q $4 \\times 4$ perspective transformation matrix that can be obtained with stereoRectify.\n *\n * @param handleMissingValues Indicates, whether the function should handle missing values (i.e. points\n * where the disparity was not computed). If handleMissingValues=true, then pixels with the minimal\n * disparity that corresponds to the outliers (see StereoMatcher::compute ) are transformed to 3D\n * points with a very large Z value (currently set to 10000).\n *\n * @param ddepth The optional output array depth. If it is -1, the output image will have CV_32F depth.\n * ddepth can also be set to CV_16S, CV_32S or CV_32F.\n */\nexport declare function reprojectImageTo3D(disparity: InputArray, _3dImage: OutputArray, Q: InputArray, handleMissingValues?: bool, ddepth?: int): void;\n/**\n * `\\\\[\\\\begin{array}{l} \\\\theta \\\\leftarrow norm(r) \\\\\\\\ r \\\\leftarrow r/ \\\\theta \\\\\\\\ R =\n * \\\\cos{\\\\theta} I + (1- \\\\cos{\\\\theta} ) r r^T + \\\\sin{\\\\theta}\n * \\\\vecthreethree{0}{-r_z}{r_y}{r_z}{0}{-r_x}{-r_y}{r_x}{0} \\\\end{array}\\\\]`\n *\n * Inverse transformation can be also done easily, since\n *\n * `\\\\[\\\\sin ( \\\\theta ) \\\\vecthreethree{0}{-r_z}{r_y}{r_z}{0}{-r_x}{-r_y}{r_x}{0} = \\\\frac{R -\n * R^T}{2}\\\\]`\n *\n * A rotation vector is a convenient and most compact representation of a rotation matrix (since any\n * rotation matrix has just 3 degrees of freedom). The representation is used in the global 3D geometry\n * optimization procedures like calibrateCamera, stereoCalibrate, or solvePnP .\n *\n * @param src Input rotation vector (3x1 or 1x3) or rotation matrix (3x3).\n *\n * @param dst Output rotation matrix (3x3) or rotation vector (3x1 or 1x3), respectively.\n *\n * @param jacobian Optional output Jacobian matrix, 3x9 or 9x3, which is a matrix of partial\n * derivatives of the output array components with respect to the input array components.\n */\nexport declare function Rodrigues(src: InputArray, dst: OutputArray, jacobian?: OutputArray): void;\n/**\n * The function computes a RQ decomposition using the given rotations. This function is used in\n * decomposeProjectionMatrix to decompose the left 3x3 submatrix of a projection matrix into a camera\n * and a rotation matrix.\n *\n * It optionally returns three rotation matrices, one for each axis, and the three Euler angles in\n * degrees (as the return value) that could be used in OpenGL. Note, there is always more than one\n * sequence of rotations about the three principal axes that results in the same orientation of an\n * object, e.g. see Slabaugh . Returned tree rotation matrices and corresponding three Euler angles are\n * only one of the possible solutions.\n *\n * @param src 3x3 input matrix.\n *\n * @param mtxR Output 3x3 upper-triangular matrix.\n *\n * @param mtxQ Output 3x3 orthogonal matrix.\n *\n * @param Qx Optional output 3x3 rotation matrix around x-axis.\n *\n * @param Qy Optional output 3x3 rotation matrix around y-axis.\n *\n * @param Qz Optional output 3x3 rotation matrix around z-axis.\n */\nexport declare function RQDecomp3x3(src: InputArray, mtxR: OutputArray, mtxQ: OutputArray, Qx?: OutputArray, Qy?: OutputArray, Qz?: OutputArray): Vec3d;\n/**\n * The function [cv::sampsonDistance] calculates and returns the first order approximation of the\n * geometric error as: `\\\\[ sd( \\\\texttt{pt1} , \\\\texttt{pt2} )= \\\\frac{(\\\\texttt{pt2}^t \\\\cdot\n * \\\\texttt{F} \\\\cdot \\\\texttt{pt1})^2} {((\\\\texttt{F} \\\\cdot \\\\texttt{pt1})(0))^2 + ((\\\\texttt{F}\n * \\\\cdot \\\\texttt{pt1})(1))^2 + ((\\\\texttt{F}^t \\\\cdot \\\\texttt{pt2})(0))^2 + ((\\\\texttt{F}^t \\\\cdot\n * \\\\texttt{pt2})(1))^2} \\\\]` The fundamental matrix may be calculated using the\n * [cv::findFundamentalMat] function. See HartleyZ00 11.4.3 for details.\n *\n * The computed Sampson distance.\n *\n * @param pt1 first homogeneous 2d point\n *\n * @param pt2 second homogeneous 2d point\n *\n * @param F fundamental matrix\n */\nexport declare function sampsonDistance(pt1: InputArray, pt2: InputArray, F: InputArray): double;\n/**\n * The function estimates the object pose given 3 object points, their corresponding image projections,\n * as well as the camera matrix and the distortion coefficients.\n *\n * The solutions are sorted by reprojection errors (lowest to highest).\n *\n * @param objectPoints Array of object points in the object coordinate space, 3x3 1-channel or 1x3/3x1\n * 3-channel. vector<Point3f> can be also passed here.\n *\n * @param imagePoints Array of corresponding image points, 3x2 1-channel or 1x3/3x1 2-channel.\n * vector<Point2f> can be also passed here.\n *\n * @param cameraMatrix Input camera matrix $A = \\vecthreethree{fx}{0}{cx}{0}{fy}{cy}{0}{0}{1}$ .\n *\n * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6\n * [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is\n * NULL/empty, the zero distortion coefficients are assumed.\n *\n * @param rvecs Output rotation vectors (see Rodrigues ) that, together with tvecs, brings points from\n * the model coordinate system to the camera coordinate system. A P3P problem has up to 4 solutions.\n *\n * @param tvecs Output translation vectors.\n *\n * @param flags Method for solving a P3P problem:\n * SOLVEPNP_P3P Method is based on the paper of X.S. Gao, X.-R. Hou, J. Tang, H.-F. Chang \"Complete\n * Solution Classification for the Perspective-Three-Point Problem\" (gao2003complete).SOLVEPNP_AP3P\n * Method is based on the paper of T. Ke and S. Roumeliotis. \"An Efficient Algebraic Solution to the\n * Perspective-Three-Point Problem\" (Ke17).\n */\nexport declare function solveP3P(objectPoints: InputArray, imagePoints: InputArray, cameraMatrix: InputArray, distCoeffs: InputArray, rvecs: OutputArrayOfArrays, tvecs: OutputArrayOfArrays, flags: int): int;\n/**\n * P3P methods ([SOLVEPNP_P3P], [SOLVEPNP_AP3P]): need 4 input points to return a unique solution.\n * [SOLVEPNP_IPPE] Input points must be >= 4 and object points must be coplanar.\n * [SOLVEPNP_IPPE_SQUARE] Special case suitable for marker pose estimation. Number of input points must\n * be 4. Object points must be defined in the following order:\n *\n * point 0: [-squareLength / 2, squareLength / 2, 0]\n * point 1: [ squareLength / 2, squareLength / 2, 0]\n * point 2: [ squareLength / 2, -squareLength / 2, 0]\n * point 3: [-squareLength / 2, -squareLength / 2, 0]\n *\n * for all the other flags, number of input points must be >= 4 and object points can be in any\n * configuration.\n *\n * The function estimates the object pose given a set of object points, their corresponding image\n * projections, as well as the camera matrix and the distortion coefficients, see the figure below\n * (more precisely, the X-axis of the camera frame is pointing to the right, the Y-axis downward and\n * the Z-axis forward).\n *\n * Points expressed in the world frame `$ \\\\bf{X}_w $` are projected into the image plane `$ \\\\left[ u,\n * v \\\\right] $` using the perspective projection model `$ \\\\Pi $` and the camera intrinsic parameters\n * matrix `$ \\\\bf{A} $`:\n *\n * `\\\\[ \\\\begin{align*} \\\\begin{bmatrix} u \\\\\\\\ v \\\\\\\\ 1 \\\\end{bmatrix} &= \\\\bf{A} \\\\hspace{0.1em} \\\\Pi\n * \\\\hspace{0.2em} ^{c}\\\\bf{M}_w \\\\begin{bmatrix} X_{w} \\\\\\\\ Y_{w} \\\\\\\\ Z_{w} \\\\\\\\ 1 \\\\end{bmatrix}\n * \\\\\\\\ \\\\begin{bmatrix} u \\\\\\\\ v \\\\\\\\ 1 \\\\end{bmatrix} &= \\\\begin{bmatrix} f_x & 0 & c_x \\\\\\\\ 0 & f_y\n * & c_y \\\\\\\\ 0 & 0 & 1 \\\\end{bmatrix} \\\\begin{bmatrix} 1 & 0 & 0 & 0 \\\\\\\\ 0 & 1 & 0 & 0 \\\\\\\\ 0 & 0 & 1\n * & 0 \\\\end{bmatrix} \\\\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\\\\\ r_{21} & r_{22} & r_{23} &\n * t_y \\\\\\\\ r_{31} & r_{32} & r_{33} & t_z \\\\\\\\ 0 & 0 & 0 & 1 \\\\end{bmatrix} \\\\begin{bmatrix} X_{w}\n * \\\\\\\\ Y_{w} \\\\\\\\ Z_{w} \\\\\\\\ 1 \\\\end{bmatrix} \\\\end{align*} \\\\]`\n *\n * The estimated pose is thus the rotation (`rvec`) and the translation (`tvec`) vectors that allow\n * transforming a 3D point expressed in the world frame into the camera frame:\n *\n * `\\\\[ \\\\begin{align*} \\\\begin{bmatrix} X_c \\\\\\\\ Y_c \\\\\\\\ Z_c \\\\\\\\ 1 \\\\end{bmatrix} &= \\\\hspace{0.2em}\n * ^{c}\\\\bf{M}_w \\\\begin{bmatrix} X_{w} \\\\\\\\ Y_{w} \\\\\\\\ Z_{w} \\\\\\\\ 1 \\\\end{bmatrix} \\\\\\\\\n * \\\\begin{bmatrix} X_c \\\\\\\\ Y_c \\\\\\\\ Z_c \\\\\\\\ 1 \\\\end{bmatrix} &= \\\\begin{bmatrix} r_{11} & r_{12} &\n * r_{13} & t_x \\\\\\\\ r_{21} & r_{22} & r_{23} & t_y \\\\\\\\ r_{31} & r_{32} & r_{33} & t_z \\\\\\\\ 0 & 0 & 0\n * & 1 \\\\end{bmatrix} \\\\begin{bmatrix} X_{w} \\\\\\\\ Y_{w} \\\\\\\\ Z_{w} \\\\\\\\ 1 \\\\end{bmatrix} \\\\end{align*}\n * \\\\]`\n *\n * An example of how to use solvePnP for planar augmented reality can be found at\n * opencv_source_code/samples/python/plane_ar.py\n * If you are using Python:\n *\n * Numpy array slices won't work as input because solvePnP requires contiguous arrays (enforced by the\n * assertion using [cv::Mat::checkVector()] around line 55 of modules/calib3d/src/solvepnp.cpp version\n * 2.4.9)\n * The P3P algorithm requires image points to be in an array of shape (N,1,2) due to its calling of\n * [cv::undistortPoints] (around line 75 of modules/calib3d/src/solvepnp.cpp version 2.4.9) which\n * requires 2-channel information.\n * Thus, given some data D = np.array(...) where D.shape = (N,M), in order to use a subset of it as,\n * e.g., imagePoints, one must effectively copy it into a new array: imagePoints =\n * np.ascontiguousarray(D[:,:2]).reshape((N,1,2))\n *\n * The methods **SOLVEPNP_DLS** and **SOLVEPNP_UPNP** cannot be used as the current implementations are\n * unstable and sometimes give completely wrong results. If you pass one of these two flags,\n * **SOLVEPNP_EPNP** method will be used instead.\n * The minimum number of points is 4 in the general case. In the case of **SOLVEPNP_P3P** and\n * **SOLVEPNP_AP3P** methods, it is required to use exactly 4 points (the first 3 points are used to\n * estimate all the solutions of the P3P problem, the last one is used to retain the best solution that\n * minimizes the reprojection error).\n * With **SOLVEPNP_ITERATIVE** method and `useExtrinsicGuess=true`, the minimum number of points is 3\n * (3 points are sufficient to compute a pose but there are up to 4 solutions). The initial solution\n * should be close to the global solution to converge.\n * With **SOLVEPNP_IPPE** input points must be >= 4 and object points must be coplanar.\n * With **SOLVEPNP_IPPE_SQUARE** this is a special case suitable for marker pose estimation. Number of\n * input points must be 4. Object points must be defined in the following order:\n *\n * point 0: [-squareLength / 2, squareLength / 2, 0]\n * point 1: [ squareLength / 2, squareLength / 2, 0]\n * point 2: [ squareLength / 2, -squareLength / 2, 0]\n * point 3: [-squareLength / 2, -squareLength / 2, 0]\n *\n * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or 1xN/Nx1\n * 3-channel, where N is the number of points. vector<Point3f> can be also passed here.\n *\n * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel, where N\n * is the number of points. vector<Point2f> can be also passed here.\n *\n * @param cameraMatrix Input camera matrix $A = \\vecthreethree{fx}{0}{cx}{0}{fy}{cy}{0}{0}{1}$ .\n *\n * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6\n * [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is\n * NULL/empty, the zero distortion coefficients are assumed.\n *\n * @param rvec Output rotation vector (see Rodrigues ) that, together with tvec, brings points from the\n * model coordinate system to the camera coordinate system.\n *\n * @param tvec Output translation vector.\n *\n * @param useExtrinsicGuess Parameter used for SOLVEPNP_ITERATIVE. If true (1), the function uses the\n * provided rvec and tvec values as initial approximations of the rotation and translation vectors,\n * respectively, and further optimizes them.\n *\n * @param flags Method for solving a PnP problem:\n * SOLVEPNP_ITERATIVE Iterative method is based on a Levenberg-Marquardt optimization. In this case the\n * function finds such a pose that minimizes reprojection error, that is the sum of squared distances\n * between the observed projections imagePoints and the projected (using projectPoints ) objectPoints\n * .SOLVEPNP_P3P Method is based on the paper of X.S. Gao, X.-R. Hou, J. Tang, H.-F. Chang \"Complete\n * Solution Classification for the Perspective-Three-Point Problem\" (gao2003complete). In this case the\n * function requires exactly four object and image points.SOLVEPNP_AP3P Method is based on the paper of\n * T. Ke, S. Roumeliotis \"An Efficient Algebraic Solution to the Perspective-Three-Point Problem\"\n * (Ke17). In this case the function requires exactly four object and image points.SOLVEPNP_EPNP Method\n * has been introduced by F. Moreno-Noguer, V. Lepetit and P. Fua in the paper \"EPnP: Efficient\n * Perspective-n-Point Camera Pose Estimation\" (lepetit2009epnp).SOLVEPNP_DLS Method is based on the\n * paper of J. Hesch and S. Roumeliotis. \"A Direct Least-Squares (DLS) Method for PnP\"\n * (hesch2011direct).SOLVEPNP_UPNP Method is based on the paper of A. Penate-Sanchez, J. Andrade-Cetto,\n * F. Moreno-Noguer. \"Exhaustive Linearization for Robust Camera Pose and Focal Length\n * Estimation\" (penate2013exhaustive). In this case the function also estimates the parameters $f_x$\n * and $f_y$ assuming that both have the same value. Then the cameraMatrix is updated with the\n * estimated focal length.SOLVEPNP_IPPE Method is based on the paper of T. Collins and A. Bartoli.\n * \"Infinitesimal Plane-Based Pose Estimation\" (Collins14). This method requires coplanar object\n * points.SOLVEPNP_IPPE_SQUARE Method is based on the paper of Toby Collins and Adrien Bartoli.\n * \"Infinitesimal Plane-Based Pose Estimation\" (Collins14). This method is suitable for marker pose\n * estimation. It requires 4 coplanar object points defined in the following order:\n * point 0: [-squareLength / 2, squareLength / 2, 0]point 1: [ squareLength / 2, squareLength / 2,\n * 0]point 2: [ squareLength / 2, -squareLength / 2, 0]point 3: [-squareLength / 2, -squareLength / 2,\n * 0]\n */\nexport declare function solvePnP(objectPoints: InputArray, imagePoints: InputArray, cameraMatrix: InputArray, distCoeffs: InputArray, rvec: OutputArray, tvec: OutputArray, useExtrinsicGuess?: bool, flags?: int): bool;\n/**\n * P3P methods ([SOLVEPNP_P3P], [SOLVEPNP_AP3P]): 3 or 4 input points. Number of returned solutions can\n * be between 0 and 4 with 3 input points.\n * [SOLVEPNP_IPPE] Input points must be >= 4 and object points must be coplanar. Returns 2 solutions.\n * [SOLVEPNP_IPPE_SQUARE] Special case suitable for marker pose estimation. Number of input points must\n * be 4 and 2 solutions are returned. Object points must be defined in the following order:\n *\n * point 0: [-squareLength / 2, squareLength / 2, 0]\n * point 1: [ squareLength / 2, squareLength / 2, 0]\n * point 2: [ squareLength / 2, -squareLength / 2, 0]\n * point 3: [-squareLength / 2, -squareLength / 2, 0]\n *\n * for all the other flags, number of input points must be >= 4 and object points can be in any\n * configuration. Only 1 solution is returned.\n *\n * The function estimates the object pose given a set of object points, their corresponding image\n * projections, as well as the camera matrix and the distortion coefficients, see the figure below\n * (more precisely, the X-axis of the camera frame is pointing to the right, the Y-axis downward and\n * the Z-axis forward).\n *\n * Points expressed in the world frame `$ \\\\bf{X}_w $` are projected into the image plane `$ \\\\left[ u,\n * v \\\\right] $` using the perspective projection model `$ \\\\Pi $` and the camera intrinsic parameters\n * matrix `$ \\\\bf{A} $`:\n *\n * `\\\\[ \\\\begin{align*} \\\\begin{bmatrix} u \\\\\\\\ v \\\\\\\\ 1 \\\\end{bmatrix} &= \\\\bf{A} \\\\hspace{0.1em} \\\\Pi\n * \\\\hspace{0.2em} ^{c}\\\\bf{M}_w \\\\begin{bmatrix} X_{w} \\\\\\\\ Y_{w} \\\\\\\\ Z_{w} \\\\\\\\ 1 \\\\end{bmatrix}\n * \\\\\\\\ \\\\begin{bmatrix} u \\\\\\\\ v \\\\\\\\ 1 \\\\end{bmatrix} &= \\\\begin{bmatrix} f_x & 0 & c_x \\\\\\\\ 0 & f_y\n * & c_y \\\\\\\\ 0 & 0 & 1 \\\\end{bmatrix} \\\\begin{bmatrix} 1 & 0 & 0 & 0 \\\\\\\\ 0 & 1 & 0 & 0 \\\\\\\\ 0 & 0 & 1\n * & 0 \\\\end{bmatrix} \\\\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\\\\\ r_{21} & r_{22} & r_{23} &\n * t_y \\\\\\\\ r_{31} & r_{32} & r_{33} & t_z \\\\\\\\ 0 & 0 & 0 & 1 \\\\end{bmatrix} \\\\begin{bmatrix} X_{w}\n * \\\\\\\\ Y_{w} \\\\\\\\ Z_{w} \\\\\\\\ 1 \\\\end{bmatrix} \\\\end{align*} \\\\]`\n *\n * The estimated pose is thus the rotation (`rvec`) and the translation (`tvec`) vectors that allow\n * transforming a 3D point expressed in the world frame into the camera frame:\n *\n * `\\\\[ \\\\begin{align*} \\\\begin{bmatrix} X_c \\\\\\\\ Y_c \\\\\\\\ Z_c \\\\\\\\ 1 \\\\end{bmatrix} &= \\\\hspace{0.2em}\n * ^{c}\\\\bf{M}_w \\\\begin{bmatrix} X_{w} \\\\\\\\ Y_{w} \\\\\\\\ Z_{w} \\\\\\\\ 1 \\\\end{bmatrix} \\\\\\\\\n * \\\\begin{bmatrix} X_c \\\\\\\\ Y_c \\\\\\\\ Z_c \\\\\\\\ 1 \\\\end{bmatrix} &= \\\\begin{bmatrix} r_{11} & r_{12} &\n * r_{13} & t_x \\\\\\\\ r_{21} & r_{22} & r_{23} & t_y \\\\\\\\ r_{31} & r_{32} & r_{33} & t_z \\\\\\\\ 0 & 0 & 0\n * & 1 \\\\end{bmatrix} \\\\begin{bmatrix} X_{w} \\\\\\\\ Y_{w} \\\\\\\\ Z_{w} \\\\\\\\ 1 \\\\end{bmatrix} \\\\end{align*}\n * \\\\]`\n *\n * An example of how to use solvePnP for planar augmented reality can be found at\n * opencv_source_code/samples/python/plane_ar.py\n * If you are using Python:\n *\n * Numpy array slices won't work as input because solvePnP requires contiguous arrays (enforced by the\n * assertion using [cv::Mat::checkVector()] around line 55 of modules/calib3d/src/solvepnp.cpp version\n * 2.4.9)\n * The P3P algorithm requires image points to be in an array of shape (N,1,2) due to its calling of\n * [cv::undistortPoints] (around line 75 of modules/calib3d/src/solvepnp.cpp version 2.4.9) which\n * requires 2-channel information.\n * Thus, given some data D = np.array(...) where D.shape = (N,M), in order to use a subset of it as,\n * e.g., imagePoints, one must effectively copy it into a new array: imagePoints =\n * np.ascontiguousarray(D[:,:2]).reshape((N,1,2))\n *\n * The methods **SOLVEPNP_DLS** and **SOLVEPNP_UPNP** cannot be used as the current implementations are\n * unstable and sometimes give completely wrong results. If you pass one of these two flags,\n * **SOLVEPNP_EPNP** method will be used instead.\n * The minimum number of points is 4 in the general case. In the case of **SOLVEPNP_P3P** and\n * **SOLVEPNP_AP3P** methods, it is required to use exactly 4 points (the first 3 points are used to\n * estimate all the solutions of the P3P problem, the last one is used to retain the best solution that\n * minimizes the reprojection error).\n * With **SOLVEPNP_ITERATIVE** method and `useExtrinsicGuess=true`, the minimum number of points is 3\n * (3 points are sufficient to compute a pose but there are up to 4 solutions). The initial solution\n * should be close to the global solution to converge.\n * With **SOLVEPNP_IPPE** input points must be >= 4 and object points must be coplanar.\n * With **SOLVEPNP_IPPE_SQUARE** this is a special case suitable for marker pose estimation. Number of\n * input points must be 4. Object points must be defined in the following order:\n *\n * point 0: [-squareLength / 2, squareLength / 2, 0]\n * point 1: [ squareLength / 2, squareLength / 2, 0]\n * point 2: [ squareLength / 2, -squareLength / 2, 0]\n * point 3: [-squareLength / 2, -squareLength / 2, 0]\n *\n * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or 1xN/Nx1\n * 3-channel, where N is the number of points. vector<Point3f> can be also passed here.\n *\n * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel, where N\n * is the number of points. vector<Point2f> can be also passed here.\n *\n * @param cameraMatrix Input camera matrix $A = \\vecthreethree{fx}{0}{cx}{0}{fy}{cy}{0}{0}{1}$ .\n *\n * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6\n * [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is\n * NULL/empty, the zero distortion coefficients are assumed.\n *\n * @param rvecs Vector of output rotation vectors (see Rodrigues ) that, together with tvecs, brings\n * points from the model coordinate system to the camera coordinate system.\n *\n * @param tvecs Vector of output translation vectors.\n *\n * @param useExtrinsicGuess Parameter used for SOLVEPNP_ITERATIVE. If true (1), the function uses the\n * provided rvec and tvec values as initial approximations of the rotation and translation vectors,\n * respectively, and further optimizes them.\n *\n * @param flags Method for solving a PnP problem:\n * SOLVEPNP_ITERATIVE Iterative method is based on a Levenberg-Marquardt optimization. In this case the\n * function finds such a pose that minimizes reprojection error, that is the sum of squared distances\n * between the observed projections imagePoints and the projected (using projectPoints ) objectPoints\n * .SOLVEPNP_P3P Method is based on the paper of X.S. Gao, X.-R. Hou, J. Tang, H.-F. Chang \"Complete\n * Solution Classification for the Perspective-Three-Point Problem\" (gao2003complete). In this case the\n * function requires exactly four object and image points.SOLVEPNP_AP3P Method is based on the paper of\n * T. Ke, S. Roumeliotis \"An Efficient Algebraic Solution to the Perspective-Three-Point Problem\"\n * (Ke17). In this case the function requires exactly four object and image points.SOLVEPNP_EPNP Method\n * has been introduced by F.Moreno-Noguer, V.Lepetit and P.Fua in the paper \"EPnP: Efficient\n * Perspective-n-Point Camera Pose Estimation\" (lepetit2009epnp).SOLVEPNP_DLS Method is based on the\n * paper of Joel A. Hesch and Stergios I. Roumeliotis. \"A Direct Least-Squares (DLS) Method for PnP\"\n * (hesch2011direct).SOLVEPNP_UPNP Method is based on the paper of A.Penate-Sanchez, J.Andrade-Cetto,\n * F.Moreno-Noguer. \"Exhaustive Linearization for Robust Camera Pose and Focal Length\n * Estimation\" (penate2013exhaustive). In this case the function also estimates the parameters $f_x$\n * and $f_y$ assuming that both have the same value. Then the cameraMatrix is updated with the\n * estimated focal length.SOLVEPNP_IPPE Method is based on the paper of T. Collins and A. Bartoli.\n * \"Infinitesimal Plane-Based Pose Estimation\" (Collins14). This method requires coplanar object\n * points.SOLVEPNP_IPPE_SQUARE Method is based on the paper of Toby Collins and Adrien Bartoli.\n * \"Infinitesimal Plane-Based Pose Estimation\" (Collins14). This method is suitable for marker pose\n * estimation. It requires 4 coplanar object points defined in the following order:\n * point 0: [-squareLength / 2, squareLength / 2, 0]point 1: [ squareLength / 2, squareLength / 2,\n * 0]point 2: [ squareLength / 2, -squareLength / 2, 0]point 3: [-squareLength / 2, -squareLength / 2,\n * 0]\n *\n * @param rvec Rotation vector used to initialize an iterative PnP refinement algorithm, when flag is\n * SOLVEPNP_ITERATIVE and useExtrinsicGuess is set to true.\n *\n * @param tvec Translation vector used to initialize an iterative PnP refinement algorithm, when flag\n * is SOLVEPNP_ITERATIVE and useExtrinsicGuess is set to true.\n *\n * @param reprojectionError Optional vector of reprojection error, that is the RMS error ( $\n * \\text{RMSE} = \\sqrt{\\frac{\\sum_{i}^{N} \\left ( \\hat{y_i} - y_i \\right )^2}{N}} $) between the input\n * image points and the 3D object points projected with the estimated pose.\n */\nexport declare function solvePnPGeneric(objectPoints: InputArray, imagePoints: InputArray, cameraMatrix: InputArray, distCoeffs: InputArray, rvecs: OutputArrayOfArrays, tvecs: OutputArrayOfArrays, useExtrinsicGuess?: bool, flags?: SolvePnPMethod, rvec?: InputArray, tvec?: InputArray, reprojectionError?: OutputArray): int;\n/**\n * The function estimates an object pose given a set of object points, their corresponding image\n * projections, as well as the camera matrix and the distortion coefficients. This function finds such\n * a pose that minimizes reprojection error, that is, the sum of squared distances between the observed\n * projections imagePoints and the projected (using [projectPoints] ) objectPoints. The use of RANSAC\n * makes the function resistant to outliers.\n *\n * An example of how to use solvePNPRansac for object detection can be found at\n * opencv_source_code/samples/cpp/tutorial_code/calib3d/real_time_pose_estimation/\n * The default method used to estimate the camera pose for the Minimal Sample Sets step is\n * [SOLVEPNP_EPNP]. Exceptions are:\n *\n * if you choose [SOLVEPNP_P3P] or [SOLVEPNP_AP3P], these methods will be used.\n * if the number of input points is equal to 4, [SOLVEPNP_P3P] is used.\n *\n * The method used to estimate the camera pose using all the inliers is defined by the flags parameters\n * unless it is equal to [SOLVEPNP_P3P] or [SOLVEPNP_AP3P]. In this case, the method [SOLVEPNP_EPNP]\n * will be used instead.\n *\n * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or 1xN/Nx1\n * 3-channel, where N is the number of points. vector<Point3f> can be also passed here.\n *\n * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel, where N\n * is the number of points. vector<Point2f> can be also passed here.\n *\n * @param cameraMatrix Input camera matrix $A = \\vecthreethree{fx}{0}{cx}{0}{fy}{cy}{0}{0}{1}$ .\n *\n * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6\n * [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is\n * NULL/empty, the zero distortion coefficients are assumed.\n *\n * @param rvec Output rotation vector (see Rodrigues ) that, together with tvec, brings points from the\n * model coordinate system to the camera coordinate system.\n *\n * @param tvec Output translation vector.\n *\n * @param useExtrinsicGuess Parameter used for SOLVEPNP_ITERATIVE. If true (1), the function uses the\n * provided rvec and tvec values as initial approximations of the rotation and translation vectors,\n * respectively, and further optimizes them.\n *\n * @param iterationsCount Number of iterations.\n *\n * @param reprojectionError Inlier threshold value used by the RANSAC procedure. The parameter value is\n * the maximum allowed distance between the observed and computed point projections to consider it an\n * inlier.\n *\n * @param confidence The probability that the algorithm produces a useful result.\n *\n * @param inliers Output vector that contains indices of inliers in objectPoints and imagePoints .\n *\n * @param flags Method for solving a PnP problem (see solvePnP ).\n */\nexport declare function solvePnPRansac(objectPoints: InputArray, imagePoints: InputArray, cameraMatrix: InputArray, distCoeffs: InputArray, rvec: OutputArray, tvec: OutputArray, useExtrinsicGuess?: bool, iterationsCount?: int, reprojectionError?: float, confidence?: double, inliers?: OutputArray, flags?: int): bool;\n/**\n * The function refines the object pose given at least 3 object points, their corresponding image\n * projections, an initial solution for the rotation and translation vector, as well as the camera\n * matrix and the distortion coefficients. The function minimizes the projection error with respect to\n * the rotation and the translation vectors, according to a Levenberg-Marquardt iterative minimization\n * Madsen04 Eade13 process.\n *\n * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or 1xN/Nx1\n * 3-channel, where N is the number of points. vector<Point3f> can also be passed here.\n *\n * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel, where N\n * is the number of points. vector<Point2f> can also be passed here.\n *\n * @param cameraMatrix Input camera matrix $A = \\vecthreethree{fx}{0}{cx}{0}{fy}{cy}{0}{0}{1}$ .\n *\n * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6\n * [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is\n * NULL/empty, the zero distortion coefficients are assumed.\n *\n * @param rvec Input/Output rotation vector (see Rodrigues ) that, together with tvec, brings points\n * from the model coordinate system to the camera coordinate system. Input values are used as an\n * initial solution.\n *\n * @param tvec Input/Output translation vector. Input values are used as an initial solution.\n *\n * @param criteria Criteria when to stop the Levenberg-Marquard iterative algorithm.\n */\nexport declare function solvePnPRefineLM(objectPoints: InputArray, imagePoints: InputArray, cameraMatrix: InputArray, distCoeffs: InputArray, rvec: InputOutputArray, tvec: InputOutputArray, criteria?: TermCriteria): void;\n/**\n * The function refines the object pose given at least 3 object points, their corresponding image\n * projections, an initial solution for the rotation and translation vector, as well as the camera\n * matrix and the distortion coefficients. The function minimizes the projection error with respect to\n * the rotation and the translation vectors, using a virtual visual servoing (VVS) Chaumette06\n * Marchand16 scheme.\n *\n * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or 1xN/Nx1\n * 3-channel, where N is the number of points. vector<Point3f> can also be passed here.\n *\n * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel, where N\n * is the number of points. vector<Point2f> can also be passed here.\n *\n * @param cameraMatrix Input camera matrix $A = \\vecthreethree{fx}{0}{cx}{0}{fy}{cy}{0}{0}{1}$ .\n *\n * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6\n * [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is\n * NULL/empty, the zero distortion coefficients are assumed.\n *\n * @param rvec Input/Output rotation vector (see Rodrigues ) that, together with tvec, brings points\n * from the model coordinate system to the camera coordinate system. Input values are used as an\n * initial solution.\n *\n * @param tvec Input/Output translation vector. Input values are used as an initial solution.\n *\n * @param criteria Criteria when to stop the Levenberg-Marquard iterative algorithm.\n *\n * @param VVSlambda Gain for the virtual visual servoing control law, equivalent to the $\\alpha$ gain\n * in the Damped Gauss-Newton formulation.\n */\nexport declare function solvePnPRefineVVS(objectPoints: InputArray, imagePoints: InputArray, cameraMatrix: InputArray, distCoeffs: InputArray, rvec: InputOutputArray, tvec: InputOutputArray, criteria?: TermCriteria, VVSlambda?: double): void;\n/**\n * The function estimates transformation between two cameras making a stereo pair. If you have a stereo\n * camera where the relative position and orientation of two cameras is fixed, and if you computed\n * poses of an object relative to the first camera and to the second camera, (R1, T1) and (R2, T2),\n * respectively (this can be done with solvePnP ), then those poses definitely relate to each other.\n * This means that, given ( `$R_1$`, `$T_1$` ), it should be possible to compute ( `$R_2$`, `$T_2$` ).\n * You only need to know the position and orientation of the second camera relative to the first\n * camera. This is what the described function does. It computes ( `$R$`, `$T$` ) so that:\n *\n * `\\\\[R_2=R*R_1\\\\]` `\\\\[T_2=R*T_1 + T,\\\\]`\n *\n * Optionally, it computes the essential matrix E:\n *\n * `\\\\[E= \\\\vecthreethree{0}{-T_2}{T_1}{T_2}{0}{-T_0}{-T_1}{T_0}{0} *R\\\\]`\n *\n * where `$T_i$` are components of the translation vector `$T$` : `$T=[T_0, T_1, T_2]^T$` . And the\n * function can also compute the fundamental matrix F:\n *\n * `\\\\[F = cameraMatrix2^{-T} E cameraMatrix1^{-1}\\\\]`\n *\n * Besides the stereo-related information, the function can also perform a full calibration of each of\n * two cameras. However, due to the high dimensionality of the parameter space and noise in the input\n * data, the function can diverge from the correct solution. If the intrinsic parameters can be\n * estimated with high accuracy for each of the cameras individually (for example, using\n * calibrateCamera ), you are recommended to do so and then pass CALIB_FIX_INTRINSIC flag to the\n * function along with the computed intrinsic parameters. Otherwise, if all the parameters are\n * estimated at once, it makes sense to restrict some parameters, for example, pass\n * CALIB_SAME_FOCAL_LENGTH and CALIB_ZERO_TANGENT_DIST flags, which is usually a reasonable assumption.\n *\n * Similarly to calibrateCamera , the function minimizes the total re-projection error for all the\n * points in all the available views from both cameras. The function returns the final value of the\n * re-projection error.\n *\n * @param objectPoints Vector of vectors of the calibration pattern points.\n *\n * @param imagePoints1 Vector of vectors of the projections of the calibration pattern points, observed\n * by the first camera.\n *\n * @param imagePoints2 Vector of vectors of the projections of the calibration pattern points, observed\n * by the second camera.\n *\n * @param cameraMatrix1 Input/output first camera matrix:\n * $\\vecthreethree{f_x^{(j)}}{0}{c_x^{(j)}}{0}{f_y^{(j)}}{c_y^{(j)}}{0}{0}{1}$ , $j = 0,\\, 1$ . If any\n * of CALIB_USE_INTRINSIC_GUESS , CALIB_FIX_ASPECT_RATIO , CALIB_FIX_INTRINSIC , or\n * CALIB_FIX_FOCAL_LENGTH are specified, some or all of the matrix components must be initialized. See\n * the flags description for details.\n *\n * @param distCoeffs1 Input/output vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4,\n * k_5, k_6 [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. The output\n * vector length depends on the flags.\n *\n * @param cameraMatrix2 Input/output second camera matrix. The parameter is similar to cameraMatrix1\n *\n * @param distCoeffs2 Input/output lens distortion coefficients for the second camera. The parameter is\n * similar to distCoeffs1 .\n *\n * @param imageSize Size of the image used only to initialize intrinsic camera matrix.\n *\n * @param R Output rotation matrix between the 1st and the 2nd camera coordinate systems.\n *\n * @param T Output translation vector between the coordinate systems of the cameras.\n *\n * @param E Output essential matrix.\n *\n * @param F Output fundamental matrix.\n *\n * @param perViewErrors Output vector of the RMS re-projection error estimated for each pattern view.\n *\n * @param flags Different flags that may be zero or a combination of the following values:\n * CALIB_FIX_INTRINSIC Fix cameraMatrix? and distCoeffs? so that only R, T, E , and F matrices are\n * estimated.CALIB_USE_INTRINSIC_GUESS Optimize some or all of the intrinsic parameters according to\n * the specified flags. Initial values are provided by the user.CALIB_USE_EXTRINSIC_GUESS R, T contain\n * valid initial values that are optimized further. Otherwise R, T are initialized to the median value\n * of the pattern views (each dimension separately).CALIB_FIX_PRINCIPAL_POINT Fix the principal points\n * during the optimization.CALIB_FIX_FOCAL_LENGTH Fix $f^{(j)}_x$ and $f^{(j)}_y$\n * .CALIB_FIX_ASPECT_RATIO Optimize $f^{(j)}_y$ . Fix the ratio $f^{(j)}_x/f^{(j)}_y$\n *\n * CALIB_SAME_FOCAL_LENGTH Enforce $f^{(0)}_x=f^{(1)}_x$ and $f^{(0)}_y=f^{(1)}_y$\n * .CALIB_ZERO_TANGENT_DIST Set tangential distortion coefficients for each camera to zeros and fix\n * there.CALIB_FIX_K1,...,CALIB_FIX_K6 Do not change the corresponding radial distortion coefficient\n * during the optimization. If CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the supplied\n * distCoeffs matrix is used. Otherwise, it is set to 0.CALIB_RATIONAL_MODEL Enable coefficients k4,\n * k5, and k6. To provide the backward compatibility, this extra flag should be explicitly specified to\n * make the calibration function use the rational model and return 8 coefficients. If the flag is not\n * set, the function computes and returns only 5 distortion coefficients.CALIB_THIN_PRISM_MODEL\n * Coefficients s1, s2, s3 and s4 are enabled. To provide the backward compatibility, this extra flag\n * should be explicitly specified to make the calibration function use the thin prism model and return\n * 12 coefficients. If the flag is not set, the function computes and returns only 5 distortion\n * coefficients.CALIB_FIX_S1_S2_S3_S4 The thin prism distortion coefficients are not changed during the\n * optimization. If CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the supplied distCoeffs\n * matrix is used. Otherwise, it is set to 0.CALIB_TILTED_MODEL Coefficients tauX and tauY are enabled.\n * To provide the backward compatibility, this extra flag should be explicitly specified to make the\n * calibration function use the tilted sensor model and return 14 coefficients. If the flag is not set,\n * the function computes and returns only 5 distortion coefficients.CALIB_FIX_TAUX_TAUY The\n * coefficients of the tilted sensor model are not changed during the optimization. If\n * CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the supplied distCoeffs matrix is used.\n * Otherwise, it is set to 0.\n *\n * @param criteria Termination criteria for the iterative optimization algorithm.\n */\nexport declare function stereoCalibrate(objectPoints: InputArrayOfArrays, imagePoints1: InputArrayOfArrays, imagePoints2: InputArrayOfArrays, cameraMatrix1: InputOutputArray, distCoeffs1: InputOutputArray, cameraMatrix2: InputOutputArray, distCoeffs2: InputOutputArray, imageSize: Size, R: InputOutputArray, T: InputOutputArray, E: OutputArray, F: OutputArray, perViewErrors: OutputArray, flags?: int, criteria?: TermCriteria): double;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function stereoCalibrate(objectPoints: InputArrayOfArrays, imagePoints1: InputArrayOfArrays, imagePoints2: InputArrayOfArrays, cameraMatrix1: InputOutputArray, distCoeffs1: InputOutputArray, cameraMatrix2: InputOutputArray, distCoeffs2: InputOutputArray, imageSize: Size, R: OutputArray, T: OutputArray, E: OutputArray, F: OutputArray, flags?: int, criteria?: TermCriteria): double;\n/**\n * The function computes the rotation matrices for each camera that (virtually) make both camera image\n * planes the same plane. Consequently, this makes all the epipolar lines parallel and thus simplifies\n * the dense stereo correspondence problem. The function takes the matrices computed by stereoCalibrate\n * as input. As output, it provides two rotation matrices and also two projection matrices in the new\n * coordinates. The function distinguishes the following two cases:\n *\n * **Horizontal stereo**: the first and the second camera views are shifted relative to each other\n * mainly along the x axis (with possible small vertical shift). In the rectified images, the\n * corresponding epipolar lines in the left and right cameras are horizontal and have the same\n * y-coordinate. P1 and P2 look like:`\\\\[\\\\texttt{P1} = \\\\begin{bmatrix} f & 0 & cx_1 & 0 \\\\\\\\ 0 & f &\n * cy & 0 \\\\\\\\ 0 & 0 & 1 & 0 \\\\end{bmatrix}\\\\]``\\\\[\\\\texttt{P2} = \\\\begin{bmatrix} f & 0 & cx_2 & T_x*f\n * \\\\\\\\ 0 & f & cy & 0 \\\\\\\\ 0 & 0 & 1 & 0 \\\\end{bmatrix} ,\\\\]`where `$T_x$` is a horizontal shift\n * between the cameras and `$cx_1=cx_2$` if CALIB_ZERO_DISPARITY is set.\n * **Vertical stereo**: the first and the second camera views are shifted relative to each other mainly\n * in vertical direction (and probably a bit in the horizontal direction too). The epipolar lines in\n * the rectified images are vertical and have the same x-coordinate. P1 and P2 look\n * like:`\\\\[\\\\texttt{P1} = \\\\begin{bmatrix} f & 0 & cx & 0 \\\\\\\\ 0 & f & cy_1 & 0 \\\\\\\\ 0 & 0 & 1 & 0\n * \\\\end{bmatrix}\\\\]``\\\\[\\\\texttt{P2} = \\\\begin{bmatrix} f & 0 & cx & 0 \\\\\\\\ 0 & f & cy_2 & T_y*f \\\\\\\\\n * 0 & 0 & 1 & 0 \\\\end{bmatrix} ,\\\\]`where `$T_y$` is a vertical shift between the cameras and\n * `$cy_1=cy_2$` if CALIB_ZERO_DISPARITY is set.\n *\n * As you can see, the first three columns of P1 and P2 will effectively be the new \"rectified\" camera\n * matrices. The matrices, together with R1 and R2 , can then be passed to initUndistortRectifyMap to\n * initialize the rectification map for each camera.\n *\n * See below the screenshot from the stereo_calib.cpp sample. Some red horizontal lines pass through\n * the corresponding image regions. This means that the images are well rectified, which is what most\n * stereo correspondence algorithms rely on. The green rectangles are roi1 and roi2 . You see that\n * their interiors are all valid pixels.\n *\n * @param cameraMatrix1 First camera matrix.\n *\n * @param distCoeffs1 First camera distortion parameters.\n *\n * @param cameraMatrix2 Second camera matrix.\n *\n * @param distCoeffs2 Second camera distortion parameters.\n *\n * @param imageSize Size of the image used for stereo calibration.\n *\n * @param R Rotation matrix between the coordinate systems of the first and the second cameras.\n *\n * @param T Translation vector between coordinate systems of the cameras.\n *\n * @param R1 Output 3x3 rectification transform (rotation matrix) for the first camera.\n *\n * @param R2 Output 3x3 rectification transform (rotation matrix) for the second camera.\n *\n * @param P1 Output 3x4 projection matrix in the new (rectified) coordinate systems for the first\n * camera.\n *\n * @param P2 Output 3x4 projection matrix in the new (rectified) coordinate systems for the second\n * camera.\n *\n * @param Q Output $4 \\times 4$ disparity-to-depth mapping matrix (see reprojectImageTo3D ).\n *\n * @param flags Operation flags that may be zero or CALIB_ZERO_DISPARITY . If the flag is set, the\n * function makes the principal points of each camera have the same pixel coordinates in the rectified\n * views. And if the flag is not set, the function may still shift the images in the horizontal or\n * vertical direction (depending on the orientation of epipolar lines) to maximize the useful image\n * area.\n *\n * @param alpha Free scaling parameter. If it is -1 or absent, the function performs the default\n * scaling. Otherwise, the parameter should be between 0 and 1. alpha=0 means that the rectified images\n * are zoomed and shifted so that only valid pixels are visible (no black areas after rectification).\n * alpha=1 means that the rectified image is decimated and shifted so that all the pixels from the\n * original images from the cameras are retained in the rectified images (no source image pixels are\n * lost). Obviously, any intermediate value yields an intermediate result between those two extreme\n * cases.\n *\n * @param newImageSize New image resolution after rectification. The same size should be passed to\n * initUndistortRectifyMap (see the stereo_calib.cpp sample in OpenCV samples directory). When (0,0) is\n * passed (default), it is set to the original imageSize . Setting it to larger value can help you\n * preserve details in the original image, especially when there is a big radial distortion.\n *\n * @param validPixROI1 Optional output rectangles inside the rectified images where all the pixels are\n * valid. If alpha=0 , the ROIs cover the whole images. Otherwise, they are likely to be smaller (see\n * the picture below).\n *\n * @param validPixROI2 Optional output rectangles inside the rectified images where all the pixels are\n * valid. If alpha=0 , the ROIs cover the whole images. Otherwise, they are likely to be smaller (see\n * the picture below).\n */\nexport declare function stereoRectify(cameraMatrix1: InputArray, distCoeffs1: InputArray, cameraMatrix2: InputArray, distCoeffs2: InputArray, imageSize: Size, R: InputArray, T: InputArray, R1: OutputArray, R2: OutputArray, P1: OutputArray, P2: OutputArray, Q: OutputArray, flags?: int, alpha?: double, newImageSize?: Size, validPixROI1?: any, validPixROI2?: any): void;\n/**\n * The function computes the rectification transformations without knowing intrinsic parameters of the\n * cameras and their relative position in the space, which explains the suffix \"uncalibrated\". Another\n * related difference from stereoRectify is that the function outputs not the rectification\n * transformations in the object (3D) space, but the planar perspective transformations encoded by the\n * homography matrices H1 and H2 . The function implements the algorithm Hartley99 .\n *\n * While the algorithm does not need to know the intrinsic parameters of the cameras, it heavily\n * depends on the epipolar geometry. Therefore, if the camera lenses have a significant distortion, it\n * would be better to correct it before computing the fundamental matrix and calling this function. For\n * example, distortion coefficients can be estimated for each head of stereo camera separately by using\n * calibrateCamera . Then, the images can be corrected using undistort , or just the point coordinates\n * can be corrected with undistortPoints .\n *\n * @param points1 Array of feature points in the first image.\n *\n * @param points2 The corresponding points in the second image. The same formats as in\n * findFundamentalMat are supported.\n *\n * @param F Input fundamental matrix. It can be computed from the same set of point pairs using\n * findFundamentalMat .\n *\n * @param imgSize Size of the image.\n *\n * @param H1 Output rectification homography matrix for the first image.\n *\n * @param H2 Output rectification homography matrix for the second image.\n *\n * @param threshold Optional threshold used to filter out the outliers. If the parameter is greater\n * than zero, all the point pairs that do not comply with the epipolar geometry (that is, the points\n * for which $|\\texttt{points2[i]}^T*\\texttt{F}*\\texttt{points1[i]}|>\\texttt{threshold}$ ) are rejected\n * prior to computing the homographies. Otherwise, all the points are considered inliers.\n */\nexport declare function stereoRectifyUncalibrated(points1: InputArray, points2: InputArray, F: InputArray, imgSize: Size, H1: OutputArray, H2: OutputArray, threshold?: double): bool;\n/**\n * The function reconstructs 3-dimensional points (in homogeneous coordinates) by using their\n * observations with a stereo camera. Projections matrices can be obtained from stereoRectify.\n *\n * Keep in mind that all input data should be of float type in order for this function to work.\n *\n * [reprojectImageTo3D]\n *\n * @param projMatr1 3x4 projection matrix of the first camera.\n *\n * @param projMatr2 3x4 projection matrix of the second camera.\n *\n * @param projPoints1 2xN array of feature points in the first image. In case of c++ version it can be\n * also a vector of feature points or two-channel matrix of size 1xN or Nx1.\n *\n * @param projPoints2 2xN array of corresponding points in the second image. In case of c++ version it\n * can be also a vector of feature points or two-channel matrix of size 1xN or Nx1.\n *\n * @param points4D 4xN array of reconstructed points in homogeneous coordinates.\n */\nexport declare function triangulatePoints(projMatr1: InputArray, projMatr2: InputArray, projPoints1: InputArray, projPoints2: InputArray, points4D: OutputArray): void;\n/**\n * The function transforms an image to compensate radial and tangential lens distortion.\n *\n * The function is simply a combination of [initUndistortRectifyMap] (with unity R ) and [remap] (with\n * bilinear interpolation). See the former function for details of the transformation being performed.\n *\n * Those pixels in the destination image, for which there is no correspondent pixels in the source\n * image, are filled with zeros (black color).\n *\n * A particular subset of the source image that will be visible in the corrected image can be regulated\n * by newCameraMatrix. You can use [getOptimalNewCameraMatrix] to compute the appropriate\n * newCameraMatrix depending on your requirements.\n *\n * The camera matrix and the distortion parameters can be determined using [calibrateCamera]. If the\n * resolution of images is different from the resolution used at the calibration stage, `$f_x, f_y,\n * c_x$` and `$c_y$` need to be scaled accordingly, while the distortion coefficients remain the same.\n *\n * @param src Input (distorted) image.\n *\n * @param dst Output (corrected) image that has the same size and type as src .\n *\n * @param cameraMatrix Input camera matrix $A = \\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}$ .\n *\n * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5,\n * k_6[, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is\n * NULL/empty, the zero distortion coefficients are assumed.\n *\n * @param newCameraMatrix Camera matrix of the distorted image. By default, it is the same as\n * cameraMatrix but you may additionally scale and shift the result by using a different matrix.\n */\nexport declare function undistort(src: InputArray, dst: OutputArray, cameraMatrix: InputArray, distCoeffs: InputArray, newCameraMatrix?: InputArray): void;\n/**\n * The function is similar to [undistort] and [initUndistortRectifyMap] but it operates on a sparse set\n * of points instead of a raster image. Also the function performs a reverse transformation to\n * projectPoints. In case of a 3D object, it does not reconstruct its 3D coordinates, but for a planar\n * object, it does, up to a translation vector, if the proper R is specified.\n *\n * For each observed point coordinate `$(u, v)$` the function computes: `\\\\[ \\\\begin{array}{l} x^{\"}\n * \\\\leftarrow (u - c_x)/f_x \\\\\\\\ y^{\"} \\\\leftarrow (v - c_y)/f_y \\\\\\\\ (x',y') = undistort(x^{\"},y^{\"},\n * \\\\texttt{distCoeffs}) \\\\\\\\ {[X\\\\,Y\\\\,W]} ^T \\\\leftarrow R*[x' \\\\, y' \\\\, 1]^T \\\\\\\\ x \\\\leftarrow X/W\n * \\\\\\\\ y \\\\leftarrow Y/W \\\\\\\\ \\\\text{only performed if P is specified:} \\\\\\\\ u' \\\\leftarrow x {f'}_x +\n * {c'}_x \\\\\\\\ v' \\\\leftarrow y {f'}_y + {c'}_y \\\\end{array} \\\\]`\n *\n * where *undistort* is an approximate iterative algorithm that estimates the normalized original point\n * coordinates out of the normalized distorted point coordinates (\"normalized\" means that the\n * coordinates do not depend on the camera matrix).\n *\n * The function can be used for both a stereo camera head or a monocular camera (when R is empty).\n *\n * @param src Observed point coordinates, 2xN/Nx2 1-channel or 1xN/Nx1 2-channel (CV_32FC2 or CV_64FC2)\n * (or vector<Point2f> ).\n *\n * @param dst Output ideal point coordinates (1xN/Nx1 2-channel or vector<Point2f> ) after undistortion\n * and reverse perspective transformation. If matrix P is identity or omitted, dst will contain\n * normalized point coordinates.\n *\n * @param cameraMatrix Camera matrix $\\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}$ .\n *\n * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5,\n * k_6[, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is\n * NULL/empty, the zero distortion coefficients are assumed.\n *\n * @param R Rectification transformation in the object space (3x3 matrix). R1 or R2 computed by\n * stereoRectify can be passed here. If the matrix is empty, the identity transformation is used.\n *\n * @param P New camera matrix (3x3) or new projection matrix (3x4) $\\begin{bmatrix} {f'}_x & 0 & {c'}_x\n * & t_x \\\\ 0 & {f'}_y & {c'}_y & t_y \\\\ 0 & 0 & 1 & t_z \\end{bmatrix}$. P1 or P2 computed by\n * stereoRectify can be passed here. If the matrix is empty, the identity new camera matrix is used.\n */\nexport declare function undistortPoints(src: InputArray, dst: OutputArray, cameraMatrix: InputArray, distCoeffs: InputArray, R?: InputArray, P?: InputArray): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n *\n * Default version of [undistortPoints] does 5 iterations to compute undistorted points.\n */\nexport declare function undistortPoints(src: InputArray, dst: OutputArray, cameraMatrix: InputArray, distCoeffs: InputArray, R: InputArray, P: InputArray, criteria: TermCriteria): void;\nexport declare function validateDisparity(disparity: InputOutputArray, cost: InputArray, minDisparity: int, numberOfDisparities: int, disp12MaxDisp?: int): void;\nexport declare const LMEDS: any;\nexport declare const RANSAC: any;\nexport declare const RHO: any;\nexport declare const CALIB_CB_ADAPTIVE_THRESH: any;\nexport declare const CALIB_CB_NORMALIZE_IMAGE: any;\nexport declare const CALIB_CB_FILTER_QUADS: any;\nexport declare const CALIB_CB_FAST_CHECK: any;\nexport declare const CALIB_CB_EXHAUSTIVE: any;\nexport declare const CALIB_CB_ACCURACY: any;\nexport declare const CALIB_CB_SYMMETRIC_GRID: any;\nexport declare const CALIB_CB_ASYMMETRIC_GRID: any;\nexport declare const CALIB_CB_CLUSTERING: any;\nexport declare const CALIB_NINTRINSIC: any;\nexport declare const CALIB_USE_INTRINSIC_GUESS: any;\nexport declare const CALIB_FIX_ASPECT_RATIO: any;\nexport declare const CALIB_FIX_PRINCIPAL_POINT: any;\nexport declare const CALIB_ZERO_TANGENT_DIST: any;\nexport declare const CALIB_FIX_FOCAL_LENGTH: any;\nexport declare const CALIB_FIX_K1: any;\nexport declare const CALIB_FIX_K2: any;\nexport declare const CALIB_FIX_K3: any;\nexport declare const CALIB_FIX_K4: any;\nexport declare const CALIB_FIX_K5: any;\nexport declare const CALIB_FIX_K6: any;\nexport declare const CALIB_RATIONAL_MODEL: any;\nexport declare const CALIB_THIN_PRISM_MODEL: any;\nexport declare const CALIB_FIX_S1_S2_S3_S4: any;\nexport declare const CALIB_TILTED_MODEL: any;\nexport declare const CALIB_FIX_TAUX_TAUY: any;\nexport declare const CALIB_USE_QR: any;\nexport declare const CALIB_FIX_TANGENT_DIST: any;\nexport declare const CALIB_FIX_INTRINSIC: any;\nexport declare const CALIB_SAME_FOCAL_LENGTH: any;\nexport declare const CALIB_ZERO_DISPARITY: any;\nexport declare const CALIB_USE_LU: any;\nexport declare const CALIB_USE_EXTRINSIC_GUESS: any;\nexport declare const FM_7POINT: any;\nexport declare const FM_8POINT: any;\nexport declare const FM_LMEDS: any;\nexport declare const FM_RANSAC: any;\nexport declare const CALIB_HAND_EYE_TSAI: HandEyeCalibrationMethod;\nexport declare const CALIB_HAND_EYE_PARK: HandEyeCalibrationMethod;\nexport declare const CALIB_HAND_EYE_HORAUD: HandEyeCalibrationMethod;\nexport declare const CALIB_HAND_EYE_ANDREFF: HandEyeCalibrationMethod;\nexport declare const CALIB_HAND_EYE_DANIILIDIS: HandEyeCalibrationMethod;\nexport declare const SOLVEPNP_ITERATIVE: SolvePnPMethod;\nexport declare const SOLVEPNP_EPNP: SolvePnPMethod;\nexport declare const SOLVEPNP_P3P: SolvePnPMethod;\nexport declare const SOLVEPNP_DLS: SolvePnPMethod;\nexport declare const SOLVEPNP_UPNP: SolvePnPMethod;\nexport declare const SOLVEPNP_AP3P: SolvePnPMethod;\n/**\n * Infinitesimal Plane-Based Pose Estimation Collins14\n *  Object points must be coplanar.\n *\n */\nexport declare const SOLVEPNP_IPPE: SolvePnPMethod;\n/**\n * Infinitesimal Plane-Based Pose Estimation Collins14\n *  This is a special case suitable for marker pose estimation.\n *  4 coplanar object points must be defined in the following order:\n *\n * point 0: [-squareLength / 2, squareLength / 2, 0]\n * point 1: [ squareLength / 2, squareLength / 2, 0]\n * point 2: [ squareLength / 2, -squareLength / 2, 0]\n * point 3: [-squareLength / 2, -squareLength / 2, 0]\n *\n */\nexport declare const SOLVEPNP_IPPE_SQUARE: SolvePnPMethod;\nexport declare const PROJ_SPHERICAL_ORTHO: UndistortTypes;\nexport declare const PROJ_SPHERICAL_EQRECT: UndistortTypes;\nexport declare type HandEyeCalibrationMethod = any;\nexport declare type SolvePnPMethod = any;\nexport declare type UndistortTypes = any;\n"},"node_modules_mirada_dist_src_types_opencv_core_cluster_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_core_cluster_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/core_cluster.d.ts","content":"import { double, InputArray, InputOutputArray, int, OutputArray, TermCriteria, _EqPredicate } from './_types';\n/**\n * The function kmeans implements a k-means algorithm that finds the centers of cluster_count clusters\n * and groups the input samples around the clusters. As an output, `$\\\\texttt{bestLabels}_i$` contains\n * a 0-based cluster index for the sample stored in the `$i^{th}$` row of the samples matrix.\n *\n * (Python) An example on K-means clustering can be found at\n * opencv_source_code/samples/python/kmeans.py\n *\n * The function returns the compactness measure that is computed as `\\\\[\\\\sum _i \\\\| \\\\texttt{samples}\n * _i - \\\\texttt{centers} _{ \\\\texttt{labels} _i} \\\\| ^2\\\\]` after every attempt. The best (minimum)\n * value is chosen and the corresponding labels and the compactness value are returned by the function.\n * Basically, you can use only the core of the function, set the number of attempts to 1, initialize\n * labels each time using a custom algorithm, pass them with the ( flags = [KMEANS_USE_INITIAL_LABELS]\n * ) flag, and then choose the best (most-compact) clustering.\n *\n * @param data Data for clustering. An array of N-Dimensional points with float coordinates is needed.\n * Examples of this array can be:\n * Mat points(count, 2, CV_32F);Mat points(count, 1, CV_32FC2);Mat points(1, count,\n * CV_32FC2);std::vector<cv::Point2f> points(sampleCount);\n *\n * @param K Number of clusters to split the set by.\n *\n * @param bestLabels Input/output integer array that stores the cluster indices for every sample.\n *\n * @param criteria The algorithm termination criteria, that is, the maximum number of iterations and/or\n * the desired accuracy. The accuracy is specified as criteria.epsilon. As soon as each of the cluster\n * centers moves by less than criteria.epsilon on some iteration, the algorithm stops.\n *\n * @param attempts Flag to specify the number of times the algorithm is executed using different\n * initial labellings. The algorithm returns the labels that yield the best compactness (see the last\n * function parameter).\n *\n * @param flags Flag that can take values of cv::KmeansFlags\n *\n * @param centers Output matrix of the cluster centers, one row per each cluster center.\n */\nexport declare function kmeans(data: InputArray, K: int, bestLabels: InputOutputArray, criteria: TermCriteria, attempts: int, flags: int, centers?: OutputArray): double;\n/**\n * The generic function partition implements an `$O(N^2)$` algorithm for splitting a set of `$N$`\n * elements into one or more equivalency classes, as described in  . The function returns the number of\n * equivalency classes.\n *\n * @param _vec Set of elements stored as a vector.\n *\n * @param labels Output vector of labels. It contains as many elements as vec. Each label labels[i] is\n * a 0-based cluster index of vec[i].\n *\n * @param predicate Equivalence predicate (pointer to a boolean function of two arguments or an\n * instance of the class that has the method bool operator()(const _Tp& a, const _Tp& b) ). The\n * predicate returns true when the elements are certainly in the same class, and returns false if they\n * may or may not be in the same class.\n */\nexport declare function partition(arg119: any, arg120: any, _vec: any, labels: any, predicate?: _EqPredicate): any;\n"},"node_modules_mirada_dist_src_types_opencv_core_hal_interface_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_core_hal_interface_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/core_hal_interface.d.ts","content":"import { cvhalDFT, int, size_t, uchar } from './_types';\n/**\n * @param context pointer to context storing all necessary data\n *\n * @param src_data source image data and step\n *\n * @param dst_data destination image data and step\n */\nexport declare function hal_ni_dct2D(context: cvhalDFT, src_data: uchar, src_step: size_t, dst_data: uchar, dst_step: size_t): cvhalDFT;\n/**\n * @param context pointer to context storing all necessary data\n */\nexport declare function hal_ni_dctFree2D(context: cvhalDFT): cvhalDFT;\n/**\n * @param context double pointer to context storing all necessary data\n *\n * @param width image dimensions\n *\n * @param depth image type (CV_32F or CV64F)\n *\n * @param flags algorithm options (combination of CV_HAL_DFT_INVERSE, ...)\n */\nexport declare function hal_ni_dctInit2D(context: cvhalDFT, width: int, height: int, depth: int, flags: int): cvhalDFT;\n/**\n * @param context pointer to context storing all necessary data\n *\n * @param src source data\n *\n * @param dst destination data\n */\nexport declare function hal_ni_dft1D(context: cvhalDFT, src: uchar, dst: uchar): cvhalDFT;\n/**\n * @param context pointer to context storing all necessary data\n *\n * @param src_data source image data and step\n *\n * @param dst_data destination image data and step\n */\nexport declare function hal_ni_dft2D(context: cvhalDFT, src_data: uchar, src_step: size_t, dst_data: uchar, dst_step: size_t): cvhalDFT;\n/**\n * @param context pointer to context storing all necessary data\n */\nexport declare function hal_ni_dftFree1D(context: cvhalDFT): cvhalDFT;\n/**\n * @param context pointer to context storing all necessary data\n */\nexport declare function hal_ni_dftFree2D(context: cvhalDFT): cvhalDFT;\n/**\n * @param context double pointer to context storing all necessary data\n *\n * @param len transformed array length\n *\n * @param count estimated transformation count\n *\n * @param depth array type (CV_32F or CV_64F)\n *\n * @param flags algorithm options (combination of CV_HAL_DFT_INVERSE, CV_HAL_DFT_SCALE, ...)\n *\n * @param needBuffer pointer to boolean variable, if valid pointer provided, then variable value should\n * be set to true to signal that additional memory buffer is needed for operations\n */\nexport declare function hal_ni_dftInit1D(context: cvhalDFT, len: int, count: int, depth: int, flags: int, needBuffer: any): cvhalDFT;\n/**\n * @param context double pointer to context storing all necessary data\n *\n * @param width image dimensions\n *\n * @param depth image type (CV_32F or CV64F)\n *\n * @param src_channels number of channels in input image\n *\n * @param dst_channels number of channels in output image\n *\n * @param flags algorithm options (combination of CV_HAL_DFT_INVERSE, ...)\n *\n * @param nonzero_rows number of nonzero rows in image, can be used for optimization\n */\nexport declare function hal_ni_dftInit2D(context: cvhalDFT, width: int, height: int, depth: int, src_channels: int, dst_channels: int, flags: int, nonzero_rows: int): cvhalDFT;\n/**\n * @param src_data Source image\n *\n * @param width Source image dimensions\n *\n * @param depth Depth of source image\n *\n * @param minVal Pointer to the returned global minimum and maximum in an array.\n *\n * @param minIdx Pointer to the returned minimum and maximum location.\n *\n * @param mask Specified array region.\n */\nexport declare function hal_ni_minMaxIdx(src_data: uchar, src_step: size_t, width: int, height: int, depth: int, minVal: any, maxVal: any, minIdx: any, maxIdx: any, mask: uchar): uchar;\n"},"node_modules_mirada_dist_src_types_opencv_DescriptorMatcher_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_DescriptorMatcher_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/DescriptorMatcher.d.ts","content":"import { bool, DMatch, FileNode, FileStorage, float, InputArray, InputArrayOfArrays, int, Mat, Ptr } from './_types';\n/**\n * It has two groups of match methods: for matching descriptors of an image with another image or with\n * an image set.\n *\n * Source:\n * [opencv2/features2d.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/features2d.hpp#L860).\n *\n */\nexport declare class DescriptorMatcher {\n    /**\n     *   If the collection is not empty, the new descriptors are added to existing train descriptors.\n     *\n     * @param descriptors Descriptors to add. Each descriptors[i] is a set of descriptors from the same\n     * train image.\n     */\n    add(descriptors: InputArrayOfArrays): InputArrayOfArrays;\n    clear(): void;\n    /**\n     * @param emptyTrainData If emptyTrainData is false, the method creates a deep copy of the object,\n     * that is, copies both parameters and train data. If emptyTrainData is true, the method creates an\n     * object copy with the current parameters but with empty train data.\n     */\n    clone(emptyTrainData?: bool): Ptr;\n    empty(): bool;\n    getTrainDescriptors(): Mat;\n    isMaskSupported(): bool;\n    /**\n     *   These extended variants of [DescriptorMatcher::match] methods find several best matches for each\n     * query descriptor. The matches are returned in the distance increasing order. See\n     * [DescriptorMatcher::match] for the details about query and train descriptors.\n     *\n     * @param queryDescriptors Query set of descriptors.\n     *\n     * @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors\n     * collection stored in the class object.\n     *\n     * @param matches Matches. Each matches[i] is k or less matches for the same query descriptor.\n     *\n     * @param k Count of best matches found per each query descriptor or less if a query descriptor has\n     * less than k possible matches in total.\n     *\n     * @param mask Mask specifying permissible matches between an input query and train matrices of\n     * descriptors.\n     *\n     * @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is\n     * false, the matches vector has the same size as queryDescriptors rows. If compactResult is true, the\n     * matches vector does not contain matches for fully masked-out query descriptors.\n     */\n    knnMatch(queryDescriptors: InputArray, trainDescriptors: InputArray, matches: DMatch, k: int, mask?: InputArray, compactResult?: bool): InputArray;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param queryDescriptors Query set of descriptors.\n     *\n     * @param matches Matches. Each matches[i] is k or less matches for the same query descriptor.\n     *\n     * @param k Count of best matches found per each query descriptor or less if a query descriptor has\n     * less than k possible matches in total.\n     *\n     * @param masks Set of masks. Each masks[i] specifies permissible matches between the input query\n     * descriptors and stored train descriptors from the i-th image trainDescCollection[i].\n     *\n     * @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is\n     * false, the matches vector has the same size as queryDescriptors rows. If compactResult is true, the\n     * matches vector does not contain matches for fully masked-out query descriptors.\n     */\n    knnMatch(queryDescriptors: InputArray, matches: DMatch, k: int, masks?: InputArrayOfArrays, compactResult?: bool): InputArray;\n    /**\n     *   In the first variant of this method, the train descriptors are passed as an input argument. In the\n     * second variant of the method, train descriptors collection that was set by [DescriptorMatcher::add]\n     * is used. Optional mask (or masks) can be passed to specify which query and training descriptors can\n     * be matched. Namely, queryDescriptors[i] can be matched with trainDescriptors[j] only if\n     * mask.at<uchar>(i,j) is non-zero.\n     *\n     * @param queryDescriptors Query set of descriptors.\n     *\n     * @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors\n     * collection stored in the class object.\n     *\n     * @param matches Matches. If a query descriptor is masked out in mask , no match is added for this\n     * descriptor. So, matches size may be smaller than the query descriptors count.\n     *\n     * @param mask Mask specifying permissible matches between an input query and train matrices of\n     * descriptors.\n     */\n    match(queryDescriptors: InputArray, trainDescriptors: InputArray, matches: DMatch, mask?: InputArray): InputArray;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param queryDescriptors Query set of descriptors.\n     *\n     * @param matches Matches. If a query descriptor is masked out in mask , no match is added for this\n     * descriptor. So, matches size may be smaller than the query descriptors count.\n     *\n     * @param masks Set of masks. Each masks[i] specifies permissible matches between the input query\n     * descriptors and stored train descriptors from the i-th image trainDescCollection[i].\n     */\n    match(queryDescriptors: InputArray, matches: DMatch, masks?: InputArrayOfArrays): InputArray;\n    /**\n     *   For each query descriptor, the methods find such training descriptors that the distance between\n     * the query descriptor and the training descriptor is equal or smaller than maxDistance. Found matches\n     * are returned in the distance increasing order.\n     *\n     * @param queryDescriptors Query set of descriptors.\n     *\n     * @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors\n     * collection stored in the class object.\n     *\n     * @param matches Found matches.\n     *\n     * @param maxDistance Threshold for the distance between matched descriptors. Distance means here\n     * metric distance (e.g. Hamming distance), not the distance between coordinates (which is measured in\n     * Pixels)!\n     *\n     * @param mask Mask specifying permissible matches between an input query and train matrices of\n     * descriptors.\n     *\n     * @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is\n     * false, the matches vector has the same size as queryDescriptors rows. If compactResult is true, the\n     * matches vector does not contain matches for fully masked-out query descriptors.\n     */\n    radiusMatch(queryDescriptors: InputArray, trainDescriptors: InputArray, matches: DMatch, maxDistance: float, mask?: InputArray, compactResult?: bool): InputArray;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param queryDescriptors Query set of descriptors.\n     *\n     * @param matches Found matches.\n     *\n     * @param maxDistance Threshold for the distance between matched descriptors. Distance means here\n     * metric distance (e.g. Hamming distance), not the distance between coordinates (which is measured in\n     * Pixels)!\n     *\n     * @param masks Set of masks. Each masks[i] specifies permissible matches between the input query\n     * descriptors and stored train descriptors from the i-th image trainDescCollection[i].\n     *\n     * @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is\n     * false, the matches vector has the same size as queryDescriptors rows. If compactResult is true, the\n     * matches vector does not contain matches for fully masked-out query descriptors.\n     */\n    radiusMatch(queryDescriptors: InputArray, matches: DMatch, maxDistance: float, masks?: InputArrayOfArrays, compactResult?: bool): InputArray;\n    read(fileName: String): String;\n    read(fn: FileNode): FileNode;\n    /**\n     *   Trains a descriptor matcher (for example, the flann index). In all methods to match, the method\n     * [train()] is run every time before matching. Some descriptor matchers (for example,\n     * BruteForceMatcher) have an empty implementation of this method. Other matchers really train their\n     * inner structures (for example, [FlannBasedMatcher] trains [flann::Index] ).\n     */\n    train(): void;\n    write(fileName: String): String;\n    write(fs: FileStorage): FileStorage;\n    write(fs: Ptr, name?: String): Ptr;\n    /**\n     * @param descriptorMatcherType Descriptor matcher type. Now the following matcher types are\n     * supported:\n     *   BruteForce (it uses L2 )BruteForce-L1BruteForce-HammingBruteForce-Hamming(2)FlannBased\n     */\n    static create(descriptorMatcherType: String): Ptr;\n    static create(matcherType: any): Ptr;\n}\nexport declare const FLANNBASED: MatcherType;\nexport declare const BRUTEFORCE: MatcherType;\nexport declare const BRUTEFORCE_L1: MatcherType;\nexport declare const BRUTEFORCE_HAMMING: MatcherType;\nexport declare const BRUTEFORCE_HAMMINGLUT: MatcherType;\nexport declare const BRUTEFORCE_SL2: MatcherType;\nexport declare type MatcherType = any;\n"},"node_modules_mirada_dist_src_types_opencv_core_utils_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_core_utils_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/core_utils.d.ts","content":"import { AsyncArray, bool, double, ErrorCallback, float, float16_t, InputArray, InputArrayOfArrays, InputOutputArray, InputOutputArrayOfArrays, int, int64, schar, short, size_t, uchar, uint64, unsigned, ushort, _Tp } from './_types';\n/**\n * The function returns the aligned pointer of the same type as the input pointer:\n * `\\\\[\\\\texttt{(_Tp*)(((size_t)ptr + n-1) & -n)}\\\\]`\n *\n * @param ptr Aligned pointer.\n *\n * @param n Alignment size that must be a power of two.\n */\nexport declare function alignPtr(arg92: any, ptr: any, n?: int): any;\n/**\n * The function returns the minimum number that is greater than or equal to sz and is divisible by n :\n * `\\\\[\\\\texttt{(sz + n-1) & -n}\\\\]`\n *\n * @param sz Buffer size to align.\n *\n * @param n Alignment size that must be a power of two.\n */\nexport declare function alignSize(sz: size_t, n: int): size_t;\n/**\n * The function returns true if the host hardware supports the specified feature. When user calls\n * setUseOptimized(false), the subsequent calls to [checkHardwareSupport()] will return false until\n * setUseOptimized(true) is called. This way user can dynamically switch on and off the optimized code\n * in OpenCV.\n *\n * @param feature The feature of interest, one of cv::CpuFeatures\n */\nexport declare function checkHardwareSupport(feature: int): bool;\n/**\n * proxy for hal::Cholesky\n */\nexport declare function Cholesky(A: any, astep: size_t, m: int, b: any, bstep: size_t, n: int): bool;\n/**\n * proxy for hal::Cholesky\n */\nexport declare function Cholesky(A: any, astep: size_t, m: int, b: any, bstep: size_t, n: int): bool;\n/**\n * The function cubeRoot computes `$\\\\sqrt[3]{\\\\texttt{val}}$`. Negative arguments are handled\n * correctly. NaN and Inf are not handled. The accuracy approaches the maximum possible accuracy for\n * single-precision data.\n *\n * @param val A function argument.\n */\nexport declare function cubeRoot(val: float): float;\nexport declare function cv_abs(arg93: any, x: _Tp): any;\nexport declare function cv_abs(x: uchar): uchar;\nexport declare function cv_abs(x: schar): schar;\nexport declare function cv_abs(x: ushort): ushort;\nexport declare function cv_abs(x: short): int;\nexport declare function CV_XADD(addr: any, delta: int): any;\n/**\n * The function computes an integer i such that: `\\\\[i \\\\le \\\\texttt{value} < i+1\\\\]`\n *\n * @param value floating-point number. If the value is outside of INT_MIN ... INT_MAX range, the result\n * is not defined.\n */\nexport declare function cvCeil(value: double): int;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function cvCeil(value: float): int;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function cvCeil(value: int): int;\n/**\n * The function computes an integer i such that: `\\\\[i \\\\le \\\\texttt{value} < i+1\\\\]`\n *\n * @param value floating-point number. If the value is outside of INT_MIN ... INT_MAX range, the result\n * is not defined.\n */\nexport declare function cvFloor(value: double): int;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function cvFloor(value: float): int;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function cvFloor(value: int): int;\n/**\n * The function returns 1 if the argument is a plus or minus infinity (as defined by IEEE754 standard)\n * and 0 otherwise.\n *\n * @param value The input floating-point value\n */\nexport declare function cvIsInf(value: double): int;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function cvIsInf(value: float): int;\n/**\n * The function returns 1 if the argument is Not A Number (as defined by IEEE754 standard), 0\n * otherwise.\n *\n * @param value The input floating-point value\n */\nexport declare function cvIsNaN(value: double): int;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function cvIsNaN(value: float): int;\n/**\n * @param value floating-point number. If the value is outside of INT_MIN ... INT_MAX range, the result\n * is not defined.\n */\nexport declare function cvRound(value: double): int;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function cvRound(value: float): int;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function cvRound(value: int): int;\n/**\n * Use this function instead of `ceil((float)a / b)` expressions.\n *\n * [alignSize]\n */\nexport declare function divUp(a: int, b: any): int;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function divUp(a: size_t, b: any): size_t;\nexport declare function dumpInputArray(argument: InputArray): String;\nexport declare function dumpInputArrayOfArrays(argument: InputArrayOfArrays): String;\nexport declare function dumpInputOutputArray(argument: InputOutputArray): String;\nexport declare function dumpInputOutputArrayOfArrays(argument: InputOutputArrayOfArrays): String;\n/**\n * By default the function prints information about the error to stderr, then it either stops if\n * [cv::setBreakOnError()] had been called before or raises the exception. It is possible to alternate\n * error processing by using [redirectError()].\n *\n * @param exc the exception raisen.\n */\nexport declare function error(exc: any): void;\n/**\n * By default the function prints information about the error to stderr, then it either stops if\n * [setBreakOnError()] had been called before or raises the exception. It is possible to alternate\n * error processing by using [redirectError()].\n *\n * [CV_Error], [CV_Error_], [CV_Assert], [CV_DbgAssert]\n *\n * @param _code - error code (Error::Code)\n *\n * @param _err - error description\n *\n * @param _func - function name. Available only when the compiler supports getting it\n *\n * @param _file - source file name where the error has occurred\n *\n * @param _line - line number in the source file where the error has occurred\n */\nexport declare function error(_code: int, _err: any, _func: any, _file: any, _line: int): void;\n/**\n * The function fastAtan2 calculates the full-range angle of an input 2D vector. The angle is measured\n * in degrees and varies from 0 to 360 degrees. The accuracy is about 0.3 degrees.\n *\n * @param y y-coordinate of the vector.\n *\n * @param x x-coordinate of the vector.\n */\nexport declare function fastAtan2(y: float, x: float): float;\n/**\n * The function deallocates the buffer allocated with fastMalloc . If NULL pointer is passed, the\n * function does nothing. C version of the function clears the pointer *pptr* to avoid problems with\n * double memory deallocation.\n *\n * @param ptr Pointer to the allocated buffer.\n */\nexport declare function fastFree(ptr: any): void;\n/**\n * The function allocates the buffer of the specified size and returns it. When the buffer size is 16\n * bytes or more, the returned buffer is aligned to 16 bytes.\n *\n * @param bufSize Allocated buffer size.\n */\nexport declare function fastMalloc(bufSize: size_t): any;\nexport declare function forEach_impl(arg94: any, arg95: any, operation: any): any;\n/**\n * Returned value is raw cmake output including version control system revision, compiler version,\n * compiler flags, enabled modules and third party libraries, etc. Output format depends on target\n * architecture.\n */\nexport declare function getBuildInformation(): any;\n/**\n * Returned value is a string containing space separated list of CPU features with following markers:\n *\n * no markers - baseline features\n * prefix `*` - features enabled in dispatcher\n * suffix `?` - features enabled but not available in HW\n *\n * Example: `SSE SSE2 SSE3 *SSE4.1 *SSE4.2 *FP16 *AVX *AVX2 *AVX512-SKX?`\n */\nexport declare function getCPUFeaturesLine(): any;\n/**\n * The function returns the current number of CPU ticks on some architectures (such as x86, x64,\n * PowerPC). On other platforms the function is equivalent to getTickCount. It can also be used for\n * very accurate time measurements, as well as for [RNG] initialization. Note that in case of multi-CPU\n * systems a thread, from which getCPUTickCount is called, can be suspended and resumed at another CPU\n * with its own counter. So, theoretically (and practically) the subsequent calls to the function do\n * not necessary return the monotonously increasing values. Also, since a modern CPU varies the CPU\n * frequency depending on the load, the number of CPU clocks spent in some code cannot be directly\n * converted to time units. Therefore, getTickCount is generally a preferable solution for measuring\n * execution time.\n */\nexport declare function getCPUTickCount(): int64;\nexport declare function getElemSize(type: int): size_t;\n/**\n * Returns empty string if feature is not defined\n */\nexport declare function getHardwareFeatureName(feature: int): String;\nexport declare function getNumberOfCPUs(): int;\n/**\n * Always returns 1 if OpenCV is built without threading support.\n *\n * The exact meaning of return value depends on the threading framework used by OpenCV library:\n *\n * `TBB` - The number of threads, that OpenCV will try to use for parallel regions. If there is any\n * tbb::thread_scheduler_init in user code conflicting with OpenCV, then function returns default\n * number of threads used by TBB library.\n * `OpenMP` - An upper bound on the number of threads that could be used to form a new team.\n * `Concurrency` - The number of threads, that OpenCV will try to use for parallel regions.\n * `GCD` - Unsupported; returns the GCD thread pool limit (512) for compatibility.\n * `C=` - The number of threads, that OpenCV will try to use for parallel regions, if before called\n * setNumThreads with threads > 0, otherwise returns the number of logical CPUs, available for the\n * process.\n *\n * [setNumThreads], [getThreadNum]\n */\nexport declare function getNumThreads(): int;\n/**\n * The exact meaning of the return value depends on the threading framework used by OpenCV library:\n *\n * `TBB` - Unsupported with current 4.1 TBB release. Maybe will be supported in future.\n * `OpenMP` - The thread number, within the current team, of the calling thread.\n * `Concurrency` - An ID for the virtual processor that the current context is executing on (0 for\n * master thread and unique number for others, but not necessary 1,2,3,...).\n * `GCD` - System calling thread's ID. Never returns 0 inside parallel region.\n * `C=` - The index of the current parallel task.\n *\n * [setNumThreads], [getNumThreads]\n */\nexport declare function getThreadNum(): int;\n/**\n * The function returns the number of ticks after the certain event (for example, when the machine was\n * turned on). It can be used to initialize [RNG] or to measure a function execution time by reading\n * the tick count before and after the function call.\n *\n * [getTickFrequency], [TickMeter]\n */\nexport declare function getTickCount(): int64;\n/**\n * The function returns the number of ticks per second. That is, the following code computes the\n * execution time in seconds:\n *\n * ```cpp\n * double t = (double)getTickCount();\n * // do something ...\n * t = ((double)getTickCount() - t)/getTickFrequency();\n * ```\n *\n * [getTickCount], [TickMeter]\n */\nexport declare function getTickFrequency(): double;\nexport declare function getVersionMajor(): int;\nexport declare function getVersionMinor(): int;\nexport declare function getVersionRevision(): int;\n/**\n * For example \"3.4.1-dev\".\n *\n * getMajorVersion, getMinorVersion, getRevisionVersion\n */\nexport declare function getVersionString(): String;\nexport declare function glob(pattern: String, result: any, recursive?: bool): void;\n/**\n * proxy for hal::LU\n */\nexport declare function LU(A: any, astep: size_t, m: int, b: any, bstep: size_t, n: int): int;\n/**\n * proxy for hal::LU\n */\nexport declare function LU(A: any, astep: size_t, m: int, b: any, bstep: size_t, n: int): int;\nexport declare function normInf(arg96: any, arg97: any, a: any, n: int): any;\nexport declare function normInf(arg98: any, arg99: any, a: any, b: any, n: int): any;\nexport declare function normL1(arg100: any, arg101: any, a: any, n: int): any;\nexport declare function normL1(arg102: any, arg103: any, a: any, b: any, n: int): any;\nexport declare function normL1(a: any, b: any, n: int): float;\nexport declare function normL1(a: uchar, b: uchar, n: int): uchar;\nexport declare function normL2Sqr(arg104: any, arg105: any, a: any, n: int): any;\nexport declare function normL2Sqr(arg106: any, arg107: any, a: any, b: any, n: int): any;\nexport declare function normL2Sqr(a: any, b: any, n: int): float;\nexport declare function parallel_for_(range: any, body: any, nstripes?: double): void;\nexport declare function parallel_for_(range: any, functor: any, nstripes?: double): void;\n/**\n * The function sets the new error handler, called from [cv::error()].\n *\n * the previous error handler\n *\n * @param errCallback the new error handler. If NULL, the default error handler is used.\n *\n * @param userdata the optional user data pointer, passed to the callback.\n *\n * @param prevUserdata the optional output parameter where the previous user data pointer is stored\n */\nexport declare function redirectError(errCallback: ErrorCallback, userdata?: any, prevUserdata?: any): ErrorCallback;\n/**\n * Use this function instead of `ceil((float)a / b) * b` expressions.\n *\n * [divUp]\n */\nexport declare function roundUp(a: int, b: any): int;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function roundUp(a: size_t, b: any): size_t;\n/**\n * The function saturate_cast resembles the standard C++ cast operations, such as static_cast<T>() and\n * others. It perform an efficient and accurate conversion from one primitive type to another (see the\n * introduction chapter). saturate in the name means that when the input value v is out of the range of\n * the target type, the result is not formed just by taking low bits of the input, but instead the\n * value is clipped. For example:\n *\n * ```cpp\n * uchar a = saturate_cast<uchar>(-100); // a = 0 (UCHAR_MIN)\n * short b = saturate_cast<short>(33333.33333); // b = 32767 (SHRT_MAX)\n * ```\n *\n *  Such clipping is done when the target type is unsigned char , signed char , unsigned short or\n * signed short . For 32-bit integers, no clipping is done.\n *\n * When the parameter is a floating-point value and the target type is an integer (8-, 16- or 32-bit),\n * the floating-point value is first rounded to the nearest integer and then clipped if needed (when\n * the target type is 8- or 16-bit).\n *\n * This operation is used in the simplest or most complex image processing functions in OpenCV.\n *\n * [add], [subtract], [multiply], [divide], [Mat::convertTo]\n *\n * @param v Function parameter.\n */\nexport declare function saturate_cast(arg108: any, v: uchar): uchar;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function saturate_cast(arg109: any, v: schar): schar;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function saturate_cast(arg110: any, v: ushort): ushort;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function saturate_cast(arg111: any, v: short): any;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function saturate_cast(arg112: any, v: unsigned): any;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function saturate_cast(arg113: any, v: int): any;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function saturate_cast(arg114: any, v: float): any;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function saturate_cast(arg115: any, v: double): any;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function saturate_cast(arg116: any, v: int64): int64;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function saturate_cast(arg117: any, v: uint64): uint64;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function saturate_cast(arg118: any, v: float16_t): any;\n/**\n * When the break-on-error mode is set, the default error handler issues a hardware exception, which\n * can make debugging more convenient.\n *\n * the previous state\n */\nexport declare function setBreakOnError(flag: bool): bool;\n/**\n * If threads == 0, OpenCV will disable threading optimizations and run all it's functions\n * sequentially. Passing threads < 0 will reset threads number to system default. This function must be\n * called outside of parallel region.\n *\n * OpenCV will try to run its functions with specified threads number, but some behaviour differs from\n * framework:\n *\n * `TBB` - User-defined parallel constructions will run with the same threads number, if another is not\n * specified. If later on user creates his own scheduler, OpenCV will use it.\n * `OpenMP` - No special defined behaviour.\n * `Concurrency` - If threads == 1, OpenCV will disable threading optimizations and run its functions\n * sequentially.\n * `GCD` - Supports only values <= 0.\n * `C=` - No special defined behaviour.\n *\n * [getNumThreads], [getThreadNum]\n *\n * @param nthreads Number of threads used by OpenCV.\n */\nexport declare function setNumThreads(nthreads: int): void;\n/**\n * The function can be used to dynamically turn on and off optimized dispatched code (code that uses\n * SSE4.2, AVX/AVX2, and other instructions on the platforms that support it). It sets a global flag\n * that is further checked by OpenCV functions. Since the flag is not checked in the inner OpenCV\n * loops, it is only safe to call the function on the very top level in your application where you can\n * be sure that no other OpenCV function is currently executed.\n *\n * By default, the optimized code is enabled unless you disable it in CMake. The current status can be\n * retrieved using useOptimized.\n *\n * @param onoff The boolean flag specifying whether the optimized code should be used (onoff=true) or\n * not (onoff=false).\n */\nexport declare function setUseOptimized(onoff: bool): void;\nexport declare function tempfile(suffix?: any): String;\nexport declare function testAsyncArray(argument: InputArray): AsyncArray;\nexport declare function testAsyncException(): AsyncArray;\n/**\n * The function returns true if the optimized code is enabled. Otherwise, it returns false.\n */\nexport declare function useOptimized(): bool;\nexport declare const CPU_MMX: CpuFeatures;\nexport declare const CPU_SSE: CpuFeatures;\nexport declare const CPU_SSE2: CpuFeatures;\nexport declare const CPU_SSE3: CpuFeatures;\nexport declare const CPU_SSSE3: CpuFeatures;\nexport declare const CPU_SSE4_1: CpuFeatures;\nexport declare const CPU_SSE4_2: CpuFeatures;\nexport declare const CPU_POPCNT: CpuFeatures;\nexport declare const CPU_FP16: CpuFeatures;\nexport declare const CPU_AVX: CpuFeatures;\nexport declare const CPU_AVX2: CpuFeatures;\nexport declare const CPU_FMA3: CpuFeatures;\nexport declare const CPU_AVX_512F: CpuFeatures;\nexport declare const CPU_AVX_512BW: CpuFeatures;\nexport declare const CPU_AVX_512CD: CpuFeatures;\nexport declare const CPU_AVX_512DQ: CpuFeatures;\nexport declare const CPU_AVX_512ER: CpuFeatures;\nexport declare const CPU_AVX_512IFMA512: CpuFeatures;\nexport declare const CPU_AVX_512IFMA: CpuFeatures;\nexport declare const CPU_AVX_512PF: CpuFeatures;\nexport declare const CPU_AVX_512VBMI: CpuFeatures;\nexport declare const CPU_AVX_512VL: CpuFeatures;\nexport declare const CPU_AVX_512VBMI2: CpuFeatures;\nexport declare const CPU_AVX_512VNNI: CpuFeatures;\nexport declare const CPU_AVX_512BITALG: CpuFeatures;\nexport declare const CPU_AVX_512VPOPCNTDQ: CpuFeatures;\nexport declare const CPU_AVX_5124VNNIW: CpuFeatures;\nexport declare const CPU_AVX_5124FMAPS: CpuFeatures;\nexport declare const CPU_NEON: CpuFeatures;\nexport declare const CPU_VSX: CpuFeatures;\nexport declare const CPU_VSX3: CpuFeatures;\nexport declare const CPU_AVX512_SKX: CpuFeatures;\nexport declare const CPU_AVX512_COMMON: CpuFeatures;\nexport declare const CPU_AVX512_KNL: CpuFeatures;\nexport declare const CPU_AVX512_KNM: CpuFeatures;\nexport declare const CPU_AVX512_CNL: CpuFeatures;\nexport declare const CPU_AVX512_CEL: CpuFeatures;\nexport declare const CPU_AVX512_ICL: CpuFeatures;\nexport declare const CPU_MAX_FEATURE: CpuFeatures;\nexport declare const SORT_EVERY_ROW: SortFlags;\n/**\n * each matrix column is sorted independently; this flag and the previous one are mutually exclusive.\n *\n */\nexport declare const SORT_EVERY_COLUMN: SortFlags;\n/**\n * each matrix row is sorted in the ascending order.\n *\n */\nexport declare const SORT_ASCENDING: SortFlags;\n/**\n * each matrix row is sorted in the descending order; this flag and the previous one are also mutually\n * exclusive.\n *\n */\nexport declare const SORT_DESCENDING: SortFlags;\nexport declare type CpuFeatures = any;\nexport declare type SortFlags = any;\n"},"node_modules_mirada_dist_src_types_opencv_core_array_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_core_array_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/core_array.d.ts","content":"import { bool, double, InputArray, InputArrayOfArrays, InputOutputArray, InputOutputArrayOfArrays, int, Mat, OutputArray, OutputArrayOfArrays, Scalar, size_t } from './_types';\n/**\n * The function [cv::absdiff] calculates: Absolute difference between two arrays when they have the\n * same size and type: `\\\\[\\\\texttt{dst}(I) = \\\\texttt{saturate} (| \\\\texttt{src1}(I) -\n * \\\\texttt{src2}(I)|)\\\\]` Absolute difference between an array and a scalar when the second array is\n * constructed from Scalar or has as many elements as the number of channels in `src1`:\n * `\\\\[\\\\texttt{dst}(I) = \\\\texttt{saturate} (| \\\\texttt{src1}(I) - \\\\texttt{src2} |)\\\\]` Absolute\n * difference between a scalar and an array when the first array is constructed from Scalar or has as\n * many elements as the number of channels in `src2`: `\\\\[\\\\texttt{dst}(I) = \\\\texttt{saturate} (|\n * \\\\texttt{src1} - \\\\texttt{src2}(I) |)\\\\]` where I is a multi-dimensional index of array elements. In\n * case of multi-channel arrays, each channel is processed independently.\n *\n * Saturation is not applied when the arrays have the depth CV_32S. You may even get a negative value\n * in the case of overflow.\n *\n * cv::abs(const Mat&)\n *\n * @param src1 first input array or a scalar.\n *\n * @param src2 second input array or a scalar.\n *\n * @param dst output array that has the same size and type as input arrays.\n */\nexport declare function absdiff(src1: InputArray, src2: InputArray, dst: OutputArray): void;\n/**\n * The function add calculates:\n *\n * Sum of two arrays when both input arrays have the same size and the same number of channels:\n * `\\\\[\\\\texttt{dst}(I) = \\\\texttt{saturate} ( \\\\texttt{src1}(I) + \\\\texttt{src2}(I)) \\\\quad\n * \\\\texttt{if mask}(I) \\\\ne0\\\\]`\n * Sum of an array and a scalar when src2 is constructed from Scalar or has the same number of elements\n * as `src1.channels()`: `\\\\[\\\\texttt{dst}(I) = \\\\texttt{saturate} ( \\\\texttt{src1}(I) + \\\\texttt{src2}\n * ) \\\\quad \\\\texttt{if mask}(I) \\\\ne0\\\\]`\n * Sum of a scalar and an array when src1 is constructed from Scalar or has the same number of elements\n * as `src2.channels()`: `\\\\[\\\\texttt{dst}(I) = \\\\texttt{saturate} ( \\\\texttt{src1} + \\\\texttt{src2}(I)\n * ) \\\\quad \\\\texttt{if mask}(I) \\\\ne0\\\\]` where `I` is a multi-dimensional index of array elements. In\n * case of multi-channel arrays, each channel is processed independently.\n *\n * The first function in the list above can be replaced with matrix expressions:\n *\n * ```cpp\n * dst = src1 + src2;\n * dst += src1; // equivalent to add(dst, src1, dst);\n * ```\n *\n *  The input arrays and the output array can all have the same or different depths. For example, you\n * can add a 16-bit unsigned array to a 8-bit signed array and store the sum as a 32-bit floating-point\n * array. Depth of the output array is determined by the dtype parameter. In the second and third cases\n * above, as well as in the first case, when src1.depth() == src2.depth(), dtype can be set to the\n * default -1. In this case, the output array will have the same depth as the input array, be it src1,\n * src2 or both.\n *\n * Saturation is not applied when the output array has the depth CV_32S. You may even get result of an\n * incorrect sign in the case of overflow.\n *\n * [subtract], [addWeighted], [scaleAdd], [Mat::convertTo]\n *\n * @param src1 first input array or a scalar.\n *\n * @param src2 second input array or a scalar.\n *\n * @param dst output array that has the same size and number of channels as the input array(s); the\n * depth is defined by dtype or src1/src2.\n *\n * @param mask optional operation mask - 8-bit single channel array, that specifies elements of the\n * output array to be changed.\n *\n * @param dtype optional depth of the output array (see the discussion below).\n */\nexport declare function add(src1: InputArray, src2: InputArray, dst: OutputArray, mask?: InputArray, dtype?: int): void;\n/**\n * The function addWeighted calculates the weighted sum of two arrays as follows: `\\\\[\\\\texttt{dst}\n * (I)= \\\\texttt{saturate} ( \\\\texttt{src1} (I)* \\\\texttt{alpha} + \\\\texttt{src2} (I)* \\\\texttt{beta} +\n * \\\\texttt{gamma} )\\\\]` where I is a multi-dimensional index of array elements. In case of\n * multi-channel arrays, each channel is processed independently. The function can be replaced with a\n * matrix expression:\n *\n * ```cpp\n * dst = src1*alpha + src2*beta + gamma;\n * ```\n *\n * Saturation is not applied when the output array has the depth CV_32S. You may even get result of an\n * incorrect sign in the case of overflow.\n *\n * [add], [subtract], [scaleAdd], [Mat::convertTo]\n *\n * @param src1 first input array.\n *\n * @param alpha weight of the first array elements.\n *\n * @param src2 second input array of the same size and channel number as src1.\n *\n * @param beta weight of the second array elements.\n *\n * @param gamma scalar added to each sum.\n *\n * @param dst output array that has the same size and number of channels as the input arrays.\n *\n * @param dtype optional depth of the output array; when both input arrays have the same depth, dtype\n * can be set to -1, which will be equivalent to src1.depth().\n */\nexport declare function addWeighted(src1: InputArray, alpha: double, src2: InputArray, beta: double, gamma: double, dst: OutputArray, dtype?: int): void;\n/**\n * see\n */\nexport declare function batchDistance(src1: InputArray, src2: InputArray, dist: OutputArray, dtype: int, nidx: OutputArray, normType?: int, K?: int, mask?: InputArray, update?: int, crosscheck?: bool): void;\n/**\n * The function [cv::bitwise_and] calculates the per-element bit-wise logical conjunction for: Two\n * arrays when src1 and src2 have the same size: `\\\\[\\\\texttt{dst} (I) = \\\\texttt{src1} (I) \\\\wedge\n * \\\\texttt{src2} (I) \\\\quad \\\\texttt{if mask} (I) \\\\ne0\\\\]` An array and a scalar when src2 is\n * constructed from Scalar or has the same number of elements as `src1.channels()`: `\\\\[\\\\texttt{dst}\n * (I) = \\\\texttt{src1} (I) \\\\wedge \\\\texttt{src2} \\\\quad \\\\texttt{if mask} (I) \\\\ne0\\\\]` A scalar and\n * an array when src1 is constructed from Scalar or has the same number of elements as\n * `src2.channels()`: `\\\\[\\\\texttt{dst} (I) = \\\\texttt{src1} \\\\wedge \\\\texttt{src2} (I) \\\\quad\n * \\\\texttt{if mask} (I) \\\\ne0\\\\]` In case of floating-point arrays, their machine-specific bit\n * representations (usually IEEE754-compliant) are used for the operation. In case of multi-channel\n * arrays, each channel is processed independently. In the second and third cases above, the scalar is\n * first converted to the array type.\n *\n * @param src1 first input array or a scalar.\n *\n * @param src2 second input array or a scalar.\n *\n * @param dst output array that has the same size and type as the input arrays.\n *\n * @param mask optional operation mask, 8-bit single channel array, that specifies elements of the\n * output array to be changed.\n */\nexport declare function bitwise_and(src1: InputArray, src2: InputArray, dst: OutputArray, mask?: InputArray): void;\n/**\n * The function [cv::bitwise_not] calculates per-element bit-wise inversion of the input array:\n * `\\\\[\\\\texttt{dst} (I) = \\\\neg \\\\texttt{src} (I)\\\\]` In case of a floating-point input array, its\n * machine-specific bit representation (usually IEEE754-compliant) is used for the operation. In case\n * of multi-channel arrays, each channel is processed independently.\n *\n * @param src input array.\n *\n * @param dst output array that has the same size and type as the input array.\n *\n * @param mask optional operation mask, 8-bit single channel array, that specifies elements of the\n * output array to be changed.\n */\nexport declare function bitwise_not(src: InputArray, dst: OutputArray, mask?: InputArray): void;\n/**\n * The function [cv::bitwise_or] calculates the per-element bit-wise logical disjunction for: Two\n * arrays when src1 and src2 have the same size: `\\\\[\\\\texttt{dst} (I) = \\\\texttt{src1} (I) \\\\vee\n * \\\\texttt{src2} (I) \\\\quad \\\\texttt{if mask} (I) \\\\ne0\\\\]` An array and a scalar when src2 is\n * constructed from Scalar or has the same number of elements as `src1.channels()`: `\\\\[\\\\texttt{dst}\n * (I) = \\\\texttt{src1} (I) \\\\vee \\\\texttt{src2} \\\\quad \\\\texttt{if mask} (I) \\\\ne0\\\\]` A scalar and an\n * array when src1 is constructed from Scalar or has the same number of elements as `src2.channels()`:\n * `\\\\[\\\\texttt{dst} (I) = \\\\texttt{src1} \\\\vee \\\\texttt{src2} (I) \\\\quad \\\\texttt{if mask} (I)\n * \\\\ne0\\\\]` In case of floating-point arrays, their machine-specific bit representations (usually\n * IEEE754-compliant) are used for the operation. In case of multi-channel arrays, each channel is\n * processed independently. In the second and third cases above, the scalar is first converted to the\n * array type.\n *\n * @param src1 first input array or a scalar.\n *\n * @param src2 second input array or a scalar.\n *\n * @param dst output array that has the same size and type as the input arrays.\n *\n * @param mask optional operation mask, 8-bit single channel array, that specifies elements of the\n * output array to be changed.\n */\nexport declare function bitwise_or(src1: InputArray, src2: InputArray, dst: OutputArray, mask?: InputArray): void;\n/**\n * The function [cv::bitwise_xor] calculates the per-element bit-wise logical \"exclusive-or\" operation\n * for: Two arrays when src1 and src2 have the same size: `\\\\[\\\\texttt{dst} (I) = \\\\texttt{src1} (I)\n * \\\\oplus \\\\texttt{src2} (I) \\\\quad \\\\texttt{if mask} (I) \\\\ne0\\\\]` An array and a scalar when src2 is\n * constructed from Scalar or has the same number of elements as `src1.channels()`: `\\\\[\\\\texttt{dst}\n * (I) = \\\\texttt{src1} (I) \\\\oplus \\\\texttt{src2} \\\\quad \\\\texttt{if mask} (I) \\\\ne0\\\\]` A scalar and\n * an array when src1 is constructed from Scalar or has the same number of elements as\n * `src2.channels()`: `\\\\[\\\\texttt{dst} (I) = \\\\texttt{src1} \\\\oplus \\\\texttt{src2} (I) \\\\quad\n * \\\\texttt{if mask} (I) \\\\ne0\\\\]` In case of floating-point arrays, their machine-specific bit\n * representations (usually IEEE754-compliant) are used for the operation. In case of multi-channel\n * arrays, each channel is processed independently. In the 2nd and 3rd cases above, the scalar is first\n * converted to the array type.\n *\n * @param src1 first input array or a scalar.\n *\n * @param src2 second input array or a scalar.\n *\n * @param dst output array that has the same size and type as the input arrays.\n *\n * @param mask optional operation mask, 8-bit single channel array, that specifies elements of the\n * output array to be changed.\n */\nexport declare function bitwise_xor(src1: InputArray, src2: InputArray, dst: OutputArray, mask?: InputArray): void;\n/**\n * The function computes and returns the coordinate of a donor pixel corresponding to the specified\n * extrapolated pixel when using the specified extrapolation border mode. For example, if you use\n * [cv::BORDER_WRAP] mode in the horizontal direction, [cv::BORDER_REFLECT_101] in the vertical\n * direction and want to compute value of the \"virtual\" pixel Point(-5, 100) in a floating-point image\n * img , it looks like:\n *\n * ```cpp\n * float val = img.at<float>(borderInterpolate(100, img.rows, cv::BORDER_REFLECT_101),\n *                           borderInterpolate(-5, img.cols, cv::BORDER_WRAP));\n * ```\n *\n *  Normally, the function is not called directly. It is used inside filtering functions and also in\n * copyMakeBorder.\n *\n * [copyMakeBorder]\n *\n * @param p 0-based coordinate of the extrapolated pixel along one of the axes, likely <0 or >= len\n *\n * @param len Length of the array along the corresponding axis.\n *\n * @param borderType Border type, one of the BorderTypes, except for BORDER_TRANSPARENT and\n * BORDER_ISOLATED . When borderType==BORDER_CONSTANT , the function always returns -1, regardless of p\n * and len.\n */\nexport declare function borderInterpolate(p: int, len: int, borderType: int): int;\n/**\n * The function [cv::calcCovarMatrix] calculates the covariance matrix and, optionally, the mean vector\n * of the set of input vectors.\n *\n * [PCA], [mulTransposed], [Mahalanobis]\n *\n * @param samples samples stored as separate matrices\n *\n * @param nsamples number of samples\n *\n * @param covar output covariance matrix of the type ctype and square size.\n *\n * @param mean input or output (depending on the flags) array as the average value of the input\n * vectors.\n *\n * @param flags operation flags as a combination of CovarFlags\n *\n * @param ctype type of the matrixl; it equals 'CV_64F' by default.\n */\nexport declare function calcCovarMatrix(samples: any, nsamples: int, covar: any, mean: any, flags: int, ctype?: int): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n *\n * use [COVAR_ROWS] or [COVAR_COLS] flag\n *\n * @param samples samples stored as rows/columns of a single matrix.\n *\n * @param covar output covariance matrix of the type ctype and square size.\n *\n * @param mean input or output (depending on the flags) array as the average value of the input\n * vectors.\n *\n * @param flags operation flags as a combination of CovarFlags\n *\n * @param ctype type of the matrixl; it equals 'CV_64F' by default.\n */\nexport declare function calcCovarMatrix(samples: InputArray, covar: OutputArray, mean: InputOutputArray, flags: int, ctype?: int): void;\n/**\n * The function [cv::cartToPolar] calculates either the magnitude, angle, or both for every 2D vector\n * (x(I),y(I)): `\\\\[\\\\begin{array}{l} \\\\texttt{magnitude} (I)=\n * \\\\sqrt{\\\\texttt{x}(I)^2+\\\\texttt{y}(I)^2} , \\\\\\\\ \\\\texttt{angle} (I)= \\\\texttt{atan2} ( \\\\texttt{y}\n * (I), \\\\texttt{x} (I))[ \\\\cdot180 / \\\\pi ] \\\\end{array}\\\\]`\n *\n * The angles are calculated with accuracy about 0.3 degrees. For the point (0,0), the angle is set to\n * 0.\n *\n * [Sobel], [Scharr]\n *\n * @param x array of x-coordinates; this must be a single-precision or double-precision floating-point\n * array.\n *\n * @param y array of y-coordinates, that must have the same size and same type as x.\n *\n * @param magnitude output array of magnitudes of the same size and type as x.\n *\n * @param angle output array of angles that has the same size and type as x; the angles are measured in\n * radians (from 0 to 2*Pi) or in degrees (0 to 360 degrees).\n *\n * @param angleInDegrees a flag, indicating whether the angles are measured in radians (which is by\n * default), or in degrees.\n */\nexport declare function cartToPolar(x: InputArray, y: InputArray, magnitude: OutputArray, angle: OutputArray, angleInDegrees?: bool): void;\n/**\n * The function [cv::checkRange] checks that every array element is neither NaN nor infinite. When\n * minVal > -DBL_MAX and maxVal < DBL_MAX, the function also checks that each value is between minVal\n * and maxVal. In case of multi-channel arrays, each channel is processed independently. If some values\n * are out of range, position of the first outlier is stored in pos (when pos != NULL). Then, the\n * function either returns false (when quiet=true) or throws an exception.\n *\n * @param a input array.\n *\n * @param quiet a flag, indicating whether the functions quietly return false when the array elements\n * are out of range or they throw an exception.\n *\n * @param pos optional output parameter, when not NULL, must be a pointer to array of src.dims\n * elements.\n *\n * @param minVal inclusive lower boundary of valid values range.\n *\n * @param maxVal exclusive upper boundary of valid values range.\n */\nexport declare function checkRange(a: InputArray, quiet?: bool, pos?: any, minVal?: double, maxVal?: double): bool;\n/**\n * The function compares: Elements of two arrays when src1 and src2 have the same size:\n * `\\\\[\\\\texttt{dst} (I) = \\\\texttt{src1} (I) \\\\,\\\\texttt{cmpop}\\\\, \\\\texttt{src2} (I)\\\\]` Elements of\n * src1 with a scalar src2 when src2 is constructed from Scalar or has a single element:\n * `\\\\[\\\\texttt{dst} (I) = \\\\texttt{src1}(I) \\\\,\\\\texttt{cmpop}\\\\, \\\\texttt{src2}\\\\]` src1 with\n * elements of src2 when src1 is constructed from Scalar or has a single element: `\\\\[\\\\texttt{dst} (I)\n * = \\\\texttt{src1} \\\\,\\\\texttt{cmpop}\\\\, \\\\texttt{src2} (I)\\\\]` When the comparison result is true,\n * the corresponding element of output array is set to 255. The comparison operations can be replaced\n * with the equivalent matrix expressions:\n *\n * ```cpp\n * Mat dst1 = src1 >= src2;\n * Mat dst2 = src1 < 8;\n * ...\n * ```\n *\n * [checkRange], [min], [max], [threshold]\n *\n * @param src1 first input array or a scalar; when it is an array, it must have a single channel.\n *\n * @param src2 second input array or a scalar; when it is an array, it must have a single channel.\n *\n * @param dst output array of type ref CV_8U that has the same size and the same number of channels as\n * the input arrays.\n *\n * @param cmpop a flag, that specifies correspondence between the arrays (cv::CmpTypes)\n */\nexport declare function compare(src1: InputArray, src2: InputArray, dst: OutputArray, cmpop: int): void;\n/**\n * The function [cv::completeSymm] copies the lower or the upper half of a square matrix to its another\n * half. The matrix diagonal remains unchanged:\n *\n * `$\\\\texttt{m}_{ij}=\\\\texttt{m}_{ji}$` for `$i > j$` if lowerToUpper=false\n * `$\\\\texttt{m}_{ij}=\\\\texttt{m}_{ji}$` for `$i < j$` if lowerToUpper=true\n *\n * [flip], [transpose]\n *\n * @param m input-output floating-point square matrix.\n *\n * @param lowerToUpper operation flag; if true, the lower half is copied to the upper half. Otherwise,\n * the upper half is copied to the lower half.\n */\nexport declare function completeSymm(m: InputOutputArray, lowerToUpper?: bool): void;\n/**\n * This function converts FP32 (single precision floating point) from/to FP16 (half precision floating\n * point). CV_16S format is used to represent FP16 data. There are two use modes (src -> dst): CV_32F\n * -> CV_16S and CV_16S -> CV_32F. The input array has to have type of CV_32F or CV_16S to represent\n * the bit depth. If the input array is neither of them, the function will raise an error. The format\n * of half precision floating point is defined in IEEE 754-2008.\n *\n * @param src input array.\n *\n * @param dst output array.\n */\nexport declare function convertFp16(src: InputArray, dst: OutputArray): void;\n/**\n * On each element of the input array, the function convertScaleAbs performs three operations\n * sequentially: scaling, taking an absolute value, conversion to an unsigned 8-bit type:\n * `\\\\[\\\\texttt{dst} (I)= \\\\texttt{saturate\\\\_cast<uchar>} (| \\\\texttt{src} (I)* \\\\texttt{alpha} +\n * \\\\texttt{beta} |)\\\\]` In case of multi-channel arrays, the function processes each channel\n * independently. When the output is not 8-bit, the operation can be emulated by calling the\n * [Mat::convertTo] method (or by using matrix expressions) and then by calculating an absolute value\n * of the result. For example:\n *\n * ```cpp\n * Mat_<float> A(30,30);\n * randu(A, Scalar(-100), Scalar(100));\n * Mat_<float> B = A*5 + 3;\n * B = abs(B);\n * // Mat_<float> B = abs(A*5+3) will also do the job,\n * // but it will allocate a temporary matrix\n * ```\n *\n * [Mat::convertTo], cv::abs(const Mat&)\n *\n * @param src input array.\n *\n * @param dst output array.\n *\n * @param alpha optional scale factor.\n *\n * @param beta optional delta added to the scaled values.\n */\nexport declare function convertScaleAbs(src: InputArray, dst: OutputArray, alpha?: double, beta?: double): void;\n/**\n * The function copies the source image into the middle of the destination image. The areas to the\n * left, to the right, above and below the copied source image will be filled with extrapolated pixels.\n * This is not what filtering functions based on it do (they extrapolate pixels on-fly), but what other\n * more complex functions, including your own, may do to simplify image boundary handling.\n *\n * The function supports the mode when src is already in the middle of dst . In this case, the function\n * does not copy src itself but simply constructs the border, for example:\n *\n * ```cpp\n * // let border be the same in all directions\n * int border=2;\n * // constructs a larger image to fit both the image and the border\n * Mat gray_buf(rgb.rows + border*2, rgb.cols + border*2, rgb.depth());\n * // select the middle part of it w/o copying data\n * Mat gray(gray_canvas, Rect(border, border, rgb.cols, rgb.rows));\n * // convert image from RGB to grayscale\n * cvtColor(rgb, gray, COLOR_RGB2GRAY);\n * // form a border in-place\n * copyMakeBorder(gray, gray_buf, border, border,\n *                border, border, BORDER_REPLICATE);\n * // now do some custom filtering ...\n * ...\n * ```\n *\n * When the source image is a part (ROI) of a bigger image, the function will try to use the pixels\n * outside of the ROI to form a border. To disable this feature and always do extrapolation, as if src\n * was not a ROI, use borderType | [BORDER_ISOLATED].\n *\n * [borderInterpolate]\n *\n * @param src Source image.\n *\n * @param dst Destination image of the same type as src and the size Size(src.cols+left+right,\n * src.rows+top+bottom) .\n *\n * @param top the top pixels\n *\n * @param bottom the bottom pixels\n *\n * @param left the left pixels\n *\n * @param right Parameter specifying how many pixels in each direction from the source image rectangle\n * to extrapolate. For example, top=1, bottom=1, left=1, right=1 mean that 1 pixel-wide border needs to\n * be built.\n *\n * @param borderType Border type. See borderInterpolate for details.\n *\n * @param value Border value if borderType==BORDER_CONSTANT .\n */\nexport declare function copyMakeBorder(src: InputArray, dst: OutputArray, top: int, bottom: int, left: int, right: int, borderType: int, value?: any): void;\n/**\n * @param src source matrix.\n *\n * @param dst Destination matrix. If it does not have a proper size or type before the operation, it is\n * reallocated.\n *\n * @param mask Operation mask of the same size as *this. Its non-zero elements indicate which matrix\n * elements need to be copied. The mask has to be of type CV_8U and can have 1 or multiple channels.\n */\nexport declare function copyTo(src: InputArray, dst: OutputArray, mask: InputArray): void;\n/**\n * The function returns the number of non-zero elements in src : `\\\\[\\\\sum _{I: \\\\; \\\\texttt{src} (I)\n * \\\\ne0 } 1\\\\]`\n *\n * [mean], [meanStdDev], [norm], [minMaxLoc], [calcCovarMatrix]\n *\n * @param src single-channel array.\n */\nexport declare function countNonZero(src: InputArray): int;\n/**\n * The function [cv::dct] performs a forward or inverse discrete Cosine transform (DCT) of a 1D or 2D\n * floating-point array:\n *\n * Forward Cosine transform of a 1D vector of N elements: `\\\\[Y = C^{(N)} \\\\cdot X\\\\]` where\n * `\\\\[C^{(N)}_{jk}= \\\\sqrt{\\\\alpha_j/N} \\\\cos \\\\left ( \\\\frac{\\\\pi(2k+1)j}{2N} \\\\right )\\\\]` and\n * `$\\\\alpha_0=1$`, `$\\\\alpha_j=2$` for *j > 0*.\n * Inverse Cosine transform of a 1D vector of N elements: `\\\\[X = \\\\left (C^{(N)} \\\\right )^{-1} \\\\cdot\n * Y = \\\\left (C^{(N)} \\\\right )^T \\\\cdot Y\\\\]` (since `$C^{(N)}$` is an orthogonal matrix, `$C^{(N)}\n * \\\\cdot \\\\left(C^{(N)}\\\\right)^T = I$` )\n * Forward 2D Cosine transform of M x N matrix: `\\\\[Y = C^{(N)} \\\\cdot X \\\\cdot \\\\left (C^{(N)} \\\\right\n * )^T\\\\]`\n * Inverse 2D Cosine transform of M x N matrix: `\\\\[X = \\\\left (C^{(N)} \\\\right )^T \\\\cdot X \\\\cdot\n * C^{(N)}\\\\]`\n *\n * The function chooses the mode of operation by looking at the flags and size of the input array:\n *\n * If (flags & [DCT_INVERSE]) == 0 , the function does a forward 1D or 2D transform. Otherwise, it is\n * an inverse 1D or 2D transform.\n * If (flags & [DCT_ROWS]) != 0 , the function performs a 1D transform of each row.\n * If the array is a single column or a single row, the function performs a 1D transform.\n * If none of the above is true, the function performs a 2D transform.\n *\n * Currently dct supports even-size arrays (2, 4, 6 ...). For data analysis and approximation, you can\n * pad the array when necessary. Also, the function performance depends very much, and not\n * monotonically, on the array size (see getOptimalDFTSize ). In the current implementation DCT of a\n * vector of size N is calculated via DFT of a vector of size N/2 . Thus, the optimal DCT size N1 >= N\n * can be calculated as:\n *\n * ```cpp\n * size_t getOptimalDCTSize(size_t N) { return 2*getOptimalDFTSize((N+1)/2); }\n * N1 = getOptimalDCTSize(N);\n * ```\n *\n * [dft] , [getOptimalDFTSize] , [idct]\n *\n * @param src input floating-point array.\n *\n * @param dst output array of the same size and type as src .\n *\n * @param flags transformation flags as a combination of cv::DftFlags (DCT_*)\n */\nexport declare function dct(src: InputArray, dst: OutputArray, flags?: int): void;\n/**\n * The function [cv::determinant] calculates and returns the determinant of the specified matrix. For\n * small matrices ( mtx.cols=mtx.rows<=3 ), the direct method is used. For larger matrices, the\n * function uses LU factorization with partial pivoting.\n *\n * For symmetric positively-determined matrices, it is also possible to use eigen decomposition to\n * calculate the determinant.\n *\n * [trace], [invert], [solve], [eigen], [MatrixExpressions]\n *\n * @param mtx input matrix that must have CV_32FC1 or CV_64FC1 type and square size.\n */\nexport declare function determinant(mtx: InputArray): double;\n/**\n * The function [cv::dft] performs one of the following:\n *\n * Forward the Fourier transform of a 1D vector of N elements: `\\\\[Y = F^{(N)} \\\\cdot X,\\\\]` where\n * `$F^{(N)}_{jk}=\\\\exp(-2\\\\pi i j k/N)$` and `$i=\\\\sqrt{-1}$`\n * Inverse the Fourier transform of a 1D vector of N elements: `\\\\[\\\\begin{array}{l} X'= \\\\left\n * (F^{(N)} \\\\right )^{-1} \\\\cdot Y = \\\\left (F^{(N)} \\\\right )^* \\\\cdot y \\\\\\\\ X = (1/N) \\\\cdot X,\n * \\\\end{array}\\\\]` where `$F^*=\\\\left(\\\\textrm{Re}(F^{(N)})-\\\\textrm{Im}(F^{(N)})\\\\right)^T$`\n * Forward the 2D Fourier transform of a M x N matrix: `\\\\[Y = F^{(M)} \\\\cdot X \\\\cdot F^{(N)}\\\\]`\n * Inverse the 2D Fourier transform of a M x N matrix: `\\\\[\\\\begin{array}{l} X'= \\\\left (F^{(M)}\n * \\\\right )^* \\\\cdot Y \\\\cdot \\\\left (F^{(N)} \\\\right )^* \\\\\\\\ X = \\\\frac{1}{M \\\\cdot N} \\\\cdot X'\n * \\\\end{array}\\\\]`\n *\n * In case of real (single-channel) data, the output spectrum of the forward Fourier transform or input\n * spectrum of the inverse Fourier transform can be represented in a packed format called *CCS*\n * (complex-conjugate-symmetrical). It was borrowed from IPL (Intel* Image Processing Library). Here is\n * how 2D *CCS* spectrum looks: `\\\\[\\\\begin{bmatrix} Re Y_{0,0} & Re Y_{0,1} & Im Y_{0,1} & Re Y_{0,2}\n * & Im Y_{0,2} & \\\\cdots & Re Y_{0,N/2-1} & Im Y_{0,N/2-1} & Re Y_{0,N/2} \\\\\\\\ Re Y_{1,0} & Re Y_{1,1}\n * & Im Y_{1,1} & Re Y_{1,2} & Im Y_{1,2} & \\\\cdots & Re Y_{1,N/2-1} & Im Y_{1,N/2-1} & Re Y_{1,N/2}\n * \\\\\\\\ Im Y_{1,0} & Re Y_{2,1} & Im Y_{2,1} & Re Y_{2,2} & Im Y_{2,2} & \\\\cdots & Re Y_{2,N/2-1} & Im\n * Y_{2,N/2-1} & Im Y_{1,N/2} \\\\\\\\ \\\\hdotsfor{9} \\\\\\\\ Re Y_{M/2-1,0} & Re Y_{M-3,1} & Im Y_{M-3,1} &\n * \\\\hdotsfor{3} & Re Y_{M-3,N/2-1} & Im Y_{M-3,N/2-1}& Re Y_{M/2-1,N/2} \\\\\\\\ Im Y_{M/2-1,0} & Re\n * Y_{M-2,1} & Im Y_{M-2,1} & \\\\hdotsfor{3} & Re Y_{M-2,N/2-1} & Im Y_{M-2,N/2-1}& Im Y_{M/2-1,N/2}\n * \\\\\\\\ Re Y_{M/2,0} & Re Y_{M-1,1} & Im Y_{M-1,1} & \\\\hdotsfor{3} & Re Y_{M-1,N/2-1} & Im\n * Y_{M-1,N/2-1}& Re Y_{M/2,N/2} \\\\end{bmatrix}\\\\]`\n *\n * In case of 1D transform of a real vector, the output looks like the first row of the matrix above.\n *\n * So, the function chooses an operation mode depending on the flags and size of the input array:\n *\n * If [DFT_ROWS] is set or the input array has a single row or single column, the function performs a\n * 1D forward or inverse transform of each row of a matrix when [DFT_ROWS] is set. Otherwise, it\n * performs a 2D transform.\n * If the input array is real and [DFT_INVERSE] is not set, the function performs a forward 1D or 2D\n * transform:\n *\n * When [DFT_COMPLEX_OUTPUT] is set, the output is a complex matrix of the same size as input.\n * When [DFT_COMPLEX_OUTPUT] is not set, the output is a real matrix of the same size as input. In case\n * of 2D transform, it uses the packed format as shown above. In case of a single 1D transform, it\n * looks like the first row of the matrix above. In case of multiple 1D transforms (when using the\n * [DFT_ROWS] flag), each row of the output matrix looks like the first row of the matrix above.\n *\n * If the input array is complex and either [DFT_INVERSE] or [DFT_REAL_OUTPUT] are not set, the output\n * is a complex array of the same size as input. The function performs a forward or inverse 1D or 2D\n * transform of the whole input array or each row of the input array independently, depending on the\n * flags DFT_INVERSE and DFT_ROWS.\n * When [DFT_INVERSE] is set and the input array is real, or it is complex but [DFT_REAL_OUTPUT] is\n * set, the output is a real array of the same size as input. The function performs a 1D or 2D inverse\n * transformation of the whole input array or each individual row, depending on the flags [DFT_INVERSE]\n * and [DFT_ROWS].\n *\n * If [DFT_SCALE] is set, the scaling is done after the transformation.\n *\n * Unlike dct , the function supports arrays of arbitrary size. But only those arrays are processed\n * efficiently, whose sizes can be factorized in a product of small prime numbers (2, 3, and 5 in the\n * current implementation). Such an efficient DFT size can be calculated using the getOptimalDFTSize\n * method.\n *\n * The sample below illustrates how to calculate a DFT-based convolution of two 2D real arrays:\n *\n * ```cpp\n * void convolveDFT(InputArray A, InputArray B, OutputArray C)\n * {\n *     // reallocate the output array if needed\n *     C.create(abs(A.rows - B.rows)+1, abs(A.cols - B.cols)+1, A.type());\n *     Size dftSize;\n *     // calculate the size of DFT transform\n *     dftSize.width = getOptimalDFTSize(A.cols + B.cols - 1);\n *     dftSize.height = getOptimalDFTSize(A.rows + B.rows - 1);\n *\n *     // allocate temporary buffers and initialize them with 0's\n *     Mat tempA(dftSize, A.type(), Scalar::all(0));\n *     Mat tempB(dftSize, B.type(), Scalar::all(0));\n *\n *     // copy A and B to the top-left corners of tempA and tempB, respectively\n *     Mat roiA(tempA, Rect(0,0,A.cols,A.rows));\n *     A.copyTo(roiA);\n *     Mat roiB(tempB, Rect(0,0,B.cols,B.rows));\n *     B.copyTo(roiB);\n *\n *     // now transform the padded A & B in-place;\n *     // use \"nonzeroRows\" hint for faster processing\n *     dft(tempA, tempA, 0, A.rows);\n *     dft(tempB, tempB, 0, B.rows);\n *\n *     // multiply the spectrums;\n *     // the function handles packed spectrum representations well\n *     mulSpectrums(tempA, tempB, tempA);\n *\n *     // transform the product back from the frequency domain.\n *     // Even though all the result rows will be non-zero,\n *     // you need only the first C.rows of them, and thus you\n *     // pass nonzeroRows == C.rows\n *     dft(tempA, tempA, DFT_INVERSE + DFT_SCALE, C.rows);\n *\n *     // now copy the result back to C.\n *     tempA(Rect(0, 0, C.cols, C.rows)).copyTo(C);\n *\n *     // all the temporary buffers will be deallocated automatically\n * }\n * ```\n *\n *  To optimize this sample, consider the following approaches:\n *\n * Since nonzeroRows != 0 is passed to the forward transform calls and since A and B are copied to the\n * top-left corners of tempA and tempB, respectively, it is not necessary to clear the whole tempA and\n * tempB. It is only necessary to clear the tempA.cols - A.cols ( tempB.cols - B.cols) rightmost\n * columns of the matrices.\n * This DFT-based convolution does not have to be applied to the whole big arrays, especially if B is\n * significantly smaller than A or vice versa. Instead, you can calculate convolution by parts. To do\n * this, you need to split the output array C into multiple tiles. For each tile, estimate which parts\n * of A and B are required to calculate convolution in this tile. If the tiles in C are too small, the\n * speed will decrease a lot because of repeated work. In the ultimate case, when each tile in C is a\n * single pixel, the algorithm becomes equivalent to the naive convolution algorithm. If the tiles are\n * too big, the temporary arrays tempA and tempB become too big and there is also a slowdown because of\n * bad cache locality. So, there is an optimal tile size somewhere in the middle.\n * If different tiles in C can be calculated in parallel and, thus, the convolution is done by parts,\n * the loop can be threaded.\n *\n * All of the above improvements have been implemented in [matchTemplate] and [filter2D] . Therefore,\n * by using them, you can get the performance even better than with the above theoretically optimal\n * implementation. Though, those two functions actually calculate cross-correlation, not convolution,\n * so you need to \"flip\" the second convolution operand B vertically and horizontally using flip .\n *\n * An example using the discrete fourier transform can be found at\n * opencv_source_code/samples/cpp/dft.cpp\n * (Python) An example using the dft functionality to perform Wiener deconvolution can be found at\n * opencv_source/samples/python/deconvolution.py\n * (Python) An example rearranging the quadrants of a Fourier image can be found at\n * opencv_source/samples/python/dft.py\n *\n * [dct] , [getOptimalDFTSize] , [mulSpectrums], [filter2D] , [matchTemplate] , [flip] , [cartToPolar]\n * , [magnitude] , [phase]\n *\n * @param src input array that could be real or complex.\n *\n * @param dst output array whose size and type depends on the flags .\n *\n * @param flags transformation flags, representing a combination of the DftFlags\n *\n * @param nonzeroRows when the parameter is not zero, the function assumes that only the first\n * nonzeroRows rows of the input array (DFT_INVERSE is not set) or only the first nonzeroRows of the\n * output array (DFT_INVERSE is set) contain non-zeros, thus, the function can handle the rest of the\n * rows more efficiently and save some time; this technique is very useful for calculating array\n * cross-correlation or convolution using DFT.\n */\nexport declare function dft(src: InputArray, dst: OutputArray, flags?: int, nonzeroRows?: int): void;\n/**\n * The function [cv::divide] divides one array by another: `\\\\[\\\\texttt{dst(I) =\n * saturate(src1(I)*scale/src2(I))}\\\\]` or a scalar by an array when there is no src1 :\n * `\\\\[\\\\texttt{dst(I) = saturate(scale/src2(I))}\\\\]`\n *\n * Different channels of multi-channel arrays are processed independently.\n *\n * For integer types when src2(I) is zero, dst(I) will also be zero.\n *\n * In case of floating point data there is no special defined behavior for zero src2(I) values. Regular\n * floating-point division is used. Expect correct IEEE-754 behaviour for floating-point data (with\n * NaN, Inf result values).\n *\n * Saturation is not applied when the output array has the depth CV_32S. You may even get result of an\n * incorrect sign in the case of overflow.\n *\n * [multiply], [add], [subtract]\n *\n * @param src1 first input array.\n *\n * @param src2 second input array of the same size and type as src1.\n *\n * @param dst output array of the same size and type as src2.\n *\n * @param scale scalar factor.\n *\n * @param dtype optional depth of the output array; if -1, dst will have depth src2.depth(), but in\n * case of an array-by-array division, you can only pass -1 when src1.depth()==src2.depth().\n */\nexport declare function divide(src1: InputArray, src2: InputArray, dst: OutputArray, scale?: double, dtype?: int): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function divide(scale: double, src2: InputArray, dst: OutputArray, dtype?: int): void;\n/**\n * The function [cv::eigen] calculates just eigenvalues, or eigenvalues and eigenvectors of the\n * symmetric matrix src:\n *\n * ```cpp\n * src*eigenvectors.row(i).t() = eigenvalues.at<srcType>(i)*eigenvectors.row(i).t()\n * ```\n *\n * Use [cv::eigenNonSymmetric] for calculation of real eigenvalues and eigenvectors of non-symmetric\n * matrix.\n *\n * [eigenNonSymmetric], [completeSymm] , [PCA]\n *\n * @param src input matrix that must have CV_32FC1 or CV_64FC1 type, square size and be symmetrical\n * (src ^T^ == src).\n *\n * @param eigenvalues output vector of eigenvalues of the same type as src; the eigenvalues are stored\n * in the descending order.\n *\n * @param eigenvectors output matrix of eigenvectors; it has the same size and type as src; the\n * eigenvectors are stored as subsequent matrix rows, in the same order as the corresponding\n * eigenvalues.\n */\nexport declare function eigen(src: InputArray, eigenvalues: OutputArray, eigenvectors?: OutputArray): bool;\n/**\n * Assumes real eigenvalues.\n * The function calculates eigenvalues and eigenvectors (optional) of the square matrix src:\n *\n * ```cpp\n * src*eigenvectors.row(i).t() = eigenvalues.at<srcType>(i)*eigenvectors.row(i).t()\n * ```\n *\n * [eigen]\n *\n * @param src input matrix (CV_32FC1 or CV_64FC1 type).\n *\n * @param eigenvalues output vector of eigenvalues (type is the same type as src).\n *\n * @param eigenvectors output matrix of eigenvectors (type is the same type as src). The eigenvectors\n * are stored as subsequent matrix rows, in the same order as the corresponding eigenvalues.\n */\nexport declare function eigenNonSymmetric(src: InputArray, eigenvalues: OutputArray, eigenvectors: OutputArray): void;\n/**\n * The function [cv::exp] calculates the exponent of every element of the input array:\n * `\\\\[\\\\texttt{dst} [I] = e^{ src(I) }\\\\]`\n *\n * The maximum relative error is about 7e-6 for single-precision input and less than 1e-10 for\n * double-precision input. Currently, the function converts denormalized values to zeros on output.\n * Special values (NaN, Inf) are not handled.\n *\n * [log] , [cartToPolar] , [polarToCart] , [phase] , [pow] , [sqrt] , [magnitude]\n *\n * @param src input array.\n *\n * @param dst output array of the same size and type as src.\n */\nexport declare function exp(src: InputArray, dst: OutputArray): void;\n/**\n * [mixChannels], [split]\n *\n * @param src input array\n *\n * @param dst output array\n *\n * @param coi index of channel to extract\n */\nexport declare function extractChannel(src: InputArray, dst: OutputArray, coi: int): void;\n/**\n * Given a binary matrix (likely returned from an operation such as [threshold()], [compare()], >, ==,\n * etc, return all of the non-zero indices as a [cv::Mat] or std::vector<cv::Point> (x,y) For example:\n *\n * ```cpp\n * cv::Mat binaryImage; // input, binary image\n * cv::Mat locations;   // output, locations of non-zero pixels\n * cv::findNonZero(binaryImage, locations);\n *\n * // access pixel coordinates\n * Point pnt = locations.at<Point>(i);\n * ```\n *\n *  or\n *\n * ```cpp\n * cv::Mat binaryImage; // input, binary image\n * vector<Point> locations;   // output, locations of non-zero pixels\n * cv::findNonZero(binaryImage, locations);\n *\n * // access pixel coordinates\n * Point pnt = locations[i];\n * ```\n *\n * @param src single-channel array\n *\n * @param idx the output array, type of cv::Mat or std::vector<Point>, corresponding to non-zero\n * indices in the input\n */\nexport declare function findNonZero(src: InputArray, idx: OutputArray): void;\n/**\n * The function [cv::flip] flips the array in one of three different ways (row and column indices are\n * 0-based): `\\\\[\\\\texttt{dst} _{ij} = \\\\left\\\\{ \\\\begin{array}{l l} \\\\texttt{src}\n * _{\\\\texttt{src.rows}-i-1,j} & if\\\\; \\\\texttt{flipCode} = 0 \\\\\\\\ \\\\texttt{src} _{i,\n * \\\\texttt{src.cols} -j-1} & if\\\\; \\\\texttt{flipCode} > 0 \\\\\\\\ \\\\texttt{src} _{ \\\\texttt{src.rows}\n * -i-1, \\\\texttt{src.cols} -j-1} & if\\\\; \\\\texttt{flipCode} < 0 \\\\\\\\ \\\\end{array} \\\\right.\\\\]` The\n * example scenarios of using the function are the following: Vertical flipping of the image (flipCode\n * == 0) to switch between top-left and bottom-left image origin. This is a typical operation in video\n * processing on Microsoft Windows* OS. Horizontal flipping of the image with the subsequent horizontal\n * shift and absolute difference calculation to check for a vertical-axis symmetry (flipCode > 0).\n * Simultaneous horizontal and vertical flipping of the image with the subsequent shift and absolute\n * difference calculation to check for a central symmetry (flipCode < 0). Reversing the order of point\n * arrays (flipCode > 0 or flipCode == 0).\n *\n * [transpose] , [repeat] , [completeSymm]\n *\n * @param src input array.\n *\n * @param dst output array of the same size and type as src.\n *\n * @param flipCode a flag to specify how to flip the array; 0 means flipping around the x-axis and\n * positive value (for example, 1) means flipping around y-axis. Negative value (for example, -1) means\n * flipping around both axes.\n */\nexport declare function flip(src: InputArray, dst: OutputArray, flipCode: int): void;\n/**\n * The function [cv::gemm] performs generalized matrix multiplication similar to the gemm functions in\n * BLAS level 3. For example, `gemm(src1, src2, alpha, src3, beta, dst, GEMM_1_T + GEMM_3_T)`\n * corresponds to `\\\\[\\\\texttt{dst} = \\\\texttt{alpha} \\\\cdot \\\\texttt{src1} ^T \\\\cdot \\\\texttt{src2} +\n * \\\\texttt{beta} \\\\cdot \\\\texttt{src3} ^T\\\\]`\n *\n * In case of complex (two-channel) data, performed a complex matrix multiplication.\n *\n * The function can be replaced with a matrix expression. For example, the above call can be replaced\n * with:\n *\n * ```cpp\n * dst = alpha*src1.t()*src2 + beta*src3.t();\n * ```\n *\n * [mulTransposed] , [transform]\n *\n * @param src1 first multiplied input matrix that could be real(CV_32FC1, CV_64FC1) or\n * complex(CV_32FC2, CV_64FC2).\n *\n * @param src2 second multiplied input matrix of the same type as src1.\n *\n * @param alpha weight of the matrix product.\n *\n * @param src3 third optional delta matrix added to the matrix product; it should have the same type as\n * src1 and src2.\n *\n * @param beta weight of src3.\n *\n * @param dst output matrix; it has the proper size and the same type as input matrices.\n *\n * @param flags operation flags (cv::GemmFlags)\n */\nexport declare function gemm(src1: InputArray, src2: InputArray, alpha: double, src3: InputArray, beta: double, dst: OutputArray, flags?: int): void;\n/**\n * DFT performance is not a monotonic function of a vector size. Therefore, when you calculate\n * convolution of two arrays or perform the spectral analysis of an array, it usually makes sense to\n * pad the input data with zeros to get a bit larger array that can be transformed much faster than the\n * original one. Arrays whose size is a power-of-two (2, 4, 8, 16, 32, ...) are the fastest to process.\n * Though, the arrays whose size is a product of 2's, 3's, and 5's (for example, 300 = 5*5*3*2*2) are\n * also processed quite efficiently.\n *\n * The function [cv::getOptimalDFTSize] returns the minimum number N that is greater than or equal to\n * vecsize so that the DFT of a vector of size N can be processed efficiently. In the current\n * implementation N = 2 ^p^ * 3 ^q^ * 5 ^r^ for some integer p, q, r.\n *\n * The function returns a negative number if vecsize is too large (very close to INT_MAX ).\n *\n * While the function cannot be used directly to estimate the optimal vector size for DCT transform\n * (since the current DCT implementation supports only even-size vectors), it can be easily processed\n * as getOptimalDFTSize((vecsize+1)/2)*2.\n *\n * [dft] , [dct] , [idft] , [idct] , [mulSpectrums]\n *\n * @param vecsize vector size.\n */\nexport declare function getOptimalDFTSize(vecsize: int): int;\n/**\n * The function horizontally concatenates two or more [cv::Mat] matrices (with the same number of\n * rows).\n *\n * ```cpp\n * cv::Mat matArray[] = { cv::Mat(4, 1, CV_8UC1, cv::Scalar(1)),\n *                        cv::Mat(4, 1, CV_8UC1, cv::Scalar(2)),\n *                        cv::Mat(4, 1, CV_8UC1, cv::Scalar(3)),};\n *\n * cv::Mat out;\n * cv::hconcat( matArray, 3, out );\n * //out:\n * //[1, 2, 3;\n * // 1, 2, 3;\n * // 1, 2, 3;\n * // 1, 2, 3]\n * ```\n *\n * [cv::vconcat(const Mat*, size_t, OutputArray)],\n *\n * [cv::vconcat(InputArrayOfArrays, OutputArray)] and\n *\n * [cv::vconcat(InputArray, InputArray, OutputArray)]\n *\n * @param src input array or vector of matrices. all of the matrices must have the same number of rows\n * and the same depth.\n *\n * @param nsrc number of matrices in src.\n *\n * @param dst output array. It has the same number of rows and depth as the src, and the sum of cols of\n * the src.\n */\nexport declare function hconcat(src: any, nsrc: size_t, dst: OutputArray): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n *\n * ```cpp\n * cv::Mat_<float> A = (cv::Mat_<float>(3, 2) << 1, 4,\n *                                               2, 5,\n *                                               3, 6);\n * cv::Mat_<float> B = (cv::Mat_<float>(3, 2) << 7, 10,\n *                                               8, 11,\n *                                               9, 12);\n *\n * cv::Mat C;\n * cv::hconcat(A, B, C);\n * //C:\n * //[1, 4, 7, 10;\n * // 2, 5, 8, 11;\n * // 3, 6, 9, 12]\n * ```\n *\n * @param src1 first input array to be considered for horizontal concatenation.\n *\n * @param src2 second input array to be considered for horizontal concatenation.\n *\n * @param dst output array. It has the same number of rows and depth as the src1 and src2, and the sum\n * of cols of the src1 and src2.\n */\nexport declare function hconcat(src1: InputArray, src2: InputArray, dst: OutputArray): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n *\n * ```cpp\n * std::vector<cv::Mat> matrices = { cv::Mat(4, 1, CV_8UC1, cv::Scalar(1)),\n *                                   cv::Mat(4, 1, CV_8UC1, cv::Scalar(2)),\n *                                   cv::Mat(4, 1, CV_8UC1, cv::Scalar(3)),};\n *\n * cv::Mat out;\n * cv::hconcat( matrices, out );\n * //out:\n * //[1, 2, 3;\n * // 1, 2, 3;\n * // 1, 2, 3;\n * // 1, 2, 3]\n * ```\n *\n * @param src input array or vector of matrices. all of the matrices must have the same number of rows\n * and the same depth.\n *\n * @param dst output array. It has the same number of rows and depth as the src, and the sum of cols of\n * the src. same depth.\n */\nexport declare function hconcat(src: InputArrayOfArrays, dst: OutputArray): void;\n/**\n * idct(src, dst, flags) is equivalent to dct(src, dst, flags | DCT_INVERSE).\n *\n * [dct], [dft], [idft], [getOptimalDFTSize]\n *\n * @param src input floating-point single-channel array.\n *\n * @param dst output array of the same size and type as src.\n *\n * @param flags operation flags.\n */\nexport declare function idct(src: InputArray, dst: OutputArray, flags?: int): void;\n/**\n * idft(src, dst, flags) is equivalent to dft(src, dst, flags | [DFT_INVERSE]) .\n *\n * None of dft and idft scales the result by default. So, you should pass [DFT_SCALE] to one of dft or\n * idft explicitly to make these transforms mutually inverse.\n *\n * [dft], [dct], [idct], [mulSpectrums], [getOptimalDFTSize]\n *\n * @param src input floating-point real or complex array.\n *\n * @param dst output array whose size and type depend on the flags.\n *\n * @param flags operation flags (see dft and DftFlags).\n *\n * @param nonzeroRows number of dst rows to process; the rest of the rows have undefined content (see\n * the convolution sample in dft description.\n */\nexport declare function idft(src: InputArray, dst: OutputArray, flags?: int, nonzeroRows?: int): void;\n/**\n * The function checks the range as follows:\n *\n * For every element of a single-channel input array: `\\\\[\\\\texttt{dst} (I)= \\\\texttt{lowerb} (I)_0\n * \\\\leq \\\\texttt{src} (I)_0 \\\\leq \\\\texttt{upperb} (I)_0\\\\]`\n * For two-channel arrays: `\\\\[\\\\texttt{dst} (I)= \\\\texttt{lowerb} (I)_0 \\\\leq \\\\texttt{src} (I)_0\n * \\\\leq \\\\texttt{upperb} (I)_0 \\\\land \\\\texttt{lowerb} (I)_1 \\\\leq \\\\texttt{src} (I)_1 \\\\leq\n * \\\\texttt{upperb} (I)_1\\\\]`\n * and so forth.\n *\n * That is, dst (I) is set to 255 (all 1 -bits) if src (I) is within the specified 1D, 2D, 3D, ... box\n * and 0 otherwise.\n *\n * When the lower and/or upper boundary parameters are scalars, the indexes (I) at lowerb and upperb in\n * the above formulas should be omitted.\n *\n * @param src first input array.\n *\n * @param lowerb inclusive lower boundary array or a scalar.\n *\n * @param upperb inclusive upper boundary array or a scalar.\n *\n * @param dst output array of the same size as src and CV_8U type.\n */\nexport declare function inRange(src: InputArray, lowerb: InputArray, upperb: InputArray, dst: OutputArray): void;\n/**\n * [mixChannels], [merge]\n *\n * @param src input array\n *\n * @param dst output array\n *\n * @param coi index of channel for insertion\n */\nexport declare function insertChannel(src: InputArray, dst: InputOutputArray, coi: int): void;\n/**\n * The function [cv::invert] inverts the matrix src and stores the result in dst . When the matrix src\n * is singular or non-square, the function calculates the pseudo-inverse matrix (the dst matrix) so\n * that norm(src*dst - I) is minimal, where I is an identity matrix.\n *\n * In case of the [DECOMP_LU] method, the function returns non-zero value if the inverse has been\n * successfully calculated and 0 if src is singular.\n *\n * In case of the [DECOMP_SVD] method, the function returns the inverse condition number of src (the\n * ratio of the smallest singular value to the largest singular value) and 0 if src is singular. The\n * [SVD] method calculates a pseudo-inverse matrix if src is singular.\n *\n * Similarly to [DECOMP_LU], the method [DECOMP_CHOLESKY] works only with non-singular square matrices\n * that should also be symmetrical and positively defined. In this case, the function stores the\n * inverted matrix in dst and returns non-zero. Otherwise, it returns 0.\n *\n * [solve], [SVD]\n *\n * @param src input floating-point M x N matrix.\n *\n * @param dst output matrix of N x M size and the same type as src.\n *\n * @param flags inversion method (cv::DecompTypes)\n */\nexport declare function invert(src: InputArray, dst: OutputArray, flags?: int): double;\n/**\n * The function [cv::log] calculates the natural logarithm of every element of the input array:\n * `\\\\[\\\\texttt{dst} (I) = \\\\log (\\\\texttt{src}(I)) \\\\]`\n *\n * Output on zero, negative and special (NaN, Inf) values is undefined.\n *\n * [exp], [cartToPolar], [polarToCart], [phase], [pow], [sqrt], [magnitude]\n *\n * @param src input array.\n *\n * @param dst output array of the same size and type as src .\n */\nexport declare function log(src: InputArray, dst: OutputArray): void;\n/**\n * The function LUT fills the output array with values from the look-up table. Indices of the entries\n * are taken from the input array. That is, the function processes each element of src as follows:\n * `\\\\[\\\\texttt{dst} (I) \\\\leftarrow \\\\texttt{lut(src(I) + d)}\\\\]` where `\\\\[d = \\\\fork{0}{if\n * \\\\(\\\\texttt{src}\\\\) has depth \\\\(\\\\texttt{CV_8U}\\\\)}{128}{if \\\\(\\\\texttt{src}\\\\) has depth\n * \\\\(\\\\texttt{CV_8S}\\\\)}\\\\]`\n *\n * [convertScaleAbs], [Mat::convertTo]\n *\n * @param src input array of 8-bit elements.\n *\n * @param lut look-up table of 256 elements; in case of multi-channel input array, the table should\n * either have a single channel (in this case the same table is used for all channels) or the same\n * number of channels as in the input array.\n *\n * @param dst output array of the same size and number of channels as src, and the same depth as lut.\n */\nexport declare function LUT(src: InputArray, lut: InputArray, dst: OutputArray): void;\n/**\n * The function [cv::magnitude] calculates the magnitude of 2D vectors formed from the corresponding\n * elements of x and y arrays: `\\\\[\\\\texttt{dst} (I) = \\\\sqrt{\\\\texttt{x}(I)^2 + \\\\texttt{y}(I)^2}\\\\]`\n *\n * [cartToPolar], [polarToCart], [phase], [sqrt]\n *\n * @param x floating-point array of x-coordinates of the vectors.\n *\n * @param y floating-point array of y-coordinates of the vectors; it must have the same size as x.\n *\n * @param magnitude output array of the same size and type as x.\n */\nexport declare function magnitude(x: InputArray, y: InputArray, magnitude: OutputArray): void;\n/**\n * The function [cv::Mahalanobis] calculates and returns the weighted distance between two vectors:\n * `\\\\[d( \\\\texttt{vec1} , \\\\texttt{vec2} )=\n * \\\\sqrt{\\\\sum_{i,j}{\\\\texttt{icovar(i,j)}\\\\cdot(\\\\texttt{vec1}(I)-\\\\texttt{vec2}(I))\\\\cdot(\\\\texttt{vec1(j)}-\\\\texttt{vec2(j)})}\n * }\\\\]` The covariance matrix may be calculated using the [calcCovarMatrix] function and then inverted\n * using the invert function (preferably using the [DECOMP_SVD] method, as the most accurate).\n *\n * @param v1 first 1D input vector.\n *\n * @param v2 second 1D input vector.\n *\n * @param icovar inverse covariance matrix.\n */\nexport declare function Mahalanobis(v1: InputArray, v2: InputArray, icovar: InputArray): double;\n/**\n * The function [cv::max] calculates the per-element maximum of two arrays: `\\\\[\\\\texttt{dst} (I)=\n * \\\\max ( \\\\texttt{src1} (I), \\\\texttt{src2} (I))\\\\]` or array and a scalar: `\\\\[\\\\texttt{dst} (I)=\n * \\\\max ( \\\\texttt{src1} (I), \\\\texttt{value} )\\\\]`\n *\n * [min], [compare], [inRange], [minMaxLoc], [MatrixExpressions]\n *\n * @param src1 first input array.\n *\n * @param src2 second input array of the same size and type as src1 .\n *\n * @param dst output array of the same size and type as src1.\n */\nexport declare function max(src1: InputArray, src2: InputArray, dst: OutputArray): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts. needed to avoid conflicts with const _Tp& std::min(const _Tp&,\n * const _Tp&, _Compare)\n */\nexport declare function max(src1: any, src2: any, dst: any): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts. needed to avoid conflicts with const _Tp& std::min(const _Tp&,\n * const _Tp&, _Compare)\n */\nexport declare function max(src1: any, src2: any, dst: any): void;\n/**\n * The function [cv::mean] calculates the mean value M of array elements, independently for each\n * channel, and return it: `\\\\[\\\\begin{array}{l} N = \\\\sum _{I: \\\\; \\\\texttt{mask} (I) \\\\ne 0} 1 \\\\\\\\\n * M_c = \\\\left ( \\\\sum _{I: \\\\; \\\\texttt{mask} (I) \\\\ne 0}{ \\\\texttt{mtx} (I)_c} \\\\right )/N\n * \\\\end{array}\\\\]` When all the mask elements are 0's, the function returns Scalar::all(0)\n *\n * [countNonZero], [meanStdDev], [norm], [minMaxLoc]\n *\n * @param src input array that should have from 1 to 4 channels so that the result can be stored in\n * Scalar_ .\n *\n * @param mask optional operation mask.\n */\nexport declare function mean(src: InputArray, mask?: InputArray): Scalar;\n/**\n * Calculates a mean and standard deviation of array elements.\n *\n * The function [cv::meanStdDev] calculates the mean and the standard deviation M of array elements\n * independently for each channel and returns it via the output parameters: `\\\\[\\\\begin{array}{l} N =\n * \\\\sum _{I, \\\\texttt{mask} (I) \\\\ne 0} 1 \\\\\\\\ \\\\texttt{mean} _c = \\\\frac{\\\\sum_{ I: \\\\;\n * \\\\texttt{mask}(I) \\\\ne 0} \\\\texttt{src} (I)_c}{N} \\\\\\\\ \\\\texttt{stddev} _c = \\\\sqrt{\\\\frac{\\\\sum_{\n * I: \\\\; \\\\texttt{mask}(I) \\\\ne 0} \\\\left ( \\\\texttt{src} (I)_c - \\\\texttt{mean} _c \\\\right )^2}{N}}\n * \\\\end{array}\\\\]` When all the mask elements are 0's, the function returns\n * mean=stddev=Scalar::all(0).\n *\n * The calculated standard deviation is only the diagonal of the complete normalized covariance matrix.\n * If the full matrix is needed, you can reshape the multi-channel array M x N to the single-channel\n * array M*N x mtx.channels() (only possible when the matrix is continuous) and then pass the matrix to\n * calcCovarMatrix .\n *\n * [countNonZero], [mean], [norm], [minMaxLoc], [calcCovarMatrix]\n *\n * @param src input array that should have from 1 to 4 channels so that the results can be stored in\n * Scalar_ 's.\n *\n * @param mean output parameter: calculated mean value.\n *\n * @param stddev output parameter: calculated standard deviation.\n *\n * @param mask optional operation mask.\n */\nexport declare function meanStdDev(src: InputArray, mean: OutputArray, stddev: OutputArray, mask?: InputArray): void;\n/**\n * The function [cv::merge] merges several arrays to make a single multi-channel array. That is, each\n * element of the output array will be a concatenation of the elements of the input arrays, where\n * elements of i-th input array are treated as mv[i].channels()-element vectors.\n *\n * The function [cv::split] does the reverse operation. If you need to shuffle channels in some other\n * advanced way, use [cv::mixChannels].\n *\n * The following example shows how to merge 3 single channel matrices into a single 3-channel matrix.\n *\n * ```cpp\n *     Mat m1 = (Mat_<uchar>(2,2) << 1,4,7,10);\n *     Mat m2 = (Mat_<uchar>(2,2) << 2,5,8,11);\n *     Mat m3 = (Mat_<uchar>(2,2) << 3,6,9,12);\n *\n *     Mat channels[3] = {m1, m2, m3};\n *     Mat m;\n *     merge(channels, 3, m);\n *     /*\n *     m =\n *     [  1,   2,   3,   4,   5,   6;\n *        7,   8,   9,  10,  11,  12]\n *     m.channels() = 3\n * \\/\n * ```\n *\n * [mixChannels], [split], [Mat::reshape]\n *\n * @param mv input array of matrices to be merged; all the matrices in mv must have the same size and\n * the same depth.\n *\n * @param count number of input matrices when mv is a plain C array; it must be greater than zero.\n *\n * @param dst output array of the same size and the same depth as mv[0]; The number of channels will be\n * equal to the parameter count.\n */\nexport declare function merge(mv: any, count: size_t, dst: OutputArray): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n *\n * @param mv input vector of matrices to be merged; all the matrices in mv must have the same size and\n * the same depth.\n *\n * @param dst output array of the same size and the same depth as mv[0]; The number of channels will be\n * the total number of channels in the matrix array.\n */\nexport declare function merge(mv: InputArrayOfArrays, dst: OutputArray): void;\n/**\n * The function [cv::min] calculates the per-element minimum of two arrays: `\\\\[\\\\texttt{dst} (I)=\n * \\\\min ( \\\\texttt{src1} (I), \\\\texttt{src2} (I))\\\\]` or array and a scalar: `\\\\[\\\\texttt{dst} (I)=\n * \\\\min ( \\\\texttt{src1} (I), \\\\texttt{value} )\\\\]`\n *\n * [max], [compare], [inRange], [minMaxLoc]\n *\n * @param src1 first input array.\n *\n * @param src2 second input array of the same size and type as src1.\n *\n * @param dst output array of the same size and type as src1.\n */\nexport declare function min(src1: InputArray, src2: InputArray, dst: OutputArray): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts. needed to avoid conflicts with const _Tp& std::min(const _Tp&,\n * const _Tp&, _Compare)\n */\nexport declare function min(src1: any, src2: any, dst: any): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts. needed to avoid conflicts with const _Tp& std::min(const _Tp&,\n * const _Tp&, _Compare)\n */\nexport declare function min(src1: any, src2: any, dst: any): void;\n/**\n * The function [cv::minMaxIdx] finds the minimum and maximum element values and their positions. The\n * extremums are searched across the whole array or, if mask is not an empty array, in the specified\n * array region. The function does not work with multi-channel arrays. If you need to find minimum or\n * maximum elements across all the channels, use [Mat::reshape] first to reinterpret the array as\n * single-channel. Or you may extract the particular channel using either extractImageCOI , or\n * mixChannels , or split . In case of a sparse matrix, the minimum is found among non-zero elements\n * only.\n *\n * When minIdx is not NULL, it must have at least 2 elements (as well as maxIdx), even if src is a\n * single-row or single-column matrix. In OpenCV (following MATLAB) each array has at least 2\n * dimensions, i.e. single-column matrix is Mx1 matrix (and therefore minIdx/maxIdx will be\n * (i1,0)/(i2,0)) and single-row matrix is 1xN matrix (and therefore minIdx/maxIdx will be\n * (0,j1)/(0,j2)).\n *\n * @param src input single-channel array.\n *\n * @param minVal pointer to the returned minimum value; NULL is used if not required.\n *\n * @param maxVal pointer to the returned maximum value; NULL is used if not required.\n *\n * @param minIdx pointer to the returned minimum location (in nD case); NULL is used if not required;\n * Otherwise, it must point to an array of src.dims elements, the coordinates of the minimum element in\n * each dimension are stored there sequentially.\n *\n * @param maxIdx pointer to the returned maximum location (in nD case). NULL is used if not required.\n *\n * @param mask specified array region\n */\nexport declare function minMaxIdx(src: InputArray, minVal: any, maxVal?: any, minIdx?: any, maxIdx?: any, mask?: InputArray): void;\n/**\n * The function [cv::minMaxLoc] finds the minimum and maximum element values and their positions. The\n * extremums are searched across the whole array or, if mask is not an empty array, in the specified\n * array region.\n *\n * The function do not work with multi-channel arrays. If you need to find minimum or maximum elements\n * across all the channels, use [Mat::reshape] first to reinterpret the array as single-channel. Or you\n * may extract the particular channel using either extractImageCOI , or mixChannels , or split .\n *\n * [max], [min], [compare], [inRange], extractImageCOI, [mixChannels], [split], [Mat::reshape]\n *\n * @param src input single-channel array.\n *\n * @param minVal pointer to the returned minimum value; NULL is used if not required.\n *\n * @param maxVal pointer to the returned maximum value; NULL is used if not required.\n *\n * @param minLoc pointer to the returned minimum location (in 2D case); NULL is used if not required.\n *\n * @param maxLoc pointer to the returned maximum location (in 2D case); NULL is used if not required.\n *\n * @param mask optional mask used to select a sub-array.\n */\nexport declare function minMaxLoc(src: InputArray, minVal: any, maxVal?: any, minLoc?: any, maxLoc?: any, mask?: InputArray): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n *\n * @param a input single-channel array.\n *\n * @param minVal pointer to the returned minimum value; NULL is used if not required.\n *\n * @param maxVal pointer to the returned maximum value; NULL is used if not required.\n *\n * @param minIdx pointer to the returned minimum location (in nD case); NULL is used if not required;\n * Otherwise, it must point to an array of src.dims elements, the coordinates of the minimum element in\n * each dimension are stored there sequentially.\n *\n * @param maxIdx pointer to the returned maximum location (in nD case). NULL is used if not required.\n */\nexport declare function minMaxLoc(a: any, minVal: any, maxVal: any, minIdx?: any, maxIdx?: any): void;\n/**\n * The function [cv::mixChannels] provides an advanced mechanism for shuffling image channels.\n *\n * [cv::split],[cv::merge],[cv::extractChannel],[cv::insertChannel] and some forms of [cv::cvtColor]\n * are partial cases of [cv::mixChannels].\n *\n * In the example below, the code splits a 4-channel BGRA image into a 3-channel BGR (with B and R\n * channels swapped) and a separate alpha-channel image:\n *\n * ```cpp\n * Mat bgra( 100, 100, CV_8UC4, Scalar(255,0,0,255) );\n * Mat bgr( bgra.rows, bgra.cols, CV_8UC3 );\n * Mat alpha( bgra.rows, bgra.cols, CV_8UC1 );\n *\n * // forming an array of matrices is a quite efficient operation,\n * // because the matrix data is not copied, only the headers\n * Mat out[] = { bgr, alpha };\n * // bgra[0] -> bgr[2], bgra[1] -> bgr[1],\n * // bgra[2] -> bgr[0], bgra[3] -> alpha[0]\n * int from_to[] = { 0,2, 1,1, 2,0, 3,3 };\n * mixChannels( &bgra, 1, out, 2, from_to, 4 );\n * ```\n *\n * Unlike many other new-style C++ functions in OpenCV (see the introduction section and [Mat::create]\n * ), [cv::mixChannels] requires the output arrays to be pre-allocated before calling the function.\n *\n * [split], [merge], [extractChannel], [insertChannel], [cvtColor]\n *\n * @param src input array or vector of matrices; all of the matrices must have the same size and the\n * same depth.\n *\n * @param nsrcs number of matrices in src.\n *\n * @param dst output array or vector of matrices; all the matrices must be allocated; their size and\n * depth must be the same as in src[0].\n *\n * @param ndsts number of matrices in dst.\n *\n * @param fromTo array of index pairs specifying which channels are copied and where; fromTo[k*2] is a\n * 0-based index of the input channel in src, fromTo[k*2+1] is an index of the output channel in dst;\n * the continuous channel numbering is used: the first input image channels are indexed from 0 to\n * src[0].channels()-1, the second input image channels are indexed from src[0].channels() to\n * src[0].channels() + src[1].channels()-1, and so on, the same scheme is used for the output image\n * channels; as a special case, when fromTo[k*2] is negative, the corresponding output channel is\n * filled with zero .\n *\n * @param npairs number of index pairs in fromTo.\n */\nexport declare function mixChannels(src: any, nsrcs: size_t, dst: any, ndsts: size_t, fromTo: any, npairs: size_t): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n *\n * @param src input array or vector of matrices; all of the matrices must have the same size and the\n * same depth.\n *\n * @param dst output array or vector of matrices; all the matrices must be allocated; their size and\n * depth must be the same as in src[0].\n *\n * @param fromTo array of index pairs specifying which channels are copied and where; fromTo[k*2] is a\n * 0-based index of the input channel in src, fromTo[k*2+1] is an index of the output channel in dst;\n * the continuous channel numbering is used: the first input image channels are indexed from 0 to\n * src[0].channels()-1, the second input image channels are indexed from src[0].channels() to\n * src[0].channels() + src[1].channels()-1, and so on, the same scheme is used for the output image\n * channels; as a special case, when fromTo[k*2] is negative, the corresponding output channel is\n * filled with zero .\n *\n * @param npairs number of index pairs in fromTo.\n */\nexport declare function mixChannels(src: InputArrayOfArrays, dst: InputOutputArrayOfArrays, fromTo: any, npairs: size_t): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n *\n * @param src input array or vector of matrices; all of the matrices must have the same size and the\n * same depth.\n *\n * @param dst output array or vector of matrices; all the matrices must be allocated; their size and\n * depth must be the same as in src[0].\n *\n * @param fromTo array of index pairs specifying which channels are copied and where; fromTo[k*2] is a\n * 0-based index of the input channel in src, fromTo[k*2+1] is an index of the output channel in dst;\n * the continuous channel numbering is used: the first input image channels are indexed from 0 to\n * src[0].channels()-1, the second input image channels are indexed from src[0].channels() to\n * src[0].channels() + src[1].channels()-1, and so on, the same scheme is used for the output image\n * channels; as a special case, when fromTo[k*2] is negative, the corresponding output channel is\n * filled with zero .\n */\nexport declare function mixChannels(src: InputArrayOfArrays, dst: InputOutputArrayOfArrays, fromTo: any): void;\n/**\n * The function [cv::mulSpectrums] performs the per-element multiplication of the two CCS-packed or\n * complex matrices that are results of a real or complex Fourier transform.\n *\n * The function, together with dft and idft , may be used to calculate convolution (pass conjB=false )\n * or correlation (pass conjB=true ) of two arrays rapidly. When the arrays are complex, they are\n * simply multiplied (per element) with an optional conjugation of the second-array elements. When the\n * arrays are real, they are assumed to be CCS-packed (see dft for details).\n *\n * @param a first input array.\n *\n * @param b second input array of the same size and type as src1 .\n *\n * @param c output array of the same size and type as src1 .\n *\n * @param flags operation flags; currently, the only supported flag is cv::DFT_ROWS, which indicates\n * that each row of src1 and src2 is an independent 1D Fourier spectrum. If you do not want to use this\n * flag, then simply add a 0 as value.\n *\n * @param conjB optional flag that conjugates the second input array before the multiplication (true)\n * or not (false).\n */\nexport declare function mulSpectrums(a: InputArray, b: InputArray, c: OutputArray, flags: int, conjB?: bool): void;\n/**\n * The function multiply calculates the per-element product of two arrays:\n *\n * `\\\\[\\\\texttt{dst} (I)= \\\\texttt{saturate} ( \\\\texttt{scale} \\\\cdot \\\\texttt{src1} (I) \\\\cdot\n * \\\\texttt{src2} (I))\\\\]`\n *\n * There is also a [MatrixExpressions] -friendly variant of the first function. See [Mat::mul] .\n *\n * For a not-per-element matrix product, see gemm .\n *\n * Saturation is not applied when the output array has the depth CV_32S. You may even get result of an\n * incorrect sign in the case of overflow.\n *\n * [add], [subtract], [divide], [scaleAdd], [addWeighted], [accumulate], [accumulateProduct],\n * [accumulateSquare], [Mat::convertTo]\n *\n * @param src1 first input array.\n *\n * @param src2 second input array of the same size and the same type as src1.\n *\n * @param dst output array of the same size and type as src1.\n *\n * @param scale optional scale factor.\n *\n * @param dtype optional depth of the output array\n */\nexport declare function multiply(src1: InputArray, src2: InputArray, dst: OutputArray, scale?: double, dtype?: int): void;\n/**\n * The function [cv::mulTransposed] calculates the product of src and its transposition:\n * `\\\\[\\\\texttt{dst} = \\\\texttt{scale} ( \\\\texttt{src} - \\\\texttt{delta} )^T ( \\\\texttt{src} -\n * \\\\texttt{delta} )\\\\]` if aTa=true , and `\\\\[\\\\texttt{dst} = \\\\texttt{scale} ( \\\\texttt{src} -\n * \\\\texttt{delta} ) ( \\\\texttt{src} - \\\\texttt{delta} )^T\\\\]` otherwise. The function is used to\n * calculate the covariance matrix. With zero delta, it can be used as a faster substitute for general\n * matrix product A*B when B=A'\n *\n * [calcCovarMatrix], [gemm], [repeat], [reduce]\n *\n * @param src input single-channel matrix. Note that unlike gemm, the function can multiply not only\n * floating-point matrices.\n *\n * @param dst output square matrix.\n *\n * @param aTa Flag specifying the multiplication ordering. See the description below.\n *\n * @param delta Optional delta matrix subtracted from src before the multiplication. When the matrix is\n * empty ( delta=noArray() ), it is assumed to be zero, that is, nothing is subtracted. If it has the\n * same size as src , it is simply subtracted. Otherwise, it is \"repeated\" (see repeat ) to cover the\n * full src and then subtracted. Type of the delta matrix, when it is not empty, must be the same as\n * the type of created output matrix. See the dtype parameter description below.\n *\n * @param scale Optional scale factor for the matrix product.\n *\n * @param dtype Optional type of the output matrix. When it is negative, the output matrix will have\n * the same type as src . Otherwise, it will be type=CV_MAT_DEPTH(dtype) that should be either CV_32F\n * or CV_64F .\n */\nexport declare function mulTransposed(src: InputArray, dst: OutputArray, aTa: bool, delta?: InputArray, scale?: double, dtype?: int): void;\n/**\n * This version of [norm] calculates the absolute norm of src1. The type of norm to calculate is\n * specified using [NormTypes].\n *\n * As example for one array consider the function `$r(x)= \\\\begin{pmatrix} x \\\\\\\\ 1-x \\\\end{pmatrix}, x\n * \\\\in [-1;1]$`. The `$ L_{1}, L_{2} $` and `$ L_{\\\\infty} $` norm for the sample value `$r(-1) =\n * \\\\begin{pmatrix} -1 \\\\\\\\ 2 \\\\end{pmatrix}$` is calculated as follows `\\\\begin{align*} \\\\| r(-1)\n * \\\\|_{L_1} &= |-1| + |2| = 3 \\\\\\\\ \\\\| r(-1) \\\\|_{L_2} &= \\\\sqrt{(-1)^{2} + (2)^{2}} = \\\\sqrt{5} \\\\\\\\\n * \\\\| r(-1) \\\\|_{L_\\\\infty} &= \\\\max(|-1|,|2|) = 2 \\\\end{align*}` and for `$r(0.5) = \\\\begin{pmatrix}\n * 0.5 \\\\\\\\ 0.5 \\\\end{pmatrix}$` the calculation is `\\\\begin{align*} \\\\| r(0.5) \\\\|_{L_1} &= |0.5| +\n * |0.5| = 1 \\\\\\\\ \\\\| r(0.5) \\\\|_{L_2} &= \\\\sqrt{(0.5)^{2} + (0.5)^{2}} = \\\\sqrt{0.5} \\\\\\\\ \\\\| r(0.5)\n * \\\\|_{L_\\\\infty} &= \\\\max(|0.5|,|0.5|) = 0.5. \\\\end{align*}` The following graphic shows all values\n * for the three norm functions `$\\\\| r(x) \\\\|_{L_1}, \\\\| r(x) \\\\|_{L_2}$` and `$\\\\| r(x)\n * \\\\|_{L_\\\\infty}$`. It is notable that the `$ L_{1} $` norm forms the upper and the `$ L_{\\\\infty} $`\n * norm forms the lower border for the example function `$ r(x) $`.\n *  When the mask parameter is specified and it is not empty, the norm is\n *\n * If normType is not specified, [NORM_L2] is used. calculated only over the region specified by the\n * mask.\n *\n * Multi-channel input arrays are treated as single-channel arrays, that is, the results for all\n * channels are combined.\n *\n * [Hamming] norms can only be calculated with CV_8U depth arrays.\n *\n * @param src1 first input array.\n *\n * @param normType type of the norm (see NormTypes).\n *\n * @param mask optional operation mask; it must have the same size as src1 and CV_8UC1 type.\n */\nexport declare function norm(src1: InputArray, normType?: int, mask?: InputArray): double;\n/**\n * This version of [cv::norm] calculates the absolute difference norm or the relative difference norm\n * of arrays src1 and src2. The type of norm to calculate is specified using [NormTypes].\n *\n * @param src1 first input array.\n *\n * @param src2 second input array of the same size and the same type as src1.\n *\n * @param normType type of the norm (see NormTypes).\n *\n * @param mask optional operation mask; it must have the same size as src1 and CV_8UC1 type.\n */\nexport declare function norm(src1: InputArray, src2: InputArray, normType?: int, mask?: InputArray): double;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n *\n * @param src first input array.\n *\n * @param normType type of the norm (see NormTypes).\n */\nexport declare function norm(src: any, normType: int): double;\n/**\n * The function [cv::normalize] normalizes scale and shift the input array elements so that `\\\\[\\\\|\n * \\\\texttt{dst} \\\\| _{L_p}= \\\\texttt{alpha}\\\\]` (where p=Inf, 1 or 2) when normType=NORM_INF, NORM_L1,\n * or NORM_L2, respectively; or so that `\\\\[\\\\min _I \\\\texttt{dst} (I)= \\\\texttt{alpha} , \\\\, \\\\, \\\\max\n * _I \\\\texttt{dst} (I)= \\\\texttt{beta}\\\\]`\n *\n * when normType=NORM_MINMAX (for dense arrays only). The optional mask specifies a sub-array to be\n * normalized. This means that the norm or min-n-max are calculated over the sub-array, and then this\n * sub-array is modified to be normalized. If you want to only use the mask to calculate the norm or\n * min-max but modify the whole array, you can use norm and [Mat::convertTo].\n *\n * In case of sparse matrices, only the non-zero values are analyzed and transformed. Because of this,\n * the range transformation for sparse matrices is not allowed since it can shift the zero level.\n *\n * Possible usage with some positive example data:\n *\n * ```cpp\n * vector<double> positiveData = { 2.0, 8.0, 10.0 };\n * vector<double> normalizedData_l1, normalizedData_l2, normalizedData_inf, normalizedData_minmax;\n *\n * // Norm to probability (total count)\n * // sum(numbers) = 20.0\n * // 2.0      0.1     (2.0/20.0)\n * // 8.0      0.4     (8.0/20.0)\n * // 10.0     0.5     (10.0/20.0)\n * normalize(positiveData, normalizedData_l1, 1.0, 0.0, NORM_L1);\n *\n * // Norm to unit vector: ||positiveData|| = 1.0\n * // 2.0      0.15\n * // 8.0      0.62\n * // 10.0     0.77\n * normalize(positiveData, normalizedData_l2, 1.0, 0.0, NORM_L2);\n *\n * // Norm to max element\n * // 2.0      0.2     (2.0/10.0)\n * // 8.0      0.8     (8.0/10.0)\n * // 10.0     1.0     (10.0/10.0)\n * normalize(positiveData, normalizedData_inf, 1.0, 0.0, NORM_INF);\n *\n * // Norm to range [0.0;1.0]\n * // 2.0      0.0     (shift to left border)\n * // 8.0      0.75    (6.0/8.0)\n * // 10.0     1.0     (shift to right border)\n * normalize(positiveData, normalizedData_minmax, 1.0, 0.0, NORM_MINMAX);\n * ```\n *\n * [norm], [Mat::convertTo], [SparseMat::convertTo]\n *\n * @param src input array.\n *\n * @param dst output array of the same size as src .\n *\n * @param alpha norm value to normalize to or the lower range boundary in case of the range\n * normalization.\n *\n * @param beta upper range boundary in case of the range normalization; it is not used for the norm\n * normalization.\n *\n * @param norm_type normalization type (see cv::NormTypes).\n *\n * @param dtype when negative, the output array has the same type as src; otherwise, it has the same\n * number of channels as src and the depth =CV_MAT_DEPTH(dtype).\n *\n * @param mask optional operation mask.\n */\nexport declare function normalize(src: InputArray, dst: InputOutputArray, alpha?: double, beta?: double, norm_type?: int, dtype?: int, mask?: InputArray): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n *\n * @param src input array.\n *\n * @param dst output array of the same size as src .\n *\n * @param alpha norm value to normalize to or the lower range boundary in case of the range\n * normalization.\n *\n * @param normType normalization type (see cv::NormTypes).\n */\nexport declare function normalize(src: any, dst: any, alpha: double, normType: int): void;\nexport declare function patchNaNs(a: InputOutputArray, val?: double): void;\n/**\n * wrap [PCA::backProject]\n */\nexport declare function PCABackProject(data: InputArray, mean: InputArray, eigenvectors: InputArray, result: OutputArray): void;\n/**\n * wrap PCA::operator()\n */\nexport declare function PCACompute(data: InputArray, mean: InputOutputArray, eigenvectors: OutputArray, maxComponents?: int): void;\n/**\n * wrap PCA::operator() and add eigenvalues output parameter\n */\nexport declare function PCACompute(data: InputArray, mean: InputOutputArray, eigenvectors: OutputArray, eigenvalues: OutputArray, maxComponents?: int): void;\n/**\n * wrap PCA::operator()\n */\nexport declare function PCACompute(data: InputArray, mean: InputOutputArray, eigenvectors: OutputArray, retainedVariance: double): void;\n/**\n * wrap PCA::operator() and add eigenvalues output parameter\n */\nexport declare function PCACompute(data: InputArray, mean: InputOutputArray, eigenvectors: OutputArray, eigenvalues: OutputArray, retainedVariance: double): void;\n/**\n * wrap [PCA::project]\n */\nexport declare function PCAProject(data: InputArray, mean: InputArray, eigenvectors: InputArray, result: OutputArray): void;\n/**\n * The function [cv::perspectiveTransform] transforms every element of src by treating it as a 2D or 3D\n * vector, in the following way: `\\\\[(x, y, z) \\\\rightarrow (x'/w, y'/w, z'/w)\\\\]` where `\\\\[(x', y',\n * z', w') = \\\\texttt{mat} \\\\cdot \\\\begin{bmatrix} x & y & z & 1 \\\\end{bmatrix}\\\\]` and `\\\\[w =\n * \\\\fork{w'}{if \\\\(w' \\\\ne 0\\\\)}{\\\\infty}{otherwise}\\\\]`\n *\n * Here a 3D vector transformation is shown. In case of a 2D vector transformation, the z component is\n * omitted.\n *\n * The function transforms a sparse set of 2D or 3D vectors. If you want to transform an image using\n * perspective transformation, use warpPerspective . If you have an inverse problem, that is, you want\n * to compute the most probable perspective transformation out of several pairs of corresponding\n * points, you can use getPerspectiveTransform or findHomography .\n *\n * [transform], [warpPerspective], [getPerspectiveTransform], [findHomography]\n *\n * @param src input two-channel or three-channel floating-point array; each element is a 2D/3D vector\n * to be transformed.\n *\n * @param dst output array of the same size and type as src.\n *\n * @param m 3x3 or 4x4 floating-point transformation matrix.\n */\nexport declare function perspectiveTransform(src: InputArray, dst: OutputArray, m: InputArray): void;\n/**\n * The function [cv::phase] calculates the rotation angle of each 2D vector that is formed from the\n * corresponding elements of x and y : `\\\\[\\\\texttt{angle} (I) = \\\\texttt{atan2} ( \\\\texttt{y} (I),\n * \\\\texttt{x} (I))\\\\]`\n *\n * The angle estimation accuracy is about 0.3 degrees. When x(I)=y(I)=0 , the corresponding angle(I) is\n * set to 0.\n *\n * @param x input floating-point array of x-coordinates of 2D vectors.\n *\n * @param y input array of y-coordinates of 2D vectors; it must have the same size and the same type as\n * x.\n *\n * @param angle output array of vector angles; it has the same size and same type as x .\n *\n * @param angleInDegrees when true, the function calculates the angle in degrees, otherwise, they are\n * measured in radians.\n */\nexport declare function phase(x: InputArray, y: InputArray, angle: OutputArray, angleInDegrees?: bool): void;\n/**\n * The function [cv::polarToCart] calculates the Cartesian coordinates of each 2D vector represented by\n * the corresponding elements of magnitude and angle: `\\\\[\\\\begin{array}{l} \\\\texttt{x} (I) =\n * \\\\texttt{magnitude} (I) \\\\cos ( \\\\texttt{angle} (I)) \\\\\\\\ \\\\texttt{y} (I) = \\\\texttt{magnitude} (I)\n * \\\\sin ( \\\\texttt{angle} (I)) \\\\\\\\ \\\\end{array}\\\\]`\n *\n * The relative accuracy of the estimated coordinates is about 1e-6.\n *\n * [cartToPolar], [magnitude], [phase], [exp], [log], [pow], [sqrt]\n *\n * @param magnitude input floating-point array of magnitudes of 2D vectors; it can be an empty matrix\n * (=Mat()), in this case, the function assumes that all the magnitudes are =1; if it is not empty, it\n * must have the same size and type as angle.\n *\n * @param angle input floating-point array of angles of 2D vectors.\n *\n * @param x output array of x-coordinates of 2D vectors; it has the same size and type as angle.\n *\n * @param y output array of y-coordinates of 2D vectors; it has the same size and type as angle.\n *\n * @param angleInDegrees when true, the input angles are measured in degrees, otherwise, they are\n * measured in radians.\n */\nexport declare function polarToCart(magnitude: InputArray, angle: InputArray, x: OutputArray, y: OutputArray, angleInDegrees?: bool): void;\n/**\n * The function [cv::pow] raises every element of the input array to power : `\\\\[\\\\texttt{dst} (I) =\n * \\\\fork{\\\\texttt{src}(I)^{power}}{if \\\\(\\\\texttt{power}\\\\) is\n * integer}{|\\\\texttt{src}(I)|^{power}}{otherwise}\\\\]`\n *\n * So, for a non-integer power exponent, the absolute values of input array elements are used. However,\n * it is possible to get true values for negative values using some extra operations. In the example\n * below, computing the 5th root of array src shows:\n *\n * ```cpp\n * Mat mask = src < 0;\n * pow(src, 1./5, dst);\n * subtract(Scalar::all(0), dst, dst, mask);\n * ```\n *\n *  For some values of power, such as integer values, 0.5 and -0.5, specialized faster algorithms are\n * used.\n *\n * Special values (NaN, Inf) are not handled.\n *\n * [sqrt], [exp], [log], [cartToPolar], [polarToCart]\n *\n * @param src input array.\n *\n * @param power exponent of power.\n *\n * @param dst output array of the same size and type as src.\n */\nexport declare function pow(src: InputArray, power: double, dst: OutputArray): void;\n/**\n * This function calculates the Peak Signal-to-Noise Ratio (PSNR) image quality metric in decibels\n * (dB), between two input arrays src1 and src2. The arrays must have the same type.\n *\n * The PSNR is calculated as follows:\n *\n * `\\\\[ \\\\texttt{PSNR} = 10 \\\\cdot \\\\log_{10}{\\\\left( \\\\frac{R^2}{MSE} \\\\right) } \\\\]`\n *\n * where R is the maximum integer value of depth (e.g. 255 in the case of CV_8U data) and MSE is the\n * mean squared error between the two arrays.\n *\n * @param src1 first input array.\n *\n * @param src2 second input array of the same size as src1.\n *\n * @param R the maximum pixel value (255 by default)\n */\nexport declare function PSNR(src1: InputArray, src2: InputArray, R?: double): double;\n/**\n * The function [cv::randn] fills the matrix dst with normally distributed random numbers with the\n * specified mean vector and the standard deviation matrix. The generated random numbers are clipped to\n * fit the value range of the output array data type.\n *\n * [RNG], [randu]\n *\n * @param dst output array of random numbers; the array must be pre-allocated and have 1 to 4 channels.\n *\n * @param mean mean value (expectation) of the generated random numbers.\n *\n * @param stddev standard deviation of the generated random numbers; it can be either a vector (in\n * which case a diagonal standard deviation matrix is assumed) or a square matrix.\n */\nexport declare function randn(dst: InputOutputArray, mean: InputArray, stddev: InputArray): void;\n/**\n * The function [cv::randShuffle] shuffles the specified 1D array by randomly choosing pairs of\n * elements and swapping them. The number of such swap operations will be dst.rows*dst.cols*iterFactor\n * .\n *\n * [RNG], [sort]\n *\n * @param dst input/output numerical 1D array.\n *\n * @param iterFactor scale factor that determines the number of random swap operations (see the details\n * below).\n *\n * @param rng optional random number generator used for shuffling; if it is zero, theRNG () is used\n * instead.\n */\nexport declare function randShuffle(dst: InputOutputArray, iterFactor?: double, rng?: any): void;\n/**\n * Non-template variant of the function fills the matrix dst with uniformly-distributed random numbers\n * from the specified range: `\\\\[\\\\texttt{low} _c \\\\leq \\\\texttt{dst} (I)_c < \\\\texttt{high} _c\\\\]`\n *\n * [RNG], [randn], [theRNG]\n *\n * @param dst output array of random numbers; the array must be pre-allocated.\n *\n * @param low inclusive lower boundary of the generated random numbers.\n *\n * @param high exclusive upper boundary of the generated random numbers.\n */\nexport declare function randu(dst: InputOutputArray, low: InputArray, high: InputArray): void;\n/**\n * The function [reduce] reduces the matrix to a vector by treating the matrix rows/columns as a set of\n * 1D vectors and performing the specified operation on the vectors until a single row/column is\n * obtained. For example, the function can be used to compute horizontal and vertical projections of a\n * raster image. In case of [REDUCE_MAX] and [REDUCE_MIN] , the output image should have the same type\n * as the source one. In case of [REDUCE_SUM] and [REDUCE_AVG] , the output may have a larger element\n * bit-depth to preserve accuracy. And multi-channel arrays are also supported in these two reduction\n * modes.\n *\n * The following code demonstrates its usage for a single channel matrix.\n *\n * ```cpp\n *         Mat m = (Mat_<uchar>(3,2) << 1,2,3,4,5,6);\n *         Mat col_sum, row_sum;\n *\n *         reduce(m, col_sum, 0, REDUCE_SUM, CV_32F);\n *         reduce(m, row_sum, 1, REDUCE_SUM, CV_32F);\n *         /*\n *         m =\n *         [  1,   2;\n *            3,   4;\n *            5,   6]\n *         col_sum =\n *         [9, 12]\n *         row_sum =\n *         [3;\n *          7;\n *          11]\n * \\/\n * ```\n *\n *  And the following code demonstrates its usage for a two-channel matrix.\n *\n * ```cpp\n *         // two channels\n *         char d[] = {1,2,3,4,5,6};\n *         Mat m(3, 1, CV_8UC2, d);\n *         Mat col_sum_per_channel;\n *         reduce(m, col_sum_per_channel, 0, REDUCE_SUM, CV_32F);\n *         /*\n *         col_sum_per_channel =\n *         [9, 12]\n * \\/\n * ```\n *\n * [repeat]\n *\n * @param src input 2D matrix.\n *\n * @param dst output vector. Its size and type is defined by dim and dtype parameters.\n *\n * @param dim dimension index along which the matrix is reduced. 0 means that the matrix is reduced to\n * a single row. 1 means that the matrix is reduced to a single column.\n *\n * @param rtype reduction operation that could be one of ReduceTypes\n *\n * @param dtype when negative, the output vector will have the same type as the input matrix,\n * otherwise, its type will be CV_MAKE_TYPE(CV_MAT_DEPTH(dtype), src.channels()).\n */\nexport declare function reduce(src: InputArray, dst: OutputArray, dim: int, rtype: int, dtype?: int): void;\n/**\n * The function [cv::repeat] duplicates the input array one or more times along each of the two axes:\n * `\\\\[\\\\texttt{dst} _{ij}= \\\\texttt{src} _{i\\\\mod src.rows, \\\\; j\\\\mod src.cols }\\\\]` The second\n * variant of the function is more convenient to use with [MatrixExpressions].\n *\n * [cv::reduce]\n *\n * @param src input array to replicate.\n *\n * @param ny Flag to specify how many times the src is repeated along the vertical axis.\n *\n * @param nx Flag to specify how many times the src is repeated along the horizontal axis.\n *\n * @param dst output array of the same type as src.\n */\nexport declare function repeat(src: InputArray, ny: int, nx: int, dst: OutputArray): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n *\n * @param src input array to replicate.\n *\n * @param ny Flag to specify how many times the src is repeated along the vertical axis.\n *\n * @param nx Flag to specify how many times the src is repeated along the horizontal axis.\n */\nexport declare function repeat(src: any, ny: int, nx: int): Mat;\n/**\n * [transpose] , [repeat] , [completeSymm], [flip], [RotateFlags]\n *\n * @param src input array.\n *\n * @param dst output array of the same type as src. The size is the same with ROTATE_180, and the rows\n * and cols are switched for ROTATE_90_CLOCKWISE and ROTATE_90_COUNTERCLOCKWISE.\n *\n * @param rotateCode an enum to specify how to rotate the array; see the enum RotateFlags\n */\nexport declare function rotate(src: InputArray, dst: OutputArray, rotateCode: int): void;\n/**\n * The function scaleAdd is one of the classical primitive linear algebra operations, known as DAXPY or\n * SAXPY in . It calculates the sum of a scaled array and another array: `\\\\[\\\\texttt{dst} (I)=\n * \\\\texttt{scale} \\\\cdot \\\\texttt{src1} (I) + \\\\texttt{src2} (I)\\\\]` The function can also be emulated\n * with a matrix expression, for example:\n *\n * ```cpp\n * Mat A(3, 3, CV_64F);\n * ...\n * A.row(0) = A.row(1)*2 + A.row(2);\n * ```\n *\n * [add], [addWeighted], [subtract], [Mat::dot], [Mat::convertTo]\n *\n * @param src1 first input array.\n *\n * @param alpha scale factor for the first array.\n *\n * @param src2 second input array of the same size and type as src1.\n *\n * @param dst output array of the same size and type as src1.\n */\nexport declare function scaleAdd(src1: InputArray, alpha: double, src2: InputArray, dst: OutputArray): void;\n/**\n * The function [cv::setIdentity] initializes a scaled identity matrix: `\\\\[\\\\texttt{mtx} (i,j)=\n * \\\\fork{\\\\texttt{value}}{ if \\\\(i=j\\\\)}{0}{otherwise}\\\\]`\n *\n * The function can also be emulated using the matrix initializers and the matrix expressions:\n *\n * ```cpp\n * Mat A = Mat::eye(4, 3, CV_32F)*5;\n * // A will be set to [[5, 0, 0], [0, 5, 0], [0, 0, 5], [0, 0, 0]]\n * ```\n *\n * [Mat::zeros], [Mat::ones], [Mat::setTo], [Mat::operator=]\n *\n * @param mtx matrix to initialize (not necessarily square).\n *\n * @param s value to assign to diagonal elements.\n */\nexport declare function setIdentity(mtx: InputOutputArray, s?: any): void;\n/**\n * The function [cv::setRNGSeed] sets state of default random number generator to custom value.\n *\n * [RNG], [randu], [randn]\n *\n * @param seed new state for default random number generator\n */\nexport declare function setRNGSeed(seed: int): void;\n/**\n * The function [cv::solve] solves a linear system or least-squares problem (the latter is possible\n * with [SVD] or QR methods, or by specifying the flag [DECOMP_NORMAL] ): `\\\\[\\\\texttt{dst} = \\\\arg\n * \\\\min _X \\\\| \\\\texttt{src1} \\\\cdot \\\\texttt{X} - \\\\texttt{src2} \\\\|\\\\]`\n *\n * If [DECOMP_LU] or [DECOMP_CHOLESKY] method is used, the function returns 1 if src1 (or\n * `$\\\\texttt{src1}^T\\\\texttt{src1}$` ) is non-singular. Otherwise, it returns 0. In the latter case,\n * dst is not valid. Other methods find a pseudo-solution in case of a singular left-hand side part.\n *\n * If you want to find a unity-norm solution of an under-defined singular system\n * `$\\\\texttt{src1}\\\\cdot\\\\texttt{dst}=0$` , the function solve will not do the work. Use [SVD::solveZ]\n * instead.\n *\n * [invert], [SVD], [eigen]\n *\n * @param src1 input matrix on the left-hand side of the system.\n *\n * @param src2 input matrix on the right-hand side of the system.\n *\n * @param dst output solution.\n *\n * @param flags solution (matrix inversion) method (DecompTypes)\n */\nexport declare function solve(src1: InputArray, src2: InputArray, dst: OutputArray, flags?: int): bool;\n/**\n * The function solveCubic finds the real roots of a cubic equation:\n *\n * if coeffs is a 4-element vector: `\\\\[\\\\texttt{coeffs} [0] x^3 + \\\\texttt{coeffs} [1] x^2 +\n * \\\\texttt{coeffs} [2] x + \\\\texttt{coeffs} [3] = 0\\\\]`\n * if coeffs is a 3-element vector: `\\\\[x^3 + \\\\texttt{coeffs} [0] x^2 + \\\\texttt{coeffs} [1] x +\n * \\\\texttt{coeffs} [2] = 0\\\\]`\n *\n * The roots are stored in the roots array.\n *\n * number of real roots. It can be 0, 1 or 2.\n *\n * @param coeffs equation coefficients, an array of 3 or 4 elements.\n *\n * @param roots output array of real roots that has 1 or 3 elements.\n */\nexport declare function solveCubic(coeffs: InputArray, roots: OutputArray): int;\n/**\n * The function [cv::solvePoly] finds real and complex roots of a polynomial equation:\n * `\\\\[\\\\texttt{coeffs} [n] x^{n} + \\\\texttt{coeffs} [n-1] x^{n-1} + ... + \\\\texttt{coeffs} [1] x +\n * \\\\texttt{coeffs} [0] = 0\\\\]`\n *\n * @param coeffs array of polynomial coefficients.\n *\n * @param roots output (complex) array of roots.\n *\n * @param maxIters maximum number of iterations the algorithm does.\n */\nexport declare function solvePoly(coeffs: InputArray, roots: OutputArray, maxIters?: int): double;\n/**\n * The function [cv::sort] sorts each matrix row or each matrix column in ascending or descending\n * order. So you should pass two operation flags to get desired behaviour. If you want to sort matrix\n * rows or columns lexicographically, you can use STL std::sort generic function with the proper\n * comparison predicate.\n *\n * [sortIdx], [randShuffle]\n *\n * @param src input single-channel array.\n *\n * @param dst output array of the same size and type as src.\n *\n * @param flags operation flags, a combination of SortFlags\n */\nexport declare function sort(src: InputArray, dst: OutputArray, flags: int): void;\n/**\n * The function [cv::sortIdx] sorts each matrix row or each matrix column in the ascending or\n * descending order. So you should pass two operation flags to get desired behaviour. Instead of\n * reordering the elements themselves, it stores the indices of sorted elements in the output array.\n * For example:\n *\n * ```cpp\n * Mat A = Mat::eye(3,3,CV_32F), B;\n * sortIdx(A, B, SORT_EVERY_ROW + SORT_ASCENDING);\n * // B will probably contain\n * // (because of equal elements in A some permutations are possible):\n * // [[1, 2, 0], [0, 2, 1], [0, 1, 2]]\n * ```\n *\n * [sort], [randShuffle]\n *\n * @param src input single-channel array.\n *\n * @param dst output integer array of the same size as src.\n *\n * @param flags operation flags that could be a combination of cv::SortFlags\n */\nexport declare function sortIdx(src: InputArray, dst: OutputArray, flags: int): void;\n/**\n * The function [cv::split] splits a multi-channel array into separate single-channel arrays:\n * `\\\\[\\\\texttt{mv} [c](I) = \\\\texttt{src} (I)_c\\\\]` If you need to extract a single channel or do some\n * other sophisticated channel permutation, use mixChannels .\n *\n * The following example demonstrates how to split a 3-channel matrix into 3 single channel matrices.\n *\n * ```cpp\n *     char d[] = {1,2,3,4,5,6,7,8,9,10,11,12};\n *     Mat m(2, 2, CV_8UC3, d);\n *     Mat channels[3];\n *     split(m, channels);\n *\n *     /*\n *     channels[0] =\n *     [  1,   4;\n *        7,  10]\n *     channels[1] =\n *     [  2,   5;\n *        8,  11]\n *     channels[2] =\n *     [  3,   6;\n *        9,  12]\n * \\/\n * ```\n *\n * [merge], [mixChannels], [cvtColor]\n *\n * @param src input multi-channel array.\n *\n * @param mvbegin output array; the number of arrays must match src.channels(); the arrays themselves\n * are reallocated, if needed.\n */\nexport declare function split(src: any, mvbegin: any): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n *\n * @param m input multi-channel array.\n *\n * @param mv output vector of arrays; the arrays themselves are reallocated, if needed.\n */\nexport declare function split(m: InputArray, mv: OutputArrayOfArrays): void;\n/**\n * The function [cv::sqrt] calculates a square root of each input array element. In case of\n * multi-channel arrays, each channel is processed independently. The accuracy is approximately the\n * same as of the built-in std::sqrt .\n *\n * @param src input floating-point array.\n *\n * @param dst output array of the same size and type as src.\n */\nexport declare function sqrt(src: InputArray, dst: OutputArray): void;\n/**\n * The function subtract calculates:\n *\n * Difference between two arrays, when both input arrays have the same size and the same number of\n * channels: `\\\\[\\\\texttt{dst}(I) = \\\\texttt{saturate} ( \\\\texttt{src1}(I) - \\\\texttt{src2}(I)) \\\\quad\n * \\\\texttt{if mask}(I) \\\\ne0\\\\]`\n * Difference between an array and a scalar, when src2 is constructed from Scalar or has the same\n * number of elements as `src1.channels()`: `\\\\[\\\\texttt{dst}(I) = \\\\texttt{saturate} (\n * \\\\texttt{src1}(I) - \\\\texttt{src2} ) \\\\quad \\\\texttt{if mask}(I) \\\\ne0\\\\]`\n * Difference between a scalar and an array, when src1 is constructed from Scalar or has the same\n * number of elements as `src2.channels()`: `\\\\[\\\\texttt{dst}(I) = \\\\texttt{saturate} ( \\\\texttt{src1}\n * - \\\\texttt{src2}(I) ) \\\\quad \\\\texttt{if mask}(I) \\\\ne0\\\\]`\n * The reverse difference between a scalar and an array in the case of `SubRS`: `\\\\[\\\\texttt{dst}(I) =\n * \\\\texttt{saturate} ( \\\\texttt{src2} - \\\\texttt{src1}(I) ) \\\\quad \\\\texttt{if mask}(I) \\\\ne0\\\\]`\n * where I is a multi-dimensional index of array elements. In case of multi-channel arrays, each\n * channel is processed independently.\n *\n * The first function in the list above can be replaced with matrix expressions:\n *\n * ```cpp\n * dst = src1 - src2;\n * dst -= src1; // equivalent to subtract(dst, src1, dst);\n * ```\n *\n *  The input arrays and the output array can all have the same or different depths. For example, you\n * can subtract to 8-bit unsigned arrays and store the difference in a 16-bit signed array. Depth of\n * the output array is determined by dtype parameter. In the second and third cases above, as well as\n * in the first case, when src1.depth() == src2.depth(), dtype can be set to the default -1. In this\n * case the output array will have the same depth as the input array, be it src1, src2 or both.\n *\n * Saturation is not applied when the output array has the depth CV_32S. You may even get result of an\n * incorrect sign in the case of overflow.\n *\n * [add], [addWeighted], [scaleAdd], [Mat::convertTo]\n *\n * @param src1 first input array or a scalar.\n *\n * @param src2 second input array or a scalar.\n *\n * @param dst output array of the same size and the same number of channels as the input array.\n *\n * @param mask optional operation mask; this is an 8-bit single channel array that specifies elements\n * of the output array to be changed.\n *\n * @param dtype optional depth of the output array\n */\nexport declare function subtract(src1: InputArray, src2: InputArray, dst: OutputArray, mask?: InputArray, dtype?: int): void;\n/**\n * The function [cv::sum] calculates and returns the sum of array elements, independently for each\n * channel.\n *\n * [countNonZero], [mean], [meanStdDev], [norm], [minMaxLoc], [reduce]\n *\n * @param src input array that must have from 1 to 4 channels.\n */\nexport declare function sum(src: InputArray): Scalar;\n/**\n * wrap [SVD::backSubst]\n */\nexport declare function SVBackSubst(w: InputArray, u: InputArray, vt: InputArray, rhs: InputArray, dst: OutputArray): void;\n/**\n * wrap [SVD::compute]\n */\nexport declare function SVDecomp(src: InputArray, w: OutputArray, u: OutputArray, vt: OutputArray, flags?: int): void;\n/**\n * The function [cv::theRNG] returns the default random number generator. For each thread, there is a\n * separate random number generator, so you can use the function safely in multi-thread environments.\n * If you just need to get a single random number using this generator or initialize an array, you can\n * use randu or randn instead. But if you are going to generate many random numbers inside a loop, it\n * is much faster to use this function to retrieve the generator and then use RNG::operator _Tp() .\n *\n * [RNG], [randu], [randn]\n */\nexport declare function theRNG(): any;\n/**\n * The function [cv::trace] returns the sum of the diagonal elements of the matrix mtx .\n * `\\\\[\\\\mathrm{tr} ( \\\\texttt{mtx} ) = \\\\sum _i \\\\texttt{mtx} (i,i)\\\\]`\n *\n * @param mtx input matrix.\n */\nexport declare function trace(mtx: InputArray): Scalar;\n/**\n * The function [cv::transform] performs the matrix transformation of every element of the array src\n * and stores the results in dst : `\\\\[\\\\texttt{dst} (I) = \\\\texttt{m} \\\\cdot \\\\texttt{src} (I)\\\\]`\n * (when m.cols=src.channels() ), or `\\\\[\\\\texttt{dst} (I) = \\\\texttt{m} \\\\cdot [ \\\\texttt{src} (I);\n * 1]\\\\]` (when m.cols=src.channels()+1 )\n *\n * Every element of the N -channel array src is interpreted as N -element vector that is transformed\n * using the M x N or M x (N+1) matrix m to M-element vector - the corresponding element of the output\n * array dst .\n *\n * The function may be used for geometrical transformation of N -dimensional points, arbitrary linear\n * color space transformation (such as various kinds of RGB to YUV transforms), shuffling the image\n * channels, and so forth.\n *\n * [perspectiveTransform], [getAffineTransform], [estimateAffine2D], [warpAffine], [warpPerspective]\n *\n * @param src input array that must have as many channels (1 to 4) as m.cols or m.cols-1.\n *\n * @param dst output array of the same size and depth as src; it has as many channels as m.rows.\n *\n * @param m transformation 2x2 or 2x3 floating-point matrix.\n */\nexport declare function transform(src: InputArray, dst: OutputArray, m: InputArray): void;\n/**\n * The function [cv::transpose] transposes the matrix src : `\\\\[\\\\texttt{dst} (i,j) = \\\\texttt{src}\n * (j,i)\\\\]`\n *\n * No complex conjugation is done in case of a complex matrix. It should be done separately if needed.\n *\n * @param src input array.\n *\n * @param dst output array of the same type as src.\n */\nexport declare function transpose(src: InputArray, dst: OutputArray): void;\n/**\n * The function vertically concatenates two or more [cv::Mat] matrices (with the same number of cols).\n *\n * ```cpp\n * cv::Mat matArray[] = { cv::Mat(1, 4, CV_8UC1, cv::Scalar(1)),\n *                        cv::Mat(1, 4, CV_8UC1, cv::Scalar(2)),\n *                        cv::Mat(1, 4, CV_8UC1, cv::Scalar(3)),};\n *\n * cv::Mat out;\n * cv::vconcat( matArray, 3, out );\n * //out:\n * //[1,   1,   1,   1;\n * // 2,   2,   2,   2;\n * // 3,   3,   3,   3]\n * ```\n *\n * [cv::hconcat(const Mat*, size_t, OutputArray)],\n *\n * [cv::hconcat(InputArrayOfArrays, OutputArray)] and\n *\n * [cv::hconcat(InputArray, InputArray, OutputArray)]\n *\n * @param src input array or vector of matrices. all of the matrices must have the same number of cols\n * and the same depth.\n *\n * @param nsrc number of matrices in src.\n *\n * @param dst output array. It has the same number of cols and depth as the src, and the sum of rows of\n * the src.\n */\nexport declare function vconcat(src: any, nsrc: size_t, dst: OutputArray): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n *\n * ```cpp\n * cv::Mat_<float> A = (cv::Mat_<float>(3, 2) << 1, 7,\n *                                               2, 8,\n *                                               3, 9);\n * cv::Mat_<float> B = (cv::Mat_<float>(3, 2) << 4, 10,\n *                                               5, 11,\n *                                               6, 12);\n *\n * cv::Mat C;\n * cv::vconcat(A, B, C);\n * //C:\n * //[1, 7;\n * // 2, 8;\n * // 3, 9;\n * // 4, 10;\n * // 5, 11;\n * // 6, 12]\n * ```\n *\n * @param src1 first input array to be considered for vertical concatenation.\n *\n * @param src2 second input array to be considered for vertical concatenation.\n *\n * @param dst output array. It has the same number of cols and depth as the src1 and src2, and the sum\n * of rows of the src1 and src2.\n */\nexport declare function vconcat(src1: InputArray, src2: InputArray, dst: OutputArray): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n *\n * ```cpp\n * std::vector<cv::Mat> matrices = { cv::Mat(1, 4, CV_8UC1, cv::Scalar(1)),\n *                                   cv::Mat(1, 4, CV_8UC1, cv::Scalar(2)),\n *                                   cv::Mat(1, 4, CV_8UC1, cv::Scalar(3)),};\n *\n * cv::Mat out;\n * cv::vconcat( matrices, out );\n * //out:\n * //[1,   1,   1,   1;\n * // 2,   2,   2,   2;\n * // 3,   3,   3,   3]\n * ```\n *\n * @param src input array or vector of matrices. all of the matrices must have the same number of cols\n * and the same depth\n *\n * @param dst output array. It has the same number of cols and depth as the src, and the sum of rows of\n * the src. same depth.\n */\nexport declare function vconcat(src: InputArrayOfArrays, dst: OutputArray): void;\nexport declare const BORDER_CONSTANT: BorderTypes;\nexport declare const BORDER_REPLICATE: BorderTypes;\nexport declare const BORDER_REFLECT: BorderTypes;\nexport declare const BORDER_WRAP: BorderTypes;\nexport declare const BORDER_REFLECT_101: BorderTypes;\nexport declare const BORDER_TRANSPARENT: BorderTypes;\nexport declare const BORDER_REFLECT101: BorderTypes;\nexport declare const BORDER_DEFAULT: BorderTypes;\nexport declare const BORDER_ISOLATED: BorderTypes;\nexport declare const CMP_EQ: CmpTypes;\nexport declare const CMP_GT: CmpTypes;\nexport declare const CMP_GE: CmpTypes;\nexport declare const CMP_LT: CmpTypes;\nexport declare const CMP_LE: CmpTypes;\nexport declare const CMP_NE: CmpTypes;\n/**\n * Gaussian elimination with the optimal pivot element chosen.\n *\n */\nexport declare const DECOMP_LU: DecompTypes;\n/**\n * singular value decomposition ([SVD]) method; the system can be over-defined and/or the matrix src1\n * can be singular\n *\n */\nexport declare const DECOMP_SVD: DecompTypes;\n/**\n * eigenvalue decomposition; the matrix src1 must be symmetrical\n *\n */\nexport declare const DECOMP_EIG: DecompTypes;\n/**\n * Cholesky `$LL^T$` factorization; the matrix src1 must be symmetrical and positively defined\n *\n */\nexport declare const DECOMP_CHOLESKY: DecompTypes;\n/**\n * QR factorization; the system can be over-defined and/or the matrix src1 can be singular\n *\n */\nexport declare const DECOMP_QR: DecompTypes;\n/**\n * while all the previous flags are mutually exclusive, this flag can be used together with any of the\n * previous; it means that the normal equations\n * `$\\\\texttt{src1}^T\\\\cdot\\\\texttt{src1}\\\\cdot\\\\texttt{dst}=\\\\texttt{src1}^T\\\\texttt{src2}$` are\n * solved instead of the original system `$\\\\texttt{src1}\\\\cdot\\\\texttt{dst}=\\\\texttt{src2}$`\n *\n */\nexport declare const DECOMP_NORMAL: DecompTypes;\n/**\n * performs an inverse 1D or 2D transform instead of the default forward transform.\n *\n */\nexport declare const DFT_INVERSE: DftFlags;\n/**\n * scales the result: divide it by the number of array elements. Normally, it is combined with\n * DFT_INVERSE.\n *\n */\nexport declare const DFT_SCALE: DftFlags;\n/**\n * performs a forward or inverse transform of every individual row of the input matrix; this flag\n * enables you to transform multiple vectors simultaneously and can be used to decrease the overhead\n * (which is sometimes several times larger than the processing itself) to perform 3D and\n * higher-dimensional transformations and so forth.\n *\n */\nexport declare const DFT_ROWS: DftFlags;\n/**\n * performs a forward transformation of 1D or 2D real array; the result, though being a complex array,\n * has complex-conjugate symmetry (*CCS*, see the function description below for details), and such an\n * array can be packed into a real array of the same size as input, which is the fastest option and\n * which is what the function does by default; however, you may wish to get a full complex array (for\n * simpler spectrum analysis, and so on) - pass the flag to enable the function to produce a full-size\n * complex output array.\n *\n */\nexport declare const DFT_COMPLEX_OUTPUT: DftFlags;\n/**\n * performs an inverse transformation of a 1D or 2D complex array; the result is normally a complex\n * array of the same size, however, if the input array has conjugate-complex symmetry (for example, it\n * is a result of forward transformation with DFT_COMPLEX_OUTPUT flag), the output is a real array;\n * while the function itself does not check whether the input is symmetrical or not, you can pass the\n * flag and then the function will assume the symmetry and produce the real output array (note that\n * when the input is packed into a real array and inverse transformation is executed, the function\n * treats the input as a packed complex-conjugate symmetrical array, and the output will also be a real\n * array).\n *\n */\nexport declare const DFT_REAL_OUTPUT: DftFlags;\n/**\n * specifies that input is complex input. If this flag is set, the input must have 2 channels. On the\n * other hand, for backwards compatibility reason, if input has 2 channels, input is already considered\n * complex.\n *\n */\nexport declare const DFT_COMPLEX_INPUT: DftFlags;\n/**\n * performs an inverse 1D or 2D transform instead of the default forward transform.\n *\n */\nexport declare const DCT_INVERSE: DftFlags;\n/**\n * performs a forward or inverse transform of every individual row of the input matrix. This flag\n * enables you to transform multiple vectors simultaneously and can be used to decrease the overhead\n * (which is sometimes several times larger than the processing itself) to perform 3D and\n * higher-dimensional transforms and so forth.\n *\n */\nexport declare const DCT_ROWS: DftFlags;\nexport declare const GEMM_1_T: GemmFlags;\nexport declare const GEMM_2_T: GemmFlags;\nexport declare const GEMM_3_T: GemmFlags;\n/**\n * `\\\\[ norm = \\\\forkthree {\\\\|\\\\texttt{src1}\\\\|_{L_{\\\\infty}} = \\\\max _I | \\\\texttt{src1} (I)|}{if\n * \\\\(\\\\texttt{normType} = \\\\texttt{NORM_INF}\\\\) } {\\\\|\\\\texttt{src1}-\\\\texttt{src2}\\\\|_{L_{\\\\infty}} =\n * \\\\max _I | \\\\texttt{src1} (I) - \\\\texttt{src2} (I)|}{if \\\\(\\\\texttt{normType} =\n * \\\\texttt{NORM_INF}\\\\) } {\\\\frac{\\\\|\\\\texttt{src1}-\\\\texttt{src2}\\\\|_{L_{\\\\infty}}\n * }{\\\\|\\\\texttt{src2}\\\\|_{L_{\\\\infty}} }}{if \\\\(\\\\texttt{normType} = \\\\texttt{NORM_RELATIVE |\n * NORM_INF}\\\\) } \\\\]`\n *\n */\nexport declare const NORM_INF: NormTypes;\n/**\n * `\\\\[ norm = \\\\forkthree {\\\\| \\\\texttt{src1} \\\\| _{L_1} = \\\\sum _I | \\\\texttt{src1} (I)|}{if\n * \\\\(\\\\texttt{normType} = \\\\texttt{NORM_L1}\\\\)} { \\\\| \\\\texttt{src1} - \\\\texttt{src2} \\\\| _{L_1} =\n * \\\\sum _I | \\\\texttt{src1} (I) - \\\\texttt{src2} (I)|}{if \\\\(\\\\texttt{normType} = \\\\texttt{NORM_L1}\\\\)\n * } { \\\\frac{\\\\|\\\\texttt{src1}-\\\\texttt{src2}\\\\|_{L_1} }{\\\\|\\\\texttt{src2}\\\\|_{L_1}} }{if\n * \\\\(\\\\texttt{normType} = \\\\texttt{NORM_RELATIVE | NORM_L1}\\\\) } \\\\]`\n *\n */\nexport declare const NORM_L1: NormTypes;\n/**\n * `\\\\[ norm = \\\\forkthree { \\\\| \\\\texttt{src1} \\\\| _{L_2} = \\\\sqrt{\\\\sum_I \\\\texttt{src1}(I)^2} }{if\n * \\\\(\\\\texttt{normType} = \\\\texttt{NORM_L2}\\\\) } { \\\\| \\\\texttt{src1} - \\\\texttt{src2} \\\\| _{L_2} =\n * \\\\sqrt{\\\\sum_I (\\\\texttt{src1}(I) - \\\\texttt{src2}(I))^2} }{if \\\\(\\\\texttt{normType} =\n * \\\\texttt{NORM_L2}\\\\) } { \\\\frac{\\\\|\\\\texttt{src1}-\\\\texttt{src2}\\\\|_{L_2}\n * }{\\\\|\\\\texttt{src2}\\\\|_{L_2}} }{if \\\\(\\\\texttt{normType} = \\\\texttt{NORM_RELATIVE | NORM_L2}\\\\) }\n * \\\\]`\n *\n */\nexport declare const NORM_L2: NormTypes;\n/**\n * `\\\\[ norm = \\\\forkthree { \\\\| \\\\texttt{src1} \\\\| _{L_2} ^{2} = \\\\sum_I \\\\texttt{src1}(I)^2} {if\n * \\\\(\\\\texttt{normType} = \\\\texttt{NORM_L2SQR}\\\\)} { \\\\| \\\\texttt{src1} - \\\\texttt{src2} \\\\| _{L_2}\n * ^{2} = \\\\sum_I (\\\\texttt{src1}(I) - \\\\texttt{src2}(I))^2 }{if \\\\(\\\\texttt{normType} =\n * \\\\texttt{NORM_L2SQR}\\\\) } { \\\\left(\\\\frac{\\\\|\\\\texttt{src1}-\\\\texttt{src2}\\\\|_{L_2}\n * }{\\\\|\\\\texttt{src2}\\\\|_{L_2}}\\\\right)^2 }{if \\\\(\\\\texttt{normType} = \\\\texttt{NORM_RELATIVE |\n * NORM_L2SQR}\\\\) } \\\\]`\n *\n */\nexport declare const NORM_L2SQR: NormTypes;\n/**\n * In the case of one input array, calculates the [Hamming] distance of the array from zero, In the\n * case of two input arrays, calculates the [Hamming] distance between the arrays.\n *\n */\nexport declare const NORM_HAMMING: NormTypes;\n/**\n * Similar to NORM_HAMMING, but in the calculation, each two bits of the input sequence will be added\n * and treated as a single bit to be used in the same calculation as NORM_HAMMING.\n *\n */\nexport declare const NORM_HAMMING2: NormTypes;\nexport declare const NORM_TYPE_MASK: NormTypes;\nexport declare const NORM_RELATIVE: NormTypes;\nexport declare const NORM_MINMAX: NormTypes;\nexport declare const ROTATE_90_CLOCKWISE: RotateFlags;\nexport declare const ROTATE_180: RotateFlags;\nexport declare const ROTATE_90_COUNTERCLOCKWISE: RotateFlags;\n/**\n * Various border types, image boundaries are denoted with `|`\n *\n * [borderInterpolate], [copyMakeBorder]\n *\n */\nexport declare type BorderTypes = any;\n/**\n * Various border types, image boundaries are denoted with `|`\n *\n * [borderInterpolate], [copyMakeBorder]\n *\n */\nexport declare type CmpTypes = any;\n/**\n * Various border types, image boundaries are denoted with `|`\n *\n * [borderInterpolate], [copyMakeBorder]\n *\n */\nexport declare type DecompTypes = any;\n/**\n * Various border types, image boundaries are denoted with `|`\n *\n * [borderInterpolate], [copyMakeBorder]\n *\n */\nexport declare type DftFlags = any;\n/**\n * Various border types, image boundaries are denoted with `|`\n *\n * [borderInterpolate], [copyMakeBorder]\n *\n */\nexport declare type GemmFlags = any;\n/**\n * Various border types, image boundaries are denoted with `|`\n *\n * [borderInterpolate], [copyMakeBorder]\n *\n */\nexport declare type NormTypes = any;\n/**\n * Various border types, image boundaries are denoted with `|`\n *\n * [borderInterpolate], [copyMakeBorder]\n *\n */\nexport declare type RotateFlags = any;\n"},"node_modules_mirada_dist_src_types_opencv_dnn_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_dnn_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/dnn.d.ts","content":"import { bool, double, InputArray, InputArrayOfArrays, int, Mat, Net, OutputArray, OutputArrayOfArrays, Size, size_t, uchar } from './_types';\n/**\n * if `crop` is true, input image is resized so one side after resize is equal to corresponding\n * dimension in `size` and another one is equal or larger. Then, crop from the center is performed. If\n * `crop` is false, direct resize without cropping and preserving aspect ratio is performed.\n *\n * 4-dimensional [Mat] with NCHW dimensions order.\n *\n * @param image input image (with 1-, 3- or 4-channels).\n *\n * @param scalefactor multiplier for image values.\n *\n * @param size spatial size for output image\n *\n * @param mean scalar with mean values which are subtracted from channels. Values are intended to be in\n * (mean-R, mean-G, mean-B) order if image has BGR ordering and swapRB is true.\n *\n * @param swapRB flag which indicates that swap first and last channels in 3-channel image is\n * necessary.\n *\n * @param crop flag which indicates whether image will be cropped after resize or not\n *\n * @param ddepth Depth of output blob. Choose CV_32F or CV_8U.\n */\nexport declare function blobFromImage(image: InputArray, scalefactor?: double, size?: any, mean?: any, swapRB?: bool, crop?: bool, ddepth?: int): Mat;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function blobFromImage(image: InputArray, blob: OutputArray, scalefactor?: double, size?: any, mean?: any, swapRB?: bool, crop?: bool, ddepth?: int): void;\n/**\n * if `crop` is true, input image is resized so one side after resize is equal to corresponding\n * dimension in `size` and another one is equal or larger. Then, crop from the center is performed. If\n * `crop` is false, direct resize without cropping and preserving aspect ratio is performed.\n *\n * 4-dimensional [Mat] with NCHW dimensions order.\n *\n * @param images input images (all with 1-, 3- or 4-channels).\n *\n * @param scalefactor multiplier for images values.\n *\n * @param size spatial size for output image\n *\n * @param mean scalar with mean values which are subtracted from channels. Values are intended to be in\n * (mean-R, mean-G, mean-B) order if image has BGR ordering and swapRB is true.\n *\n * @param swapRB flag which indicates that swap first and last channels in 3-channel image is\n * necessary.\n *\n * @param crop flag which indicates whether image will be cropped after resize or not\n *\n * @param ddepth Depth of output blob. Choose CV_32F or CV_8U.\n */\nexport declare function blobFromImages(images: InputArrayOfArrays, scalefactor?: double, size?: Size, mean?: any, swapRB?: bool, crop?: bool, ddepth?: int): Mat;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function blobFromImages(images: InputArrayOfArrays, blob: OutputArray, scalefactor?: double, size?: Size, mean?: any, swapRB?: bool, crop?: bool, ddepth?: int): void;\nexport declare function getAvailableBackends(): any;\nexport declare function getAvailableTargets(be: Backend): any;\n/**\n * @param blob_ 4 dimensional array (images, channels, height, width) in floating point precision\n * (CV_32F) from which you would like to extract the images.\n *\n * @param images_ array of 2D Mat containing the images extracted from the blob in floating point\n * precision (CV_32F). They are non normalized neither mean added. The number of returned images equals\n * the first dimension of the blob (batch size). Every image has a number of channels equals to the\n * second dimension of the blob (depth).\n */\nexport declare function imagesFromBlob(blob_: any, images_: OutputArrayOfArrays): any;\n/**\n * @param bboxes a set of bounding boxes to apply NMS.\n *\n * @param scores a set of corresponding confidences.\n *\n * @param score_threshold a threshold used to filter boxes by score.\n *\n * @param nms_threshold a threshold used in non maximum suppression.\n *\n * @param indices the kept indices of bboxes after NMS.\n *\n * @param eta a coefficient in adaptive threshold formula: $nms\\_threshold_{i+1}=eta\\cdot\n * nms\\_threshold_i$.\n *\n * @param top_k if >0, keep at most top_k picked indices.\n */\nexport declare function NMSBoxes(bboxes: any, scores: any, score_threshold: any, nms_threshold: any, indices: any, eta?: any, top_k?: any): void;\nexport declare function NMSBoxes(bboxes: any, scores: any, score_threshold: any, nms_threshold: any, indices: any, eta?: any, top_k?: any): void;\nexport declare function NMSBoxes(bboxes: any, scores: any, score_threshold: any, nms_threshold: any, indices: any, eta?: any, top_k?: any): void;\n/**\n * [Net] object.\n * This function automatically detects an origin framework of trained model and calls an appropriate\n * function such [readNetFromCaffe], [readNetFromTensorflow], [readNetFromTorch] or\n * [readNetFromDarknet]. An order of `model` and `config` arguments does not matter.\n *\n * @param model Binary file contains trained weights. The following file extensions are expected for\n * models from different frameworks:\n * .caffemodel (Caffe, http://caffe.berkeleyvision.org/)*.pb (TensorFlow,\n * https://www.tensorflow.org/)*.t7 | *.net (Torch, http://torch.ch/)*.weights (Darknet,\n * https://pjreddie.com/darknet/)*.bin (DLDT, https://software.intel.com/openvino-toolkit)*.onnx (ONNX,\n * https://onnx.ai/)\n *\n * @param config Text file contains network configuration. It could be a file with the following\n * extensions:\n * .prototxt (Caffe, http://caffe.berkeleyvision.org/)*.pbtxt (TensorFlow,\n * https://www.tensorflow.org/)*.cfg (Darknet, https://pjreddie.com/darknet/)*.xml (DLDT,\n * https://software.intel.com/openvino-toolkit)\n *\n * @param framework Explicit framework name tag to determine a format.\n */\nexport declare function readNet(model: any, config?: any, framework?: any): Net;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n *\n * [Net] object.\n *\n * @param framework Name of origin framework.\n *\n * @param bufferModel A buffer with a content of binary file with weights\n *\n * @param bufferConfig A buffer with a content of text file contains network configuration.\n */\nexport declare function readNet(framework: any, bufferModel: uchar, bufferConfig?: uchar): uchar;\n/**\n * [Net] object.\n *\n * @param prototxt path to the .prototxt file with text description of the network architecture.\n *\n * @param caffeModel path to the .caffemodel file with learned network.\n */\nexport declare function readNetFromCaffe(prototxt: any, caffeModel?: any): Net;\n/**\n * [Net] object.\n *\n * @param bufferProto buffer containing the content of the .prototxt file\n *\n * @param bufferModel buffer containing the content of the .caffemodel file\n */\nexport declare function readNetFromCaffe(bufferProto: uchar, bufferModel?: uchar): uchar;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n *\n * [Net] object.\n *\n * @param bufferProto buffer containing the content of the .prototxt file\n *\n * @param lenProto length of bufferProto\n *\n * @param bufferModel buffer containing the content of the .caffemodel file\n *\n * @param lenModel length of bufferModel\n */\nexport declare function readNetFromCaffe(bufferProto: any, lenProto: size_t, bufferModel?: any, lenModel?: size_t): Net;\n/**\n * Network object that ready to do forward, throw an exception in failure cases.\n *\n * [Net] object.\n *\n * @param cfgFile path to the .cfg file with text description of the network architecture.\n *\n * @param darknetModel path to the .weights file with learned network.\n */\nexport declare function readNetFromDarknet(cfgFile: any, darknetModel?: any): Net;\n/**\n * [Net] object.\n *\n * @param bufferCfg A buffer contains a content of .cfg file with text description of the network\n * architecture.\n *\n * @param bufferModel A buffer contains a content of .weights file with learned network.\n */\nexport declare function readNetFromDarknet(bufferCfg: uchar, bufferModel?: uchar): uchar;\n/**\n * [Net] object.\n *\n * @param bufferCfg A buffer contains a content of .cfg file with text description of the network\n * architecture.\n *\n * @param lenCfg Number of bytes to read from bufferCfg\n *\n * @param bufferModel A buffer contains a content of .weights file with learned network.\n *\n * @param lenModel Number of bytes to read from bufferModel\n */\nexport declare function readNetFromDarknet(bufferCfg: any, lenCfg: size_t, bufferModel?: any, lenModel?: size_t): Net;\n/**\n * [Net] object. Networks imported from Intel's [Model] Optimizer are launched in Intel's Inference\n * Engine backend.\n *\n * @param xml XML configuration file with network's topology.\n *\n * @param bin Binary file with trained weights.\n */\nexport declare function readNetFromModelOptimizer(xml: any, bin: any): Net;\n/**\n * Network object that ready to do forward, throw an exception in failure cases.\n *\n * @param onnxFile path to the .onnx file with text description of the network architecture.\n */\nexport declare function readNetFromONNX(onnxFile: any): Net;\n/**\n * Network object that ready to do forward, throw an exception in failure cases.\n *\n * @param buffer memory address of the first byte of the buffer.\n *\n * @param sizeBuffer size of the buffer.\n */\nexport declare function readNetFromONNX(buffer: any, sizeBuffer: size_t): Net;\n/**\n * Network object that ready to do forward, throw an exception in failure cases.\n *\n * @param buffer in-memory buffer that stores the ONNX model bytes.\n */\nexport declare function readNetFromONNX(buffer: uchar): uchar;\n/**\n * [Net] object.\n *\n * @param model path to the .pb file with binary protobuf description of the network architecture\n *\n * @param config path to the .pbtxt file that contains text graph definition in protobuf format.\n * Resulting Net object is built by text graph using weights from a binary one that let us make it more\n * flexible.\n */\nexport declare function readNetFromTensorflow(model: any, config?: any): Net;\n/**\n * [Net] object.\n *\n * @param bufferModel buffer containing the content of the pb file\n *\n * @param bufferConfig buffer containing the content of the pbtxt file\n */\nexport declare function readNetFromTensorflow(bufferModel: uchar, bufferConfig?: uchar): uchar;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n *\n * @param bufferModel buffer containing the content of the pb file\n *\n * @param lenModel length of bufferModel\n *\n * @param bufferConfig buffer containing the content of the pbtxt file\n *\n * @param lenConfig length of bufferConfig\n */\nexport declare function readNetFromTensorflow(bufferModel: any, lenModel: size_t, bufferConfig?: any, lenConfig?: size_t): Net;\n/**\n * [Net] object.\n *\n * Ascii mode of Torch serializer is more preferable, because binary mode extensively use `long` type\n * of C language, which has various bit-length on different systems.\n * The loading file must contain serialized  object with importing network. Try to eliminate a custom\n * objects from serialazing data to avoid importing errors.\n *\n * List of supported layers (i.e. object instances derived from Torch nn.Module class):\n *\n * nn.Sequential\n * nn.Parallel\n * nn.Concat\n * nn.Linear\n * nn.SpatialConvolution\n * nn.SpatialMaxPooling, nn.SpatialAveragePooling\n * nn.ReLU, nn.TanH, nn.Sigmoid\n * nn.Reshape\n * nn.SoftMax, nn.LogSoftMax\n *\n * Also some equivalents of these classes from cunn, cudnn, and fbcunn may be successfully imported.\n *\n * @param model path to the file, dumped from Torch by using torch.save() function.\n *\n * @param isBinary specifies whether the network was serialized in ascii mode or binary.\n *\n * @param evaluate specifies testing phase of network. If true, it's similar to evaluate() method in\n * Torch.\n */\nexport declare function readNetFromTorch(model: any, isBinary?: bool, evaluate?: bool): Net;\n/**\n * [Mat].\n *\n * @param path to the .pb file with input tensor.\n */\nexport declare function readTensorFromONNX(path: any): Mat;\n/**\n * This function has the same limitations as [readNetFromTorch()].\n */\nexport declare function readTorchBlob(filename: any, isBinary?: bool): Mat;\n/**\n * Shrinked model has no origin float32 weights so it can't be used in origin Caffe framework anymore.\n * However the structure of data is taken from NVidia's Caffe fork: . So the resulting model may be\n * used there.\n *\n * @param src Path to origin model from Caffe framework contains single precision floating point\n * weights (usually has .caffemodel extension).\n *\n * @param dst Path to destination model with updated weights.\n *\n * @param layersTypes Set of layers types which parameters will be converted. By default, converts only\n * Convolutional and Fully-Connected layers' weights.\n */\nexport declare function shrinkCaffeModel(src: any, dst: any, layersTypes?: any): void;\n/**\n * To reduce output file size, trained weights are not included.\n *\n * @param model A path to binary network.\n *\n * @param output A path to output text file to be created.\n */\nexport declare function writeTextGraph(model: any, output: any): void;\n/**\n * DNN_BACKEND_DEFAULT equals to DNN_BACKEND_INFERENCE_ENGINE if OpenCV is built with Intel's Inference\n * Engine library or DNN_BACKEND_OPENCV otherwise.\n *\n */\nexport declare const DNN_BACKEND_DEFAULT: Backend;\nexport declare const DNN_BACKEND_HALIDE: Backend;\nexport declare const DNN_BACKEND_INFERENCE_ENGINE: Backend;\nexport declare const DNN_BACKEND_OPENCV: Backend;\nexport declare const DNN_BACKEND_VKCOM: Backend;\nexport declare const DNN_TARGET_CPU: Target;\nexport declare const DNN_TARGET_OPENCL: Target;\nexport declare const DNN_TARGET_OPENCL_FP16: Target;\nexport declare const DNN_TARGET_MYRIAD: Target;\nexport declare const DNN_TARGET_VULKAN: Target;\nexport declare const DNN_TARGET_FPGA: Target;\n/**\n * [Net::setPreferableBackend]\n *\n */\nexport declare type Backend = any;\n/**\n * [Net::setPreferableBackend]\n *\n */\nexport declare type Target = any;\n"},"node_modules_mirada_dist_src_types_opencv_DynamicBitset_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_DynamicBitset_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/DynamicBitset.d.ts","content":"import { bool, size_t } from './_types';\n/**\n * Class re-implementing the boost version of it This helps not depending on boost, it also does not do\n * the bound checks and has a way to reset a block for speed\n *\n * Source:\n * [opencv2/flann/dynamic_bitset.h](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/flann/dynamic_bitset.h#L150).\n *\n */\nexport declare class DynamicBitset {\n    /**\n     *   default constructor\n     */\n    constructor();\n    /**\n     *   only constructor we use in our code\n     *\n     * @param sz the size of the bitset (in bits)\n     */\n    constructor(sz: size_t);\n    /**\n     *   Sets all the bits to 0\n     */\n    clear(): void;\n    /**\n     *   true if the bitset is empty\n     */\n    empty(): bool;\n    /**\n     *   set all the bits to 0\n     */\n    reset(): void;\n    reset(index: size_t): void;\n    reset_block(index: size_t): void;\n    /**\n     *   resize the bitset so that it contains at least sz bits\n     */\n    resize(sz: size_t): void;\n    /**\n     *   set a bit to true\n     *\n     * @param index the index of the bit to set to 1\n     */\n    set(index: size_t): void;\n    /**\n     *   gives the number of contained bits\n     */\n    size(): size_t;\n    /**\n     *   check if a bit is set\n     *\n     *   true if the bit is set\n     *\n     * @param index the index of the bit to check\n     */\n    test(index: size_t): bool;\n}\n"},"node_modules_mirada_dist_src_types_opencv_Exception_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_Exception_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/Exception.d.ts","content":"import { int } from './_types';\n/**\n * This class encapsulates all or almost all necessary information about the error happened in the\n * program. The exception is usually constructed and thrown implicitly via CV_Error and CV_Error_\n * macros.\n *\n * [error](#db/de0/group__core__utils_1gacbd081fdb20423a63cf731569ba70b2b})\n *\n * Source:\n * [opencv2/core.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/core.hpp#L135).\n *\n */\nexport declare class Exception {\n    /**\n     *   CVStatus\n     *\n     */\n    code: int;\n    err: String;\n    file: String;\n    func: String;\n    line: int;\n    msg: String;\n    /**\n     *   Default constructor\n     */\n    constructor();\n    /**\n     *   Full constructor. Normally the constructor is not called explicitly. Instead, the macros\n     * [CV_Error()], [CV_Error_()] and [CV_Assert()] are used.\n     */\n    constructor(_code: int, _err: String, _func: String, _file: String, _line: int);\n    formatMessage(): void;\n    /**\n     *   the error description and the context as a text string.\n     */\n    what(): any;\n}\n"},"node_modules_mirada_dist_src_types_opencv_features2d_draw_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_features2d_draw_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/features2d_draw.d.ts","content":"import { InputArray, InputOutputArray } from './_types';\n/**\n * For Python API, flags are modified as cv.DRAW_MATCHES_FLAGS_DEFAULT,\n * cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS, cv.DRAW_MATCHES_FLAGS_DRAW_OVER_OUTIMG,\n * cv.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS\n *\n * @param image Source image.\n *\n * @param keypoints Keypoints from the source image.\n *\n * @param outImage Output image. Its content depends on the flags value defining what is drawn in the\n * output image. See possible flags bit values below.\n *\n * @param color Color of keypoints.\n *\n * @param flags Flags setting drawing features. Possible flags bit values are defined by\n * DrawMatchesFlags. See details above in drawMatches .\n */\nexport declare function drawKeypoints(image: InputArray, keypoints: any, outImage: InputOutputArray, color?: any, flags?: DrawMatchesFlags): void;\n/**\n * This function draws matches of keypoints from two images in the output image. Match is a line\n * connecting two keypoints (circles). See [cv::DrawMatchesFlags].\n *\n * @param img1 First source image.\n *\n * @param keypoints1 Keypoints from the first source image.\n *\n * @param img2 Second source image.\n *\n * @param keypoints2 Keypoints from the second source image.\n *\n * @param matches1to2 Matches from the first image to the second one, which means that keypoints1[i]\n * has a corresponding point in keypoints2[matches[i]] .\n *\n * @param outImg Output image. Its content depends on the flags value defining what is drawn in the\n * output image. See possible flags bit values below.\n *\n * @param matchColor Color of matches (lines and connected keypoints). If matchColor==Scalar::all(-1) ,\n * the color is generated randomly.\n *\n * @param singlePointColor Color of single keypoints (circles), which means that keypoints do not have\n * the matches. If singlePointColor==Scalar::all(-1) , the color is generated randomly.\n *\n * @param matchesMask Mask determining which matches are drawn. If the mask is empty, all matches are\n * drawn.\n *\n * @param flags Flags setting drawing features. Possible flags bit values are defined by\n * DrawMatchesFlags.\n */\nexport declare function drawMatches(img1: InputArray, keypoints1: any, img2: InputArray, keypoints2: any, matches1to2: any, outImg: InputOutputArray, matchColor?: any, singlePointColor?: any, matchesMask?: any, flags?: DrawMatchesFlags): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function drawMatches(img1: InputArray, keypoints1: any, img2: InputArray, keypoints2: any, matches1to2: any, outImg: InputOutputArray, matchColor?: any, singlePointColor?: any, matchesMask?: any, flags?: DrawMatchesFlags): void;\n/**\n * Output image matrix will be created ([Mat::create]), i.e. existing memory of output image may be\n * reused. Two source image, matches and single keypoints will be drawn. For each keypoint only the\n * center point will be drawn (without the circle around keypoint with keypoint size and orientation).\n *\n */\nexport declare const DEFAULT: DrawMatchesFlags;\n/**\n * Output image matrix will not be created ([Mat::create]). Matches will be drawn on existing content\n * of output image.\n *\n */\nexport declare const DRAW_OVER_OUTIMG: DrawMatchesFlags;\nexport declare const NOT_DRAW_SINGLE_POINTS: DrawMatchesFlags;\n/**\n * For each keypoint the circle around keypoint with keypoint size and orientation will be drawn.\n *\n */\nexport declare const DRAW_RICH_KEYPOINTS: DrawMatchesFlags;\nexport declare type DrawMatchesFlags = any;\n"},"node_modules_mirada_dist_src_types_opencv_HOGDescriptor_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_HOGDescriptor_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/HOGDescriptor.d.ts","content":"import { bool, DetectionROI, double, FileNode, FileStorage, float, InputArray, InputOutputArray, int, Point, Rect, Size, size_t, UMat } from './_types';\n/**\n * the HOG descriptor algorithm introduced by Navneet Dalal and Bill Triggs Dalal2005 .\n *\n * useful links:\n *\n * Source:\n * [opencv2/objdetect.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/objdetect.hpp#L377).\n *\n */\nexport declare class HOGDescriptor {\n    blockSize: Size;\n    blockStride: Size;\n    cellSize: Size;\n    derivAperture: int;\n    free_coef: float;\n    gammaCorrection: bool;\n    histogramNormType: any;\n    L2HysThreshold: double;\n    nbins: int;\n    nlevels: int;\n    oclSvmDetector: UMat;\n    signedGradient: bool;\n    svmDetector: any;\n    winSigma: double;\n    winSize: Size;\n    /**\n     *   aqual to [HOGDescriptor](Size(64,128), Size(16,16), Size(8,8), Size(8,8), 9 )\n     */\n    constructor();\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param _winSize sets winSize with given value.\n     *\n     * @param _blockSize sets blockSize with given value.\n     *\n     * @param _blockStride sets blockStride with given value.\n     *\n     * @param _cellSize sets cellSize with given value.\n     *\n     * @param _nbins sets nbins with given value.\n     *\n     * @param _derivAperture sets derivAperture with given value.\n     *\n     * @param _winSigma sets winSigma with given value.\n     *\n     * @param _histogramNormType sets histogramNormType with given value.\n     *\n     * @param _L2HysThreshold sets L2HysThreshold with given value.\n     *\n     * @param _gammaCorrection sets gammaCorrection with given value.\n     *\n     * @param _nlevels sets nlevels with given value.\n     *\n     * @param _signedGradient sets signedGradient with given value.\n     */\n    constructor(_winSize: Size, _blockSize: Size, _blockStride: Size, _cellSize: Size, _nbins: int, _derivAperture?: int, _winSigma?: double, _histogramNormType?: any, _L2HysThreshold?: double, _gammaCorrection?: bool, _nlevels?: int, _signedGradient?: bool);\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param filename The file name containing HOGDescriptor properties and coefficients for the linear\n     * SVM classifier.\n     */\n    constructor(filename: String);\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param d the HOGDescriptor which cloned to create a new one.\n     */\n    constructor(d: HOGDescriptor);\n    checkDetectorSize(): bool;\n    /**\n     * @param img Matrix of the type CV_8U containing an image where HOG features will be calculated.\n     *\n     * @param descriptors Matrix of the type CV_32F\n     *\n     * @param winStride Window stride. It must be a multiple of block stride.\n     *\n     * @param padding Padding\n     *\n     * @param locations Vector of Point\n     */\n    compute(img: InputArray, descriptors: any, winStride?: Size, padding?: Size, locations?: Point): InputArray;\n    /**\n     * @param img Matrix contains the image to be computed\n     *\n     * @param grad Matrix of type CV_32FC2 contains computed gradients\n     *\n     * @param angleOfs Matrix of type CV_8UC2 contains quantized gradient orientations\n     *\n     * @param paddingTL Padding from top-left\n     *\n     * @param paddingBR Padding from bottom-right\n     */\n    computeGradient(img: InputArray, grad: InputOutputArray, angleOfs: InputOutputArray, paddingTL?: Size, paddingBR?: Size): InputArray;\n    /**\n     * @param c cloned HOGDescriptor\n     */\n    copyTo(c: HOGDescriptor): HOGDescriptor;\n    /**\n     * @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.\n     *\n     * @param foundLocations Vector of point where each point contains left-top corner point of detected\n     * object boundaries.\n     *\n     * @param weights Vector that will contain confidence values for each detected object.\n     *\n     * @param hitThreshold Threshold for the distance between features and SVM classifying plane. Usually\n     * it is 0 and should be specified in the detector coefficients (as the last free coefficient). But if\n     * the free coefficient is omitted (which is allowed), you can specify it manually here.\n     *\n     * @param winStride Window stride. It must be a multiple of block stride.\n     *\n     * @param padding Padding\n     *\n     * @param searchLocations Vector of Point includes set of requested locations to be evaluated.\n     */\n    detect(img: InputArray, foundLocations: Point, weights: any, hitThreshold?: double, winStride?: Size, padding?: Size, searchLocations?: Point): InputArray;\n    /**\n     * @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.\n     *\n     * @param foundLocations Vector of point where each point contains left-top corner point of detected\n     * object boundaries.\n     *\n     * @param hitThreshold Threshold for the distance between features and SVM classifying plane. Usually\n     * it is 0 and should be specified in the detector coefficients (as the last free coefficient). But if\n     * the free coefficient is omitted (which is allowed), you can specify it manually here.\n     *\n     * @param winStride Window stride. It must be a multiple of block stride.\n     *\n     * @param padding Padding\n     *\n     * @param searchLocations Vector of Point includes locations to search.\n     */\n    detect(img: InputArray, foundLocations: Point, hitThreshold?: double, winStride?: Size, padding?: Size, searchLocations?: Point): InputArray;\n    /**\n     * @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.\n     *\n     * @param foundLocations Vector of rectangles where each rectangle contains the detected object.\n     *\n     * @param foundWeights Vector that will contain confidence values for each detected object.\n     *\n     * @param hitThreshold Threshold for the distance between features and SVM classifying plane. Usually\n     * it is 0 and should be specified in the detector coefficients (as the last free coefficient). But if\n     * the free coefficient is omitted (which is allowed), you can specify it manually here.\n     *\n     * @param winStride Window stride. It must be a multiple of block stride.\n     *\n     * @param padding Padding\n     *\n     * @param scale Coefficient of the detection window increase.\n     *\n     * @param finalThreshold Final threshold\n     *\n     * @param useMeanshiftGrouping indicates grouping algorithm\n     */\n    detectMultiScale(img: InputArray, foundLocations: Rect, foundWeights: any, hitThreshold?: double, winStride?: Size, padding?: Size, scale?: double, finalThreshold?: double, useMeanshiftGrouping?: bool): InputArray;\n    /**\n     * @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.\n     *\n     * @param foundLocations Vector of rectangles where each rectangle contains the detected object.\n     *\n     * @param hitThreshold Threshold for the distance between features and SVM classifying plane. Usually\n     * it is 0 and should be specified in the detector coefficients (as the last free coefficient). But if\n     * the free coefficient is omitted (which is allowed), you can specify it manually here.\n     *\n     * @param winStride Window stride. It must be a multiple of block stride.\n     *\n     * @param padding Padding\n     *\n     * @param scale Coefficient of the detection window increase.\n     *\n     * @param finalThreshold Final threshold\n     *\n     * @param useMeanshiftGrouping indicates grouping algorithm\n     */\n    detectMultiScale(img: InputArray, foundLocations: Rect, hitThreshold?: double, winStride?: Size, padding?: Size, scale?: double, finalThreshold?: double, useMeanshiftGrouping?: bool): InputArray;\n    /**\n     * @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.\n     *\n     * @param foundLocations Vector of rectangles where each rectangle contains the detected object.\n     *\n     * @param locations Vector of DetectionROI\n     *\n     * @param hitThreshold Threshold for the distance between features and SVM classifying plane. Usually\n     * it is 0 and should be specified in the detector coefficients (as the last free coefficient). But if\n     * the free coefficient is omitted (which is allowed), you can specify it manually here.\n     *\n     * @param groupThreshold Minimum possible number of rectangles minus 1. The threshold is used in a\n     * group of rectangles to retain it.\n     */\n    detectMultiScaleROI(img: InputArray, foundLocations: any, locations: DetectionROI, hitThreshold?: double, groupThreshold?: int): InputArray;\n    /**\n     * @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.\n     *\n     * @param locations Vector of Point\n     *\n     * @param foundLocations Vector of Point where each Point is detected object's top-left point.\n     *\n     * @param confidences confidences\n     *\n     * @param hitThreshold Threshold for the distance between features and SVM classifying plane. Usually\n     * it is 0 and should be specified in the detector coefficients (as the last free coefficient). But if\n     * the free coefficient is omitted (which is allowed), you can specify it manually here\n     *\n     * @param winStride winStride\n     *\n     * @param padding padding\n     */\n    detectROI(img: InputArray, locations: any, foundLocations: any, confidences: any, hitThreshold?: double, winStride?: any, padding?: any): InputArray;\n    getDescriptorSize(): size_t;\n    getWinSigma(): double;\n    /**\n     * @param rectList Input/output vector of rectangles. Output vector includes retained and grouped\n     * rectangles. (The Python list is not modified in place.)\n     *\n     * @param weights Input/output vector of weights of rectangles. Output vector includes weights of\n     * retained and grouped rectangles. (The Python list is not modified in place.)\n     *\n     * @param groupThreshold Minimum possible number of rectangles minus 1. The threshold is used in a\n     * group of rectangles to retain it.\n     *\n     * @param eps Relative difference between sides of the rectangles to merge them into a group.\n     */\n    groupRectangles(rectList: any, weights: any, groupThreshold: int, eps: double): any;\n    /**\n     * @param filename Path of the file to read.\n     *\n     * @param objname The optional name of the node to read (if empty, the first top-level node will be\n     * used).\n     */\n    load(filename: String, objname?: String): String;\n    /**\n     * @param fn File node\n     */\n    read(fn: FileNode): FileNode;\n    /**\n     * @param filename File name\n     *\n     * @param objname Object name\n     */\n    save(filename: String, objname?: String): String;\n    /**\n     * @param svmdetector coefficients for the linear SVM classifier.\n     */\n    setSVMDetector(svmdetector: InputArray): InputArray;\n    /**\n     * @param fs File storage\n     *\n     * @param objname Object name\n     */\n    write(fs: FileStorage, objname: String): FileStorage;\n    static getDaimlerPeopleDetector(): any;\n    static getDefaultPeopleDetector(): any;\n}\nexport declare const DEFAULT_NLEVELS: any;\nexport declare const DESCR_FORMAT_COL_BY_COL: DescriptorStorageFormat;\nexport declare const DESCR_FORMAT_ROW_BY_ROW: DescriptorStorageFormat;\nexport declare const L2Hys: HistogramNormType;\nexport declare type DescriptorStorageFormat = any;\nexport declare type HistogramNormType = any;\n"},"node_modules_mirada_dist_src_types_opencv_FlannBasedMatcher_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_FlannBasedMatcher_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/FlannBasedMatcher.d.ts","content":"import { bool, FileNode, FileStorage, InputArrayOfArrays, Ptr } from './_types';\n/**\n * This matcher trains [cv::flann::Index](#d1/db2/classcv_1_1flann_1_1Index}) on a train descriptor\n * collection and calls its nearest search methods to find the best matches. So, this matcher may be\n * faster when matching a large train collection than the brute force matcher.\n * [FlannBasedMatcher](#dc/de2/classcv_1_1FlannBasedMatcher}) does not support masking permissible\n * matches of descriptor sets because [flann::Index](#d1/db2/classcv_1_1flann_1_1Index}) does not\n * support this. :\n *\n * Source:\n * [opencv2/features2d.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/features2d.hpp#L1187).\n *\n */\nexport declare class FlannBasedMatcher {\n    constructor(indexParams?: Ptr, searchParams?: Ptr);\n    /**\n     *   If the collection is not empty, the new descriptors are added to existing train descriptors.\n     *\n     * @param descriptors Descriptors to add. Each descriptors[i] is a set of descriptors from the same\n     * train image.\n     */\n    add(descriptors: InputArrayOfArrays): InputArrayOfArrays;\n    clear(): void;\n    /**\n     * @param emptyTrainData If emptyTrainData is false, the method creates a deep copy of the object,\n     * that is, copies both parameters and train data. If emptyTrainData is true, the method creates an\n     * object copy with the current parameters but with empty train data.\n     */\n    clone(emptyTrainData?: bool): Ptr;\n    isMaskSupported(): bool;\n    read(fn: FileNode): FileNode;\n    /**\n     *   Trains a descriptor matcher (for example, the flann index). In all methods to match, the method\n     * [train()] is run every time before matching. Some descriptor matchers (for example,\n     * BruteForceMatcher) have an empty implementation of this method. Other matchers really train their\n     * inner structures (for example, [FlannBasedMatcher] trains [flann::Index] ).\n     */\n    train(): void;\n    write(fs: FileStorage): FileStorage;\n    static create(): Ptr;\n}\n"},"node_modules_mirada_dist_src_types_opencv_imgproc_color_conversions_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_imgproc_color_conversions_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/imgproc_color_conversions.d.ts","content":"import { InputArray, int, OutputArray } from './_types';\n/**\n * The function converts an input image from one color space to another. In case of a transformation\n * to-from RGB color space, the order of the channels should be specified explicitly (RGB or BGR). Note\n * that the default color format in OpenCV is often referred to as RGB but it is actually BGR (the\n * bytes are reversed). So the first byte in a standard (24-bit) color image will be an 8-bit Blue\n * component, the second byte will be Green, and the third byte will be Red. The fourth, fifth, and\n * sixth bytes would then be the second pixel (Blue, then Green, then Red), and so on.\n *\n * The conventional ranges for R, G, and B channel values are:\n *\n * 0 to 255 for CV_8U images\n * 0 to 65535 for CV_16U images\n * 0 to 1 for CV_32F images\n *\n * In case of linear transformations, the range does not matter. But in case of a non-linear\n * transformation, an input RGB image should be normalized to the proper value range to get the correct\n * results, for example, for RGB `$\\\\rightarrow$` L*u*v* transformation. For example, if you have a\n * 32-bit floating-point image directly converted from an 8-bit image without any scaling, then it will\n * have the 0..255 value range instead of 0..1 assumed by the function. So, before calling [cvtColor] ,\n * you need first to scale the image down:\n *\n * ```cpp\n * img *= 1./255;\n * cvtColor(img, img, COLOR_BGR2Luv);\n * ```\n *\n *  If you use [cvtColor] with 8-bit images, the conversion will have some information lost. For many\n * applications, this will not be noticeable but it is recommended to use 32-bit images in applications\n * that need the full range of colors or that convert an image before an operation and then convert\n * back.\n *\n * If conversion adds the alpha channel, its value will set to the maximum of corresponding channel\n * range: 255 for CV_8U, 65535 for CV_16U, 1 for CV_32F.\n *\n * [Color conversions]\n *\n * @param src input image: 8-bit unsigned, 16-bit unsigned ( CV_16UC... ), or single-precision\n * floating-point.\n *\n * @param dst output image of the same size and depth as src.\n *\n * @param code color space conversion code (see ColorConversionCodes).\n *\n * @param dstCn number of channels in the destination image; if the parameter is 0, the number of the\n * channels is derived automatically from src and code.\n */\nexport declare function cvtColor(src: InputArray, dst: OutputArray, code: int, dstCn?: int): void;\n/**\n * This function only supports YUV420 to RGB conversion as of now.\n *\n * @param src1 8-bit image (CV_8U) of the Y plane.\n *\n * @param src2 image containing interleaved U/V plane.\n *\n * @param dst output image.\n *\n * @param code Specifies the type of conversion. It can take any of the following values:\n * COLOR_YUV2BGR_NV12COLOR_YUV2RGB_NV12COLOR_YUV2BGRA_NV12COLOR_YUV2RGBA_NV12COLOR_YUV2BGR_NV21COLOR_YUV2RGB_NV21COLOR_YUV2BGRA_NV21COLOR_YUV2RGBA_NV21\n */\nexport declare function cvtColorTwoPlane(src1: InputArray, src2: InputArray, dst: OutputArray, code: int): void;\n/**\n * The function can do the following transformations:\n *\n * Demosaicing using bilinear interpolation[COLOR_BayerBG2BGR] , [COLOR_BayerGB2BGR] ,\n * [COLOR_BayerRG2BGR] , [COLOR_BayerGR2BGR][COLOR_BayerBG2GRAY] , [COLOR_BayerGB2GRAY] ,\n * [COLOR_BayerRG2GRAY] , [COLOR_BayerGR2GRAY]\n * Demosaicing using Variable Number of Gradients.[COLOR_BayerBG2BGR_VNG] , [COLOR_BayerGB2BGR_VNG] ,\n * [COLOR_BayerRG2BGR_VNG] , [COLOR_BayerGR2BGR_VNG]\n * Edge-Aware Demosaicing.[COLOR_BayerBG2BGR_EA] , [COLOR_BayerGB2BGR_EA] , [COLOR_BayerRG2BGR_EA] ,\n * [COLOR_BayerGR2BGR_EA]\n * Demosaicing with alpha channel[COLOR_BayerBG2BGRA] , [COLOR_BayerGB2BGRA] , [COLOR_BayerRG2BGRA] ,\n * [COLOR_BayerGR2BGRA]\n *\n * [cvtColor]\n *\n * @param src input image: 8-bit unsigned or 16-bit unsigned.\n *\n * @param dst output image of the same size and depth as src.\n *\n * @param code Color space conversion code (see the description below).\n *\n * @param dstCn number of channels in the destination image; if the parameter is 0, the number of the\n * channels is derived automatically from src and code.\n */\nexport declare function demosaicing(src: InputArray, dst: OutputArray, code: int, dstCn?: int): void;\nexport declare const COLOR_BGR2BGRA: ColorConversionCodes;\nexport declare const COLOR_RGB2RGBA: ColorConversionCodes;\nexport declare const COLOR_BGRA2BGR: ColorConversionCodes;\nexport declare const COLOR_RGBA2RGB: ColorConversionCodes;\nexport declare const COLOR_BGR2RGBA: ColorConversionCodes;\nexport declare const COLOR_RGB2BGRA: ColorConversionCodes;\nexport declare const COLOR_RGBA2BGR: ColorConversionCodes;\nexport declare const COLOR_BGRA2RGB: ColorConversionCodes;\nexport declare const COLOR_BGR2RGB: ColorConversionCodes;\nexport declare const COLOR_RGB2BGR: ColorConversionCodes;\nexport declare const COLOR_BGRA2RGBA: ColorConversionCodes;\nexport declare const COLOR_RGBA2BGRA: ColorConversionCodes;\nexport declare const COLOR_BGR2GRAY: ColorConversionCodes;\nexport declare const COLOR_RGB2GRAY: ColorConversionCodes;\nexport declare const COLOR_GRAY2BGR: ColorConversionCodes;\nexport declare const COLOR_GRAY2RGB: ColorConversionCodes;\nexport declare const COLOR_GRAY2BGRA: ColorConversionCodes;\nexport declare const COLOR_GRAY2RGBA: ColorConversionCodes;\nexport declare const COLOR_BGRA2GRAY: ColorConversionCodes;\nexport declare const COLOR_RGBA2GRAY: ColorConversionCodes;\nexport declare const COLOR_BGR2BGR565: ColorConversionCodes;\nexport declare const COLOR_RGB2BGR565: ColorConversionCodes;\nexport declare const COLOR_BGR5652BGR: ColorConversionCodes;\nexport declare const COLOR_BGR5652RGB: ColorConversionCodes;\nexport declare const COLOR_BGRA2BGR565: ColorConversionCodes;\nexport declare const COLOR_RGBA2BGR565: ColorConversionCodes;\nexport declare const COLOR_BGR5652BGRA: ColorConversionCodes;\nexport declare const COLOR_BGR5652RGBA: ColorConversionCodes;\nexport declare const COLOR_GRAY2BGR565: ColorConversionCodes;\nexport declare const COLOR_BGR5652GRAY: ColorConversionCodes;\nexport declare const COLOR_BGR2BGR555: ColorConversionCodes;\nexport declare const COLOR_RGB2BGR555: ColorConversionCodes;\nexport declare const COLOR_BGR5552BGR: ColorConversionCodes;\nexport declare const COLOR_BGR5552RGB: ColorConversionCodes;\nexport declare const COLOR_BGRA2BGR555: ColorConversionCodes;\nexport declare const COLOR_RGBA2BGR555: ColorConversionCodes;\nexport declare const COLOR_BGR5552BGRA: ColorConversionCodes;\nexport declare const COLOR_BGR5552RGBA: ColorConversionCodes;\nexport declare const COLOR_GRAY2BGR555: ColorConversionCodes;\nexport declare const COLOR_BGR5552GRAY: ColorConversionCodes;\nexport declare const COLOR_BGR2XYZ: ColorConversionCodes;\nexport declare const COLOR_RGB2XYZ: ColorConversionCodes;\nexport declare const COLOR_XYZ2BGR: ColorConversionCodes;\nexport declare const COLOR_XYZ2RGB: ColorConversionCodes;\nexport declare const COLOR_BGR2YCrCb: ColorConversionCodes;\nexport declare const COLOR_RGB2YCrCb: ColorConversionCodes;\nexport declare const COLOR_YCrCb2BGR: ColorConversionCodes;\nexport declare const COLOR_YCrCb2RGB: ColorConversionCodes;\nexport declare const COLOR_BGR2HSV: ColorConversionCodes;\nexport declare const COLOR_RGB2HSV: ColorConversionCodes;\nexport declare const COLOR_BGR2Lab: ColorConversionCodes;\nexport declare const COLOR_RGB2Lab: ColorConversionCodes;\nexport declare const COLOR_BGR2Luv: ColorConversionCodes;\nexport declare const COLOR_RGB2Luv: ColorConversionCodes;\nexport declare const COLOR_BGR2HLS: ColorConversionCodes;\nexport declare const COLOR_RGB2HLS: ColorConversionCodes;\nexport declare const COLOR_HSV2BGR: ColorConversionCodes;\nexport declare const COLOR_HSV2RGB: ColorConversionCodes;\nexport declare const COLOR_Lab2BGR: ColorConversionCodes;\nexport declare const COLOR_Lab2RGB: ColorConversionCodes;\nexport declare const COLOR_Luv2BGR: ColorConversionCodes;\nexport declare const COLOR_Luv2RGB: ColorConversionCodes;\nexport declare const COLOR_HLS2BGR: ColorConversionCodes;\nexport declare const COLOR_HLS2RGB: ColorConversionCodes;\nexport declare const COLOR_BGR2HSV_FULL: ColorConversionCodes;\nexport declare const COLOR_RGB2HSV_FULL: ColorConversionCodes;\nexport declare const COLOR_BGR2HLS_FULL: ColorConversionCodes;\nexport declare const COLOR_RGB2HLS_FULL: ColorConversionCodes;\nexport declare const COLOR_HSV2BGR_FULL: ColorConversionCodes;\nexport declare const COLOR_HSV2RGB_FULL: ColorConversionCodes;\nexport declare const COLOR_HLS2BGR_FULL: ColorConversionCodes;\nexport declare const COLOR_HLS2RGB_FULL: ColorConversionCodes;\nexport declare const COLOR_LBGR2Lab: ColorConversionCodes;\nexport declare const COLOR_LRGB2Lab: ColorConversionCodes;\nexport declare const COLOR_LBGR2Luv: ColorConversionCodes;\nexport declare const COLOR_LRGB2Luv: ColorConversionCodes;\nexport declare const COLOR_Lab2LBGR: ColorConversionCodes;\nexport declare const COLOR_Lab2LRGB: ColorConversionCodes;\nexport declare const COLOR_Luv2LBGR: ColorConversionCodes;\nexport declare const COLOR_Luv2LRGB: ColorConversionCodes;\nexport declare const COLOR_BGR2YUV: ColorConversionCodes;\nexport declare const COLOR_RGB2YUV: ColorConversionCodes;\nexport declare const COLOR_YUV2BGR: ColorConversionCodes;\nexport declare const COLOR_YUV2RGB: ColorConversionCodes;\nexport declare const COLOR_YUV2RGB_NV12: ColorConversionCodes;\nexport declare const COLOR_YUV2BGR_NV12: ColorConversionCodes;\nexport declare const COLOR_YUV2RGB_NV21: ColorConversionCodes;\nexport declare const COLOR_YUV2BGR_NV21: ColorConversionCodes;\nexport declare const COLOR_YUV420sp2RGB: ColorConversionCodes;\nexport declare const COLOR_YUV420sp2BGR: ColorConversionCodes;\nexport declare const COLOR_YUV2RGBA_NV12: ColorConversionCodes;\nexport declare const COLOR_YUV2BGRA_NV12: ColorConversionCodes;\nexport declare const COLOR_YUV2RGBA_NV21: ColorConversionCodes;\nexport declare const COLOR_YUV2BGRA_NV21: ColorConversionCodes;\nexport declare const COLOR_YUV420sp2RGBA: ColorConversionCodes;\nexport declare const COLOR_YUV420sp2BGRA: ColorConversionCodes;\nexport declare const COLOR_YUV2RGB_YV12: ColorConversionCodes;\nexport declare const COLOR_YUV2BGR_YV12: ColorConversionCodes;\nexport declare const COLOR_YUV2RGB_IYUV: ColorConversionCodes;\nexport declare const COLOR_YUV2BGR_IYUV: ColorConversionCodes;\nexport declare const COLOR_YUV2RGB_I420: ColorConversionCodes;\nexport declare const COLOR_YUV2BGR_I420: ColorConversionCodes;\nexport declare const COLOR_YUV420p2RGB: ColorConversionCodes;\nexport declare const COLOR_YUV420p2BGR: ColorConversionCodes;\nexport declare const COLOR_YUV2RGBA_YV12: ColorConversionCodes;\nexport declare const COLOR_YUV2BGRA_YV12: ColorConversionCodes;\nexport declare const COLOR_YUV2RGBA_IYUV: ColorConversionCodes;\nexport declare const COLOR_YUV2BGRA_IYUV: ColorConversionCodes;\nexport declare const COLOR_YUV2RGBA_I420: ColorConversionCodes;\nexport declare const COLOR_YUV2BGRA_I420: ColorConversionCodes;\nexport declare const COLOR_YUV420p2RGBA: ColorConversionCodes;\nexport declare const COLOR_YUV420p2BGRA: ColorConversionCodes;\nexport declare const COLOR_YUV2GRAY_420: ColorConversionCodes;\nexport declare const COLOR_YUV2GRAY_NV21: ColorConversionCodes;\nexport declare const COLOR_YUV2GRAY_NV12: ColorConversionCodes;\nexport declare const COLOR_YUV2GRAY_YV12: ColorConversionCodes;\nexport declare const COLOR_YUV2GRAY_IYUV: ColorConversionCodes;\nexport declare const COLOR_YUV2GRAY_I420: ColorConversionCodes;\nexport declare const COLOR_YUV420sp2GRAY: ColorConversionCodes;\nexport declare const COLOR_YUV420p2GRAY: ColorConversionCodes;\nexport declare const COLOR_YUV2RGB_UYVY: ColorConversionCodes;\nexport declare const COLOR_YUV2BGR_UYVY: ColorConversionCodes;\nexport declare const COLOR_YUV2RGB_Y422: ColorConversionCodes;\nexport declare const COLOR_YUV2BGR_Y422: ColorConversionCodes;\nexport declare const COLOR_YUV2RGB_UYNV: ColorConversionCodes;\nexport declare const COLOR_YUV2BGR_UYNV: ColorConversionCodes;\nexport declare const COLOR_YUV2RGBA_UYVY: ColorConversionCodes;\nexport declare const COLOR_YUV2BGRA_UYVY: ColorConversionCodes;\nexport declare const COLOR_YUV2RGBA_Y422: ColorConversionCodes;\nexport declare const COLOR_YUV2BGRA_Y422: ColorConversionCodes;\nexport declare const COLOR_YUV2RGBA_UYNV: ColorConversionCodes;\nexport declare const COLOR_YUV2BGRA_UYNV: ColorConversionCodes;\nexport declare const COLOR_YUV2RGB_YUY2: ColorConversionCodes;\nexport declare const COLOR_YUV2BGR_YUY2: ColorConversionCodes;\nexport declare const COLOR_YUV2RGB_YVYU: ColorConversionCodes;\nexport declare const COLOR_YUV2BGR_YVYU: ColorConversionCodes;\nexport declare const COLOR_YUV2RGB_YUYV: ColorConversionCodes;\nexport declare const COLOR_YUV2BGR_YUYV: ColorConversionCodes;\nexport declare const COLOR_YUV2RGB_YUNV: ColorConversionCodes;\nexport declare const COLOR_YUV2BGR_YUNV: ColorConversionCodes;\nexport declare const COLOR_YUV2RGBA_YUY2: ColorConversionCodes;\nexport declare const COLOR_YUV2BGRA_YUY2: ColorConversionCodes;\nexport declare const COLOR_YUV2RGBA_YVYU: ColorConversionCodes;\nexport declare const COLOR_YUV2BGRA_YVYU: ColorConversionCodes;\nexport declare const COLOR_YUV2RGBA_YUYV: ColorConversionCodes;\nexport declare const COLOR_YUV2BGRA_YUYV: ColorConversionCodes;\nexport declare const COLOR_YUV2RGBA_YUNV: ColorConversionCodes;\nexport declare const COLOR_YUV2BGRA_YUNV: ColorConversionCodes;\nexport declare const COLOR_YUV2GRAY_UYVY: ColorConversionCodes;\nexport declare const COLOR_YUV2GRAY_YUY2: ColorConversionCodes;\nexport declare const COLOR_YUV2GRAY_Y422: ColorConversionCodes;\nexport declare const COLOR_YUV2GRAY_UYNV: ColorConversionCodes;\nexport declare const COLOR_YUV2GRAY_YVYU: ColorConversionCodes;\nexport declare const COLOR_YUV2GRAY_YUYV: ColorConversionCodes;\nexport declare const COLOR_YUV2GRAY_YUNV: ColorConversionCodes;\nexport declare const COLOR_RGBA2mRGBA: ColorConversionCodes;\nexport declare const COLOR_mRGBA2RGBA: ColorConversionCodes;\nexport declare const COLOR_RGB2YUV_I420: ColorConversionCodes;\nexport declare const COLOR_BGR2YUV_I420: ColorConversionCodes;\nexport declare const COLOR_RGB2YUV_IYUV: ColorConversionCodes;\nexport declare const COLOR_BGR2YUV_IYUV: ColorConversionCodes;\nexport declare const COLOR_RGBA2YUV_I420: ColorConversionCodes;\nexport declare const COLOR_BGRA2YUV_I420: ColorConversionCodes;\nexport declare const COLOR_RGBA2YUV_IYUV: ColorConversionCodes;\nexport declare const COLOR_BGRA2YUV_IYUV: ColorConversionCodes;\nexport declare const COLOR_RGB2YUV_YV12: ColorConversionCodes;\nexport declare const COLOR_BGR2YUV_YV12: ColorConversionCodes;\nexport declare const COLOR_RGBA2YUV_YV12: ColorConversionCodes;\nexport declare const COLOR_BGRA2YUV_YV12: ColorConversionCodes;\nexport declare const COLOR_BayerBG2BGR: ColorConversionCodes;\nexport declare const COLOR_BayerGB2BGR: ColorConversionCodes;\nexport declare const COLOR_BayerRG2BGR: ColorConversionCodes;\nexport declare const COLOR_BayerGR2BGR: ColorConversionCodes;\nexport declare const COLOR_BayerBG2RGB: ColorConversionCodes;\nexport declare const COLOR_BayerGB2RGB: ColorConversionCodes;\nexport declare const COLOR_BayerRG2RGB: ColorConversionCodes;\nexport declare const COLOR_BayerGR2RGB: ColorConversionCodes;\nexport declare const COLOR_BayerBG2GRAY: ColorConversionCodes;\nexport declare const COLOR_BayerGB2GRAY: ColorConversionCodes;\nexport declare const COLOR_BayerRG2GRAY: ColorConversionCodes;\nexport declare const COLOR_BayerGR2GRAY: ColorConversionCodes;\nexport declare const COLOR_BayerBG2BGR_VNG: ColorConversionCodes;\nexport declare const COLOR_BayerGB2BGR_VNG: ColorConversionCodes;\nexport declare const COLOR_BayerRG2BGR_VNG: ColorConversionCodes;\nexport declare const COLOR_BayerGR2BGR_VNG: ColorConversionCodes;\nexport declare const COLOR_BayerBG2RGB_VNG: ColorConversionCodes;\nexport declare const COLOR_BayerGB2RGB_VNG: ColorConversionCodes;\nexport declare const COLOR_BayerRG2RGB_VNG: ColorConversionCodes;\nexport declare const COLOR_BayerGR2RGB_VNG: ColorConversionCodes;\nexport declare const COLOR_BayerBG2BGR_EA: ColorConversionCodes;\nexport declare const COLOR_BayerGB2BGR_EA: ColorConversionCodes;\nexport declare const COLOR_BayerRG2BGR_EA: ColorConversionCodes;\nexport declare const COLOR_BayerGR2BGR_EA: ColorConversionCodes;\nexport declare const COLOR_BayerBG2RGB_EA: ColorConversionCodes;\nexport declare const COLOR_BayerGB2RGB_EA: ColorConversionCodes;\nexport declare const COLOR_BayerRG2RGB_EA: ColorConversionCodes;\nexport declare const COLOR_BayerGR2RGB_EA: ColorConversionCodes;\nexport declare const COLOR_BayerBG2BGRA: ColorConversionCodes;\nexport declare const COLOR_BayerGB2BGRA: ColorConversionCodes;\nexport declare const COLOR_BayerRG2BGRA: ColorConversionCodes;\nexport declare const COLOR_BayerGR2BGRA: ColorConversionCodes;\nexport declare const COLOR_BayerBG2RGBA: ColorConversionCodes;\nexport declare const COLOR_BayerGB2RGBA: ColorConversionCodes;\nexport declare const COLOR_BayerRG2RGBA: ColorConversionCodes;\nexport declare const COLOR_BayerGR2RGBA: ColorConversionCodes;\nexport declare const COLOR_COLORCVT_MAX: ColorConversionCodes;\n/**\n * the color conversion codes\n *\n * [Color conversions]\n *\n */\nexport declare type ColorConversionCodes = any;\n"},"node_modules_mirada_dist_src_types_opencv_imgproc_draw_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_imgproc_draw_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/imgproc_draw.d.ts","content":"import { bool, double, InputArray, InputArrayOfArrays, InputOutputArray, int, Point, Point2d, Rect, Scalar, Size, Size2d, Size2l } from './_types';\n/**\n * The function [cv::arrowedLine] draws an arrow between pt1 and pt2 points in the image. See also\n * [line].\n *\n * @param img Image.\n *\n * @param pt1 The point the arrow starts from.\n *\n * @param pt2 The point the arrow points to.\n *\n * @param color Line color.\n *\n * @param thickness Line thickness.\n *\n * @param line_type Type of the line. See LineTypes\n *\n * @param shift Number of fractional bits in the point coordinates.\n *\n * @param tipLength The length of the arrow tip in relation to the arrow length\n */\nexport declare function arrowedLine(img: InputOutputArray, pt1: Point, pt2: Point, color: any, thickness?: int, line_type?: int, shift?: int, tipLength?: double): void;\n/**\n * The function [cv::circle] draws a simple or filled circle with a given center and radius.\n *\n * @param img Image where the circle is drawn.\n *\n * @param center Center of the circle.\n *\n * @param radius Radius of the circle.\n *\n * @param color Circle color.\n *\n * @param thickness Thickness of the circle outline, if positive. Negative values, like FILLED, mean\n * that a filled circle is to be drawn.\n *\n * @param lineType Type of the circle boundary. See LineTypes\n *\n * @param shift Number of fractional bits in the coordinates of the center and in the radius value.\n */\nexport declare function circle(img: InputOutputArray, center: Point, radius: int, color: any, thickness?: int, lineType?: int, shift?: int): void;\n/**\n * The function [cv::clipLine] calculates a part of the line segment that is entirely within the\n * specified rectangle. it returns false if the line segment is completely outside the rectangle.\n * Otherwise, it returns true .\n *\n * @param imgSize Image size. The image rectangle is Rect(0, 0, imgSize.width, imgSize.height) .\n *\n * @param pt1 First line point.\n *\n * @param pt2 Second line point.\n */\nexport declare function clipLine(imgSize: Size, pt1: any, pt2: any): bool;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n *\n * @param imgSize Image size. The image rectangle is Rect(0, 0, imgSize.width, imgSize.height) .\n *\n * @param pt1 First line point.\n *\n * @param pt2 Second line point.\n */\nexport declare function clipLine(imgSize: Size2l, pt1: any, pt2: any): bool;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n *\n * @param imgRect Image rectangle.\n *\n * @param pt1 First line point.\n *\n * @param pt2 Second line point.\n */\nexport declare function clipLine(imgRect: Rect, pt1: any, pt2: any): bool;\n/**\n * The function draws contour outlines in the image if `$\\\\texttt{thickness} \\\\ge 0$` or fills the area\n * bounded by the contours if `$\\\\texttt{thickness}<0$` . The example below shows how to retrieve\n * connected components from the binary image and label them: :\n *\n * ```cpp\n * #include \"opencv2/imgproc.hpp\"\n * #include \"opencv2/highgui.hpp\"\n *\n * using namespace cv;\n * using namespace std;\n *\n * int main( int argc, char** argv )\n * {\n *     Mat src;\n *     // the first command-line parameter must be a filename of the binary\n *     // (black-n-white) image\n *     if( argc != 2 || !(src=imread(argv[1], 0)).data)\n *         return -1;\n *\n *     Mat dst = Mat::zeros(src.rows, src.cols, CV_8UC3);\n *\n *     src = src > 1;\n *     namedWindow( \"Source\", 1 );\n *     imshow( \"Source\", src );\n *\n *     vector<vector<Point> > contours;\n *     vector<Vec4i> hierarchy;\n *\n *     findContours( src, contours, hierarchy,\n *         RETR_CCOMP, CHAIN_APPROX_SIMPLE );\n *\n *     // iterate through all the top-level contours,\n *     // draw each connected component with its own random color\n *     int idx = 0;\n *     for( ; idx >= 0; idx = hierarchy[idx][0] )\n *     {\n *         Scalar color( rand()&255, rand()&255, rand()&255 );\n *         drawContours( dst, contours, idx, color, FILLED, 8, hierarchy );\n *     }\n *\n *     namedWindow( \"Components\", 1 );\n *     imshow( \"Components\", dst );\n *     waitKey(0);\n * }\n * ```\n *\n * When thickness=[FILLED], the function is designed to handle connected components with holes\n * correctly even when no hierarchy date is provided. This is done by analyzing all the outlines\n * together using even-odd rule. This may give incorrect results if you have a joint collection of\n * separately retrieved contours. In order to solve this problem, you need to call [drawContours]\n * separately for each sub-group of contours, or iterate over the collection using contourIdx\n * parameter.\n *\n * @param image Destination image.\n *\n * @param contours All the input contours. Each contour is stored as a point vector.\n *\n * @param contourIdx Parameter indicating a contour to draw. If it is negative, all the contours are\n * drawn.\n *\n * @param color Color of the contours.\n *\n * @param thickness Thickness of lines the contours are drawn with. If it is negative (for example,\n * thickness=FILLED ), the contour interiors are drawn.\n *\n * @param lineType Line connectivity. See LineTypes\n *\n * @param hierarchy Optional information about hierarchy. It is only needed if you want to draw only\n * some of the contours (see maxLevel ).\n *\n * @param maxLevel Maximal level for drawn contours. If it is 0, only the specified contour is drawn.\n * If it is 1, the function draws the contour(s) and all the nested contours. If it is 2, the function\n * draws the contours, all the nested contours, all the nested-to-nested contours, and so on. This\n * parameter is only taken into account when there is hierarchy available.\n *\n * @param offset Optional contour shift parameter. Shift all the drawn contours by the specified\n * $\\texttt{offset}=(dx,dy)$ .\n */\nexport declare function drawContours(image: InputOutputArray, contours: InputArrayOfArrays, contourIdx: int, color: any, thickness?: int, lineType?: int, hierarchy?: InputArray, maxLevel?: int, offset?: Point): void;\n/**\n * The function [cv::drawMarker] draws a marker on a given position in the image. For the moment\n * several marker types are supported, see [MarkerTypes] for more information.\n *\n * @param img Image.\n *\n * @param position The point where the crosshair is positioned.\n *\n * @param color Line color.\n *\n * @param markerType The specific type of marker you want to use, see MarkerTypes\n *\n * @param markerSize The length of the marker axis [default = 20 pixels]\n *\n * @param thickness Line thickness.\n *\n * @param line_type Type of the line, See LineTypes\n */\nexport declare function drawMarker(img: InputOutputArray, position: Point, color: any, markerType?: int, markerSize?: int, thickness?: int, line_type?: int): void;\n/**\n * The function [cv::ellipse] with more parameters draws an ellipse outline, a filled ellipse, an\n * elliptic arc, or a filled ellipse sector. The drawing code uses general parametric form. A\n * piecewise-linear curve is used to approximate the elliptic arc boundary. If you need more control of\n * the ellipse rendering, you can retrieve the curve using [ellipse2Poly] and then render it with\n * [polylines] or fill it with [fillPoly]. If you use the first variant of the function and want to\n * draw the whole ellipse, not an arc, pass `startAngle=0` and `endAngle=360`. If `startAngle` is\n * greater than `endAngle`, they are swapped. The figure below explains the meaning of the parameters\n * to draw the blue arc.\n *\n * @param img Image.\n *\n * @param center Center of the ellipse.\n *\n * @param axes Half of the size of the ellipse main axes.\n *\n * @param angle Ellipse rotation angle in degrees.\n *\n * @param startAngle Starting angle of the elliptic arc in degrees.\n *\n * @param endAngle Ending angle of the elliptic arc in degrees.\n *\n * @param color Ellipse color.\n *\n * @param thickness Thickness of the ellipse arc outline, if positive. Otherwise, this indicates that a\n * filled ellipse sector is to be drawn.\n *\n * @param lineType Type of the ellipse boundary. See LineTypes\n *\n * @param shift Number of fractional bits in the coordinates of the center and values of axes.\n */\nexport declare function ellipse(img: InputOutputArray, center: Point, axes: Size, angle: double, startAngle: double, endAngle: double, color: any, thickness?: int, lineType?: int, shift?: int): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n *\n * @param img Image.\n *\n * @param box Alternative ellipse representation via RotatedRect. This means that the function draws an\n * ellipse inscribed in the rotated rectangle.\n *\n * @param color Ellipse color.\n *\n * @param thickness Thickness of the ellipse arc outline, if positive. Otherwise, this indicates that a\n * filled ellipse sector is to be drawn.\n *\n * @param lineType Type of the ellipse boundary. See LineTypes\n */\nexport declare function ellipse(img: InputOutputArray, box: any, color: any, thickness?: int, lineType?: int): void;\n/**\n * The function ellipse2Poly computes the vertices of a polyline that approximates the specified\n * elliptic arc. It is used by [ellipse]. If `arcStart` is greater than `arcEnd`, they are swapped.\n *\n * @param center Center of the arc.\n *\n * @param axes Half of the size of the ellipse main axes. See ellipse for details.\n *\n * @param angle Rotation angle of the ellipse in degrees. See ellipse for details.\n *\n * @param arcStart Starting angle of the elliptic arc in degrees.\n *\n * @param arcEnd Ending angle of the elliptic arc in degrees.\n *\n * @param delta Angle between the subsequent polyline vertices. It defines the approximation accuracy.\n *\n * @param pts Output vector of polyline vertices.\n */\nexport declare function ellipse2Poly(center: Point, axes: Size, angle: int, arcStart: int, arcEnd: int, delta: int, pts: any): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n *\n * @param center Center of the arc.\n *\n * @param axes Half of the size of the ellipse main axes. See ellipse for details.\n *\n * @param angle Rotation angle of the ellipse in degrees. See ellipse for details.\n *\n * @param arcStart Starting angle of the elliptic arc in degrees.\n *\n * @param arcEnd Ending angle of the elliptic arc in degrees.\n *\n * @param delta Angle between the subsequent polyline vertices. It defines the approximation accuracy.\n *\n * @param pts Output vector of polyline vertices.\n */\nexport declare function ellipse2Poly(center: Point2d, axes: Size2d, angle: int, arcStart: int, arcEnd: int, delta: int, pts: any): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function fillConvexPoly(img: InputOutputArray, pts: any, npts: int, color: any, lineType?: int, shift?: int): void;\n/**\n * The function [cv::fillConvexPoly] draws a filled convex polygon. This function is much faster than\n * the function [fillPoly] . It can fill not only convex polygons but any monotonic polygon without\n * self-intersections, that is, a polygon whose contour intersects every horizontal line (scan line)\n * twice at the most (though, its top-most and/or the bottom edge could be horizontal).\n *\n * @param img Image.\n *\n * @param points Polygon vertices.\n *\n * @param color Polygon color.\n *\n * @param lineType Type of the polygon boundaries. See LineTypes\n *\n * @param shift Number of fractional bits in the vertex coordinates.\n */\nexport declare function fillConvexPoly(img: InputOutputArray, points: InputArray, color: any, lineType?: int, shift?: int): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function fillPoly(img: InputOutputArray, pts: any, npts: any, ncontours: int, color: any, lineType?: int, shift?: int, offset?: Point): void;\n/**\n * The function [cv::fillPoly] fills an area bounded by several polygonal contours. The function can\n * fill complex areas, for example, areas with holes, contours with self-intersections (some of their\n * parts), and so forth.\n *\n * @param img Image.\n *\n * @param pts Array of polygons where each polygon is represented as an array of points.\n *\n * @param color Polygon color.\n *\n * @param lineType Type of the polygon boundaries. See LineTypes\n *\n * @param shift Number of fractional bits in the vertex coordinates.\n *\n * @param offset Optional offset of all points of the contours.\n */\nexport declare function fillPoly(img: InputOutputArray, pts: InputArrayOfArrays, color: any, lineType?: int, shift?: int, offset?: Point): void;\n/**\n * The fontSize to use for [cv::putText]\n *\n * [cv::putText]\n *\n * @param fontFace Font to use, see cv::HersheyFonts.\n *\n * @param pixelHeight Pixel height to compute the fontScale for\n *\n * @param thickness Thickness of lines used to render the text.See putText for details.\n */\nexport declare function getFontScaleFromHeight(fontFace: any, pixelHeight: any, thickness?: any): double;\n/**\n * The function [cv::getTextSize] calculates and returns the size of a box that contains the specified\n * text. That is, the following code renders some text, the tight box surrounding it, and the baseline:\n * :\n *\n * ```cpp\n * String text = \"Funny text inside the box\";\n * int fontFace = FONT_HERSHEY_SCRIPT_SIMPLEX;\n * double fontScale = 2;\n * int thickness = 3;\n *\n * Mat img(600, 800, CV_8UC3, Scalar::all(0));\n *\n * int baseline=0;\n * Size textSize = getTextSize(text, fontFace,\n *                             fontScale, thickness, &baseline);\n * baseline += thickness;\n *\n * // center the text\n * Point textOrg((img.cols - textSize.width)/2,\n *               (img.rows + textSize.height)/2);\n *\n * // draw the box\n * rectangle(img, textOrg + Point(0, baseline),\n *           textOrg + Point(textSize.width, -textSize.height),\n *           Scalar(0,0,255));\n * // ... and the baseline first\n * line(img, textOrg + Point(0, thickness),\n *      textOrg + Point(textSize.width, thickness),\n *      Scalar(0, 0, 255));\n *\n * // then put the text itself\n * putText(img, text, textOrg, fontFace, fontScale,\n *         Scalar::all(255), thickness, 8);\n * ```\n *\n * The size of a box that contains the specified text.\n *\n * [putText]\n *\n * @param text Input text string.\n *\n * @param fontFace Font to use, see HersheyFonts.\n *\n * @param fontScale Font scale factor that is multiplied by the font-specific base size.\n *\n * @param thickness Thickness of lines used to render the text. See putText for details.\n *\n * @param baseLine y-coordinate of the baseline relative to the bottom-most text point.\n */\nexport declare function getTextSize(text: any, fontFace: int, fontScale: double, thickness: int, baseLine: any): Size;\n/**\n * The function line draws the line segment between pt1 and pt2 points in the image. The line is\n * clipped by the image boundaries. For non-antialiased lines with integer coordinates, the 8-connected\n * or 4-connected Bresenham algorithm is used. Thick lines are drawn with rounding endings. Antialiased\n * lines are drawn using Gaussian filtering.\n *\n * @param img Image.\n *\n * @param pt1 First point of the line segment.\n *\n * @param pt2 Second point of the line segment.\n *\n * @param color Line color.\n *\n * @param thickness Line thickness.\n *\n * @param lineType Type of the line. See LineTypes.\n *\n * @param shift Number of fractional bits in the point coordinates.\n */\nexport declare function line(img: InputOutputArray, pt1: Point, pt2: Point, color: any, thickness?: int, lineType?: int, shift?: int): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function polylines(img: InputOutputArray, pts: any, npts: any, ncontours: int, isClosed: bool, color: any, thickness?: int, lineType?: int, shift?: int): void;\n/**\n * The function [cv::polylines] draws one or more polygonal curves.\n *\n * @param img Image.\n *\n * @param pts Array of polygonal curves.\n *\n * @param isClosed Flag indicating whether the drawn polylines are closed or not. If they are closed,\n * the function draws a line from the last vertex of each curve to its first vertex.\n *\n * @param color Polyline color.\n *\n * @param thickness Thickness of the polyline edges.\n *\n * @param lineType Type of the line segments. See LineTypes\n *\n * @param shift Number of fractional bits in the vertex coordinates.\n */\nexport declare function polylines(img: InputOutputArray, pts: InputArrayOfArrays, isClosed: bool, color: any, thickness?: int, lineType?: int, shift?: int): void;\n/**\n * The function [cv::putText] renders the specified text string in the image. Symbols that cannot be\n * rendered using the specified font are replaced by question marks. See [getTextSize] for a text\n * rendering code example.\n *\n * @param img Image.\n *\n * @param text Text string to be drawn.\n *\n * @param org Bottom-left corner of the text string in the image.\n *\n * @param fontFace Font type, see HersheyFonts.\n *\n * @param fontScale Font scale factor that is multiplied by the font-specific base size.\n *\n * @param color Text color.\n *\n * @param thickness Thickness of the lines used to draw a text.\n *\n * @param lineType Line type. See LineTypes\n *\n * @param bottomLeftOrigin When true, the image data origin is at the bottom-left corner. Otherwise, it\n * is at the top-left corner.\n */\nexport declare function putText(img: InputOutputArray, text: any, org: Point, fontFace: int, fontScale: double, color: Scalar, thickness?: int, lineType?: int, bottomLeftOrigin?: bool): void;\n/**\n * The function [cv::rectangle] draws a rectangle outline or a filled rectangle whose two opposite\n * corners are pt1 and pt2.\n *\n * @param img Image.\n *\n * @param pt1 Vertex of the rectangle.\n *\n * @param pt2 Vertex of the rectangle opposite to pt1 .\n *\n * @param color Rectangle color or brightness (grayscale image).\n *\n * @param thickness Thickness of lines that make up the rectangle. Negative values, like FILLED, mean\n * that the function has to draw a filled rectangle.\n *\n * @param lineType Type of the line. See LineTypes\n *\n * @param shift Number of fractional bits in the point coordinates.\n */\nexport declare function rectangle(img: InputOutputArray, pt1: Point, pt2: Point, color: any, thickness?: int, lineType?: int, shift?: int): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n *\n * use `rec` parameter as alternative specification of the drawn rectangle: `r.tl() and\n * r.br()-Point(1,1)` are opposite corners\n */\nexport declare function rectangle(img: InputOutputArray, rec: Rect, color: any, thickness?: int, lineType?: int, shift?: int): void;\nexport declare const FONT_HERSHEY_SIMPLEX: HersheyFonts;\nexport declare const FONT_HERSHEY_PLAIN: HersheyFonts;\nexport declare const FONT_HERSHEY_DUPLEX: HersheyFonts;\nexport declare const FONT_HERSHEY_COMPLEX: HersheyFonts;\nexport declare const FONT_HERSHEY_TRIPLEX: HersheyFonts;\nexport declare const FONT_HERSHEY_COMPLEX_SMALL: HersheyFonts;\nexport declare const FONT_HERSHEY_SCRIPT_SIMPLEX: HersheyFonts;\nexport declare const FONT_HERSHEY_SCRIPT_COMPLEX: HersheyFonts;\nexport declare const FONT_ITALIC: HersheyFonts;\nexport declare const FILLED: LineTypes;\nexport declare const LINE_4: LineTypes;\nexport declare const LINE_8: LineTypes;\nexport declare const LINE_AA: LineTypes;\nexport declare const MARKER_CROSS: MarkerTypes;\nexport declare const MARKER_TILTED_CROSS: MarkerTypes;\nexport declare const MARKER_STAR: MarkerTypes;\nexport declare const MARKER_DIAMOND: MarkerTypes;\nexport declare const MARKER_SQUARE: MarkerTypes;\nexport declare const MARKER_TRIANGLE_UP: MarkerTypes;\nexport declare const MARKER_TRIANGLE_DOWN: MarkerTypes;\n/**\n * Only a subset of Hershey fonts  are supported\n *\n */\nexport declare type HersheyFonts = any;\n/**\n * Only a subset of Hershey fonts  are supported\n *\n */\nexport declare type LineTypes = any;\n/**\n * Only a subset of Hershey fonts  are supported\n *\n */\nexport declare type MarkerTypes = any;\n"},"node_modules_mirada_dist_src_types_opencv_imgproc_feature_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_imgproc_feature_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/imgproc_feature.d.ts","content":"import { bool, double, InputArray, InputOutputArray, int, OutputArray, Size, TermCriteria } from './_types';\n/**\n * The function finds edges in the input image and marks them in the output map edges using the Canny\n * algorithm. The smallest value between threshold1 and threshold2 is used for edge linking. The\n * largest value is used to find initial segments of strong edges. See\n *\n * @param image 8-bit input image.\n *\n * @param edges output edge map; single channels 8-bit image, which has the same size as image .\n *\n * @param threshold1 first threshold for the hysteresis procedure.\n *\n * @param threshold2 second threshold for the hysteresis procedure.\n *\n * @param apertureSize aperture size for the Sobel operator.\n *\n * @param L2gradient a flag, indicating whether a more accurate $L_2$ norm $=\\sqrt{(dI/dx)^2 +\n * (dI/dy)^2}$ should be used to calculate the image gradient magnitude ( L2gradient=true ), or whether\n * the default $L_1$ norm $=|dI/dx|+|dI/dy|$ is enough ( L2gradient=false ).\n */\nexport declare function Canny(image: InputArray, edges: OutputArray, threshold1: double, threshold2: double, apertureSize?: int, L2gradient?: bool): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n *\n * Finds edges in an image using the Canny algorithm with custom image gradient.\n *\n * @param dx 16-bit x derivative of input image (CV_16SC1 or CV_16SC3).\n *\n * @param dy 16-bit y derivative of input image (same type as dx).\n *\n * @param edges output edge map; single channels 8-bit image, which has the same size as image .\n *\n * @param threshold1 first threshold for the hysteresis procedure.\n *\n * @param threshold2 second threshold for the hysteresis procedure.\n *\n * @param L2gradient a flag, indicating whether a more accurate $L_2$ norm $=\\sqrt{(dI/dx)^2 +\n * (dI/dy)^2}$ should be used to calculate the image gradient magnitude ( L2gradient=true ), or whether\n * the default $L_1$ norm $=|dI/dx|+|dI/dy|$ is enough ( L2gradient=false ).\n */\nexport declare function Canny(dx: InputArray, dy: InputArray, edges: OutputArray, threshold1: double, threshold2: double, L2gradient?: bool): void;\n/**\n * For every pixel `$p$` , the function cornerEigenValsAndVecs considers a blockSize `$\\\\times$`\n * blockSize neighborhood `$S(p)$` . It calculates the covariation matrix of derivatives over the\n * neighborhood as:\n *\n * `\\\\[M = \\\\begin{bmatrix} \\\\sum _{S(p)}(dI/dx)^2 & \\\\sum _{S(p)}dI/dx dI/dy \\\\\\\\ \\\\sum _{S(p)}dI/dx\n * dI/dy & \\\\sum _{S(p)}(dI/dy)^2 \\\\end{bmatrix}\\\\]`\n *\n * where the derivatives are computed using the Sobel operator.\n *\n * After that, it finds eigenvectors and eigenvalues of `$M$` and stores them in the destination image\n * as `$(\\\\lambda_1, \\\\lambda_2, x_1, y_1, x_2, y_2)$` where\n *\n * `$\\\\lambda_1, \\\\lambda_2$` are the non-sorted eigenvalues of `$M$`\n * `$x_1, y_1$` are the eigenvectors corresponding to `$\\\\lambda_1$`\n * `$x_2, y_2$` are the eigenvectors corresponding to `$\\\\lambda_2$`\n *\n * The output of the function can be used for robust edge or corner detection.\n *\n * [cornerMinEigenVal], [cornerHarris], [preCornerDetect]\n *\n * @param src Input single-channel 8-bit or floating-point image.\n *\n * @param dst Image to store the results. It has the same size as src and the type CV_32FC(6) .\n *\n * @param blockSize Neighborhood size (see details below).\n *\n * @param ksize Aperture parameter for the Sobel operator.\n *\n * @param borderType Pixel extrapolation method. See BorderTypes.\n */\nexport declare function cornerEigenValsAndVecs(src: InputArray, dst: OutputArray, blockSize: int, ksize: int, borderType?: int): void;\n/**\n * The function runs the Harris corner detector on the image. Similarly to cornerMinEigenVal and\n * cornerEigenValsAndVecs , for each pixel `$(x, y)$` it calculates a `$2\\\\times2$` gradient covariance\n * matrix `$M^{(x,y)}$` over a `$\\\\texttt{blockSize} \\\\times \\\\texttt{blockSize}$` neighborhood. Then,\n * it computes the following characteristic:\n *\n * `\\\\[\\\\texttt{dst} (x,y) = \\\\mathrm{det} M^{(x,y)} - k \\\\cdot \\\\left ( \\\\mathrm{tr} M^{(x,y)} \\\\right\n * )^2\\\\]`\n *\n * Corners in the image can be found as the local maxima of this response map.\n *\n * @param src Input single-channel 8-bit or floating-point image.\n *\n * @param dst Image to store the Harris detector responses. It has the type CV_32FC1 and the same size\n * as src .\n *\n * @param blockSize Neighborhood size (see the details on cornerEigenValsAndVecs ).\n *\n * @param ksize Aperture parameter for the Sobel operator.\n *\n * @param k Harris detector free parameter. See the formula above.\n *\n * @param borderType Pixel extrapolation method. See BorderTypes.\n */\nexport declare function cornerHarris(src: InputArray, dst: OutputArray, blockSize: int, ksize: int, k: double, borderType?: int): void;\n/**\n * The function is similar to cornerEigenValsAndVecs but it calculates and stores only the minimal\n * eigenvalue of the covariance matrix of derivatives, that is, `$\\\\min(\\\\lambda_1, \\\\lambda_2)$` in\n * terms of the formulae in the cornerEigenValsAndVecs description.\n *\n * @param src Input single-channel 8-bit or floating-point image.\n *\n * @param dst Image to store the minimal eigenvalues. It has the type CV_32FC1 and the same size as src\n * .\n *\n * @param blockSize Neighborhood size (see the details on cornerEigenValsAndVecs ).\n *\n * @param ksize Aperture parameter for the Sobel operator.\n *\n * @param borderType Pixel extrapolation method. See BorderTypes.\n */\nexport declare function cornerMinEigenVal(src: InputArray, dst: OutputArray, blockSize: int, ksize?: int, borderType?: int): void;\n/**\n * The function iterates to find the sub-pixel accurate location of corners or radial saddle points, as\n * shown on the figure below.\n *\n *  Sub-pixel accurate corner locator is based on the observation that every vector from the center\n * `$q$` to a point `$p$` located within a neighborhood of `$q$` is orthogonal to the image gradient at\n * `$p$` subject to image and measurement noise. Consider the expression:\n *\n * `\\\\[\\\\epsilon _i = {DI_{p_i}}^T \\\\cdot (q - p_i)\\\\]`\n *\n * where `${DI_{p_i}}$` is an image gradient at one of the points `$p_i$` in a neighborhood of `$q$` .\n * The value of `$q$` is to be found so that `$\\\\epsilon_i$` is minimized. A system of equations may be\n * set up with `$\\\\epsilon_i$` set to zero:\n *\n * `\\\\[\\\\sum _i(DI_{p_i} \\\\cdot {DI_{p_i}}^T) \\\\cdot q - \\\\sum _i(DI_{p_i} \\\\cdot {DI_{p_i}}^T \\\\cdot\n * p_i)\\\\]`\n *\n * where the gradients are summed within a neighborhood (\"search window\") of `$q$` . Calling the first\n * gradient term `$G$` and the second gradient term `$b$` gives:\n *\n * `\\\\[q = G^{-1} \\\\cdot b\\\\]`\n *\n * The algorithm sets the center of the neighborhood window at this new center `$q$` and then iterates\n * until the center stays within a set threshold.\n *\n * @param image Input single-channel, 8-bit or float image.\n *\n * @param corners Initial coordinates of the input corners and refined coordinates provided for output.\n *\n * @param winSize Half of the side length of the search window. For example, if winSize=Size(5,5) ,\n * then a $(5*2+1) \\times (5*2+1) = 11 \\times 11$ search window is used.\n *\n * @param zeroZone Half of the size of the dead region in the middle of the search zone over which the\n * summation in the formula below is not done. It is used sometimes to avoid possible singularities of\n * the autocorrelation matrix. The value of (-1,-1) indicates that there is no such a size.\n *\n * @param criteria Criteria for termination of the iterative process of corner refinement. That is, the\n * process of corner position refinement stops either after criteria.maxCount iterations or when the\n * corner position moves by less than criteria.epsilon on some iteration.\n */\nexport declare function cornerSubPix(image: InputArray, corners: InputOutputArray, winSize: Size, zeroZone: Size, criteria: TermCriteria): void;\n/**\n * The [LineSegmentDetector] algorithm is defined using the standard values. Only advanced users may\n * want to edit those, as to tailor it for their own application.\n *\n * Implementation has been removed due original code license conflict\n *\n * @param _refine The way found lines will be refined, see LineSegmentDetectorModes\n *\n * @param _scale The scale of the image that will be used to find the lines. Range (0..1].\n *\n * @param _sigma_scale Sigma for Gaussian filter. It is computed as sigma = _sigma_scale/_scale.\n *\n * @param _quant Bound to the quantization error on the gradient norm.\n *\n * @param _ang_th Gradient angle tolerance in degrees.\n *\n * @param _log_eps Detection threshold: -log10(NFA) > log_eps. Used only when advance refinement is\n * chosen.\n *\n * @param _density_th Minimal density of aligned region points in the enclosing rectangle.\n *\n * @param _n_bins Number of bins in pseudo-ordering of gradient modulus.\n */\nexport declare function createLineSegmentDetector(_refine?: int, _scale?: double, _sigma_scale?: double, _quant?: double, _ang_th?: double, _log_eps?: double, _density_th?: double, _n_bins?: int): any;\n/**\n * The function finds the most prominent corners in the image or in the specified image region, as\n * described in Shi94\n *\n * Function calculates the corner quality measure at every source image pixel using the\n * [cornerMinEigenVal] or [cornerHarris] .\n * Function performs a non-maximum suppression (the local maximums in *3 x 3* neighborhood are\n * retained).\n * The corners with the minimal eigenvalue less than `$\\\\texttt{qualityLevel} \\\\cdot \\\\max_{x,y}\n * qualityMeasureMap(x,y)$` are rejected.\n * The remaining corners are sorted by the quality measure in the descending order.\n * Function throws away each corner for which there is a stronger corner at a distance less than\n * maxDistance.\n *\n * The function can be used to initialize a point-based tracker of an object.\n *\n * If the function is called with different values A and B of the parameter qualityLevel , and A > B,\n * the vector of returned corners with qualityLevel=A will be the prefix of the output vector with\n * qualityLevel=B .\n *\n * [cornerMinEigenVal], [cornerHarris], [calcOpticalFlowPyrLK], [estimateRigidTransform],\n *\n * @param image Input 8-bit or floating-point 32-bit, single-channel image.\n *\n * @param corners Output vector of detected corners.\n *\n * @param maxCorners Maximum number of corners to return. If there are more corners than are found, the\n * strongest of them is returned. maxCorners <= 0 implies that no limit on the maximum is set and all\n * detected corners are returned.\n *\n * @param qualityLevel Parameter characterizing the minimal accepted quality of image corners. The\n * parameter value is multiplied by the best corner quality measure, which is the minimal eigenvalue\n * (see cornerMinEigenVal ) or the Harris function response (see cornerHarris ). The corners with the\n * quality measure less than the product are rejected. For example, if the best corner has the quality\n * measure = 1500, and the qualityLevel=0.01 , then all the corners with the quality measure less than\n * 15 are rejected.\n *\n * @param minDistance Minimum possible Euclidean distance between the returned corners.\n *\n * @param mask Optional region of interest. If the image is not empty (it needs to have the type\n * CV_8UC1 and the same size as image ), it specifies the region in which the corners are detected.\n *\n * @param blockSize Size of an average block for computing a derivative covariation matrix over each\n * pixel neighborhood. See cornerEigenValsAndVecs .\n *\n * @param useHarrisDetector Parameter indicating whether to use a Harris detector (see cornerHarris) or\n * cornerMinEigenVal.\n *\n * @param k Free parameter of the Harris detector.\n */\nexport declare function goodFeaturesToTrack(image: InputArray, corners: OutputArray, maxCorners: int, qualityLevel: double, minDistance: double, mask?: InputArray, blockSize?: int, useHarrisDetector?: bool, k?: double): void;\nexport declare function goodFeaturesToTrack(image: InputArray, corners: OutputArray, maxCorners: int, qualityLevel: double, minDistance: double, mask: InputArray, blockSize: int, gradientSize: int, useHarrisDetector?: bool, k?: double): void;\n/**\n * The function finds circles in a grayscale image using a modification of the Hough transform.\n *\n * Example: :\n *\n * ```cpp\n * #include <opencv2/imgproc.hpp>\n * #include <opencv2/highgui.hpp>\n * #include <math.h>\n *\n * using namespace cv;\n * using namespace std;\n *\n * int main(int argc, char** argv)\n * {\n *     Mat img, gray;\n *     if( argc != 2 || !(img=imread(argv[1], 1)).data)\n *         return -1;\n *     cvtColor(img, gray, COLOR_BGR2GRAY);\n *     // smooth it, otherwise a lot of false circles may be detected\n *     GaussianBlur( gray, gray, Size(9, 9), 2, 2 );\n *     vector<Vec3f> circles;\n *     HoughCircles(gray, circles, HOUGH_GRADIENT,\n *                  2, gray.rows/4, 200, 100 );\n *     for( size_t i = 0; i < circles.size(); i++ )\n *     {\n *          Point center(cvRound(circles[i][0]), cvRound(circles[i][1]));\n *          int radius = cvRound(circles[i][2]);\n *          // draw the circle center\n *          circle( img, center, 3, Scalar(0,255,0), -1, 8, 0 );\n *          // draw the circle outline\n *          circle( img, center, radius, Scalar(0,0,255), 3, 8, 0 );\n *     }\n *     namedWindow( \"circles\", 1 );\n *     imshow( \"circles\", img );\n *\n *     waitKey(0);\n *     return 0;\n * }\n * ```\n *\n * Usually the function detects the centers of circles well. However, it may fail to find correct\n * radii. You can assist to the function by specifying the radius range ( minRadius and maxRadius ) if\n * you know it. Or, you may set maxRadius to a negative number to return centers only without radius\n * search, and find the correct radius using an additional procedure.\n *\n * [fitEllipse], [minEnclosingCircle]\n *\n * @param image 8-bit, single-channel, grayscale input image.\n *\n * @param circles Output vector of found circles. Each vector is encoded as 3 or 4 element\n * floating-point vector $(x, y, radius)$ or $(x, y, radius, votes)$ .\n *\n * @param method Detection method, see HoughModes. Currently, the only implemented method is\n * HOUGH_GRADIENT\n *\n * @param dp Inverse ratio of the accumulator resolution to the image resolution. For example, if dp=1\n * , the accumulator has the same resolution as the input image. If dp=2 , the accumulator has half as\n * big width and height.\n *\n * @param minDist Minimum distance between the centers of the detected circles. If the parameter is too\n * small, multiple neighbor circles may be falsely detected in addition to a true one. If it is too\n * large, some circles may be missed.\n *\n * @param param1 First method-specific parameter. In case of HOUGH_GRADIENT , it is the higher\n * threshold of the two passed to the Canny edge detector (the lower one is twice smaller).\n *\n * @param param2 Second method-specific parameter. In case of HOUGH_GRADIENT , it is the accumulator\n * threshold for the circle centers at the detection stage. The smaller it is, the more false circles\n * may be detected. Circles, corresponding to the larger accumulator values, will be returned first.\n *\n * @param minRadius Minimum circle radius.\n *\n * @param maxRadius Maximum circle radius. If <= 0, uses the maximum image dimension. If < 0, returns\n * centers without finding the radius.\n */\nexport declare function HoughCircles(image: InputArray, circles: OutputArray, method: int, dp: double, minDist: double, param1?: double, param2?: double, minRadius?: int, maxRadius?: int): void;\n/**\n * The function implements the standard or standard multi-scale Hough transform algorithm for line\n * detection. See  for a good explanation of Hough transform.\n *\n * @param image 8-bit, single-channel binary source image. The image may be modified by the function.\n *\n * @param lines Output vector of lines. Each line is represented by a 2 or 3 element vector $(\\rho,\n * \\theta)$ or $(\\rho, \\theta, \\textrm{votes})$ . $\\rho$ is the distance from the coordinate origin\n * $(0,0)$ (top-left corner of the image). $\\theta$ is the line rotation angle in radians ( $0 \\sim\n * \\textrm{vertical line}, \\pi/2 \\sim \\textrm{horizontal line}$ ). $\\textrm{votes}$ is the value of\n * accumulator.\n *\n * @param rho Distance resolution of the accumulator in pixels.\n *\n * @param theta Angle resolution of the accumulator in radians.\n *\n * @param threshold Accumulator threshold parameter. Only those lines are returned that get enough\n * votes ( $>\\texttt{threshold}$ ).\n *\n * @param srn For the multi-scale Hough transform, it is a divisor for the distance resolution rho .\n * The coarse accumulator distance resolution is rho and the accurate accumulator resolution is rho/srn\n * . If both srn=0 and stn=0 , the classical Hough transform is used. Otherwise, both these parameters\n * should be positive.\n *\n * @param stn For the multi-scale Hough transform, it is a divisor for the distance resolution theta.\n *\n * @param min_theta For standard and multi-scale Hough transform, minimum angle to check for lines.\n * Must fall between 0 and max_theta.\n *\n * @param max_theta For standard and multi-scale Hough transform, maximum angle to check for lines.\n * Must fall between min_theta and CV_PI.\n */\nexport declare function HoughLines(image: InputArray, lines: OutputArray, rho: double, theta: double, threshold: int, srn?: double, stn?: double, min_theta?: double, max_theta?: double): void;\n/**\n * The function implements the probabilistic Hough transform algorithm for line detection, described in\n * Matas00\n *\n * See the line detection example below:\n *\n * ```cpp\n * #include <opencv2/imgproc.hpp>\n * #include <opencv2/highgui.hpp>\n *\n * using namespace cv;\n * using namespace std;\n *\n * int main(int argc, char** argv)\n * {\n *     Mat src, dst, color_dst;\n *     if( argc != 2 || !(src=imread(argv[1], 0)).data)\n *         return -1;\n *\n *     Canny( src, dst, 50, 200, 3 );\n *     cvtColor( dst, color_dst, COLOR_GRAY2BGR );\n *\n *     vector<Vec4i> lines;\n *     HoughLinesP( dst, lines, 1, CV_PI/180, 80, 30, 10 );\n *     for( size_t i = 0; i < lines.size(); i++ )\n *     {\n *         line( color_dst, Point(lines[i][0], lines[i][1]),\n *         Point( lines[i][2], lines[i][3]), Scalar(0,0,255), 3, 8 );\n *     }\n *     namedWindow( \"Source\", 1 );\n *     imshow( \"Source\", src );\n *\n *     namedWindow( \"Detected Lines\", 1 );\n *     imshow( \"Detected Lines\", color_dst );\n *\n *     waitKey(0);\n *     return 0;\n * }\n * ```\n *\n *  This is a sample picture the function parameters have been tuned for:\n *\n *  And this is the output of the above program in case of the probabilistic Hough transform:\n *\n * [LineSegmentDetector]\n *\n * @param image 8-bit, single-channel binary source image. The image may be modified by the function.\n *\n * @param lines Output vector of lines. Each line is represented by a 4-element vector $(x_1, y_1, x_2,\n * y_2)$ , where $(x_1,y_1)$ and $(x_2, y_2)$ are the ending points of each detected line segment.\n *\n * @param rho Distance resolution of the accumulator in pixels.\n *\n * @param theta Angle resolution of the accumulator in radians.\n *\n * @param threshold Accumulator threshold parameter. Only those lines are returned that get enough\n * votes ( $>\\texttt{threshold}$ ).\n *\n * @param minLineLength Minimum line length. Line segments shorter than that are rejected.\n *\n * @param maxLineGap Maximum allowed gap between points on the same line to link them.\n */\nexport declare function HoughLinesP(image: InputArray, lines: OutputArray, rho: double, theta: double, threshold: int, minLineLength?: double, maxLineGap?: double): void;\n/**\n * The function finds lines in a set of points using a modification of the Hough transform.\n *\n * ```cpp\n * #include <opencv2/core.hpp>\n * #include <opencv2/imgproc.hpp>\n *\n * using namespace cv;\n * using namespace std;\n *\n * int main()\n * {\n *     Mat lines;\n *     vector<Vec3d> line3d;\n *     vector<Point2f> point;\n *     const static float Points[20][2] = {\n *     { 0.0f,   369.0f }, { 10.0f,  364.0f }, { 20.0f,  358.0f }, { 30.0f,  352.0f },\n *     { 40.0f,  346.0f }, { 50.0f,  341.0f }, { 60.0f,  335.0f }, { 70.0f,  329.0f },\n *     { 80.0f,  323.0f }, { 90.0f,  318.0f }, { 100.0f, 312.0f }, { 110.0f, 306.0f },\n *     { 120.0f, 300.0f }, { 130.0f, 295.0f }, { 140.0f, 289.0f }, { 150.0f, 284.0f },\n *     { 160.0f, 277.0f }, { 170.0f, 271.0f }, { 180.0f, 266.0f }, { 190.0f, 260.0f }\n *     };\n *\n *     for (int i = 0; i < 20; i++)\n *     {\n *         point.push_back(Point2f(Points[i][0],Points[i][1]));\n *     }\n *\n *     double rhoMin = 0.0f, rhoMax = 360.0f, rhoStep = 1;\n *     double thetaMin = 0.0f, thetaMax = CV_PI / 2.0f, thetaStep = CV_PI / 180.0f;\n *\n *     HoughLinesPointSet(point, lines, 20, 1,\n *                        rhoMin, rhoMax, rhoStep,\n *                        thetaMin, thetaMax, thetaStep);\n *\n *     lines.copyTo(line3d);\n *     printf(\"votes:%d, rho:%.7f, theta:%.7f\\\\n\",(int)line3d.at(0).val[0], line3d.at(0).val[1],\n * line3d.at(0).val[2]);\n * }\n * ```\n *\n * @param _point Input vector of points. Each vector must be encoded as a Point vector $(x,y)$. Type\n * must be CV_32FC2 or CV_32SC2.\n *\n * @param _lines Output vector of found lines. Each vector is encoded as a vector<Vec3d> $(votes, rho,\n * theta)$. The larger the value of 'votes', the higher the reliability of the Hough line.\n *\n * @param lines_max Max count of hough lines.\n *\n * @param threshold Accumulator threshold parameter. Only those lines are returned that get enough\n * votes ( $>\\texttt{threshold}$ )\n *\n * @param min_rho Minimum Distance value of the accumulator in pixels.\n *\n * @param max_rho Maximum Distance value of the accumulator in pixels.\n *\n * @param rho_step Distance resolution of the accumulator in pixels.\n *\n * @param min_theta Minimum angle value of the accumulator in radians.\n *\n * @param max_theta Maximum angle value of the accumulator in radians.\n *\n * @param theta_step Angle resolution of the accumulator in radians.\n */\nexport declare function HoughLinesPointSet(_point: InputArray, _lines: OutputArray, lines_max: int, threshold: int, min_rho: double, max_rho: double, rho_step: double, min_theta: double, max_theta: double, theta_step: double): void;\n/**\n * The function calculates the complex spatial derivative-based function of the source image\n *\n * `\\\\[\\\\texttt{dst} = (D_x \\\\texttt{src} )^2 \\\\cdot D_{yy} \\\\texttt{src} + (D_y \\\\texttt{src} )^2\n * \\\\cdot D_{xx} \\\\texttt{src} - 2 D_x \\\\texttt{src} \\\\cdot D_y \\\\texttt{src} \\\\cdot D_{xy}\n * \\\\texttt{src}\\\\]`\n *\n * where `$D_x$`, `$D_y$` are the first image derivatives, `$D_{xx}$`, `$D_{yy}$` are the second image\n * derivatives, and `$D_{xy}$` is the mixed derivative.\n *\n * The corners can be found as local maximums of the functions, as shown below:\n *\n * ```cpp\n * Mat corners, dilated_corners;\n * preCornerDetect(image, corners, 3);\n * // dilation with 3x3 rectangular structuring element\n * dilate(corners, dilated_corners, Mat(), 1);\n * Mat corner_mask = corners == dilated_corners;\n * ```\n *\n * @param src Source single-channel 8-bit of floating-point image.\n *\n * @param dst Output image that has the type CV_32F and the same size as src .\n *\n * @param ksize Aperture size of the Sobel .\n *\n * @param borderType Pixel extrapolation method. See BorderTypes.\n */\nexport declare function preCornerDetect(src: InputArray, dst: OutputArray, ksize: int, borderType?: int): void;\n/**\n * classical or standard Hough transform. Every line is represented by two floating-point numbers\n * `$(\\\\rho, \\\\theta)$` , where `$\\\\rho$` is a distance between (0,0) point and the line, and\n * `$\\\\theta$` is the angle between x-axis and the normal to the line. Thus, the matrix must be (the\n * created sequence will be) of CV_32FC2 type\n *\n */\nexport declare const HOUGH_STANDARD: HoughModes;\n/**\n * probabilistic Hough transform (more efficient in case if the picture contains a few long linear\n * segments). It returns line segments rather than the whole line. Each segment is represented by\n * starting and ending points, and the matrix must be (the created sequence will be) of the CV_32SC4\n * type.\n *\n */\nexport declare const HOUGH_PROBABILISTIC: HoughModes;\n/**\n * multi-scale variant of the classical Hough transform. The lines are encoded the same way as\n * HOUGH_STANDARD.\n *\n */\nexport declare const HOUGH_MULTI_SCALE: HoughModes;\nexport declare const HOUGH_GRADIENT: HoughModes;\nexport declare const LSD_REFINE_NONE: LineSegmentDetectorModes;\nexport declare const LSD_REFINE_STD: LineSegmentDetectorModes;\n/**\n * Advanced refinement. Number of false alarms is calculated, lines are refined through increase of\n * precision, decrement in size, etc.\n *\n */\nexport declare const LSD_REFINE_ADV: LineSegmentDetectorModes;\nexport declare type HoughModes = any;\nexport declare type LineSegmentDetectorModes = any;\n"},"node_modules_mirada_dist_src_types_opencv_imgproc_filter_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_imgproc_filter_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/imgproc_filter.d.ts","content":"import { bool, double, InputArray, int, Mat, OutputArray, OutputArrayOfArrays, Point, Scalar, Size, TermCriteria } from './_types';\n/**\n * The function applies bilateral filtering to the input image, as described in  bilateralFilter can\n * reduce unwanted noise very well while keeping edges fairly sharp. However, it is very slow compared\n * to most filters.\n *\n * Sigma values*: For simplicity, you can set the 2 sigma values to be the same. If they are small (<\n * 10), the filter will not have much effect, whereas if they are large (> 150), they will have a very\n * strong effect, making the image look \"cartoonish\".\n *\n * Filter size*: Large filters (d > 5) are very slow, so it is recommended to use d=5 for real-time\n * applications, and perhaps d=9 for offline applications that need heavy noise filtering.\n *\n * This filter does not work inplace.\n *\n * @param src Source 8-bit or floating-point, 1-channel or 3-channel image.\n *\n * @param dst Destination image of the same size and type as src .\n *\n * @param d Diameter of each pixel neighborhood that is used during filtering. If it is non-positive,\n * it is computed from sigmaSpace.\n *\n * @param sigmaColor Filter sigma in the color space. A larger value of the parameter means that\n * farther colors within the pixel neighborhood (see sigmaSpace) will be mixed together, resulting in\n * larger areas of semi-equal color.\n *\n * @param sigmaSpace Filter sigma in the coordinate space. A larger value of the parameter means that\n * farther pixels will influence each other as long as their colors are close enough (see sigmaColor ).\n * When d>0, it specifies the neighborhood size regardless of sigmaSpace. Otherwise, d is proportional\n * to sigmaSpace.\n *\n * @param borderType border mode used to extrapolate pixels outside of the image, see BorderTypes\n */\nexport declare function bilateralFilter(src: InputArray, dst: OutputArray, d: int, sigmaColor: double, sigmaSpace: double, borderType?: int): void;\n/**\n * The function smooths an image using the kernel:\n *\n * `\\\\[\\\\texttt{K} = \\\\frac{1}{\\\\texttt{ksize.width*ksize.height}} \\\\begin{bmatrix} 1 & 1 & 1 & \\\\cdots\n * & 1 & 1 \\\\\\\\ 1 & 1 & 1 & \\\\cdots & 1 & 1 \\\\\\\\ \\\\hdotsfor{6} \\\\\\\\ 1 & 1 & 1 & \\\\cdots & 1 & 1 \\\\\\\\\n * \\\\end{bmatrix}\\\\]`\n *\n * The call `blur(src, dst, ksize, anchor, borderType)` is equivalent to `boxFilter(src, dst,\n * src.type(), anchor, true, borderType)`.\n *\n * [boxFilter], [bilateralFilter], [GaussianBlur], [medianBlur]\n *\n * @param src input image; it can have any number of channels, which are processed independently, but\n * the depth should be CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.\n *\n * @param dst output image of the same size and type as src.\n *\n * @param ksize blurring kernel size.\n *\n * @param anchor anchor point; default value Point(-1,-1) means that the anchor is at the kernel\n * center.\n *\n * @param borderType border mode used to extrapolate pixels outside of the image, see BorderTypes\n */\nexport declare function blur(src: InputArray, dst: OutputArray, ksize: Size, anchor?: Point, borderType?: int): void;\n/**\n * The function smooths an image using the kernel:\n *\n * `\\\\[\\\\texttt{K} = \\\\alpha \\\\begin{bmatrix} 1 & 1 & 1 & \\\\cdots & 1 & 1 \\\\\\\\ 1 & 1 & 1 & \\\\cdots & 1\n * & 1 \\\\\\\\ \\\\hdotsfor{6} \\\\\\\\ 1 & 1 & 1 & \\\\cdots & 1 & 1 \\\\end{bmatrix}\\\\]`\n *\n * where\n *\n * `\\\\[\\\\alpha = \\\\fork{\\\\frac{1}{\\\\texttt{ksize.width*ksize.height}}}{when\n * \\\\texttt{normalize=true}}{1}{otherwise}\\\\]`\n *\n * Unnormalized box filter is useful for computing various integral characteristics over each pixel\n * neighborhood, such as covariance matrices of image derivatives (used in dense optical flow\n * algorithms, and so on). If you need to compute pixel sums over variable-size windows, use\n * [integral].\n *\n * [blur], [bilateralFilter], [GaussianBlur], [medianBlur], [integral]\n *\n * @param src input image.\n *\n * @param dst output image of the same size and type as src.\n *\n * @param ddepth the output image depth (-1 to use src.depth()).\n *\n * @param ksize blurring kernel size.\n *\n * @param anchor anchor point; default value Point(-1,-1) means that the anchor is at the kernel\n * center.\n *\n * @param normalize flag, specifying whether the kernel is normalized by its area or not.\n *\n * @param borderType border mode used to extrapolate pixels outside of the image, see BorderTypes\n */\nexport declare function boxFilter(src: InputArray, dst: OutputArray, ddepth: int, ksize: Size, anchor?: Point, normalize?: bool, borderType?: int): void;\n/**\n * The function constructs a vector of images and builds the Gaussian pyramid by recursively applying\n * pyrDown to the previously built pyramid layers, starting from `dst[0]==src`.\n *\n * @param src Source image. Check pyrDown for the list of supported types.\n *\n * @param dst Destination vector of maxlevel+1 images of the same type as src. dst[0] will be the same\n * as src. dst[1] is the next pyramid layer, a smoothed and down-sized src, and so on.\n *\n * @param maxlevel 0-based index of the last (the smallest) pyramid layer. It must be non-negative.\n *\n * @param borderType Pixel extrapolation method, see BorderTypes (BORDER_CONSTANT isn't supported)\n */\nexport declare function buildPyramid(src: InputArray, dst: OutputArrayOfArrays, maxlevel: int, borderType?: int): void;\n/**\n * The function dilates the source image using the specified structuring element that determines the\n * shape of a pixel neighborhood over which the maximum is taken: `\\\\[\\\\texttt{dst} (x,y) = \\\\max\n * _{(x',y'): \\\\, \\\\texttt{element} (x',y') \\\\ne0 } \\\\texttt{src} (x+x',y+y')\\\\]`\n *\n * The function supports the in-place mode. Dilation can be applied several ( iterations ) times. In\n * case of multi-channel images, each channel is processed independently.\n *\n * [erode], [morphologyEx], [getStructuringElement]\n *\n * @param src input image; the number of channels can be arbitrary, but the depth should be one of\n * CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.\n *\n * @param dst output image of the same size and type as src.\n *\n * @param kernel structuring element used for dilation; if elemenat=Mat(), a 3 x 3 rectangular\n * structuring element is used. Kernel can be created using getStructuringElement\n *\n * @param anchor position of the anchor within the element; default value (-1, -1) means that the\n * anchor is at the element center.\n *\n * @param iterations number of times dilation is applied.\n *\n * @param borderType pixel extrapolation method, see BorderTypes\n *\n * @param borderValue border value in case of a constant border\n */\nexport declare function dilate(src: InputArray, dst: OutputArray, kernel: InputArray, anchor?: Point, iterations?: int, borderType?: int, borderValue?: any): void;\n/**\n * The function erodes the source image using the specified structuring element that determines the\n * shape of a pixel neighborhood over which the minimum is taken:\n *\n * `\\\\[\\\\texttt{dst} (x,y) = \\\\min _{(x',y'): \\\\, \\\\texttt{element} (x',y') \\\\ne0 } \\\\texttt{src}\n * (x+x',y+y')\\\\]`\n *\n * The function supports the in-place mode. Erosion can be applied several ( iterations ) times. In\n * case of multi-channel images, each channel is processed independently.\n *\n * [dilate], [morphologyEx], [getStructuringElement]\n *\n * @param src input image; the number of channels can be arbitrary, but the depth should be one of\n * CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.\n *\n * @param dst output image of the same size and type as src.\n *\n * @param kernel structuring element used for erosion; if element=Mat(), a 3 x 3 rectangular\n * structuring element is used. Kernel can be created using getStructuringElement.\n *\n * @param anchor position of the anchor within the element; default value (-1, -1) means that the\n * anchor is at the element center.\n *\n * @param iterations number of times erosion is applied.\n *\n * @param borderType pixel extrapolation method, see BorderTypes\n *\n * @param borderValue border value in case of a constant border\n */\nexport declare function erode(src: InputArray, dst: OutputArray, kernel: InputArray, anchor?: Point, iterations?: int, borderType?: int, borderValue?: any): void;\n/**\n * The function applies an arbitrary linear filter to an image. In-place operation is supported. When\n * the aperture is partially outside the image, the function interpolates outlier pixel values\n * according to the specified border mode.\n *\n * The function does actually compute correlation, not the convolution:\n *\n * `\\\\[\\\\texttt{dst} (x,y) = \\\\sum _{ \\\\stackrel{0\\\\leq x' < \\\\texttt{kernel.cols},}{0\\\\leq y' <\n * \\\\texttt{kernel.rows}} } \\\\texttt{kernel} (x',y')* \\\\texttt{src} (x+x'- \\\\texttt{anchor.x} ,y+y'-\n * \\\\texttt{anchor.y} )\\\\]`\n *\n * That is, the kernel is not mirrored around the anchor point. If you need a real convolution, flip\n * the kernel using [flip] and set the new anchor to `(kernel.cols - anchor.x - 1, kernel.rows -\n * anchor.y - 1)`.\n *\n * The function uses the DFT-based algorithm in case of sufficiently large kernels (~`11 x 11` or\n * larger) and the direct algorithm for small kernels.\n *\n * [sepFilter2D], [dft], [matchTemplate]\n *\n * @param src input image.\n *\n * @param dst output image of the same size and the same number of channels as src.\n *\n * @param ddepth desired depth of the destination image, see combinations\n *\n * @param kernel convolution kernel (or rather a correlation kernel), a single-channel floating point\n * matrix; if you want to apply different kernels to different channels, split the image into separate\n * color planes using split and process them individually.\n *\n * @param anchor anchor of the kernel that indicates the relative position of a filtered point within\n * the kernel; the anchor should lie within the kernel; default value (-1,-1) means that the anchor is\n * at the kernel center.\n *\n * @param delta optional value added to the filtered pixels before storing them in dst.\n *\n * @param borderType pixel extrapolation method, see BorderTypes\n */\nexport declare function filter2D(src: InputArray, dst: OutputArray, ddepth: int, kernel: InputArray, anchor?: Point, delta?: double, borderType?: int): void;\n/**\n * The function convolves the source image with the specified Gaussian kernel. In-place filtering is\n * supported.\n *\n * [sepFilter2D], [filter2D], [blur], [boxFilter], [bilateralFilter], [medianBlur]\n *\n * @param src input image; the image can have any number of channels, which are processed\n * independently, but the depth should be CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.\n *\n * @param dst output image of the same size and type as src.\n *\n * @param ksize Gaussian kernel size. ksize.width and ksize.height can differ but they both must be\n * positive and odd. Or, they can be zero's and then they are computed from sigma.\n *\n * @param sigmaX Gaussian kernel standard deviation in X direction.\n *\n * @param sigmaY Gaussian kernel standard deviation in Y direction; if sigmaY is zero, it is set to be\n * equal to sigmaX, if both sigmas are zeros, they are computed from ksize.width and ksize.height,\n * respectively (see getGaussianKernel for details); to fully control the result regardless of possible\n * future modifications of all this semantics, it is recommended to specify all of ksize, sigmaX, and\n * sigmaY.\n *\n * @param borderType pixel extrapolation method, see BorderTypes\n */\nexport declare function GaussianBlur(src: InputArray, dst: OutputArray, ksize: Size, sigmaX: double, sigmaY?: double, borderType?: int): void;\n/**\n * The function computes and returns the filter coefficients for spatial image derivatives. When\n * `ksize=FILTER_SCHARR`, the Scharr `$3 \\\\times 3$` kernels are generated (see [Scharr]). Otherwise,\n * Sobel kernels are generated (see [Sobel]). The filters are normally passed to [sepFilter2D] or to\n *\n * @param kx Output matrix of row filter coefficients. It has the type ktype .\n *\n * @param ky Output matrix of column filter coefficients. It has the type ktype .\n *\n * @param dx Derivative order in respect of x.\n *\n * @param dy Derivative order in respect of y.\n *\n * @param ksize Aperture size. It can be FILTER_SCHARR, 1, 3, 5, or 7.\n *\n * @param normalize Flag indicating whether to normalize (scale down) the filter coefficients or not.\n * Theoretically, the coefficients should have the denominator $=2^{ksize*2-dx-dy-2}$. If you are going\n * to filter floating-point images, you are likely to use the normalized kernels. But if you compute\n * derivatives of an 8-bit image, store the results in a 16-bit image, and wish to preserve all the\n * fractional bits, you may want to set normalize=false .\n *\n * @param ktype Type of filter coefficients. It can be CV_32f or CV_64F .\n */\nexport declare function getDerivKernels(kx: OutputArray, ky: OutputArray, dx: int, dy: int, ksize: int, normalize?: bool, ktype?: int): void;\n/**\n * For more details about gabor filter equations and parameters, see: .\n *\n * @param ksize Size of the filter returned.\n *\n * @param sigma Standard deviation of the gaussian envelope.\n *\n * @param theta Orientation of the normal to the parallel stripes of a Gabor function.\n *\n * @param lambd Wavelength of the sinusoidal factor.\n *\n * @param gamma Spatial aspect ratio.\n *\n * @param psi Phase offset.\n *\n * @param ktype Type of filter coefficients. It can be CV_32F or CV_64F .\n */\nexport declare function getGaborKernel(ksize: Size, sigma: double, theta: double, lambd: double, gamma: double, psi?: double, ktype?: int): Mat;\n/**\n * The function computes and returns the `$\\\\texttt{ksize} \\\\times 1$` matrix of Gaussian filter\n * coefficients:\n *\n * `\\\\[G_i= \\\\alpha *e^{-(i-( \\\\texttt{ksize} -1)/2)^2/(2* \\\\texttt{sigma}^2)},\\\\]`\n *\n * where `$i=0..\\\\texttt{ksize}-1$` and `$\\\\alpha$` is the scale factor chosen so that `$\\\\sum_i\n * G_i=1$`.\n *\n * Two of such generated kernels can be passed to sepFilter2D. Those functions automatically recognize\n * smoothing kernels (a symmetrical kernel with sum of weights equal to 1) and handle them accordingly.\n * You may also use the higher-level GaussianBlur.\n *\n * [sepFilter2D], [getDerivKernels], [getStructuringElement], [GaussianBlur]\n *\n * @param ksize Aperture size. It should be odd ( $\\texttt{ksize} \\mod 2 = 1$ ) and positive.\n *\n * @param sigma Gaussian standard deviation. If it is non-positive, it is computed from ksize as sigma\n * = 0.3*((ksize-1)*0.5 - 1) + 0.8.\n *\n * @param ktype Type of filter coefficients. It can be CV_32F or CV_64F .\n */\nexport declare function getGaussianKernel(ksize: int, sigma: double, ktype?: int): Mat;\n/**\n * The function constructs and returns the structuring element that can be further passed to [erode],\n * [dilate] or [morphologyEx]. But you can also construct an arbitrary binary mask yourself and use it\n * as the structuring element.\n *\n * @param shape Element shape that could be one of MorphShapes\n *\n * @param ksize Size of the structuring element.\n *\n * @param anchor Anchor position within the element. The default value $(-1, -1)$ means that the anchor\n * is at the center. Note that only the shape of a cross-shaped element depends on the anchor position.\n * In other cases the anchor just regulates how much the result of the morphological operation is\n * shifted.\n */\nexport declare function getStructuringElement(shape: int, ksize: Size, anchor?: Point): Mat;\n/**\n * The function calculates the Laplacian of the source image by adding up the second x and y\n * derivatives calculated using the Sobel operator:\n *\n * `\\\\[\\\\texttt{dst} = \\\\Delta \\\\texttt{src} = \\\\frac{\\\\partial^2 \\\\texttt{src}}{\\\\partial x^2} +\n * \\\\frac{\\\\partial^2 \\\\texttt{src}}{\\\\partial y^2}\\\\]`\n *\n * This is done when `ksize > 1`. When `ksize == 1`, the Laplacian is computed by filtering the image\n * with the following `$3 \\\\times 3$` aperture:\n *\n * `\\\\[\\\\vecthreethree {0}{1}{0}{1}{-4}{1}{0}{1}{0}\\\\]`\n *\n * [Sobel], [Scharr]\n *\n * @param src Source image.\n *\n * @param dst Destination image of the same size and the same number of channels as src .\n *\n * @param ddepth Desired depth of the destination image.\n *\n * @param ksize Aperture size used to compute the second-derivative filters. See getDerivKernels for\n * details. The size must be positive and odd.\n *\n * @param scale Optional scale factor for the computed Laplacian values. By default, no scaling is\n * applied. See getDerivKernels for details.\n *\n * @param delta Optional delta value that is added to the results prior to storing them in dst .\n *\n * @param borderType Pixel extrapolation method, see BorderTypes\n */\nexport declare function Laplacian(src: InputArray, dst: OutputArray, ddepth: int, ksize?: int, scale?: double, delta?: double, borderType?: int): void;\n/**\n * The function smoothes an image using the median filter with the `$\\\\texttt{ksize} \\\\times\n * \\\\texttt{ksize}$` aperture. Each channel of a multi-channel image is processed independently.\n * In-place operation is supported.\n *\n * The median filter uses [BORDER_REPLICATE] internally to cope with border pixels, see [BorderTypes]\n *\n * [bilateralFilter], [blur], [boxFilter], [GaussianBlur]\n *\n * @param src input 1-, 3-, or 4-channel image; when ksize is 3 or 5, the image depth should be CV_8U,\n * CV_16U, or CV_32F, for larger aperture sizes, it can only be CV_8U.\n *\n * @param dst destination array of the same size and type as src.\n *\n * @param ksize aperture linear size; it must be odd and greater than 1, for example: 3, 5, 7 ...\n */\nexport declare function medianBlur(src: InputArray, dst: OutputArray, ksize: int): void;\nexport declare function morphologyDefaultBorderValue(): Scalar;\n/**\n * The function [cv::morphologyEx] can perform advanced morphological transformations using an erosion\n * and dilation as basic operations.\n *\n * Any of the operations can be done in-place. In case of multi-channel images, each channel is\n * processed independently.\n *\n * [dilate], [erode], [getStructuringElement]\n *\n * The number of iterations is the number of times erosion or dilatation operation will be applied. For\n * instance, an opening operation ([MORPH_OPEN]) with two iterations is equivalent to apply\n * successively: erode -> erode -> dilate -> dilate (and not erode -> dilate -> erode -> dilate).\n *\n * @param src Source image. The number of channels can be arbitrary. The depth should be one of CV_8U,\n * CV_16U, CV_16S, CV_32F or CV_64F.\n *\n * @param dst Destination image of the same size and type as source image.\n *\n * @param op Type of a morphological operation, see MorphTypes\n *\n * @param kernel Structuring element. It can be created using getStructuringElement.\n *\n * @param anchor Anchor position with the kernel. Negative values mean that the anchor is at the kernel\n * center.\n *\n * @param iterations Number of times erosion and dilation are applied.\n *\n * @param borderType Pixel extrapolation method, see BorderTypes\n *\n * @param borderValue Border value in case of a constant border. The default value has a special\n * meaning.\n */\nexport declare function morphologyEx(src: InputArray, dst: OutputArray, op: int, kernel: InputArray, anchor?: Point, iterations?: int, borderType?: int, borderValue?: any): void;\n/**\n * By default, size of the output image is computed as `Size((src.cols+1)/2, (src.rows+1)/2)`, but in\n * any case, the following conditions should be satisfied:\n *\n * `\\\\[\\\\begin{array}{l} | \\\\texttt{dstsize.width} *2-src.cols| \\\\leq 2 \\\\\\\\ | \\\\texttt{dstsize.height}\n * *2-src.rows| \\\\leq 2 \\\\end{array}\\\\]`\n *\n * The function performs the downsampling step of the Gaussian pyramid construction. First, it\n * convolves the source image with the kernel:\n *\n * `\\\\[\\\\frac{1}{256} \\\\begin{bmatrix} 1 & 4 & 6 & 4 & 1 \\\\\\\\ 4 & 16 & 24 & 16 & 4 \\\\\\\\ 6 & 24 & 36 &\n * 24 & 6 \\\\\\\\ 4 & 16 & 24 & 16 & 4 \\\\\\\\ 1 & 4 & 6 & 4 & 1 \\\\end{bmatrix}\\\\]`\n *\n * Then, it downsamples the image by rejecting even rows and columns.\n *\n * @param src input image.\n *\n * @param dst output image; it has the specified size and the same type as src.\n *\n * @param dstsize size of the output image.\n *\n * @param borderType Pixel extrapolation method, see BorderTypes (BORDER_CONSTANT isn't supported)\n */\nexport declare function pyrDown(src: InputArray, dst: OutputArray, dstsize?: any, borderType?: int): void;\n/**\n * The function implements the filtering stage of meanshift segmentation, that is, the output of the\n * function is the filtered \"posterized\" image with color gradients and fine-grain texture flattened.\n * At every pixel (X,Y) of the input image (or down-sized input image, see below) the function executes\n * meanshift iterations, that is, the pixel (X,Y) neighborhood in the joint space-color hyperspace is\n * considered:\n *\n * `\\\\[(x,y): X- \\\\texttt{sp} \\\\le x \\\\le X+ \\\\texttt{sp} , Y- \\\\texttt{sp} \\\\le y \\\\le Y+ \\\\texttt{sp}\n * , ||(R,G,B)-(r,g,b)|| \\\\le \\\\texttt{sr}\\\\]`\n *\n * where (R,G,B) and (r,g,b) are the vectors of color components at (X,Y) and (x,y), respectively\n * (though, the algorithm does not depend on the color space used, so any 3-component color space can\n * be used instead). Over the neighborhood the average spatial value (X',Y') and average color vector\n * (R',G',B') are found and they act as the neighborhood center on the next iteration:\n *\n * `\\\\[(X,Y)~(X',Y'), (R,G,B)~(R',G',B').\\\\]`\n *\n * After the iterations over, the color components of the initial pixel (that is, the pixel from where\n * the iterations started) are set to the final value (average color at the last iteration):\n *\n * `\\\\[I(X,Y) <- (R*,G*,B*)\\\\]`\n *\n * When maxLevel > 0, the gaussian pyramid of maxLevel+1 levels is built, and the above procedure is\n * run on the smallest layer first. After that, the results are propagated to the larger layer and the\n * iterations are run again only on those pixels where the layer colors differ by more than sr from the\n * lower-resolution layer of the pyramid. That makes boundaries of color regions sharper. Note that the\n * results will be actually different from the ones obtained by running the meanshift procedure on the\n * whole original image (i.e. when maxLevel==0).\n *\n * @param src The source 8-bit, 3-channel image.\n *\n * @param dst The destination image of the same format and the same size as the source.\n *\n * @param sp The spatial window radius.\n *\n * @param sr The color window radius.\n *\n * @param maxLevel Maximum level of the pyramid for the segmentation.\n *\n * @param termcrit Termination criteria: when to stop meanshift iterations.\n */\nexport declare function pyrMeanShiftFiltering(src: InputArray, dst: OutputArray, sp: double, sr: double, maxLevel?: int, termcrit?: TermCriteria): void;\n/**\n * By default, size of the output image is computed as `Size(src.cols\\\\*2, (src.rows\\\\*2)`, but in any\n * case, the following conditions should be satisfied:\n *\n * `\\\\[\\\\begin{array}{l} | \\\\texttt{dstsize.width} -src.cols*2| \\\\leq ( \\\\texttt{dstsize.width} \\\\mod\n * 2) \\\\\\\\ | \\\\texttt{dstsize.height} -src.rows*2| \\\\leq ( \\\\texttt{dstsize.height} \\\\mod 2)\n * \\\\end{array}\\\\]`\n *\n * The function performs the upsampling step of the Gaussian pyramid construction, though it can\n * actually be used to construct the Laplacian pyramid. First, it upsamples the source image by\n * injecting even zero rows and columns and then convolves the result with the same kernel as in\n * pyrDown multiplied by 4.\n *\n * @param src input image.\n *\n * @param dst output image. It has the specified size and the same type as src .\n *\n * @param dstsize size of the output image.\n *\n * @param borderType Pixel extrapolation method, see BorderTypes (only BORDER_DEFAULT is supported)\n */\nexport declare function pyrUp(src: InputArray, dst: OutputArray, dstsize?: any, borderType?: int): void;\n/**\n * The function computes the first x- or y- spatial image derivative using the Scharr operator. The\n * call\n *\n * `\\\\[\\\\texttt{Scharr(src, dst, ddepth, dx, dy, scale, delta, borderType)}\\\\]`\n *\n * is equivalent to\n *\n * `\\\\[\\\\texttt{Sobel(src, dst, ddepth, dx, dy, FILTER_SCHARR, scale, delta, borderType)} .\\\\]`\n *\n * [cartToPolar]\n *\n * @param src input image.\n *\n * @param dst output image of the same size and the same number of channels as src.\n *\n * @param ddepth output image depth, see combinations\n *\n * @param dx order of the derivative x.\n *\n * @param dy order of the derivative y.\n *\n * @param scale optional scale factor for the computed derivative values; by default, no scaling is\n * applied (see getDerivKernels for details).\n *\n * @param delta optional delta value that is added to the results prior to storing them in dst.\n *\n * @param borderType pixel extrapolation method, see BorderTypes\n */\nexport declare function Scharr(src: InputArray, dst: OutputArray, ddepth: int, dx: int, dy: int, scale?: double, delta?: double, borderType?: int): void;\n/**\n * The function applies a separable linear filter to the image. That is, first, every row of src is\n * filtered with the 1D kernel kernelX. Then, every column of the result is filtered with the 1D kernel\n * kernelY. The final result shifted by delta is stored in dst .\n *\n * [filter2D], [Sobel], [GaussianBlur], [boxFilter], [blur]\n *\n * @param src Source image.\n *\n * @param dst Destination image of the same size and the same number of channels as src .\n *\n * @param ddepth Destination image depth, see combinations\n *\n * @param kernelX Coefficients for filtering each row.\n *\n * @param kernelY Coefficients for filtering each column.\n *\n * @param anchor Anchor position within the kernel. The default value $(-1,-1)$ means that the anchor\n * is at the kernel center.\n *\n * @param delta Value added to the filtered results before storing them.\n *\n * @param borderType Pixel extrapolation method, see BorderTypes\n */\nexport declare function sepFilter2D(src: InputArray, dst: OutputArray, ddepth: int, kernelX: InputArray, kernelY: InputArray, anchor?: Point, delta?: double, borderType?: int): void;\n/**\n * In all cases except one, the `$\\\\texttt{ksize} \\\\times \\\\texttt{ksize}$` separable kernel is used to\n * calculate the derivative. When `$\\\\texttt{ksize = 1}$`, the `$3 \\\\times 1$` or `$1 \\\\times 3$`\n * kernel is used (that is, no Gaussian smoothing is done). `ksize = 1` can only be used for the first\n * or the second x- or y- derivatives.\n *\n * There is also the special value `ksize = [FILTER_SCHARR] (-1)` that corresponds to the `$3\\\\times3$`\n * Scharr filter that may give more accurate results than the `$3\\\\times3$` Sobel. The Scharr aperture\n * is\n *\n * `\\\\[\\\\vecthreethree{-3}{0}{3}{-10}{0}{10}{-3}{0}{3}\\\\]`\n *\n * for the x-derivative, or transposed for the y-derivative.\n *\n * The function calculates an image derivative by convolving the image with the appropriate kernel:\n *\n * `\\\\[\\\\texttt{dst} = \\\\frac{\\\\partial^{xorder+yorder} \\\\texttt{src}}{\\\\partial x^{xorder} \\\\partial\n * y^{yorder}}\\\\]`\n *\n * The Sobel operators combine Gaussian smoothing and differentiation, so the result is more or less\n * resistant to the noise. Most often, the function is called with ( xorder = 1, yorder = 0, ksize = 3)\n * or ( xorder = 0, yorder = 1, ksize = 3) to calculate the first x- or y- image derivative. The first\n * case corresponds to a kernel of:\n *\n * `\\\\[\\\\vecthreethree{-1}{0}{1}{-2}{0}{2}{-1}{0}{1}\\\\]`\n *\n * The second case corresponds to a kernel of:\n *\n * `\\\\[\\\\vecthreethree{-1}{-2}{-1}{0}{0}{0}{1}{2}{1}\\\\]`\n *\n * [Scharr], [Laplacian], [sepFilter2D], [filter2D], [GaussianBlur], [cartToPolar]\n *\n * @param src input image.\n *\n * @param dst output image of the same size and the same number of channels as src .\n *\n * @param ddepth output image depth, see combinations; in the case of 8-bit input images it will result\n * in truncated derivatives.\n *\n * @param dx order of the derivative x.\n *\n * @param dy order of the derivative y.\n *\n * @param ksize size of the extended Sobel kernel; it must be 1, 3, 5, or 7.\n *\n * @param scale optional scale factor for the computed derivative values; by default, no scaling is\n * applied (see getDerivKernels for details).\n *\n * @param delta optional delta value that is added to the results prior to storing them in dst.\n *\n * @param borderType pixel extrapolation method, see BorderTypes\n */\nexport declare function Sobel(src: InputArray, dst: OutputArray, ddepth: int, dx: int, dy: int, ksize?: int, scale?: double, delta?: double, borderType?: int): void;\n/**\n * Equivalent to calling:\n *\n * ```cpp\n * Sobel( src, dx, CV_16SC1, 1, 0, 3 );\n * Sobel( src, dy, CV_16SC1, 0, 1, 3 );\n * ```\n *\n * [Sobel]\n *\n * @param src input image.\n *\n * @param dx output image with first-order derivative in x.\n *\n * @param dy output image with first-order derivative in y.\n *\n * @param ksize size of Sobel kernel. It must be 3.\n *\n * @param borderType pixel extrapolation method, see BorderTypes\n */\nexport declare function spatialGradient(src: InputArray, dx: OutputArray, dy: OutputArray, ksize?: int, borderType?: int): void;\n/**\n * For every pixel `$ (x, y) $` in the source image, the function calculates the sum of squares of\n * those neighboring pixel values which overlap the filter placed over the pixel `$ (x, y) $`.\n *\n * The unnormalized square box filter can be useful in computing local image statistics such as the the\n * local variance and standard deviation around the neighborhood of a pixel.\n *\n * [boxFilter]\n *\n * @param src input image\n *\n * @param dst output image of the same size and type as _src\n *\n * @param ddepth the output image depth (-1 to use src.depth())\n *\n * @param ksize kernel size\n *\n * @param anchor kernel anchor point. The default value of Point(-1, -1) denotes that the anchor is at\n * the kernel center.\n *\n * @param normalize flag, specifying whether the kernel is to be normalized by it's area or not.\n *\n * @param borderType border mode used to extrapolate pixels outside of the image, see BorderTypes\n */\nexport declare function sqrBoxFilter(src: InputArray, dst: OutputArray, ddepth: int, ksize: Size, anchor?: Point, normalize?: bool, borderType?: int): void;\nexport declare const MORPH_RECT: MorphShapes;\n/**\n * a cross-shaped structuring element: `\\\\[E_{ij} = \\\\fork{1}{if i=\\\\texttt{anchor.y} or\n * j=\\\\texttt{anchor.x}}{0}{otherwise}\\\\]`\n *\n */\nexport declare const MORPH_CROSS: MorphShapes;\n/**\n * an elliptic structuring element, that is, a filled ellipse inscribed into the rectangle Rect(0, 0,\n * esize.width, 0.esize.height)\n *\n */\nexport declare const MORPH_ELLIPSE: MorphShapes;\nexport declare const MORPH_ERODE: MorphTypes;\nexport declare const MORPH_DILATE: MorphTypes;\n/**\n * an opening operation `\\\\[\\\\texttt{dst} = \\\\mathrm{open} ( \\\\texttt{src} , \\\\texttt{element} )=\n * \\\\mathrm{dilate} ( \\\\mathrm{erode} ( \\\\texttt{src} , \\\\texttt{element} ))\\\\]`\n *\n */\nexport declare const MORPH_OPEN: MorphTypes;\n/**\n * a closing operation `\\\\[\\\\texttt{dst} = \\\\mathrm{close} ( \\\\texttt{src} , \\\\texttt{element} )=\n * \\\\mathrm{erode} ( \\\\mathrm{dilate} ( \\\\texttt{src} , \\\\texttt{element} ))\\\\]`\n *\n */\nexport declare const MORPH_CLOSE: MorphTypes;\n/**\n * a morphological gradient `\\\\[\\\\texttt{dst} = \\\\mathrm{morph\\\\_grad} ( \\\\texttt{src} ,\n * \\\\texttt{element} )= \\\\mathrm{dilate} ( \\\\texttt{src} , \\\\texttt{element} )- \\\\mathrm{erode} (\n * \\\\texttt{src} , \\\\texttt{element} )\\\\]`\n *\n */\nexport declare const MORPH_GRADIENT: MorphTypes;\n/**\n * \"top hat\" `\\\\[\\\\texttt{dst} = \\\\mathrm{tophat} ( \\\\texttt{src} , \\\\texttt{element} )= \\\\texttt{src}\n * - \\\\mathrm{open} ( \\\\texttt{src} , \\\\texttt{element} )\\\\]`\n *\n */\nexport declare const MORPH_TOPHAT: MorphTypes;\n/**\n * \"black hat\" `\\\\[\\\\texttt{dst} = \\\\mathrm{blackhat} ( \\\\texttt{src} , \\\\texttt{element} )=\n * \\\\mathrm{close} ( \\\\texttt{src} , \\\\texttt{element} )- \\\\texttt{src}\\\\]`\n *\n */\nexport declare const MORPH_BLACKHAT: MorphTypes;\n/**\n * \"hit or miss\" .- Only supported for CV_8UC1 binary images. A tutorial can be found in the\n * documentation\n *\n */\nexport declare const MORPH_HITMISS: MorphTypes;\nexport declare const FILTER_SCHARR: SpecialFilter;\nexport declare type MorphShapes = any;\nexport declare type MorphTypes = any;\nexport declare type SpecialFilter = any;\n"},"node_modules_mirada_dist_src_types_opencv_imgproc_hist_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_imgproc_hist_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/imgproc_hist.d.ts","content":"import { bool, double, float, InputArray, InputArrayOfArrays, int, OutputArray, Size } from './_types';\n/**\n * The function [cv::calcBackProject] calculates the back project of the histogram. That is, similarly\n * to [calcHist] , at each location (x, y) the function collects the values from the selected channels\n * in the input images and finds the corresponding histogram bin. But instead of incrementing it, the\n * function reads the bin value, scales it by scale , and stores in backProject(x,y) . In terms of\n * statistics, the function computes probability of each element value in respect with the empirical\n * probability distribution represented by the histogram. See how, for example, you can find and track\n * a bright-colored object in a scene:\n *\n * Before tracking, show the object to the camera so that it covers almost the whole frame. Calculate a\n * hue histogram. The histogram may have strong maximums, corresponding to the dominant colors in the\n * object.\n * When tracking, calculate a back projection of a hue plane of each input video frame using that\n * pre-computed histogram. Threshold the back projection to suppress weak colors. It may also make\n * sense to suppress pixels with non-sufficient color saturation and too dark or too bright pixels.\n * Find connected components in the resulting picture and choose, for example, the largest component.\n *\n * This is an approximate algorithm of the CamShift color object tracker.\n *\n * [calcHist], [compareHist]\n *\n * @param images Source arrays. They all should have the same depth, CV_8U, CV_16U or CV_32F , and the\n * same size. Each of them can have an arbitrary number of channels.\n *\n * @param nimages Number of source images.\n *\n * @param channels The list of channels used to compute the back projection. The number of channels\n * must match the histogram dimensionality. The first array channels are numerated from 0 to\n * images[0].channels()-1 , the second array channels are counted from images[0].channels() to\n * images[0].channels() + images[1].channels()-1, and so on.\n *\n * @param hist Input histogram that can be dense or sparse.\n *\n * @param backProject Destination back projection array that is a single-channel array of the same size\n * and depth as images[0] .\n *\n * @param ranges Array of arrays of the histogram bin boundaries in each dimension. See calcHist .\n *\n * @param scale Optional scale factor for the output back projection.\n *\n * @param uniform Flag indicating whether the histogram is uniform or not (see above).\n */\nexport declare function calcBackProject(images: any, nimages: int, channels: any, hist: InputArray, backProject: OutputArray, ranges: any, scale?: double, uniform?: bool): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function calcBackProject(images: any, nimages: int, channels: any, hist: any, backProject: OutputArray, ranges: any, scale?: double, uniform?: bool): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function calcBackProject(images: InputArrayOfArrays, channels: any, hist: InputArray, dst: OutputArray, ranges: any, scale: double): void;\n/**\n * The function [cv::calcHist] calculates the histogram of one or more arrays. The elements of a tuple\n * used to increment a histogram bin are taken from the corresponding input arrays at the same\n * location. The sample below shows how to compute a 2D Hue-Saturation histogram for a color image. :\n *\n * ```cpp\n * #include <opencv2/imgproc.hpp>\n * #include <opencv2/highgui.hpp>\n *\n * using namespace cv;\n *\n * int main( int argc, char** argv )\n * {\n *     Mat src, hsv;\n *     if( argc != 2 || !(src=imread(argv[1], 1)).data )\n *         return -1;\n *\n *     cvtColor(src, hsv, COLOR_BGR2HSV);\n *\n *     // Quantize the hue to 30 levels\n *     // and the saturation to 32 levels\n *     int hbins = 30, sbins = 32;\n *     int histSize[] = {hbins, sbins};\n *     // hue varies from 0 to 179, see cvtColor\n *     float hranges[] = { 0, 180 };\n *     // saturation varies from 0 (black-gray-white) to\n *     // 255 (pure spectrum color)\n *     float sranges[] = { 0, 256 };\n *     const float* ranges[] = { hranges, sranges };\n *     MatND hist;\n *     // we compute the histogram from the 0-th and 1-st channels\n *     int channels[] = {0, 1};\n *\n *     calcHist( &hsv, 1, channels, Mat(), // do not use mask\n *              hist, 2, histSize, ranges,\n *              true, // the histogram is uniform\n *              false );\n *     double maxVal=0;\n *     minMaxLoc(hist, 0, &maxVal, 0, 0);\n *\n *     int scale = 10;\n *     Mat histImg = Mat::zeros(sbins*scale, hbins*10, CV_8UC3);\n *\n *     for( int h = 0; h < hbins; h++ )\n *         for( int s = 0; s < sbins; s++ )\n *         {\n *             float binVal = hist.at<float>(h, s);\n *             int intensity = cvRound(binVal*255/maxVal);\n *             rectangle( histImg, Point(h*scale, s*scale),\n *                         Point( (h+1)*scale - 1, (s+1)*scale - 1),\n *                         Scalar::all(intensity),\n *                         -1 );\n *         }\n *\n *     namedWindow( \"Source\", 1 );\n *     imshow( \"Source\", src );\n *\n *     namedWindow( \"H-S Histogram\", 1 );\n *     imshow( \"H-S Histogram\", histImg );\n *     waitKey();\n * }\n * ```\n *\n * @param images Source arrays. They all should have the same depth, CV_8U, CV_16U or CV_32F , and the\n * same size. Each of them can have an arbitrary number of channels.\n *\n * @param nimages Number of source images.\n *\n * @param channels List of the dims channels used to compute the histogram. The first array channels\n * are numerated from 0 to images[0].channels()-1 , the second array channels are counted from\n * images[0].channels() to images[0].channels() + images[1].channels()-1, and so on.\n *\n * @param mask Optional mask. If the matrix is not empty, it must be an 8-bit array of the same size as\n * images[i] . The non-zero mask elements mark the array elements counted in the histogram.\n *\n * @param hist Output histogram, which is a dense or sparse dims -dimensional array.\n *\n * @param dims Histogram dimensionality that must be positive and not greater than CV_MAX_DIMS (equal\n * to 32 in the current OpenCV version).\n *\n * @param histSize Array of histogram sizes in each dimension.\n *\n * @param ranges Array of the dims arrays of the histogram bin boundaries in each dimension. When the\n * histogram is uniform ( uniform =true), then for each dimension i it is enough to specify the lower\n * (inclusive) boundary $L_0$ of the 0-th histogram bin and the upper (exclusive) boundary\n * $U_{\\texttt{histSize}[i]-1}$ for the last histogram bin histSize[i]-1 . That is, in case of a\n * uniform histogram each of ranges[i] is an array of 2 elements. When the histogram is not uniform (\n * uniform=false ), then each of ranges[i] contains histSize[i]+1 elements: $L_0, U_0=L_1, U_1=L_2,\n * ..., U_{\\texttt{histSize[i]}-2}=L_{\\texttt{histSize[i]}-1}, U_{\\texttt{histSize[i]}-1}$ . The array\n * elements, that are not between $L_0$ and $U_{\\texttt{histSize[i]}-1}$ , are not counted in the\n * histogram.\n *\n * @param uniform Flag indicating whether the histogram is uniform or not (see above).\n *\n * @param accumulate Accumulation flag. If it is set, the histogram is not cleared in the beginning\n * when it is allocated. This feature enables you to compute a single histogram from several sets of\n * arrays, or to update the histogram in time.\n */\nexport declare function calcHist(images: any, nimages: int, channels: any, mask: InputArray, hist: OutputArray, dims: int, histSize: any, ranges: any, uniform?: bool, accumulate?: bool): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n *\n * this variant uses SparseMat for output\n */\nexport declare function calcHist(images: any, nimages: int, channels: any, mask: InputArray, hist: any, dims: int, histSize: any, ranges: any, uniform?: bool, accumulate?: bool): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function calcHist(images: InputArrayOfArrays, channels: any, mask: InputArray, hist: OutputArray, histSize: any, ranges: any, accumulate?: bool): void;\n/**\n * The function [cv::compareHist] compares two dense or two sparse histograms using the specified\n * method.\n *\n * The function returns `$d(H_1, H_2)$` .\n *\n * While the function works well with 1-, 2-, 3-dimensional dense histograms, it may not be suitable\n * for high-dimensional sparse histograms. In such histograms, because of aliasing and sampling\n * problems, the coordinates of non-zero histogram bins can slightly shift. To compare such histograms\n * or more general sparse configurations of weighted points, consider using the [EMD] function.\n *\n * @param H1 First compared histogram.\n *\n * @param H2 Second compared histogram of the same size as H1 .\n *\n * @param method Comparison method, see HistCompMethods\n */\nexport declare function compareHist(H1: InputArray, H2: InputArray, method: int): double;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function compareHist(H1: any, H2: any, method: int): double;\n/**\n * @param clipLimit Threshold for contrast limiting.\n *\n * @param tileGridSize Size of grid for histogram equalization. Input image will be divided into\n * equally sized rectangular tiles. tileGridSize defines the number of tiles in row and column.\n */\nexport declare function createCLAHE(clipLimit?: double, tileGridSize?: Size): any;\n/**\n * The function computes the earth mover distance and/or a lower boundary of the distance between the\n * two weighted point configurations. One of the applications described in RubnerSept98, Rubner2000 is\n * multi-dimensional histogram comparison for image retrieval. EMD is a transportation problem that is\n * solved using some modification of a simplex algorithm, thus the complexity is exponential in the\n * worst case, though, on average it is much faster. In the case of a real metric the lower boundary\n * can be calculated even faster (using linear-time algorithm) and it can be used to determine roughly\n * whether the two signatures are far enough so that they cannot relate to the same object.\n *\n * @param signature1 First signature, a $\\texttt{size1}\\times \\texttt{dims}+1$ floating-point matrix.\n * Each row stores the point weight followed by the point coordinates. The matrix is allowed to have a\n * single column (weights only) if the user-defined cost matrix is used. The weights must be\n * non-negative and have at least one non-zero value.\n *\n * @param signature2 Second signature of the same format as signature1 , though the number of rows may\n * be different. The total weights may be different. In this case an extra \"dummy\" point is added to\n * either signature1 or signature2. The weights must be non-negative and have at least one non-zero\n * value.\n *\n * @param distType Used metric. See DistanceTypes.\n *\n * @param cost User-defined $\\texttt{size1}\\times \\texttt{size2}$ cost matrix. Also, if a cost matrix\n * is used, lower boundary lowerBound cannot be calculated because it needs a metric function.\n *\n * @param lowerBound Optional input/output parameter: lower boundary of a distance between the two\n * signatures that is a distance between mass centers. The lower boundary may not be calculated if the\n * user-defined cost matrix is used, the total weights of point configurations are not equal, or if the\n * signatures consist of weights only (the signature matrices have a single column). You must**\n * initialize *lowerBound . If the calculated distance between mass centers is greater or equal to\n * *lowerBound (it means that the signatures are far enough), the function does not calculate EMD. In\n * any case *lowerBound is set to the calculated distance between mass centers on return. Thus, if you\n * want to calculate both distance between mass centers and EMD, *lowerBound should be set to 0.\n *\n * @param flow Resultant $\\texttt{size1} \\times \\texttt{size2}$ flow matrix: $\\texttt{flow}_{i,j}$ is a\n * flow from $i$ -th point of signature1 to $j$ -th point of signature2 .\n */\nexport declare function EMD(signature1: InputArray, signature2: InputArray, distType: int, cost?: InputArray, lowerBound?: any, flow?: OutputArray): float;\n/**\n * The function equalizes the histogram of the input image using the following algorithm:\n *\n * Calculate the histogram `$H$` for src .\n * Normalize the histogram so that the sum of histogram bins is 255.\n * Compute the integral of the histogram: `\\\\[H'_i = \\\\sum _{0 \\\\le j < i} H(j)\\\\]`\n * Transform the image using `$H'$` as a look-up table: `$\\\\texttt{dst}(x,y) = H'(\\\\texttt{src}(x,y))$`\n *\n * The algorithm normalizes the brightness and increases the contrast of the image.\n *\n * @param src Source 8-bit single channel image.\n *\n * @param dst Destination image of the same size and type as src .\n */\nexport declare function equalizeHist(src: InputArray, dst: OutputArray): void;\nexport declare function wrapperEMD(signature1: InputArray, signature2: InputArray, distType: int, cost?: InputArray, lowerBound?: any, flow?: OutputArray): float;\n/**\n * Correlation `\\\\[d(H_1,H_2) = \\\\frac{\\\\sum_I (H_1(I) - \\\\bar{H_1}) (H_2(I) -\n * \\\\bar{H_2})}{\\\\sqrt{\\\\sum_I(H_1(I) - \\\\bar{H_1})^2 \\\\sum_I(H_2(I) - \\\\bar{H_2})^2}}\\\\]` where\n * `\\\\[\\\\bar{H_k} = \\\\frac{1}{N} \\\\sum _J H_k(J)\\\\]` and `$N$` is a total number of histogram bins.\n *\n */\nexport declare const HISTCMP_CORREL: HistCompMethods;\n/**\n * Chi-Square `\\\\[d(H_1,H_2) = \\\\sum _I \\\\frac{\\\\left(H_1(I)-H_2(I)\\\\right)^2}{H_1(I)}\\\\]`\n *\n */\nexport declare const HISTCMP_CHISQR: HistCompMethods;\n/**\n * Intersection `\\\\[d(H_1,H_2) = \\\\sum _I \\\\min (H_1(I), H_2(I))\\\\]`\n *\n */\nexport declare const HISTCMP_INTERSECT: HistCompMethods;\n/**\n * Bhattacharyya distance (In fact, OpenCV computes Hellinger distance, which is related to\n * Bhattacharyya coefficient.) `\\\\[d(H_1,H_2) = \\\\sqrt{1 - \\\\frac{1}{\\\\sqrt{\\\\bar{H_1} \\\\bar{H_2} N^2}}\n * \\\\sum_I \\\\sqrt{H_1(I) \\\\cdot H_2(I)}}\\\\]`\n *\n */\nexport declare const HISTCMP_BHATTACHARYYA: HistCompMethods;\nexport declare const HISTCMP_HELLINGER: HistCompMethods;\n/**\n * Alternative Chi-Square `\\\\[d(H_1,H_2) = 2 * \\\\sum _I\n * \\\\frac{\\\\left(H_1(I)-H_2(I)\\\\right)^2}{H_1(I)+H_2(I)}\\\\]` This alternative formula is regularly used\n * for texture comparison. See e.g. Puzicha1997\n *\n */\nexport declare const HISTCMP_CHISQR_ALT: HistCompMethods;\n/**\n * Kullback-Leibler divergence `\\\\[d(H_1,H_2) = \\\\sum _I H_1(I) \\\\log\n * \\\\left(\\\\frac{H_1(I)}{H_2(I)}\\\\right)\\\\]`\n *\n */\nexport declare const HISTCMP_KL_DIV: HistCompMethods;\n/**\n * Histogram comparison methods\n *\n */\nexport declare type HistCompMethods = any;\n"},"node_modules_mirada_dist_src_types_opencv_imgproc_misc_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_imgproc_misc_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/imgproc_misc.d.ts","content":"import { double, InputArray, InputOutputArray, int, OutputArray, Point, Rect, Scalar } from './_types';\n/**\n * The function transforms a grayscale image to a binary image according to the formulae:\n *\n * **THRESH_BINARY** `\\\\[dst(x,y) = \\\\fork{\\\\texttt{maxValue}}{if \\\\(src(x,y) >\n * T(x,y)\\\\)}{0}{otherwise}\\\\]`\n * **THRESH_BINARY_INV** `\\\\[dst(x,y) = \\\\fork{0}{if \\\\(src(x,y) >\n * T(x,y)\\\\)}{\\\\texttt{maxValue}}{otherwise}\\\\]` where `$T(x,y)$` is a threshold calculated\n * individually for each pixel (see adaptiveMethod parameter).\n *\n * The function can process the image in-place.\n *\n * [threshold], [blur], [GaussianBlur]\n *\n * @param src Source 8-bit single-channel image.\n *\n * @param dst Destination image of the same size and the same type as src.\n *\n * @param maxValue Non-zero value assigned to the pixels for which the condition is satisfied\n *\n * @param adaptiveMethod Adaptive thresholding algorithm to use, see AdaptiveThresholdTypes. The\n * BORDER_REPLICATE | BORDER_ISOLATED is used to process boundaries.\n *\n * @param thresholdType Thresholding type that must be either THRESH_BINARY or THRESH_BINARY_INV, see\n * ThresholdTypes.\n *\n * @param blockSize Size of a pixel neighborhood that is used to calculate a threshold value for the\n * pixel: 3, 5, 7, and so on.\n *\n * @param C Constant subtracted from the mean or weighted mean (see the details below). Normally, it is\n * positive but may be zero or negative as well.\n */\nexport declare function adaptiveThreshold(src: InputArray, dst: OutputArray, maxValue: double, adaptiveMethod: int, thresholdType: int, blockSize: int, C: double): void;\n/**\n * Performs linear blending of two images: `\\\\[ \\\\texttt{dst}(i,j) =\n * \\\\texttt{weights1}(i,j)*\\\\texttt{src1}(i,j) + \\\\texttt{weights2}(i,j)*\\\\texttt{src2}(i,j) \\\\]`\n *\n * @param src1 It has a type of CV_8UC(n) or CV_32FC(n), where n is a positive integer.\n *\n * @param src2 It has the same type and size as src1.\n *\n * @param weights1 It has a type of CV_32FC1 and the same size with src1.\n *\n * @param weights2 It has a type of CV_32FC1 and the same size with src1.\n *\n * @param dst It is created if it does not have the same size and type with src1.\n */\nexport declare function blendLinear(src1: InputArray, src2: InputArray, weights1: InputArray, weights2: InputArray, dst: OutputArray): void;\n/**\n * The function [cv::distanceTransform] calculates the approximate or precise distance from every\n * binary image pixel to the nearest zero pixel. For zero image pixels, the distance will obviously be\n * zero.\n *\n * When maskSize == [DIST_MASK_PRECISE] and distanceType == [DIST_L2] , the function runs the algorithm\n * described in Felzenszwalb04 . This algorithm is parallelized with the TBB library.\n *\n * In other cases, the algorithm Borgefors86 is used. This means that for a pixel the function finds\n * the shortest path to the nearest zero pixel consisting of basic shifts: horizontal, vertical,\n * diagonal, or knight's move (the latest is available for a `$5\\\\times 5$` mask). The overall distance\n * is calculated as a sum of these basic distances. Since the distance function should be symmetric,\n * all of the horizontal and vertical shifts must have the same cost (denoted as a ), all the diagonal\n * shifts must have the same cost (denoted as `b`), and all knight's moves must have the same cost\n * (denoted as `c`). For the [DIST_C] and [DIST_L1] types, the distance is calculated precisely,\n * whereas for [DIST_L2] (Euclidean distance) the distance can be calculated only with a relative error\n * (a `$5\\\\times 5$` mask gives more accurate results). For `a`,`b`, and `c`, OpenCV uses the values\n * suggested in the original paper:\n *\n * DIST_L1: `a = 1, b = 2`\n * DIST_L2:\n *\n * `3 x 3`: `a=0.955, b=1.3693`\n * `5 x 5`: `a=1, b=1.4, c=2.1969`\n *\n * DIST_C: `a = 1, b = 1`\n *\n * Typically, for a fast, coarse distance estimation [DIST_L2], a `$3\\\\times 3$` mask is used. For a\n * more accurate distance estimation [DIST_L2], a `$5\\\\times 5$` mask or the precise algorithm is used.\n * Note that both the precise and the approximate algorithms are linear on the number of pixels.\n *\n * This variant of the function does not only compute the minimum distance for each pixel `$(x, y)$`\n * but also identifies the nearest connected component consisting of zero pixels\n * (labelType==[DIST_LABEL_CCOMP]) or the nearest zero pixel (labelType==[DIST_LABEL_PIXEL]). Index of\n * the component/pixel is stored in `labels(x, y)`. When labelType==[DIST_LABEL_CCOMP], the function\n * automatically finds connected components of zero pixels in the input image and marks them with\n * distinct labels. When labelType==[DIST_LABEL_CCOMP], the function scans through the input image and\n * marks all the zero pixels with distinct labels.\n *\n * In this mode, the complexity is still linear. That is, the function provides a very fast way to\n * compute the Voronoi diagram for a binary image. Currently, the second variant can use only the\n * approximate distance transform algorithm, i.e. maskSize=[DIST_MASK_PRECISE] is not supported yet.\n *\n * @param src 8-bit, single-channel (binary) source image.\n *\n * @param dst Output image with calculated distances. It is a 8-bit or 32-bit floating-point,\n * single-channel image of the same size as src.\n *\n * @param labels Output 2D array of labels (the discrete Voronoi diagram). It has the type CV_32SC1 and\n * the same size as src.\n *\n * @param distanceType Type of distance, see DistanceTypes\n *\n * @param maskSize Size of the distance transform mask, see DistanceTransformMasks. DIST_MASK_PRECISE\n * is not supported by this variant. In case of the DIST_L1 or DIST_C distance type, the parameter is\n * forced to 3 because a $3\\times 3$ mask gives the same result as $5\\times 5$ or any larger aperture.\n *\n * @param labelType Type of the label array to build, see DistanceTransformLabelTypes.\n */\nexport declare function distanceTransform(src: InputArray, dst: OutputArray, labels: OutputArray, distanceType: int, maskSize: int, labelType?: int): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n *\n * @param src 8-bit, single-channel (binary) source image.\n *\n * @param dst Output image with calculated distances. It is a 8-bit or 32-bit floating-point,\n * single-channel image of the same size as src .\n *\n * @param distanceType Type of distance, see DistanceTypes\n *\n * @param maskSize Size of the distance transform mask, see DistanceTransformMasks. In case of the\n * DIST_L1 or DIST_C distance type, the parameter is forced to 3 because a $3\\times 3$ mask gives the\n * same result as $5\\times 5$ or any larger aperture.\n *\n * @param dstType Type of output image. It can be CV_8U or CV_32F. Type CV_8U can be used only for the\n * first variant of the function and distanceType == DIST_L1.\n */\nexport declare function distanceTransform(src: InputArray, dst: OutputArray, distanceType: int, maskSize: int, dstType?: int): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n *\n * variant without `mask` parameter\n */\nexport declare function floodFill(image: InputOutputArray, seedPoint: Point, newVal: Scalar, rect?: any, loDiff?: Scalar, upDiff?: Scalar, flags?: int): int;\n/**\n * The function [cv::floodFill] fills a connected component starting from the seed point with the\n * specified color. The connectivity is determined by the color/brightness closeness of the neighbor\n * pixels. The pixel at `$(x,y)$` is considered to belong to the repainted domain if:\n *\n * in case of a grayscale image and floating range `\\\\[\\\\texttt{src} (x',y')- \\\\texttt{loDiff} \\\\leq\n * \\\\texttt{src} (x,y) \\\\leq \\\\texttt{src} (x',y')+ \\\\texttt{upDiff}\\\\]`\n * in case of a grayscale image and fixed range `\\\\[\\\\texttt{src} ( \\\\texttt{seedPoint} .x,\n * \\\\texttt{seedPoint} .y)- \\\\texttt{loDiff} \\\\leq \\\\texttt{src} (x,y) \\\\leq \\\\texttt{src} (\n * \\\\texttt{seedPoint} .x, \\\\texttt{seedPoint} .y)+ \\\\texttt{upDiff}\\\\]`\n * in case of a color image and floating range `\\\\[\\\\texttt{src} (x',y')_r- \\\\texttt{loDiff} _r \\\\leq\n * \\\\texttt{src} (x,y)_r \\\\leq \\\\texttt{src} (x',y')_r+ \\\\texttt{upDiff} _r,\\\\]` `\\\\[\\\\texttt{src}\n * (x',y')_g- \\\\texttt{loDiff} _g \\\\leq \\\\texttt{src} (x,y)_g \\\\leq \\\\texttt{src} (x',y')_g+\n * \\\\texttt{upDiff} _g\\\\]` and `\\\\[\\\\texttt{src} (x',y')_b- \\\\texttt{loDiff} _b \\\\leq \\\\texttt{src}\n * (x,y)_b \\\\leq \\\\texttt{src} (x',y')_b+ \\\\texttt{upDiff} _b\\\\]`\n * in case of a color image and fixed range `\\\\[\\\\texttt{src} ( \\\\texttt{seedPoint} .x,\n * \\\\texttt{seedPoint} .y)_r- \\\\texttt{loDiff} _r \\\\leq \\\\texttt{src} (x,y)_r \\\\leq \\\\texttt{src} (\n * \\\\texttt{seedPoint} .x, \\\\texttt{seedPoint} .y)_r+ \\\\texttt{upDiff} _r,\\\\]` `\\\\[\\\\texttt{src} (\n * \\\\texttt{seedPoint} .x, \\\\texttt{seedPoint} .y)_g- \\\\texttt{loDiff} _g \\\\leq \\\\texttt{src} (x,y)_g\n * \\\\leq \\\\texttt{src} ( \\\\texttt{seedPoint} .x, \\\\texttt{seedPoint} .y)_g+ \\\\texttt{upDiff} _g\\\\]` and\n * `\\\\[\\\\texttt{src} ( \\\\texttt{seedPoint} .x, \\\\texttt{seedPoint} .y)_b- \\\\texttt{loDiff} _b \\\\leq\n * \\\\texttt{src} (x,y)_b \\\\leq \\\\texttt{src} ( \\\\texttt{seedPoint} .x, \\\\texttt{seedPoint} .y)_b+\n * \\\\texttt{upDiff} _b\\\\]`\n *\n * where `$src(x',y')$` is the value of one of pixel neighbors that is already known to belong to the\n * component. That is, to be added to the connected component, a color/brightness of the pixel should\n * be close enough to:\n *\n * Color/brightness of one of its neighbors that already belong to the connected component in case of a\n * floating range.\n * Color/brightness of the seed point in case of a fixed range.\n *\n * Use these functions to either mark a connected component with the specified color in-place, or build\n * a mask and then extract the contour, or copy the region to another image, and so on.\n *\n * Since the mask is larger than the filled image, a pixel `$(x, y)$` in image corresponds to the pixel\n * `$(x+1, y+1)$` in the mask .\n *\n * [findContours]\n *\n * @param image Input/output 1- or 3-channel, 8-bit, or floating-point image. It is modified by the\n * function unless the FLOODFILL_MASK_ONLY flag is set in the second variant of the function. See the\n * details below.\n *\n * @param mask Operation mask that should be a single-channel 8-bit image, 2 pixels wider and 2 pixels\n * taller than image. Since this is both an input and output parameter, you must take responsibility of\n * initializing it. Flood-filling cannot go across non-zero pixels in the input mask. For example, an\n * edge detector output can be used as a mask to stop filling at edges. On output, pixels in the mask\n * corresponding to filled pixels in the image are set to 1 or to the a value specified in flags as\n * described below. Additionally, the function fills the border of the mask with ones to simplify\n * internal processing. It is therefore possible to use the same mask in multiple calls to the function\n * to make sure the filled areas do not overlap.\n *\n * @param seedPoint Starting point.\n *\n * @param newVal New value of the repainted domain pixels.\n *\n * @param rect Optional output parameter set by the function to the minimum bounding rectangle of the\n * repainted domain.\n *\n * @param loDiff Maximal lower brightness/color difference between the currently observed pixel and one\n * of its neighbors belonging to the component, or a seed pixel being added to the component.\n *\n * @param upDiff Maximal upper brightness/color difference between the currently observed pixel and one\n * of its neighbors belonging to the component, or a seed pixel being added to the component.\n *\n * @param flags Operation flags. The first 8 bits contain a connectivity value. The default value of 4\n * means that only the four nearest neighbor pixels (those that share an edge) are considered. A\n * connectivity value of 8 means that the eight nearest neighbor pixels (those that share a corner)\n * will be considered. The next 8 bits (8-16) contain a value between 1 and 255 with which to fill the\n * mask (the default value is 1). For example, 4 | ( 255 << 8 ) will consider 4 nearest neighbours and\n * fill the mask with a value of 255. The following additional options occupy higher bits and therefore\n * may be further combined with the connectivity and mask fill values using bit-wise or (|), see\n * FloodFillFlags.\n */\nexport declare function floodFill(image: InputOutputArray, mask: InputOutputArray, seedPoint: Point, newVal: Scalar, rect?: any, loDiff?: Scalar, upDiff?: Scalar, flags?: int): int;\n/**\n * The function implements the .\n *\n * @param img Input 8-bit 3-channel image.\n *\n * @param mask Input/output 8-bit single-channel mask. The mask is initialized by the function when\n * mode is set to GC_INIT_WITH_RECT. Its elements may have one of the GrabCutClasses.\n *\n * @param rect ROI containing a segmented object. The pixels outside of the ROI are marked as \"obvious\n * background\". The parameter is only used when mode==GC_INIT_WITH_RECT .\n *\n * @param bgdModel Temporary array for the background model. Do not modify it while you are processing\n * the same image.\n *\n * @param fgdModel Temporary arrays for the foreground model. Do not modify it while you are processing\n * the same image.\n *\n * @param iterCount Number of iterations the algorithm should make before returning the result. Note\n * that the result can be refined with further calls with mode==GC_INIT_WITH_MASK or mode==GC_EVAL .\n *\n * @param mode Operation mode that could be one of the GrabCutModes\n */\nexport declare function grabCut(img: InputArray, mask: InputOutputArray, rect: Rect, bgdModel: InputOutputArray, fgdModel: InputOutputArray, iterCount: int, mode?: int): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function integral(src: InputArray, sum: OutputArray, sdepth?: int): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function integral(src: InputArray, sum: OutputArray, sqsum: OutputArray, sdepth?: int, sqdepth?: int): void;\n/**\n * The function calculates one or more integral images for the source image as follows:\n *\n * `\\\\[\\\\texttt{sum} (X,Y) = \\\\sum _{x<X,y<Y} \\\\texttt{image} (x,y)\\\\]`\n *\n * `\\\\[\\\\texttt{sqsum} (X,Y) = \\\\sum _{x<X,y<Y} \\\\texttt{image} (x,y)^2\\\\]`\n *\n * `\\\\[\\\\texttt{tilted} (X,Y) = \\\\sum _{y<Y,abs(x-X+1) \\\\leq Y-y-1} \\\\texttt{image} (x,y)\\\\]`\n *\n * Using these integral images, you can calculate sum, mean, and standard deviation over a specific\n * up-right or rotated rectangular region of the image in a constant time, for example:\n *\n * `\\\\[\\\\sum _{x_1 \\\\leq x < x_2, \\\\, y_1 \\\\leq y < y_2} \\\\texttt{image} (x,y) = \\\\texttt{sum}\n * (x_2,y_2)- \\\\texttt{sum} (x_1,y_2)- \\\\texttt{sum} (x_2,y_1)+ \\\\texttt{sum} (x_1,y_1)\\\\]`\n *\n * It makes possible to do a fast blurring or fast block correlation with a variable window size, for\n * example. In case of multi-channel images, sums for each channel are accumulated independently.\n *\n * As a practical example, the next figure shows the calculation of the integral of a straight\n * rectangle Rect(3,3,3,2) and of a tilted rectangle Rect(5,1,2,3) . The selected pixels in the\n * original image are shown, as well as the relative pixels in the integral images sum and tilted .\n *\n * @param src input image as $W \\times H$, 8-bit or floating-point (32f or 64f).\n *\n * @param sum integral image as $(W+1)\\times (H+1)$ , 32-bit integer or floating-point (32f or 64f).\n *\n * @param sqsum integral image for squared pixel values; it is $(W+1)\\times (H+1)$, double-precision\n * floating-point (64f) array.\n *\n * @param tilted integral for the image rotated by 45 degrees; it is $(W+1)\\times (H+1)$ array with the\n * same data type as sum.\n *\n * @param sdepth desired depth of the integral and the tilted integral images, CV_32S, CV_32F, or\n * CV_64F.\n *\n * @param sqdepth desired depth of the integral image of squared pixel values, CV_32F or CV_64F.\n */\nexport declare function integral(src: InputArray, sum: OutputArray, sqsum: OutputArray, tilted: OutputArray, sdepth?: int, sqdepth?: int): void;\n/**\n * The function applies fixed-level thresholding to a multiple-channel array. The function is typically\n * used to get a bi-level (binary) image out of a grayscale image ( [compare] could be also used for\n * this purpose) or for removing a noise, that is, filtering out pixels with too small or too large\n * values. There are several types of thresholding supported by the function. They are determined by\n * type parameter.\n *\n * Also, the special values [THRESH_OTSU] or [THRESH_TRIANGLE] may be combined with one of the above\n * values. In these cases, the function determines the optimal threshold value using the Otsu's or\n * Triangle algorithm and uses it instead of the specified thresh.\n *\n * Currently, the Otsu's and Triangle methods are implemented only for 8-bit single-channel images.\n *\n * the computed threshold value if Otsu's or Triangle methods used.\n *\n * [adaptiveThreshold], [findContours], [compare], [min], [max]\n *\n * @param src input array (multiple-channel, 8-bit or 32-bit floating point).\n *\n * @param dst output array of the same size and type and the same number of channels as src.\n *\n * @param thresh threshold value.\n *\n * @param maxval maximum value to use with the THRESH_BINARY and THRESH_BINARY_INV thresholding types.\n *\n * @param type thresholding type (see ThresholdTypes).\n */\nexport declare function threshold(src: InputArray, dst: OutputArray, thresh: double, maxval: double, type: int): double;\n/**\n * The function implements one of the variants of watershed, non-parametric marker-based segmentation\n * algorithm, described in Meyer92 .\n *\n * Before passing the image to the function, you have to roughly outline the desired regions in the\n * image markers with positive (>0) indices. So, every region is represented as one or more connected\n * components with the pixel values 1, 2, 3, and so on. Such markers can be retrieved from a binary\n * mask using [findContours] and [drawContours] (see the watershed.cpp demo). The markers are \"seeds\"\n * of the future image regions. All the other pixels in markers , whose relation to the outlined\n * regions is not known and should be defined by the algorithm, should be set to 0's. In the function\n * output, each pixel in markers is set to a value of the \"seed\" components or to -1 at boundaries\n * between the regions.\n *\n * Any two neighbor connected components are not necessarily separated by a watershed boundary (-1's\n * pixels); for example, they can touch each other in the initial marker image passed to the function.\n *\n * [findContours]\n *\n * @param image Input 8-bit 3-channel image.\n *\n * @param markers Input/output 32-bit single-channel image (map) of markers. It should have the same\n * size as image .\n */\nexport declare function watershed(image: InputArray, markers: InputOutputArray): void;\n/**\n * the threshold value `$T(x,y)$` is a mean of the `$\\\\texttt{blockSize} \\\\times \\\\texttt{blockSize}$`\n * neighborhood of `$(x, y)$` minus C\n *\n */\nexport declare const ADAPTIVE_THRESH_MEAN_C: AdaptiveThresholdTypes;\n/**\n * the threshold value `$T(x, y)$` is a weighted sum (cross-correlation with a Gaussian window) of the\n * `$\\\\texttt{blockSize} \\\\times \\\\texttt{blockSize}$` neighborhood of `$(x, y)$` minus C . The default\n * sigma (standard deviation) is used for the specified blockSize . See [getGaussianKernel]\n *\n */\nexport declare const ADAPTIVE_THRESH_GAUSSIAN_C: AdaptiveThresholdTypes;\n/**\n * each connected component of zeros in src (as well as all the non-zero pixels closest to the\n * connected component) will be assigned the same label\n *\n */\nexport declare const DIST_LABEL_CCOMP: DistanceTransformLabelTypes;\n/**\n * each zero pixel (and all the non-zero pixels closest to it) gets its own label.\n *\n */\nexport declare const DIST_LABEL_PIXEL: DistanceTransformLabelTypes;\nexport declare const DIST_MASK_3: DistanceTransformMasks;\nexport declare const DIST_MASK_5: DistanceTransformMasks;\nexport declare const DIST_MASK_PRECISE: DistanceTransformMasks;\nexport declare const DIST_USER: DistanceTypes;\nexport declare const DIST_L1: DistanceTypes;\nexport declare const DIST_L2: DistanceTypes;\nexport declare const DIST_C: DistanceTypes;\nexport declare const DIST_L12: DistanceTypes;\nexport declare const DIST_FAIR: DistanceTypes;\nexport declare const DIST_WELSCH: DistanceTypes;\nexport declare const DIST_HUBER: DistanceTypes;\n/**\n * If set, the difference between the current pixel and seed pixel is considered. Otherwise, the\n * difference between neighbor pixels is considered (that is, the range is floating).\n *\n */\nexport declare const FLOODFILL_FIXED_RANGE: FloodFillFlags;\n/**\n * If set, the function does not change the image ( newVal is ignored), and only fills the mask with\n * the value specified in bits 8-16 of flags as described above. This option only make sense in\n * function variants that have the mask parameter.\n *\n */\nexport declare const FLOODFILL_MASK_ONLY: FloodFillFlags;\nexport declare const GC_BGD: GrabCutClasses;\nexport declare const GC_FGD: GrabCutClasses;\nexport declare const GC_PR_BGD: GrabCutClasses;\nexport declare const GC_PR_FGD: GrabCutClasses;\n/**\n * The function initializes the state and the mask using the provided rectangle. After that it runs\n * iterCount iterations of the algorithm.\n *\n */\nexport declare const GC_INIT_WITH_RECT: GrabCutModes;\n/**\n * The function initializes the state using the provided mask. Note that GC_INIT_WITH_RECT and\n * GC_INIT_WITH_MASK can be combined. Then, all the pixels outside of the ROI are automatically\n * initialized with GC_BGD .\n *\n */\nexport declare const GC_INIT_WITH_MASK: GrabCutModes;\n/**\n * The value means that the algorithm should just resume.\n *\n */\nexport declare const GC_EVAL: GrabCutModes;\n/**\n * The value means that the algorithm should just run the grabCut algorithm (a single iteration) with\n * the fixed model\n *\n */\nexport declare const GC_EVAL_FREEZE_MODEL: GrabCutModes;\nexport declare const THRESH_BINARY: ThresholdTypes;\nexport declare const THRESH_BINARY_INV: ThresholdTypes;\nexport declare const THRESH_TRUNC: ThresholdTypes;\nexport declare const THRESH_TOZERO: ThresholdTypes;\nexport declare const THRESH_TOZERO_INV: ThresholdTypes;\nexport declare const THRESH_MASK: ThresholdTypes;\nexport declare const THRESH_OTSU: ThresholdTypes;\nexport declare const THRESH_TRIANGLE: ThresholdTypes;\n/**\n * adaptive threshold algorithm\n *\n * [adaptiveThreshold]\n *\n */\nexport declare type AdaptiveThresholdTypes = any;\n/**\n * adaptive threshold algorithm\n *\n * [adaptiveThreshold]\n *\n */\nexport declare type DistanceTransformLabelTypes = any;\n/**\n * adaptive threshold algorithm\n *\n * [adaptiveThreshold]\n *\n */\nexport declare type DistanceTransformMasks = any;\n/**\n * adaptive threshold algorithm\n *\n * [adaptiveThreshold]\n *\n */\nexport declare type DistanceTypes = any;\n/**\n * adaptive threshold algorithm\n *\n * [adaptiveThreshold]\n *\n */\nexport declare type FloodFillFlags = any;\n/**\n * adaptive threshold algorithm\n *\n * [adaptiveThreshold]\n *\n */\nexport declare type GrabCutClasses = any;\n/**\n * adaptive threshold algorithm\n *\n * [adaptiveThreshold]\n *\n */\nexport declare type GrabCutModes = any;\n/**\n * adaptive threshold algorithm\n *\n * [adaptiveThreshold]\n *\n */\nexport declare type ThresholdTypes = any;\n"},"node_modules_mirada_dist_src_types_opencv_imgproc_object_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_imgproc_object_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/imgproc_object.d.ts","content":"import { InputArray, int, OutputArray } from './_types';\n/**\n * The function slides through image , compares the overlapped patches of size `$w \\\\times h$` against\n * templ using the specified method and stores the comparison results in result . Here are the formulae\n * for the available comparison methods ( `$I$` denotes image, `$T$` template, `$R$` result ). The\n * summation is done over template and/or the image patch: `$x' = 0...w-1, y' = 0...h-1$`\n *\n * After the function finishes the comparison, the best matches can be found as global minimums (when\n * [TM_SQDIFF] was used) or maximums (when [TM_CCORR] or [TM_CCOEFF] was used) using the [minMaxLoc]\n * function. In case of a color image, template summation in the numerator and each sum in the\n * denominator is done over all of the channels and separate mean values are used for each channel.\n * That is, the function can take a color template and a color image. The result will still be a\n * single-channel image, which is easier to analyze.\n *\n * @param image Image where the search is running. It must be 8-bit or 32-bit floating-point.\n *\n * @param templ Searched template. It must be not greater than the source image and have the same data\n * type.\n *\n * @param result Map of comparison results. It must be single-channel 32-bit floating-point. If image\n * is $W \\times H$ and templ is $w \\times h$ , then result is $(W-w+1) \\times (H-h+1)$ .\n *\n * @param method Parameter specifying the comparison method, see TemplateMatchModes\n *\n * @param mask Mask of searched template. It must have the same datatype and size with templ. It is not\n * set by default. Currently, only the TM_SQDIFF and TM_CCORR_NORMED methods are supported.\n */\nexport declare function matchTemplate(image: InputArray, templ: InputArray, result: OutputArray, method: int, mask?: InputArray): void;\nexport declare const TM_SQDIFF: TemplateMatchModes;\nexport declare const TM_SQDIFF_NORMED: TemplateMatchModes;\nexport declare const TM_CCORR: TemplateMatchModes;\nexport declare const TM_CCORR_NORMED: TemplateMatchModes;\n/**\n * `\\\\[R(x,y)= \\\\sum _{x',y'} (T'(x',y') \\\\cdot I'(x+x',y+y'))\\\\]` where `\\\\[\\\\begin{array}{l}\n * T'(x',y')=T(x',y') - 1/(w \\\\cdot h) \\\\cdot \\\\sum _{x'',y''} T(x'',y'') \\\\\\\\\n * I'(x+x',y+y')=I(x+x',y+y') - 1/(w \\\\cdot h) \\\\cdot \\\\sum _{x'',y''} I(x+x'',y+y'') \\\\end{array}\\\\]`\n *\n */\nexport declare const TM_CCOEFF: TemplateMatchModes;\nexport declare const TM_CCOEFF_NORMED: TemplateMatchModes;\nexport declare type TemplateMatchModes = any;\n"},"node_modules_mirada_dist_src_types_opencv_imgproc_shape_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_imgproc_shape_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/imgproc_shape.d.ts","content":"import { bool, double, float, InputArray, int, Moments, OutputArray, OutputArrayOfArrays, Point, Point2f, Rect, RotatedRect } from './_types';\n/**\n * The function [cv::approxPolyDP] approximates a curve or a polygon with another curve/polygon with\n * less vertices so that the distance between them is less or equal to the specified precision. It uses\n * the Douglas-Peucker algorithm\n *\n * @param curve Input vector of a 2D point stored in std::vector or Mat\n *\n * @param approxCurve Result of the approximation. The type should match the type of the input curve.\n *\n * @param epsilon Parameter specifying the approximation accuracy. This is the maximum distance between\n * the original curve and its approximation.\n *\n * @param closed If true, the approximated curve is closed (its first and last vertices are connected).\n * Otherwise, it is not closed.\n */\nexport declare function approxPolyDP(curve: InputArray, approxCurve: OutputArray, epsilon: double, closed: bool): void;\n/**\n * The function computes a curve length or a closed contour perimeter.\n *\n * @param curve Input vector of 2D points, stored in std::vector or Mat.\n *\n * @param closed Flag indicating whether the curve is closed or not.\n */\nexport declare function arcLength(curve: InputArray, closed: bool): double;\n/**\n * The function calculates and returns the minimal up-right bounding rectangle for the specified point\n * set or non-zero pixels of gray-scale image.\n *\n * @param array Input gray-scale image or 2D point set, stored in std::vector or Mat.\n */\nexport declare function boundingRect(array: InputArray): Rect;\n/**\n * The function finds the four vertices of a rotated rectangle. This function is useful to draw the\n * rectangle. In C++, instead of using this function, you can directly use [RotatedRect::points]\n * method. Please visit the [tutorial on Creating Bounding rotated boxes and ellipses for contours] for\n * more information.\n *\n * @param box The input rotated rectangle. It may be the output of\n *\n * @param points The output array of four vertices of rectangles.\n */\nexport declare function boxPoints(box: RotatedRect, points: OutputArray): void;\n/**\n * image with 4 or 8 way connectivity - returns N, the total number of labels [0, N-1] where 0\n * represents the background label. ltype specifies the output label image type, an important\n * consideration based on the total number of labels or alternatively the total number of pixels in the\n * source image. ccltype specifies the connected components labeling algorithm to use, currently Grana\n * (BBDT) and Wu's (SAUF) algorithms are supported, see the [ConnectedComponentsAlgorithmsTypes] for\n * details. Note that SAUF algorithm forces a row major ordering of labels while BBDT does not. This\n * function uses parallel version of both Grana and Wu's algorithms if at least one allowed parallel\n * framework is enabled and if the rows of the image are at least twice the number returned by\n * [getNumberOfCPUs].\n *\n * @param image the 8-bit single-channel image to be labeled\n *\n * @param labels destination labeled image\n *\n * @param connectivity 8 or 4 for 8-way or 4-way connectivity respectively\n *\n * @param ltype output image label type. Currently CV_32S and CV_16U are supported.\n *\n * @param ccltype connected components algorithm type (see the ConnectedComponentsAlgorithmsTypes).\n */\nexport declare function connectedComponents(image: InputArray, labels: OutputArray, connectivity: int, ltype: int, ccltype: int): int;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n *\n * @param image the 8-bit single-channel image to be labeled\n *\n * @param labels destination labeled image\n *\n * @param connectivity 8 or 4 for 8-way or 4-way connectivity respectively\n *\n * @param ltype output image label type. Currently CV_32S and CV_16U are supported.\n */\nexport declare function connectedComponents(image: InputArray, labels: OutputArray, connectivity?: int, ltype?: int): int;\n/**\n * image with 4 or 8 way connectivity - returns N, the total number of labels [0, N-1] where 0\n * represents the background label. ltype specifies the output label image type, an important\n * consideration based on the total number of labels or alternatively the total number of pixels in the\n * source image. ccltype specifies the connected components labeling algorithm to use, currently\n * Grana's (BBDT) and Wu's (SAUF) algorithms are supported, see the\n * [ConnectedComponentsAlgorithmsTypes] for details. Note that SAUF algorithm forces a row major\n * ordering of labels while BBDT does not. This function uses parallel version of both Grana and Wu's\n * algorithms (statistics included) if at least one allowed parallel framework is enabled and if the\n * rows of the image are at least twice the number returned by [getNumberOfCPUs].\n *\n * @param image the 8-bit single-channel image to be labeled\n *\n * @param labels destination labeled image\n *\n * @param stats statistics output for each label, including the background label, see below for\n * available statistics. Statistics are accessed via stats(label, COLUMN) where COLUMN is one of\n * ConnectedComponentsTypes. The data type is CV_32S.\n *\n * @param centroids centroid output for each label, including the background label. Centroids are\n * accessed via centroids(label, 0) for x and centroids(label, 1) for y. The data type CV_64F.\n *\n * @param connectivity 8 or 4 for 8-way or 4-way connectivity respectively\n *\n * @param ltype output image label type. Currently CV_32S and CV_16U are supported.\n *\n * @param ccltype connected components algorithm type (see ConnectedComponentsAlgorithmsTypes).\n */\nexport declare function connectedComponentsWithStats(image: InputArray, labels: OutputArray, stats: OutputArray, centroids: OutputArray, connectivity: int, ltype: int, ccltype: int): int;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n *\n * @param image the 8-bit single-channel image to be labeled\n *\n * @param labels destination labeled image\n *\n * @param stats statistics output for each label, including the background label, see below for\n * available statistics. Statistics are accessed via stats(label, COLUMN) where COLUMN is one of\n * ConnectedComponentsTypes. The data type is CV_32S.\n *\n * @param centroids centroid output for each label, including the background label. Centroids are\n * accessed via centroids(label, 0) for x and centroids(label, 1) for y. The data type CV_64F.\n *\n * @param connectivity 8 or 4 for 8-way or 4-way connectivity respectively\n *\n * @param ltype output image label type. Currently CV_32S and CV_16U are supported.\n */\nexport declare function connectedComponentsWithStats(image: InputArray, labels: OutputArray, stats: OutputArray, centroids: OutputArray, connectivity?: int, ltype?: int): int;\n/**\n * The function computes a contour area. Similarly to moments , the area is computed using the Green\n * formula. Thus, the returned area and the number of non-zero pixels, if you draw the contour using\n * [drawContours] or [fillPoly] , can be different. Also, the function will most certainly give a wrong\n * results for contours with self-intersections.\n *\n * Example:\n *\n * ```cpp\n * vector<Point> contour;\n * contour.push_back(Point2f(0, 0));\n * contour.push_back(Point2f(10, 0));\n * contour.push_back(Point2f(10, 10));\n * contour.push_back(Point2f(5, 4));\n *\n * double area0 = contourArea(contour);\n * vector<Point> approx;\n * approxPolyDP(contour, approx, 5, true);\n * double area1 = contourArea(approx);\n *\n * cout << \"area0 =\" << area0 << endl <<\n *         \"area1 =\" << area1 << endl <<\n *         \"approx poly vertices\" << approx.size() << endl;\n * ```\n *\n * @param contour Input vector of 2D points (contour vertices), stored in std::vector or Mat.\n *\n * @param oriented Oriented area flag. If it is true, the function returns a signed area value,\n * depending on the contour orientation (clockwise or counter-clockwise). Using this feature you can\n * determine orientation of a contour by taking the sign of an area. By default, the parameter is\n * false, which means that the absolute value is returned.\n */\nexport declare function contourArea(contour: InputArray, oriented?: bool): double;\n/**\n * The function [cv::convexHull] finds the convex hull of a 2D point set using the Sklansky's algorithm\n * Sklansky82 that has *O(N logN)* complexity in the current implementation.\n *\n * `points` and `hull` should be different arrays, inplace processing isn't supported.\n * Check [the corresponding tutorial] for more details.\n *\n * useful links:\n *\n * @param points Input 2D point set, stored in std::vector or Mat.\n *\n * @param hull Output convex hull. It is either an integer vector of indices or vector of points. In\n * the first case, the hull elements are 0-based indices of the convex hull points in the original\n * array (since the set of convex hull points is a subset of the original point set). In the second\n * case, hull elements are the convex hull points themselves.\n *\n * @param clockwise Orientation flag. If it is true, the output convex hull is oriented clockwise.\n * Otherwise, it is oriented counter-clockwise. The assumed coordinate system has its X axis pointing\n * to the right, and its Y axis pointing upwards.\n *\n * @param returnPoints Operation flag. In case of a matrix, when the flag is true, the function returns\n * convex hull points. Otherwise, it returns indices of the convex hull points. When the output array\n * is std::vector, the flag is ignored, and the output depends on the type of the vector:\n * std::vector<int> implies returnPoints=false, std::vector<Point> implies returnPoints=true.\n */\nexport declare function convexHull(points: InputArray, hull: OutputArray, clockwise?: bool, returnPoints?: bool): void;\n/**\n * The figure below displays convexity defects of a hand contour:\n *\n * @param contour Input contour.\n *\n * @param convexhull Convex hull obtained using convexHull that should contain indices of the contour\n * points that make the hull.\n *\n * @param convexityDefects The output vector of convexity defects. In C++ and the new Python/Java\n * interface each convexity defect is represented as 4-element integer vector (a.k.a. Vec4i):\n * (start_index, end_index, farthest_pt_index, fixpt_depth), where indices are 0-based indices in the\n * original contour of the convexity defect beginning, end and the farthest point, and fixpt_depth is\n * fixed-point approximation (with 8 fractional bits) of the distance between the farthest contour\n * point and the hull. That is, to get the floating-point value of the depth will be fixpt_depth/256.0.\n */\nexport declare function convexityDefects(contour: InputArray, convexhull: InputArray, convexityDefects: OutputArray): void;\nexport declare function createGeneralizedHoughBallard(): any;\nexport declare function createGeneralizedHoughGuil(): any;\n/**\n * The function retrieves contours from the binary image using the algorithm Suzuki85 . The contours\n * are a useful tool for shape analysis and object detection and recognition. See squares.cpp in the\n * OpenCV sample directory.\n *\n * Since opencv 3.2 source image is not modified by this function.\n *\n * @param image Source, an 8-bit single-channel image. Non-zero pixels are treated as 1's. Zero pixels\n * remain 0's, so the image is treated as binary . You can use compare, inRange, threshold ,\n * adaptiveThreshold, Canny, and others to create a binary image out of a grayscale or color one. If\n * mode equals to RETR_CCOMP or RETR_FLOODFILL, the input can also be a 32-bit integer image of labels\n * (CV_32SC1).\n *\n * @param contours Detected contours. Each contour is stored as a vector of points (e.g.\n * std::vector<std::vector<cv::Point> >).\n *\n * @param hierarchy Optional output vector (e.g. std::vector<cv::Vec4i>), containing information about\n * the image topology. It has as many elements as the number of contours. For each i-th contour\n * contours[i], the elements hierarchy[i][0] , hierarchy[i][1] , hierarchy[i][2] , and hierarchy[i][3]\n * are set to 0-based indices in contours of the next and previous contours at the same hierarchical\n * level, the first child contour and the parent contour, respectively. If for the contour i there are\n * no next, previous, parent, or nested contours, the corresponding elements of hierarchy[i] will be\n * negative.\n *\n * @param mode Contour retrieval mode, see RetrievalModes\n *\n * @param method Contour approximation method, see ContourApproximationModes\n *\n * @param offset Optional offset by which every contour point is shifted. This is useful if the\n * contours are extracted from the image ROI and then they should be analyzed in the whole image\n * context.\n */\nexport declare function findContours(image: InputArray, contours: OutputArrayOfArrays, hierarchy: OutputArray, mode: int, method: int, offset?: Point): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function findContours(image: InputArray, contours: OutputArrayOfArrays, mode: int, method: int, offset?: Point): void;\n/**\n * The function calculates the ellipse that fits (in a least-squares sense) a set of 2D points best of\n * all. It returns the rotated rectangle in which the ellipse is inscribed. The first algorithm\n * described by Fitzgibbon95 is used. Developer should keep in mind that it is possible that the\n * returned ellipse/rotatedRect data contains negative indices, due to the data points being close to\n * the border of the containing [Mat] element.\n *\n * @param points Input 2D point set, stored in std::vector<> or Mat\n */\nexport declare function fitEllipse(points: InputArray): RotatedRect;\n/**\n * The function calculates the ellipse that fits a set of 2D points. It returns the rotated rectangle\n * in which the ellipse is inscribed. The Approximate Mean Square (AMS) proposed by Taubin1991 is used.\n *\n * For an ellipse, this basis set is `$ \\\\chi= \\\\left(x^2, x y, y^2, x, y, 1\\\\right) $`, which is a set\n * of six free coefficients `$\n * A^T=\\\\left\\\\{A_{\\\\text{xx}},A_{\\\\text{xy}},A_{\\\\text{yy}},A_x,A_y,A_0\\\\right\\\\} $`. However, to\n * specify an ellipse, all that is needed is five numbers; the major and minor axes lengths `$ (a,b)\n * $`, the position `$ (x_0,y_0) $`, and the orientation `$ \\\\theta $`. This is because the basis set\n * includes lines, quadratics, parabolic and hyperbolic functions as well as elliptical functions as\n * possible fits. If the fit is found to be a parabolic or hyperbolic function then the standard\n * [fitEllipse] method is used. The AMS method restricts the fit to parabolic, hyperbolic and\n * elliptical curves by imposing the condition that `$ A^T ( D_x^T D_x + D_y^T D_y) A = 1 $` where the\n * matrices `$ Dx $` and `$ Dy $` are the partial derivatives of the design matrix `$ D $` with respect\n * to x and y. The matrices are formed row by row applying the following to each of the points in the\n * set: `\\\\begin{align*} D(i,:)&=\\\\left\\\\{x_i^2, x_i y_i, y_i^2, x_i, y_i, 1\\\\right\\\\} &\n * D_x(i,:)&=\\\\left\\\\{2 x_i,y_i,0,1,0,0\\\\right\\\\} & D_y(i,:)&=\\\\left\\\\{0,x_i,2 y_i,0,1,0\\\\right\\\\}\n * \\\\end{align*}` The AMS method minimizes the cost function `\\\\begin{equation*} \\\\epsilon ^2=\\\\frac{\n * A^T D^T D A }{ A^T (D_x^T D_x + D_y^T D_y) A^T } \\\\end{equation*}`\n *\n * The minimum cost is found by solving the generalized eigenvalue problem.\n *\n * `\\\\begin{equation*} D^T D A = \\\\lambda \\\\left( D_x^T D_x + D_y^T D_y\\\\right) A \\\\end{equation*}`\n *\n * @param points Input 2D point set, stored in std::vector<> or Mat\n */\nexport declare function fitEllipseAMS(points: InputArray): RotatedRect;\n/**\n * The function calculates the ellipse that fits a set of 2D points. It returns the rotated rectangle\n * in which the ellipse is inscribed. The Direct least square (Direct) method by Fitzgibbon1999 is\n * used.\n *\n * For an ellipse, this basis set is `$ \\\\chi= \\\\left(x^2, x y, y^2, x, y, 1\\\\right) $`, which is a set\n * of six free coefficients `$\n * A^T=\\\\left\\\\{A_{\\\\text{xx}},A_{\\\\text{xy}},A_{\\\\text{yy}},A_x,A_y,A_0\\\\right\\\\} $`. However, to\n * specify an ellipse, all that is needed is five numbers; the major and minor axes lengths `$ (a,b)\n * $`, the position `$ (x_0,y_0) $`, and the orientation `$ \\\\theta $`. This is because the basis set\n * includes lines, quadratics, parabolic and hyperbolic functions as well as elliptical functions as\n * possible fits. The Direct method confines the fit to ellipses by ensuring that `$ 4 A_{xx} A_{yy}-\n * A_{xy}^2 > 0 $`. The condition imposed is that `$ 4 A_{xx} A_{yy}- A_{xy}^2=1 $` which satisfies the\n * inequality and as the coefficients can be arbitrarily scaled is not overly restrictive.\n *\n * `\\\\begin{equation*} \\\\epsilon ^2= A^T D^T D A \\\\quad \\\\text{with} \\\\quad A^T C A =1 \\\\quad\n * \\\\text{and} \\\\quad C=\\\\left(\\\\begin{matrix} 0 & 0 & 2 & 0 & 0 & 0 \\\\\\\\ 0 & -1 & 0 & 0 & 0 & 0 \\\\\\\\ 2\n * & 0 & 0 & 0 & 0 & 0 \\\\\\\\ 0 & 0 & 0 & 0 & 0 & 0 \\\\\\\\ 0 & 0 & 0 & 0 & 0 & 0 \\\\\\\\ 0 & 0 & 0 & 0 & 0 & 0\n * \\\\end{matrix} \\\\right) \\\\end{equation*}`\n *\n * The minimum cost is found by solving the generalized eigenvalue problem.\n *\n * `\\\\begin{equation*} D^T D A = \\\\lambda \\\\left( C\\\\right) A \\\\end{equation*}`\n *\n * The system produces only one positive eigenvalue `$ \\\\lambda$` which is chosen as the solution with\n * its eigenvector `$\\\\mathbf{u}$`. These are used to find the coefficients\n *\n * `\\\\begin{equation*} A = \\\\sqrt{\\\\frac{1}{\\\\mathbf{u}^T C \\\\mathbf{u}}} \\\\mathbf{u} \\\\end{equation*}`\n * The scaling factor guarantees that `$A^T C A =1$`.\n *\n * @param points Input 2D point set, stored in std::vector<> or Mat\n */\nexport declare function fitEllipseDirect(points: InputArray): RotatedRect;\n/**\n * The function fitLine fits a line to a 2D or 3D point set by minimizing `$\\\\sum_i \\\\rho(r_i)$` where\n * `$r_i$` is a distance between the `$i^{th}$` point, the line and `$\\\\rho(r)$` is a distance\n * function, one of the following:\n *\n * DIST_L2 `\\\\[\\\\rho (r) = r^2/2 \\\\quad \\\\text{(the simplest and the fastest least-squares method)}\\\\]`\n * DIST_L1 `\\\\[\\\\rho (r) = r\\\\]`\n * DIST_L12 `\\\\[\\\\rho (r) = 2 \\\\cdot ( \\\\sqrt{1 + \\\\frac{r^2}{2}} - 1)\\\\]`\n * DIST_FAIR `\\\\[\\\\rho \\\\left (r \\\\right ) = C^2 \\\\cdot \\\\left ( \\\\frac{r}{C} - \\\\log{\\\\left(1 +\n * \\\\frac{r}{C}\\\\right)} \\\\right ) \\\\quad \\\\text{where} \\\\quad C=1.3998\\\\]`\n * DIST_WELSCH `\\\\[\\\\rho \\\\left (r \\\\right ) = \\\\frac{C^2}{2} \\\\cdot \\\\left ( 1 -\n * \\\\exp{\\\\left(-\\\\left(\\\\frac{r}{C}\\\\right)^2\\\\right)} \\\\right ) \\\\quad \\\\text{where} \\\\quad\n * C=2.9846\\\\]`\n * DIST_HUBER `\\\\[\\\\rho (r) = \\\\fork{r^2/2}{if \\\\(r < C\\\\)}{C \\\\cdot (r-C/2)}{otherwise} \\\\quad\n * \\\\text{where} \\\\quad C=1.345\\\\]`\n *\n * The algorithm is based on the M-estimator (  ) technique that iteratively fits the line using the\n * weighted least-squares algorithm. After each iteration the weights `$w_i$` are adjusted to be\n * inversely proportional to `$\\\\rho(r_i)$` .\n *\n * @param points Input vector of 2D or 3D points, stored in std::vector<> or Mat.\n *\n * @param line Output line parameters. In case of 2D fitting, it should be a vector of 4 elements (like\n * Vec4f) - (vx, vy, x0, y0), where (vx, vy) is a normalized vector collinear to the line and (x0, y0)\n * is a point on the line. In case of 3D fitting, it should be a vector of 6 elements (like Vec6f) -\n * (vx, vy, vz, x0, y0, z0), where (vx, vy, vz) is a normalized vector collinear to the line and (x0,\n * y0, z0) is a point on the line.\n *\n * @param distType Distance used by the M-estimator, see DistanceTypes\n *\n * @param param Numerical parameter ( C ) for some types of distances. If it is 0, an optimal value is\n * chosen.\n *\n * @param reps Sufficient accuracy for the radius (distance between the coordinate origin and the\n * line).\n *\n * @param aeps Sufficient accuracy for the angle. 0.01 would be a good default value for reps and aeps.\n */\nexport declare function fitLine(points: InputArray, line: OutputArray, distType: int, param: double, reps: double, aeps: double): void;\n/**\n * The function calculates seven Hu invariants (introduced in Hu62; see also ) defined as:\n *\n * `\\\\[\\\\begin{array}{l} hu[0]= \\\\eta _{20}+ \\\\eta _{02} \\\\\\\\ hu[1]=( \\\\eta _{20}- \\\\eta _{02})^{2}+4\n * \\\\eta _{11}^{2} \\\\\\\\ hu[2]=( \\\\eta _{30}-3 \\\\eta _{12})^{2}+ (3 \\\\eta _{21}- \\\\eta _{03})^{2} \\\\\\\\\n * hu[3]=( \\\\eta _{30}+ \\\\eta _{12})^{2}+ ( \\\\eta _{21}+ \\\\eta _{03})^{2} \\\\\\\\ hu[4]=( \\\\eta _{30}-3\n * \\\\eta _{12})( \\\\eta _{30}+ \\\\eta _{12})[( \\\\eta _{30}+ \\\\eta _{12})^{2}-3( \\\\eta _{21}+ \\\\eta\n * _{03})^{2}]+(3 \\\\eta _{21}- \\\\eta _{03})( \\\\eta _{21}+ \\\\eta _{03})[3( \\\\eta _{30}+ \\\\eta\n * _{12})^{2}-( \\\\eta _{21}+ \\\\eta _{03})^{2}] \\\\\\\\ hu[5]=( \\\\eta _{20}- \\\\eta _{02})[( \\\\eta _{30}+\n * \\\\eta _{12})^{2}- ( \\\\eta _{21}+ \\\\eta _{03})^{2}]+4 \\\\eta _{11}( \\\\eta _{30}+ \\\\eta _{12})( \\\\eta\n * _{21}+ \\\\eta _{03}) \\\\\\\\ hu[6]=(3 \\\\eta _{21}- \\\\eta _{03})( \\\\eta _{21}+ \\\\eta _{03})[3( \\\\eta\n * _{30}+ \\\\eta _{12})^{2}-( \\\\eta _{21}+ \\\\eta _{03})^{2}]-( \\\\eta _{30}-3 \\\\eta _{12})( \\\\eta _{21}+\n * \\\\eta _{03})[3( \\\\eta _{30}+ \\\\eta _{12})^{2}-( \\\\eta _{21}+ \\\\eta _{03})^{2}] \\\\\\\\ \\\\end{array}\\\\]`\n *\n * where `$\\\\eta_{ji}$` stands for `$\\\\texttt{Moments::nu}_{ji}$` .\n *\n * These values are proved to be invariants to the image scale, rotation, and reflection except the\n * seventh one, whose sign is changed by reflection. This invariance is proved with the assumption of\n * infinite image resolution. In case of raster images, the computed Hu invariants for the original and\n * transformed images are a bit different.\n *\n * [matchShapes]\n *\n * @param moments Input moments computed with moments .\n *\n * @param hu Output Hu invariants.\n */\nexport declare function HuMoments(moments: any, hu: double): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function HuMoments(m: any, hu: OutputArray): void;\nexport declare function intersectConvexConvex(_p1: InputArray, _p2: InputArray, _p12: OutputArray, handleNested?: bool): float;\n/**\n * The function tests whether the input contour is convex or not. The contour must be simple, that is,\n * without self-intersections. Otherwise, the function output is undefined.\n *\n * @param contour Input vector of 2D points, stored in std::vector<> or Mat\n */\nexport declare function isContourConvex(contour: InputArray): bool;\n/**\n * The function compares two shapes. All three implemented methods use the Hu invariants (see\n * [HuMoments])\n *\n * @param contour1 First contour or grayscale image.\n *\n * @param contour2 Second contour or grayscale image.\n *\n * @param method Comparison method, see ShapeMatchModes\n *\n * @param parameter Method-specific parameter (not supported now).\n */\nexport declare function matchShapes(contour1: InputArray, contour2: InputArray, method: int, parameter: double): double;\n/**\n * The function calculates and returns the minimum-area bounding rectangle (possibly rotated) for a\n * specified point set. Developer should keep in mind that the returned [RotatedRect] can contain\n * negative indices when data is close to the containing [Mat] element boundary.\n *\n * @param points Input vector of 2D points, stored in std::vector<> or Mat\n */\nexport declare function minAreaRect(points: InputArray): RotatedRect;\n/**\n * The function finds the minimal enclosing circle of a 2D point set using an iterative algorithm.\n *\n * @param points Input vector of 2D points, stored in std::vector<> or Mat\n *\n * @param center Output center of the circle.\n *\n * @param radius Output radius of the circle.\n */\nexport declare function minEnclosingCircle(points: InputArray, center: any, radius: any): void;\n/**\n * The function finds a triangle of minimum area enclosing the given set of 2D points and returns its\n * area. The output for a given 2D point set is shown in the image below. 2D points are depicted in\n * red* and the enclosing triangle in *yellow*.\n *\n *  The implementation of the algorithm is based on O'Rourke's ORourke86 and Klee and Laskowski's\n * KleeLaskowski85 papers. O'Rourke provides a `$\\\\theta(n)$` algorithm for finding the minimal\n * enclosing triangle of a 2D convex polygon with n vertices. Since the [minEnclosingTriangle] function\n * takes a 2D point set as input an additional preprocessing step of computing the convex hull of the\n * 2D point set is required. The complexity of the [convexHull] function is `$O(n log(n))$` which is\n * higher than `$\\\\theta(n)$`. Thus the overall complexity of the function is `$O(n log(n))$`.\n *\n * @param points Input vector of 2D points with depth CV_32S or CV_32F, stored in std::vector<> or Mat\n *\n * @param triangle Output vector of three 2D points defining the vertices of the triangle. The depth of\n * the OutputArray must be CV_32F.\n */\nexport declare function minEnclosingTriangle(points: InputArray, triangle: OutputArray): double;\n/**\n * The function computes moments, up to the 3rd order, of a vector shape or a rasterized shape. The\n * results are returned in the structure [cv::Moments].\n *\n * moments.\n *\n * Only applicable to contour moments calculations from Python bindings: Note that the numpy type for\n * the input array should be either np.int32 or np.float32.\n *\n * [contourArea], [arcLength]\n *\n * @param array Raster image (single-channel, 8-bit or floating-point 2D array) or an array ( $1 \\times\n * N$ or $N \\times 1$ ) of 2D points (Point or Point2f ).\n *\n * @param binaryImage If it is true, all non-zero image pixels are treated as 1's. The parameter is\n * used for images only.\n */\nexport declare function moments(array: InputArray, binaryImage?: bool): Moments;\n/**\n * The function determines whether the point is inside a contour, outside, or lies on an edge (or\n * coincides with a vertex). It returns positive (inside), negative (outside), or zero (on an edge)\n * value, correspondingly. When measureDist=false , the return value is +1, -1, and 0, respectively.\n * Otherwise, the return value is a signed distance between the point and the nearest contour edge.\n *\n * See below a sample output of the function where each image pixel is tested against the contour:\n *\n * @param contour Input contour.\n *\n * @param pt Point tested against the contour.\n *\n * @param measureDist If true, the function estimates the signed distance from the point to the nearest\n * contour edge. Otherwise, the function only checks if the point is inside a contour or not.\n */\nexport declare function pointPolygonTest(contour: InputArray, pt: Point2f, measureDist: bool): double;\n/**\n * If there is then the vertices of the intersecting region are returned as well.\n *\n * Below are some examples of intersection configurations. The hatched pattern indicates the\n * intersecting region and the red vertices are returned by the function.\n *\n * One of [RectanglesIntersectTypes]\n *\n * @param rect1 First rectangle\n *\n * @param rect2 Second rectangle\n *\n * @param intersectingRegion The output array of the vertices of the intersecting region. It returns at\n * most 8 vertices. Stored as std::vector<cv::Point2f> or cv::Mat as Mx1 of type CV_32FC2.\n */\nexport declare function rotatedRectangleIntersection(rect1: any, rect2: any, intersectingRegion: OutputArray): int;\nexport declare const CCL_WU: ConnectedComponentsAlgorithmsTypes;\nexport declare const CCL_DEFAULT: ConnectedComponentsAlgorithmsTypes;\nexport declare const CCL_GRANA: ConnectedComponentsAlgorithmsTypes;\n/**\n * The leftmost (x) coordinate which is the inclusive start of the bounding box in the horizontal\n * direction.\n *\n */\nexport declare const CC_STAT_LEFT: ConnectedComponentsTypes;\n/**\n * The topmost (y) coordinate which is the inclusive start of the bounding box in the vertical\n * direction.\n *\n */\nexport declare const CC_STAT_TOP: ConnectedComponentsTypes;\nexport declare const CC_STAT_WIDTH: ConnectedComponentsTypes;\nexport declare const CC_STAT_HEIGHT: ConnectedComponentsTypes;\nexport declare const CC_STAT_AREA: ConnectedComponentsTypes;\nexport declare const CC_STAT_MAX: ConnectedComponentsTypes;\n/**\n * stores absolutely all the contour points. That is, any 2 subsequent points (x1,y1) and (x2,y2) of\n * the contour will be either horizontal, vertical or diagonal neighbors, that is,\n * max(abs(x1-x2),abs(y2-y1))==1.\n *\n */\nexport declare const CHAIN_APPROX_NONE: ContourApproximationModes;\n/**\n * compresses horizontal, vertical, and diagonal segments and leaves only their end points. For\n * example, an up-right rectangular contour is encoded with 4 points.\n *\n */\nexport declare const CHAIN_APPROX_SIMPLE: ContourApproximationModes;\n/**\n * applies one of the flavors of the Teh-Chin chain approximation algorithm TehChin89\n *\n */\nexport declare const CHAIN_APPROX_TC89_L1: ContourApproximationModes;\n/**\n * applies one of the flavors of the Teh-Chin chain approximation algorithm TehChin89\n *\n */\nexport declare const CHAIN_APPROX_TC89_KCOS: ContourApproximationModes;\nexport declare const INTERSECT_NONE: RectanglesIntersectTypes;\nexport declare const INTERSECT_PARTIAL: RectanglesIntersectTypes;\nexport declare const INTERSECT_FULL: RectanglesIntersectTypes;\n/**\n * retrieves only the extreme outer contours. It sets `hierarchy[i][2]=hierarchy[i][3]=-1` for all the\n * contours.\n *\n */\nexport declare const RETR_EXTERNAL: RetrievalModes;\n/**\n * retrieves all of the contours without establishing any hierarchical relationships.\n *\n */\nexport declare const RETR_LIST: RetrievalModes;\n/**\n * retrieves all of the contours and organizes them into a two-level hierarchy. At the top level, there\n * are external boundaries of the components. At the second level, there are boundaries of the holes.\n * If there is another contour inside a hole of a connected component, it is still put at the top\n * level.\n *\n */\nexport declare const RETR_CCOMP: RetrievalModes;\n/**\n * retrieves all of the contours and reconstructs a full hierarchy of nested contours.\n *\n */\nexport declare const RETR_TREE: RetrievalModes;\nexport declare const RETR_FLOODFILL: RetrievalModes;\nexport declare const CONTOURS_MATCH_I1: ShapeMatchModes;\nexport declare const CONTOURS_MATCH_I2: ShapeMatchModes;\nexport declare const CONTOURS_MATCH_I3: ShapeMatchModes;\nexport declare type ConnectedComponentsAlgorithmsTypes = any;\nexport declare type ConnectedComponentsTypes = any;\nexport declare type ContourApproximationModes = any;\nexport declare type RectanglesIntersectTypes = any;\nexport declare type RetrievalModes = any;\nexport declare type ShapeMatchModes = any;\n"},"node_modules_mirada_dist_src_types_opencv_index_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_index_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/index.d.ts","content":"import * as _CV from './_types';\nexport declare type CV = typeof _CV;\nexport * from './_hacks';\nexport * from './_types';\n"},"node_modules_mirada_dist_src_types_opencv_imgproc_transform_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_imgproc_transform_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/imgproc_transform.d.ts","content":"import { bool, double, InputArray, int, Mat, OutputArray, Point2f, Size } from './_types';\n/**\n * The function converts a pair of maps for remap from one representation to another. The following\n * options ( (map1.type(), map2.type()) `$\\\\rightarrow$` (dstmap1.type(), dstmap2.type()) ) are\n * supported:\n *\n * `$\\\\texttt{(CV_32FC1, CV_32FC1)} \\\\rightarrow \\\\texttt{(CV_16SC2, CV_16UC1)}$`. This is the most\n * frequently used conversion operation, in which the original floating-point maps (see remap ) are\n * converted to a more compact and much faster fixed-point representation. The first output array\n * contains the rounded coordinates and the second array (created only when nninterpolation=false )\n * contains indices in the interpolation tables.\n * `$\\\\texttt{(CV_32FC2)} \\\\rightarrow \\\\texttt{(CV_16SC2, CV_16UC1)}$`. The same as above but the\n * original maps are stored in one 2-channel matrix.\n * Reverse conversion. Obviously, the reconstructed floating-point maps will not be exactly the same as\n * the originals.\n *\n * [remap], [undistort], [initUndistortRectifyMap]\n *\n * @param map1 The first input map of type CV_16SC2, CV_32FC1, or CV_32FC2 .\n *\n * @param map2 The second input map of type CV_16UC1, CV_32FC1, or none (empty matrix), respectively.\n *\n * @param dstmap1 The first output map that has the type dstmap1type and the same size as src .\n *\n * @param dstmap2 The second output map.\n *\n * @param dstmap1type Type of the first output map that should be CV_16SC2, CV_32FC1, or CV_32FC2 .\n *\n * @param nninterpolation Flag indicating whether the fixed-point maps are used for the\n * nearest-neighbor or for a more complex interpolation.\n */\nexport declare function convertMaps(map1: InputArray, map2: InputArray, dstmap1: OutputArray, dstmap2: OutputArray, dstmap1type: int, nninterpolation?: bool): void;\n/**\n * The function calculates the `$2 \\\\times 3$` matrix of an affine transform so that:\n *\n * `\\\\[\\\\begin{bmatrix} x'_i \\\\\\\\ y'_i \\\\end{bmatrix} = \\\\texttt{map_matrix} \\\\cdot \\\\begin{bmatrix}\n * x_i \\\\\\\\ y_i \\\\\\\\ 1 \\\\end{bmatrix}\\\\]`\n *\n * where\n *\n * `\\\\[dst(i)=(x'_i,y'_i), src(i)=(x_i, y_i), i=0,1,2\\\\]`\n *\n * [warpAffine], [transform]\n *\n * @param src Coordinates of triangle vertices in the source image.\n *\n * @param dst Coordinates of the corresponding triangle vertices in the destination image.\n */\nexport declare function getAffineTransform(src: any, dst: any): Mat;\nexport declare function getAffineTransform(src: InputArray, dst: InputArray): Mat;\n/**\n * The function calculates the `$3 \\\\times 3$` matrix of a perspective transform so that:\n *\n * `\\\\[\\\\begin{bmatrix} t_i x'_i \\\\\\\\ t_i y'_i \\\\\\\\ t_i \\\\end{bmatrix} = \\\\texttt{map_matrix} \\\\cdot\n * \\\\begin{bmatrix} x_i \\\\\\\\ y_i \\\\\\\\ 1 \\\\end{bmatrix}\\\\]`\n *\n * where\n *\n * `\\\\[dst(i)=(x'_i,y'_i), src(i)=(x_i, y_i), i=0,1,2,3\\\\]`\n *\n * [findHomography], [warpPerspective], [perspectiveTransform]\n *\n * @param src Coordinates of quadrangle vertices in the source image.\n *\n * @param dst Coordinates of the corresponding quadrangle vertices in the destination image.\n *\n * @param solveMethod method passed to cv::solve (DecompTypes)\n */\nexport declare function getPerspectiveTransform(src: InputArray, dst: InputArray, solveMethod?: int): Mat;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function getPerspectiveTransform(src: any, dst: any, solveMethod?: int): Mat;\n/**\n * The function getRectSubPix extracts pixels from src:\n *\n * `\\\\[patch(x, y) = src(x + \\\\texttt{center.x} - ( \\\\texttt{dst.cols} -1)*0.5, y + \\\\texttt{center.y}\n * - ( \\\\texttt{dst.rows} -1)*0.5)\\\\]`\n *\n * where the values of the pixels at non-integer coordinates are retrieved using bilinear\n * interpolation. Every channel of multi-channel images is processed independently. Also the image\n * should be a single channel or three channel image. While the center of the rectangle must be inside\n * the image, parts of the rectangle may be outside.\n *\n * [warpAffine], [warpPerspective]\n *\n * @param image Source image.\n *\n * @param patchSize Size of the extracted patch.\n *\n * @param center Floating point coordinates of the center of the extracted rectangle within the source\n * image. The center must be inside the image.\n *\n * @param patch Extracted patch that has the size patchSize and the same number of channels as src .\n *\n * @param patchType Depth of the extracted pixels. By default, they have the same depth as src .\n */\nexport declare function getRectSubPix(image: InputArray, patchSize: Size, center: Point2f, patch: OutputArray, patchType?: int): void;\n/**\n * The function calculates the following matrix:\n *\n * `\\\\[\\\\begin{bmatrix} \\\\alpha & \\\\beta & (1- \\\\alpha ) \\\\cdot \\\\texttt{center.x} - \\\\beta \\\\cdot\n * \\\\texttt{center.y} \\\\\\\\ - \\\\beta & \\\\alpha & \\\\beta \\\\cdot \\\\texttt{center.x} + (1- \\\\alpha ) \\\\cdot\n * \\\\texttt{center.y} \\\\end{bmatrix}\\\\]`\n *\n * where\n *\n * `\\\\[\\\\begin{array}{l} \\\\alpha = \\\\texttt{scale} \\\\cdot \\\\cos \\\\texttt{angle} , \\\\\\\\ \\\\beta =\n * \\\\texttt{scale} \\\\cdot \\\\sin \\\\texttt{angle} \\\\end{array}\\\\]`\n *\n * The transformation maps the rotation center to itself. If this is not the target, adjust the shift.\n *\n * [getAffineTransform], [warpAffine], [transform]\n *\n * @param center Center of the rotation in the source image.\n *\n * @param angle Rotation angle in degrees. Positive values mean counter-clockwise rotation (the\n * coordinate origin is assumed to be the top-left corner).\n *\n * @param scale Isotropic scale factor.\n */\nexport declare function getRotationMatrix2D(center: Point2f, angle: double, scale: double): Mat;\n/**\n * The function computes an inverse affine transformation represented by `$2 \\\\times 3$` matrix M:\n *\n * `\\\\[\\\\begin{bmatrix} a_{11} & a_{12} & b_1 \\\\\\\\ a_{21} & a_{22} & b_2 \\\\end{bmatrix}\\\\]`\n *\n * The result is also a `$2 \\\\times 3$` matrix of the same type as M.\n *\n * @param M Original affine transformation.\n *\n * @param iM Output reverse affine transformation.\n */\nexport declare function invertAffineTransform(M: InputArray, iM: OutputArray): void;\nexport declare function linearPolar(src: InputArray, dst: OutputArray, center: Point2f, maxRadius: double, flags: int): void;\nexport declare function logPolar(src: InputArray, dst: OutputArray, center: Point2f, M: double, flags: int): void;\n/**\n * The function remap transforms the source image using the specified map:\n *\n * `\\\\[\\\\texttt{dst} (x,y) = \\\\texttt{src} (map_x(x,y),map_y(x,y))\\\\]`\n *\n * where values of pixels with non-integer coordinates are computed using one of available\n * interpolation methods. `$map_x$` and `$map_y$` can be encoded as separate floating-point maps in\n * `$map_1$` and `$map_2$` respectively, or interleaved floating-point maps of `$(x,y)$` in `$map_1$`,\n * or fixed-point maps created by using convertMaps. The reason you might want to convert from floating\n * to fixed-point representations of a map is that they can yield much faster (2x) remapping\n * operations. In the converted case, `$map_1$` contains pairs (cvFloor(x), cvFloor(y)) and `$map_2$`\n * contains indices in a table of interpolation coefficients.\n *\n * This function cannot operate in-place.\n *\n * Due to current implementation limitations the size of an input and output images should be less than\n * 32767x32767.\n *\n * @param src Source image.\n *\n * @param dst Destination image. It has the same size as map1 and the same type as src .\n *\n * @param map1 The first map of either (x,y) points or just x values having the type CV_16SC2 ,\n * CV_32FC1, or CV_32FC2. See convertMaps for details on converting a floating point representation to\n * fixed-point for speed.\n *\n * @param map2 The second map of y values having the type CV_16UC1, CV_32FC1, or none (empty map if\n * map1 is (x,y) points), respectively.\n *\n * @param interpolation Interpolation method (see InterpolationFlags). The method INTER_AREA is not\n * supported by this function.\n *\n * @param borderMode Pixel extrapolation method (see BorderTypes). When borderMode=BORDER_TRANSPARENT,\n * it means that the pixels in the destination image that corresponds to the \"outliers\" in the source\n * image are not modified by the function.\n *\n * @param borderValue Value used in case of a constant border. By default, it is 0.\n */\nexport declare function remap(src: InputArray, dst: OutputArray, map1: InputArray, map2: InputArray, interpolation: int, borderMode?: int, borderValue?: any): void;\n/**\n * The function resize resizes the image src down to or up to the specified size. Note that the initial\n * dst type or size are not taken into account. Instead, the size and type are derived from the\n * `src`,`dsize`,`fx`, and `fy`. If you want to resize src so that it fits the pre-created dst, you may\n * call the function as follows:\n *\n * ```cpp\n * // explicitly specify dsize=dst.size(); fx and fy will be computed from that.\n * resize(src, dst, dst.size(), 0, 0, interpolation);\n * ```\n *\n *  If you want to decimate the image by factor of 2 in each direction, you can call the function this\n * way:\n *\n * ```cpp\n * // specify fx and fy and let the function compute the destination image size.\n * resize(src, dst, Size(), 0.5, 0.5, interpolation);\n * ```\n *\n *  To shrink an image, it will generally look best with [INTER_AREA] interpolation, whereas to enlarge\n * an image, it will generally look best with c::INTER_CUBIC (slow) or [INTER_LINEAR] (faster but still\n * looks OK).\n *\n * [warpAffine], [warpPerspective], [remap]\n *\n * @param src input image.\n *\n * @param dst output image; it has the size dsize (when it is non-zero) or the size computed from\n * src.size(), fx, and fy; the type of dst is the same as of src.\n *\n * @param dsize output image size; if it equals zero, it is computed as: \\[\\texttt{dsize =\n * Size(round(fx*src.cols), round(fy*src.rows))}\\] Either dsize or both fx and fy must be non-zero.\n *\n * @param fx scale factor along the horizontal axis; when it equals 0, it is computed as\n * \\[\\texttt{(double)dsize.width/src.cols}\\]\n *\n * @param fy scale factor along the vertical axis; when it equals 0, it is computed as\n * \\[\\texttt{(double)dsize.height/src.rows}\\]\n *\n * @param interpolation interpolation method, see InterpolationFlags\n */\nexport declare function resize(src: InputArray, dst: OutputArray, dsize: Size, fx?: double, fy?: double, interpolation?: int): void;\n/**\n * The function warpAffine transforms the source image using the specified matrix:\n *\n * `\\\\[\\\\texttt{dst} (x,y) = \\\\texttt{src} ( \\\\texttt{M} _{11} x + \\\\texttt{M} _{12} y + \\\\texttt{M}\n * _{13}, \\\\texttt{M} _{21} x + \\\\texttt{M} _{22} y + \\\\texttt{M} _{23})\\\\]`\n *\n * when the flag [WARP_INVERSE_MAP] is set. Otherwise, the transformation is first inverted with\n * [invertAffineTransform] and then put in the formula above instead of M. The function cannot operate\n * in-place.\n *\n * [warpPerspective], [resize], [remap], [getRectSubPix], [transform]\n *\n * @param src input image.\n *\n * @param dst output image that has the size dsize and the same type as src .\n *\n * @param M $2\\times 3$ transformation matrix.\n *\n * @param dsize size of the output image.\n *\n * @param flags combination of interpolation methods (see InterpolationFlags) and the optional flag\n * WARP_INVERSE_MAP that means that M is the inverse transformation (\n * $\\texttt{dst}\\rightarrow\\texttt{src}$ ).\n *\n * @param borderMode pixel extrapolation method (see BorderTypes); when borderMode=BORDER_TRANSPARENT,\n * it means that the pixels in the destination image corresponding to the \"outliers\" in the source\n * image are not modified by the function.\n *\n * @param borderValue value used in case of a constant border; by default, it is 0.\n */\nexport declare function warpAffine(src: InputArray, dst: OutputArray, M: InputArray, dsize: Size, flags?: int, borderMode?: int, borderValue?: any): void;\n/**\n * The function warpPerspective transforms the source image using the specified matrix:\n *\n * `\\\\[\\\\texttt{dst} (x,y) = \\\\texttt{src} \\\\left ( \\\\frac{M_{11} x + M_{12} y + M_{13}}{M_{31} x +\n * M_{32} y + M_{33}} , \\\\frac{M_{21} x + M_{22} y + M_{23}}{M_{31} x + M_{32} y + M_{33}} \\\\right\n * )\\\\]`\n *\n * when the flag [WARP_INVERSE_MAP] is set. Otherwise, the transformation is first inverted with invert\n * and then put in the formula above instead of M. The function cannot operate in-place.\n *\n * [warpAffine], [resize], [remap], [getRectSubPix], [perspectiveTransform]\n *\n * @param src input image.\n *\n * @param dst output image that has the size dsize and the same type as src .\n *\n * @param M $3\\times 3$ transformation matrix.\n *\n * @param dsize size of the output image.\n *\n * @param flags combination of interpolation methods (INTER_LINEAR or INTER_NEAREST) and the optional\n * flag WARP_INVERSE_MAP, that sets M as the inverse transformation (\n * $\\texttt{dst}\\rightarrow\\texttt{src}$ ).\n *\n * @param borderMode pixel extrapolation method (BORDER_CONSTANT or BORDER_REPLICATE).\n *\n * @param borderValue value used in case of a constant border; by default, it equals 0.\n */\nexport declare function warpPerspective(src: InputArray, dst: OutputArray, M: InputArray, dsize: Size, flags?: int, borderMode?: int, borderValue?: any): void;\n/**\n * <a name=\"da/d54/group__imgproc__transform_1polar_remaps_reference_image\"></a>\n *  Transform the source image using the following transformation: `\\\\[ dst(\\\\rho , \\\\phi ) = src(x,y)\n * \\\\]`\n *\n * where `\\\\[ \\\\begin{array}{l} \\\\vec{I} = (x - center.x, \\\\;y - center.y) \\\\\\\\ \\\\phi = Kangle \\\\cdot\n * \\\\texttt{angle} (\\\\vec{I}) \\\\\\\\ \\\\rho = \\\\left\\\\{\\\\begin{matrix} Klin \\\\cdot \\\\texttt{magnitude}\n * (\\\\vec{I}) & default \\\\\\\\ Klog \\\\cdot log_e(\\\\texttt{magnitude} (\\\\vec{I})) & if \\\\; semilog \\\\\\\\\n * \\\\end{matrix}\\\\right. \\\\end{array} \\\\]`\n *\n * and `\\\\[ \\\\begin{array}{l} Kangle = dsize.height / 2\\\\Pi \\\\\\\\ Klin = dsize.width / maxRadius \\\\\\\\\n * Klog = dsize.width / log_e(maxRadius) \\\\\\\\ \\\\end{array} \\\\]`\n *\n * Polar mapping can be linear or semi-log. Add one of [WarpPolarMode] to `flags` to specify the polar\n * mapping mode.\n *\n * Linear is the default mode.\n *\n * The semilog mapping emulates the human \"foveal\" vision that permit very high acuity on the line of\n * sight (central vision) in contrast to peripheral vision where acuity is minor.\n *\n * if both values in `dsize <=0` (default), the destination image will have (almost) same area of\n * source bounding circle: `\\\\[\\\\begin{array}{l} dsize.area \\\\leftarrow (maxRadius^2 \\\\cdot \\\\Pi) \\\\\\\\\n * dsize.width = \\\\texttt{cvRound}(maxRadius) \\\\\\\\ dsize.height = \\\\texttt{cvRound}(maxRadius \\\\cdot\n * \\\\Pi) \\\\\\\\ \\\\end{array}\\\\]`\n * if only `dsize.height <= 0`, the destination image area will be proportional to the bounding circle\n * area but scaled by `Kx * Kx`: `\\\\[\\\\begin{array}{l} dsize.height = \\\\texttt{cvRound}(dsize.width\n * \\\\cdot \\\\Pi) \\\\\\\\ \\\\end{array} \\\\]`\n * if both values in `dsize > 0`, the destination image will have the given size therefore the area of\n * the bounding circle will be scaled to `dsize`.\n *\n * You can get reverse mapping adding [WARP_INVERSE_MAP] to `flags`\n *\n * ```cpp\n *         // direct transform\n *         warpPolar(src, lin_polar_img, Size(),center, maxRadius, flags);                     //\n * linear Polar\n *         warpPolar(src, log_polar_img, Size(),center, maxRadius, flags + WARP_POLAR_LOG);    //\n * semilog Polar\n *         // inverse transform\n *         warpPolar(lin_polar_img, recovered_lin_polar_img, src.size(), center, maxRadius, flags +\n * WARP_INVERSE_MAP);\n *         warpPolar(log_polar_img, recovered_log_polar, src.size(), center, maxRadius, flags +\n * WARP_POLAR_LOG + WARP_INVERSE_MAP);\n * ```\n *\n *  In addiction, to calculate the original coordinate from a polar mapped coordinate `$(rho, phi)->(x,\n * y)$`:\n *\n * ```cpp\n *         double angleRad, magnitude;\n *         double Kangle = dst.rows / CV_2PI;\n *         angleRad = phi / Kangle;\n *         if (flags & WARP_POLAR_LOG)\n *         {\n *             double Klog = dst.cols / std::log(maxRadius);\n *             magnitude = std::exp(rho / Klog);\n *         }\n *         else\n *         {\n *             double Klin = dst.cols / maxRadius;\n *             magnitude = rho / Klin;\n *         }\n *         int x = cvRound(center.x + magnitude * cos(angleRad));\n *         int y = cvRound(center.y + magnitude * sin(angleRad));\n * ```\n *\n * The function can not operate in-place.\n * To calculate magnitude and angle in degrees [cartToPolar] is used internally thus angles are\n * measured from 0 to 360 with accuracy about 0.3 degrees.\n * This function uses [remap]. Due to current implementation limitations the size of an input and\n * output images should be less than 32767x32767.\n *\n * [cv::remap]\n *\n * @param src Source image.\n *\n * @param dst Destination image. It will have same type as src.\n *\n * @param dsize The destination image size (see description for valid options).\n *\n * @param center The transformation center.\n *\n * @param maxRadius The radius of the bounding circle to transform. It determines the inverse magnitude\n * scale parameter too.\n *\n * @param flags A combination of interpolation methods, InterpolationFlags + WarpPolarMode.\n * Add WARP_POLAR_LINEAR to select linear polar mapping (default)Add WARP_POLAR_LOG to select semilog\n * polar mappingAdd WARP_INVERSE_MAP for reverse mapping.\n */\nexport declare function warpPolar(src: InputArray, dst: OutputArray, dsize: Size, center: Point2f, maxRadius: double, flags: int): void;\n/**\n * nearest neighbor interpolation\n *\n */\nexport declare const INTER_NEAREST: InterpolationFlags;\n/**\n * bilinear interpolation\n *\n */\nexport declare const INTER_LINEAR: InterpolationFlags;\n/**\n * bicubic interpolation\n *\n */\nexport declare const INTER_CUBIC: InterpolationFlags;\n/**\n * resampling using pixel area relation. It may be a preferred method for image decimation, as it gives\n * moire'-free results. But when the image is zoomed, it is similar to the INTER_NEAREST method.\n *\n */\nexport declare const INTER_AREA: InterpolationFlags;\n/**\n * Lanczos interpolation over 8x8 neighborhood\n *\n */\nexport declare const INTER_LANCZOS4: InterpolationFlags;\n/**\n * Bit exact bilinear interpolation\n *\n */\nexport declare const INTER_LINEAR_EXACT: InterpolationFlags;\n/**\n * mask for interpolation codes\n *\n */\nexport declare const INTER_MAX: InterpolationFlags;\n/**\n * flag, fills all of the destination image pixels. If some of them correspond to outliers in the\n * source image, they are set to zero\n *\n */\nexport declare const WARP_FILL_OUTLIERS: InterpolationFlags;\n/**\n * flag, inverse transformation\n *\n * For example, [linearPolar] or [logPolar] transforms:\n *\n * flag is **not** set: `$dst( \\\\rho , \\\\phi ) = src(x,y)$`\n * flag is set: `$dst(x,y) = src( \\\\rho , \\\\phi )$`\n *\n */\nexport declare const WARP_INVERSE_MAP: InterpolationFlags;\nexport declare const INTER_BITS: InterpolationMasks;\nexport declare const INTER_BITS2: InterpolationMasks;\nexport declare const INTER_TAB_SIZE: InterpolationMasks;\nexport declare const INTER_TAB_SIZE2: InterpolationMasks;\nexport declare const WARP_POLAR_LINEAR: WarpPolarMode;\nexport declare const WARP_POLAR_LOG: WarpPolarMode;\nexport declare type InterpolationFlags = any;\nexport declare type InterpolationMasks = any;\nexport declare type WarpPolarMode = any;\n"},"node_modules_mirada_dist_src_types_opencv_Logger_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_Logger_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/Logger.d.ts","content":"import { int } from './_types';\nexport declare class Logger {\n    static error(fmt: any, arg121: any): int;\n    static fatal(fmt: any, arg122: any): int;\n    static info(fmt: any, arg123: any): int;\n    /**\n     *   Print log message\n     *\n     * @param level Log level\n     *\n     * @param fmt Message format\n     */\n    static log(level: int, fmt: any, arg124: any): int;\n    /**\n     *   Sets the logging destination\n     *\n     * @param name Filename or NULL for console\n     */\n    static setDestination(name: any): void;\n    /**\n     *   Sets the logging level. All messages with lower priority will be ignored.\n     *\n     * @param level Logging level\n     */\n    static setLevel(level: int): void;\n    static warn(fmt: any, arg125: any): int;\n}\n"},"node_modules_mirada_dist_src_types_opencv_LshTable_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_LshTable_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/LshTable.d.ts","content":"import { Bucket, BucketKey, LshStats, Matrix, size_t } from './_types';\n/**\n * Lsh hash table. As its key is a sub-feature, and as usually the size of it is pretty small, we keep\n * it as a continuous memory array. The value is an index in the corpus of features (we keep it as an\n * unsigned int for pure memory reasons, it could be a size_t)\n *\n * Source:\n * [opencv2/flann/lsh_table.h](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/flann/lsh_table.h#L261).\n *\n */\nexport declare class LshTable {\n    /**\n     *   Default constructor\n     */\n    constructor();\n    /**\n     *   Default constructor Create the mask and allocate the memory\n     *\n     * @param feature_size is the size of the feature (considered as a ElementType[])\n     *\n     * @param key_size is the number of bits that are turned on in the feature\n     */\n    constructor(feature_size: any, key_size: any);\n    constructor(feature_size: any, subsignature_size: any);\n    /**\n     *   Add a feature to the table\n     *\n     * @param value the value to store for that feature\n     *\n     * @param feature the feature itself\n     */\n    add(value: any, feature: any): void;\n    /**\n     *   Add a set of features to the table\n     *\n     * @param dataset the values to store\n     */\n    add(dataset: Matrix): Matrix;\n    /**\n     *   Get a bucket given the key\n     */\n    getBucketFromKey(key: BucketKey): Bucket;\n    /**\n     *   Compute the sub-signature of a feature\n     */\n    getKey(arg50: any): size_t;\n    /**\n     *   Return the Subsignature of a feature\n     *\n     * @param feature the feature to analyze\n     */\n    getKey(feature: any): size_t;\n    /**\n     *   Get statistics about the table\n     */\n    getStats(): LshStats;\n    getStats(): LshStats;\n}\nexport declare const kArray: SpeedLevel;\nexport declare const kBitsetHash: SpeedLevel;\nexport declare const kHash: SpeedLevel;\n/**\n * defines the speed fo the implementation kArray uses a vector for storing data kBitsetHash uses a\n * hash map but checks for the validity of a key with a bitset kHash uses a hash map only\n *\n */\nexport declare type SpeedLevel = any;\n"},"node_modules_mirada_dist_src_types_opencv_MatExpr_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_MatExpr_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/MatExpr.d.ts","content":"import { double, int, Mat, MatOp, Scalar } from './_types';\n/**\n * <a name=\"d1/d10/classcv_1_1MatExpr_1MatrixExpressions\"></a>This is a list of implemented matrix\n * operations that can be combined in arbitrary complex expressions (here A, B stand for matrices (\n * [Mat](#d3/d63/classcv_1_1Mat}) ), s for a scalar ( Scalar ), alpha for a real-valued scalar ( double\n * )):\n *\n * Addition, subtraction, negation: `A+B`, `A-B`, `A+s`, `A-s`, `s+A`, `s-A`, `-A`\n * Scaling: `A*alpha`\n * Per-element multiplication and division: `A.mul(B)`, `A/B`, `alpha/A`\n * Matrix multiplication: `A*B`\n * Transposition: `A.t()` (means A)\n * Matrix inversion and pseudo-inversion, solving linear systems and least-squares problems:\n * `A.inv([method]) (~ A<sup>-1</sup>)`, `A.inv([method])*B (~ X: AX=B)`\n * Comparison: `A cmpop B`, `A cmpop alpha`, `alpha cmpop A`, where *cmpop* is one of `>`, `>=`, `==`,\n * `!=`, `<=`, `<`. The result of comparison is an 8-bit single channel mask whose elements are set to\n * 255 (if the particular element or pair of elements satisfy the condition) or 0.\n * Bitwise logical operations: `A logicop B`, `A logicop s`, `s logicop A`, `~A`, where *logicop* is\n * one of `&`, `|`, `^`.\n * Element-wise minimum and maximum: `min(A, B)`, `min(A, alpha)`, `max(A, B)`, `max(A, alpha)`\n * Element-wise absolute value: `abs(A)`\n * Cross-product, dot-product: `A.cross(B)`, `A.dot(B)`\n * Any function of matrix or matrices and scalars that returns a matrix or a scalar, such as norm,\n * mean, sum, countNonZero, trace, determinant, repeat, and others.\n * Matrix initializers ( [Mat::eye()](#d3/d63/classcv_1_1Mat_1a2cf9b9acde7a9852542bbc20ef851ed2}),\n * [Mat::zeros()](#d3/d63/classcv_1_1Mat_1a0b57b6a326c8876d944d188a46e0f556}),\n * [Mat::ones()](#d3/d63/classcv_1_1Mat_1a69ae0402d116fc9c71908d8508dc2f09}) ), matrix comma-separated\n * initializers, matrix constructors and operators that extract sub-matrices (see\n * [Mat](#d3/d63/classcv_1_1Mat}) description).\n * Mat_<destination_type>() constructors to cast the result to the proper type.\n *\n * Comma-separated initializers and probably some other operations may require additional explicit\n * Mat() or Mat_<T>() constructor calls to resolve a possible ambiguity.\n * Here are examples of matrix expressions:\n *\n * ```cpp\n * // compute pseudo-inverse of A, equivalent to A.inv(DECOMP_SVD)\n * SVD svd(A);\n * Mat pinvA = svd.vt.t()*Mat::diag(1./svd.w)*svd.u.t();\n *\n * // compute the new vector of parameters in the Levenberg-Marquardt algorithm\n * x -= (A.t()*A + lambda*Mat::eye(A.cols,A.cols,A.type())).inv(DECOMP_CHOLESKY)*(A.t()*err);\n *\n * // sharpen image using \"unsharp mask\" algorithm\n * Mat blurred; double sigma = 1, threshold = 5, amount = 1;\n * GaussianBlur(img, blurred, Size(), sigma, sigma);\n * Mat lowContrastMask = abs(img - blurred) < threshold;\n * Mat sharpened = img*(1+amount) + blurred*(-amount);\n * img.copyTo(sharpened, lowContrastMask);\n * ```\n *\n * Source:\n * [opencv2/core/mat.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/core/mat.hpp#L3557).\n *\n */\nexport declare class MatExpr extends Mat {\n    a: Mat;\n    alpha: double;\n    b: Mat;\n    beta: double;\n    c: Mat;\n    flags: int;\n    op: MatOp;\n    s: Scalar;\n    constructor();\n    constructor(m: Mat);\n    constructor(_op: MatOp, _flags: int, _a?: Mat, _b?: Mat, _c?: Mat, _alpha?: double, _beta?: double, _s?: Scalar);\n    col(x: int): MatExpr;\n    cross(m: Mat): Mat;\n    diag(d?: int): MatExpr;\n    dot(m: Mat): Mat;\n    inv(method?: int): MatExpr;\n    mul(e: MatExpr, scale?: double): MatExpr;\n    mul(m: Mat, scale?: double): MatExpr;\n    row(y: int): MatExpr;\n    t(): MatExpr;\n    type(): int;\n}\n"},"node_modules_mirada_dist_src_types_opencv_MatOp_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_MatOp_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/MatOp.d.ts","content":"import { double, int, Mat, MatExpr, Scalar, Size } from './_types';\nexport declare class MatOp {\n    constructor();\n    abs(expr: MatExpr, res: MatExpr): MatExpr;\n    add(expr1: MatExpr, expr2: MatExpr, res: MatExpr): MatExpr;\n    add(expr1: MatExpr, s: Scalar, res: MatExpr): MatExpr;\n    assign(expr: MatExpr, m: Mat, type?: int): MatExpr;\n    augAssignAdd(expr: MatExpr, m: Mat): MatExpr;\n    augAssignAnd(expr: MatExpr, m: Mat): MatExpr;\n    augAssignDivide(expr: MatExpr, m: Mat): MatExpr;\n    augAssignMultiply(expr: MatExpr, m: Mat): MatExpr;\n    augAssignOr(expr: MatExpr, m: Mat): MatExpr;\n    augAssignSubtract(expr: MatExpr, m: Mat): MatExpr;\n    augAssignXor(expr: MatExpr, m: Mat): MatExpr;\n    diag(expr: MatExpr, d: int, res: MatExpr): MatExpr;\n    divide(expr1: MatExpr, expr2: MatExpr, res: MatExpr, scale?: double): MatExpr;\n    divide(s: double, expr: MatExpr, res: MatExpr): MatExpr;\n    elementWise(expr: MatExpr): MatExpr;\n    invert(expr: MatExpr, method: int, res: MatExpr): MatExpr;\n    matmul(expr1: MatExpr, expr2: MatExpr, res: MatExpr): MatExpr;\n    multiply(expr1: MatExpr, expr2: MatExpr, res: MatExpr, scale?: double): MatExpr;\n    multiply(expr1: MatExpr, s: double, res: MatExpr): MatExpr;\n    roi(expr: MatExpr, rowRange: Range, colRange: Range, res: MatExpr): MatExpr;\n    size(expr: MatExpr): Size;\n    subtract(expr1: MatExpr, expr2: MatExpr, res: MatExpr): MatExpr;\n    subtract(s: Scalar, expr: MatExpr, res: MatExpr): Scalar;\n    transpose(expr: MatExpr, res: MatExpr): MatExpr;\n    type(expr: MatExpr): MatExpr;\n}\n"},"node_modules_mirada_dist_src_types_opencv_Matx_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_Matx_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/Matx.d.ts","content":"import { diag_type, int, Matx_AddOp, Matx_DivOp, Matx_MatMulOp, Matx_MulOp, Matx_ScaleOp, Matx_SubOp, Matx_TOp, Vec, _T2, _Tp } from './_types';\n/**\n * If you need a more flexible type, use [Mat](#d3/d63/classcv_1_1Mat}) . The elements of the matrix M\n * are accessible using the M(i,j) notation. Most of the common matrix operations (see also\n * [MatrixExpressions](#d1/d10/classcv_1_1MatExpr_1MatrixExpressions}) ) are available. To do an\n * operation on [Matx](#de/de1/classcv_1_1Matx}) that is not implemented, you can easily convert the\n * matrix to [Mat](#d3/d63/classcv_1_1Mat}) and backwards:\n *\n * ```cpp\n * Matx33f m(1, 2, 3,\n *           4, 5, 6,\n *           7, 8, 9);\n * cout << sum(Mat(m*m.t())) << endl;\n * ```\n *\n *  Except of the plain constructor which takes a list of elements, [Matx](#de/de1/classcv_1_1Matx})\n * can be initialized from a C-array:\n *\n * ```cpp\n * float values[] = { 1, 2, 3};\n * Matx31f m(values);\n * ```\n *\n *  In case if C++11 features are available, std::initializer_list can be also used to initialize\n * [Matx](#de/de1/classcv_1_1Matx}):\n *\n * ```cpp\n * Matx31f m = { 1, 2, 3};\n * ```\n *\n * Source:\n * [opencv2/core/matx.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/core/matx.hpp#L1185).\n *\n */\nexport declare class Matx {\n    val: _Tp;\n    constructor();\n    constructor(v0: _Tp);\n    constructor(v0: _Tp, v1: _Tp);\n    constructor(v0: _Tp, v1: _Tp, v2: _Tp);\n    constructor(v0: _Tp, v1: _Tp, v2: _Tp, v3: _Tp);\n    constructor(v0: _Tp, v1: _Tp, v2: _Tp, v3: _Tp, v4: _Tp);\n    constructor(v0: _Tp, v1: _Tp, v2: _Tp, v3: _Tp, v4: _Tp, v5: _Tp);\n    constructor(v0: _Tp, v1: _Tp, v2: _Tp, v3: _Tp, v4: _Tp, v5: _Tp, v6: _Tp);\n    constructor(v0: _Tp, v1: _Tp, v2: _Tp, v3: _Tp, v4: _Tp, v5: _Tp, v6: _Tp, v7: _Tp);\n    constructor(v0: _Tp, v1: _Tp, v2: _Tp, v3: _Tp, v4: _Tp, v5: _Tp, v6: _Tp, v7: _Tp, v8: _Tp);\n    constructor(v0: _Tp, v1: _Tp, v2: _Tp, v3: _Tp, v4: _Tp, v5: _Tp, v6: _Tp, v7: _Tp, v8: _Tp, v9: _Tp);\n    constructor(v0: _Tp, v1: _Tp, v2: _Tp, v3: _Tp, v4: _Tp, v5: _Tp, v6: _Tp, v7: _Tp, v8: _Tp, v9: _Tp, v10: _Tp, v11: _Tp);\n    constructor(v0: _Tp, v1: _Tp, v2: _Tp, v3: _Tp, v4: _Tp, v5: _Tp, v6: _Tp, v7: _Tp, v8: _Tp, v9: _Tp, v10: _Tp, v11: _Tp, v12: _Tp, v13: _Tp);\n    constructor(v0: _Tp, v1: _Tp, v2: _Tp, v3: _Tp, v4: _Tp, v5: _Tp, v6: _Tp, v7: _Tp, v8: _Tp, v9: _Tp, v10: _Tp, v11: _Tp, v12: _Tp, v13: _Tp, v14: _Tp, v15: _Tp);\n    constructor(vals: any);\n    constructor(arg334: any);\n    constructor(a: Matx, b: Matx, arg335: Matx_AddOp);\n    constructor(a: Matx, b: Matx, arg336: Matx_SubOp);\n    constructor(arg337: any, a: Matx, alpha: _T2, arg338: Matx_ScaleOp);\n    constructor(a: Matx, b: Matx, arg339: Matx_MulOp);\n    constructor(a: Matx, b: Matx, arg340: Matx_DivOp);\n    constructor(l: int, a: Matx, b: Matx, arg341: Matx_MatMulOp);\n    constructor(a: Matx, arg342: Matx_TOp);\n    col(i: int): Matx;\n    ddot(v: Matx): Matx;\n    diag(): diag_type;\n    div(a: Matx): Matx;\n    dot(v: Matx): Matx;\n    get_minor(m1: int, n1: int, base_row: int, base_col: int): Matx;\n    inv(method?: int, p_is_ok?: any): Matx;\n    mul(a: Matx): Matx;\n    reshape(m1: int, n1: int): Matx;\n    row(i: int): Matx;\n    solve(l: int, rhs: Matx, flags?: int): Matx;\n    solve(rhs: Vec, method: int): Vec;\n    t(): Matx;\n    static all(alpha: _Tp): Matx;\n    static diag(d: diag_type): Matx;\n    static eye(): Matx;\n    static ones(): Matx;\n    static randn(a: _Tp, b: _Tp): Matx;\n    static randu(a: _Tp, b: _Tp): Matx;\n    static zeros(): Matx;\n}\nexport declare const rows: any;\nexport declare const cols: any;\nexport declare const channels: any;\nexport declare const shortdim: any;\n"},"node_modules_mirada_dist_src_types_opencv_Node_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_Node_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/Node.d.ts","content":"import { double, int } from './_types';\nexport declare class Node {\n    /**\n     *   Class index normalized to 0..class_count-1 range and assigned to the node. It is used internally\n     * in classification trees and tree ensembles.\n     *\n     */\n    classIdx: int;\n    /**\n     *   Default direction where to go (-1: left or +1: right). It helps in the case of missing values.\n     *\n     */\n    defaultDir: int;\n    left: int;\n    parent: int;\n    right: int;\n    split: int;\n    /**\n     *   Value at the node: a class label in case of classification or estimated function value in case of\n     * regression.\n     *\n     */\n    value: double;\n    constructor();\n}\n"},"node_modules_mirada_dist_src_types_opencv_objdetect_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_objdetect_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/objdetect.d.ts","content":"import { double, int, Size } from './_types';\nexport declare function createFaceDetectionMaskGenerator(): any;\n/**\n * The function is a wrapper for the generic function partition . It clusters all the input rectangles\n * using the rectangle equivalence criteria that combines rectangles with similar sizes and similar\n * locations. The similarity is defined by eps. When eps=0 , no clustering is done at all. If\n * `$\\\\texttt{eps}\\\\rightarrow +\\\\inf$` , all the rectangles are put in one cluster. Then, the small\n * clusters containing less than or equal to groupThreshold rectangles are rejected. In each other\n * cluster, the average rectangle is computed and put into the output rectangle list.\n *\n * @param rectList Input/output vector of rectangles. Output vector includes retained and grouped\n * rectangles. (The Python list is not modified in place.)\n *\n * @param groupThreshold Minimum possible number of rectangles minus 1. The threshold is used in a\n * group of rectangles to retain it.\n *\n * @param eps Relative difference between sides of the rectangles to merge them into a group.\n */\nexport declare function groupRectangles(rectList: any, groupThreshold: int, eps?: double): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function groupRectangles(rectList: any, weights: any, groupThreshold: int, eps?: double): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function groupRectangles(rectList: any, groupThreshold: int, eps: double, weights: any, levelWeights: any): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function groupRectangles(rectList: any, rejectLevels: any, levelWeights: any, groupThreshold: int, eps?: double): void;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function groupRectangles_meanshift(rectList: any, foundWeights: any, foundScales: any, detectThreshold?: double, winDetSize?: Size): void;\nexport declare const CASCADE_DO_CANNY_PRUNING: any;\nexport declare const CASCADE_SCALE_IMAGE: any;\nexport declare const CASCADE_FIND_BIGGEST_OBJECT: any;\nexport declare const CASCADE_DO_ROUGH_SEARCH: any;\n"},"node_modules_mirada_dist_src_types_opencv_Mat_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_Mat_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/Mat.d.ts","content":"import { AccessFlag, bool, double, InputArray, int, MatAllocator, MatCommaInitializer_, MatConstIterator_, MatExpr, MatIterator_, MatSize, MatStep, Matx, Mat_, OutputArray, Point, Point3_, Point_, Rect, Scalar, Size, size_t, typename, uchar, UMat, UMatData, UMatUsageFlags, Vec } from './_types';\n/**\n * <a name=\"d3/d63/classcv_1_1Mat_1CVMat_Details\"></a> The class [Mat](#d3/d63/classcv_1_1Mat})\n * represents an n-dimensional dense numerical single-channel or multi-channel array. It can be used to\n * store real or complex-valued vectors and matrices, grayscale or color images, voxel volumes, vector\n * fields, point clouds, tensors, histograms (though, very high-dimensional histograms may be better\n * stored in a [SparseMat](#dd/da9/classcv_1_1SparseMat}) ). The data layout of the array `M` is\n * defined by the array `M.step[]`, so that the address of element `$(i_0,...,i_{M.dims-1})$`, where\n * `$0\\\\leq i_k<M.size[k]$`, is computed as: `\\\\[addr(M_{i_0,...,i_{M.dims-1}}) = M.data +\n * M.step[0]*i_0 + M.step[1]*i_1 + ... + M.step[M.dims-1]*i_{M.dims-1}\\\\]` In case of a 2-dimensional\n * array, the above formula is reduced to: `\\\\[addr(M_{i,j}) = M.data + M.step[0]*i + M.step[1]*j\\\\]`\n * Note that `M.step[i] >= M.step[i+1]` (in fact, `M.step[i] >= M.step[i+1]*M.size[i+1]` ). This means\n * that 2-dimensional matrices are stored row-by-row, 3-dimensional matrices are stored plane-by-plane,\n * and so on. M.step[M.dims-1] is minimal and always equal to the element size M.elemSize() .\n *\n * So, the data layout in [Mat](#d3/d63/classcv_1_1Mat}) is compatible with the majority of dense array\n * types from the standard toolkits and SDKs, such as Numpy (ndarray), Win32 (independent device\n * bitmaps), and others, that is, with any array that uses *steps* (or *strides*) to compute the\n * position of a pixel. Due to this compatibility, it is possible to make a\n * [Mat](#d3/d63/classcv_1_1Mat}) header for user-allocated data and process it in-place using OpenCV\n * functions.\n *\n * There are many different ways to create a [Mat](#d3/d63/classcv_1_1Mat}) object. The most popular\n * options are listed below:\n *\n * Use the create(nrows, ncols, type) method or the similar Mat(nrows, ncols, type[, fillValue])\n * constructor. A new array of the specified size and type is allocated. type has the same meaning as\n * in the cvCreateMat method. For example, CV_8UC1 means a 8-bit single-channel array, CV_32FC2 means a\n * 2-channel (complex) floating-point array, and so on.\n *\n * ```cpp\n * // make a 7x7 complex matrix filled with 1+3j.\n * Mat M(7,7,CV_32FC2,Scalar(1,3));\n * // and now turn M to a 100x60 15-channel 8-bit matrix.\n * // The old content will be deallocated\n * M.create(100,60,CV_8UC(15));\n * ```\n *\n *  As noted in the introduction to this chapter,\n * [create()](#d3/d63/classcv_1_1Mat_1a55ced2c8d844d683ea9a725c60037ad0}) allocates only a new array\n * when the shape or type of the current array are different from the specified ones.\n * Create a multi-dimensional array:\n *\n * ```cpp\n * // create a 100x100x100 8-bit array\n * int sz[] = {100, 100, 100};\n * Mat bigCube(3, sz, CV_8U, Scalar::all(0));\n * ```\n *\n *  It passes the number of dimensions =1 to the [Mat](#d3/d63/classcv_1_1Mat}) constructor but the\n * created array will be 2-dimensional with the number of columns set to 1. So,\n * [Mat::dims](#d3/d63/classcv_1_1Mat_1a39cf614aa52567e9a945cd2609bd767b}) is always >= 2 (can also be\n * 0 when the array is empty).\n * Use a copy constructor or assignment operator where there can be an array or expression on the right\n * side (see below). As noted in the introduction, the array assignment is an O(1) operation because it\n * only copies the header and increases the reference counter. The\n * [Mat::clone()](#d3/d63/classcv_1_1Mat_1adff2ea98da45eae0833e73582dd4a660}) method can be used to get\n * a full (deep) copy of the array when you need it.\n * Construct a header for a part of another array. It can be a single row, single column, several rows,\n * several columns, rectangular region in the array (called a *minor* in algebra) or a diagonal. Such\n * operations are also O(1) because the new header references the same data. You can actually modify a\n * part of the array using this feature, for example:\n *\n * ```cpp\n * // add the 5-th row, multiplied by 3 to the 3rd row\n * M.row(3) = M.row(3) + M.row(5)*3;\n * // now copy the 7-th column to the 1-st column\n * // M.col(1) = M.col(7); // this will not work\n * Mat M1 = M.col(1);\n * M.col(7).copyTo(M1);\n * // create a new 320x240 image\n * Mat img(Size(320,240),CV_8UC3);\n * // select a ROI\n * Mat roi(img, Rect(10,10,100,100));\n * // fill the ROI with (0,255,0) (which is green in RGB space);\n * // the original 320x240 image will be modified\n * roi = Scalar(0,255,0);\n * ```\n *\n *  Due to the additional datastart and dataend members, it is possible to compute a relative sub-array\n * position in the main *container* array using\n * [locateROI()](#d3/d63/classcv_1_1Mat_1a40b5b3371a9c2a4b2b8ce0c8068d7c96}):\n *\n * ```cpp\n * Mat A = Mat::eye(10, 10, CV_32S);\n * // extracts A columns, 1 (inclusive) to 3 (exclusive).\n * Mat B = A(Range::all(), Range(1, 3));\n * // extracts B rows, 5 (inclusive) to 9 (exclusive).\n * // that is, C \\\\~ A(Range(5, 9), Range(1, 3))\n * Mat C = B(Range(5, 9), Range::all());\n * Size size; Point ofs;\n * C.locateROI(size, ofs);\n * // size will be (width=10,height=10) and the ofs will be (x=1, y=5)\n * ```\n *\n *  As in case of whole matrices, if you need a deep copy, use the\n * `[clone()](#d3/d63/classcv_1_1Mat_1adff2ea98da45eae0833e73582dd4a660})` method of the extracted\n * sub-matrices.\n * Make a header for user-allocated data. It can be useful to do the following:\n *\n * Process \"foreign\" data using OpenCV (for example, when you implement a DirectShow* filter or a\n * processing module for gstreamer, and so on). For example:\n *\n * ```cpp\n * void process_video_frame(const unsigned char* pixels,\n *                          int width, int height, int step)\n * {\n *     Mat img(height, width, CV_8UC3, pixels, step);\n *     GaussianBlur(img, img, Size(7,7), 1.5, 1.5);\n * }\n * ```\n *\n * Quickly initialize small matrices and/or get a super-fast element access.\n *\n * ```cpp\n * double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}};\n * Mat M = Mat(3, 3, CV_64F, m).inv();\n * ```\n *\n * Use MATLAB-style array initializers,\n * [zeros()](#d3/d63/classcv_1_1Mat_1a0b57b6a326c8876d944d188a46e0f556}),\n * [ones()](#d3/d63/classcv_1_1Mat_1a69ae0402d116fc9c71908d8508dc2f09}),\n * [eye()](#d3/d63/classcv_1_1Mat_1a2cf9b9acde7a9852542bbc20ef851ed2}), for example:\n *\n * ```cpp\n * // create a double-precision identity matrix and add it to M.\n * M += Mat::eye(M.rows, M.cols, CV_64F);\n * ```\n *\n * Use a comma-separated initializer:\n *\n * ```cpp\n * // create a 3x3 double-precision identity matrix\n * Mat M = (Mat_<double>(3,3) << 1, 0, 0, 0, 1, 0, 0, 0, 1);\n * ```\n *\n *  With this approach, you first call a constructor of the [Mat](#d3/d63/classcv_1_1Mat}) class with\n * the proper parameters, and then you just put `<< operator` followed by comma-separated values that\n * can be constants, variables, expressions, and so on. Also, note the extra parentheses required to\n * avoid compilation errors.\n *\n * Once the array is created, it is automatically managed via a reference-counting mechanism. If the\n * array header is built on top of user-allocated data, you should handle the data by yourself. The\n * array data is deallocated when no one points to it. If you want to release the data pointed by a\n * array header before the array destructor is called, use\n * [Mat::release()](#d3/d63/classcv_1_1Mat_1ae48d4913285518e2c21a3457017e716e}).\n *\n * The next important thing to learn about the array class is element access. This manual already\n * described how to compute an address of each array element. Normally, you are not required to use the\n * formula directly in the code. If you know the array element type (which can be retrieved using the\n * method [Mat::type()](#d3/d63/classcv_1_1Mat_1af2d2652e552d7de635988f18a84b53e5}) ), you can access\n * the element `$M_{ij}$` of a 2-dimensional array as:\n *\n * ```cpp\n * M.at<double>(i,j) += 1.f;\n * ```\n *\n *  assuming that `M` is a double-precision floating-point array. There are several variants of the\n * method at for a different number of dimensions.\n *\n * If you need to process a whole row of a 2D array, the most efficient way is to get the pointer to\n * the row first, and then just use the plain C operator [] :\n *\n * ```cpp\n * // compute sum of positive matrix elements\n * // (assuming that M is a double-precision matrix)\n * double sum=0;\n * for(int i = 0; i < M.rows; i++)\n * {\n *     const double* Mi = M.ptr<double>(i);\n *     for(int j = 0; j < M.cols; j++)\n *         sum += std::max(Mi[j], 0.);\n * }\n * ```\n *\n *  Some operations, like the one above, do not actually depend on the array shape. They just process\n * elements of an array one by one (or elements from multiple arrays that have the same coordinates,\n * for example, array addition). Such operations are called *element-wise*. It makes sense to check\n * whether all the input/output arrays are continuous, namely, have no gaps at the end of each row. If\n * yes, process them as a long single row:\n *\n * ```cpp\n * // compute the sum of positive matrix elements, optimized variant\n * double sum=0;\n * int cols = M.cols, rows = M.rows;\n * if(M.isContinuous())\n * {\n *     cols *= rows;\n *     rows = 1;\n * }\n * for(int i = 0; i < rows; i++)\n * {\n *     const double* Mi = M.ptr<double>(i);\n *     for(int j = 0; j < cols; j++)\n *         sum += std::max(Mi[j], 0.);\n * }\n * ```\n *\n *  In case of the continuous matrix, the outer loop body is executed just once. So, the overhead is\n * smaller, which is especially noticeable in case of small matrices.\n *\n * Finally, there are STL-style iterators that are smart enough to skip gaps between successive rows:\n *\n * ```cpp\n * // compute sum of positive matrix elements, iterator-based variant\n * double sum=0;\n * MatConstIterator_<double> it = M.begin<double>(), it_end = M.end<double>();\n * for(; it != it_end; ++it)\n *     sum += std::max(*it, 0.);\n * ```\n *\n *  The matrix iterators are random-access iterators, so they can be passed to any STL algorithm,\n * including [std::sort()](#d2/de8/group__core__array_1ga45dd56da289494ce874be2324856898f}).\n *\n * Matrix Expressions and arithmetic see [MatExpr](#d1/d10/classcv_1_1MatExpr})\n *\n * Source:\n * [opencv2/core/mat.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/core/mat.hpp#L2073).\n *\n */\nexport declare class Mat extends Mat_ {\n    allocator: MatAllocator;\n    cols: int;\n    data: uchar;\n    dataend: uchar;\n    datalimit: uchar;\n    datastart: uchar;\n    dims: int;\n    /**\n     *   includes several bit-fields:\n     *\n     * the magic signature\n     * continuity flag\n     * depth\n     * number of channels\n     *\n     */\n    flags: int;\n    rows: int;\n    size: MatSize;\n    step: MatStep;\n    u: UMatData;\n    /**\n     *   These are various constructors that form a matrix. As noted in the AutomaticAllocation, often the\n     * default constructor is enough, and the proper matrix will be allocated by an OpenCV function. The\n     * constructed matrix can further be assigned to another matrix or matrix expression or can be\n     * allocated with [Mat::create] . In the former case, the old content is de-referenced.\n     */\n    constructor();\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param rows Number of rows in a 2D array.\n     *\n     * @param cols Number of columns in a 2D array.\n     *\n     * @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),\n     * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.\n     */\n    constructor(rows: int, cols: int, type: int);\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param size 2D array size: Size(cols, rows) . In the Size() constructor, the number of rows and\n     * the number of columns go in the reverse order.\n     *\n     * @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),\n     * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.\n     */\n    constructor(size: Size, type: int);\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param rows Number of rows in a 2D array.\n     *\n     * @param cols Number of columns in a 2D array.\n     *\n     * @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),\n     * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.\n     *\n     * @param s An optional value to initialize each matrix element with. To set all the matrix elements\n     * to the particular value after the construction, use the assignment operator Mat::operator=(const\n     * Scalar& value) .\n     */\n    constructor(rows: int, cols: int, type: int, s: Scalar);\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param size 2D array size: Size(cols, rows) . In the Size() constructor, the number of rows and\n     * the number of columns go in the reverse order.\n     *\n     * @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),\n     * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.\n     *\n     * @param s An optional value to initialize each matrix element with. To set all the matrix elements\n     * to the particular value after the construction, use the assignment operator Mat::operator=(const\n     * Scalar& value) .\n     */\n    constructor(size: Size, type: int, s: Scalar);\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param ndims Array dimensionality.\n     *\n     * @param sizes Array of integers specifying an n-dimensional array shape.\n     *\n     * @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),\n     * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.\n     */\n    constructor(ndims: int, sizes: any, type: int);\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param sizes Array of integers specifying an n-dimensional array shape.\n     *\n     * @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),\n     * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.\n     */\n    constructor(sizes: any, type: int);\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param ndims Array dimensionality.\n     *\n     * @param sizes Array of integers specifying an n-dimensional array shape.\n     *\n     * @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),\n     * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.\n     *\n     * @param s An optional value to initialize each matrix element with. To set all the matrix elements\n     * to the particular value after the construction, use the assignment operator Mat::operator=(const\n     * Scalar& value) .\n     */\n    constructor(ndims: int, sizes: any, type: int, s: Scalar);\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param sizes Array of integers specifying an n-dimensional array shape.\n     *\n     * @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),\n     * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.\n     *\n     * @param s An optional value to initialize each matrix element with. To set all the matrix elements\n     * to the particular value after the construction, use the assignment operator Mat::operator=(const\n     * Scalar& value) .\n     */\n    constructor(sizes: any, type: int, s: Scalar);\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param m Array that (as a whole or partly) is assigned to the constructed matrix. No data is\n     * copied by these constructors. Instead, the header pointing to m data or its sub-array is constructed\n     * and associated with it. The reference counter, if any, is incremented. So, when you modify the\n     * matrix formed using such a constructor, you also modify the corresponding elements of m . If you\n     * want to have an independent copy of the sub-array, use Mat::clone() .\n     */\n    constructor(m: Mat);\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param rows Number of rows in a 2D array.\n     *\n     * @param cols Number of columns in a 2D array.\n     *\n     * @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),\n     * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.\n     *\n     * @param data Pointer to the user data. Matrix constructors that take data and step parameters do\n     * not allocate matrix data. Instead, they just initialize the matrix header that points to the\n     * specified data, which means that no data is copied. This operation is very efficient and can be used\n     * to process external data using OpenCV functions. The external data is not automatically deallocated,\n     * so you should take care of it.\n     *\n     * @param step Number of bytes each matrix row occupies. The value should include the padding bytes\n     * at the end of each row, if any. If the parameter is missing (set to AUTO_STEP ), no padding is\n     * assumed and the actual step is calculated as cols*elemSize(). See Mat::elemSize.\n     */\n    constructor(rows: int, cols: int, type: int, data: any, step?: size_t);\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param size 2D array size: Size(cols, rows) . In the Size() constructor, the number of rows and\n     * the number of columns go in the reverse order.\n     *\n     * @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),\n     * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.\n     *\n     * @param data Pointer to the user data. Matrix constructors that take data and step parameters do\n     * not allocate matrix data. Instead, they just initialize the matrix header that points to the\n     * specified data, which means that no data is copied. This operation is very efficient and can be used\n     * to process external data using OpenCV functions. The external data is not automatically deallocated,\n     * so you should take care of it.\n     *\n     * @param step Number of bytes each matrix row occupies. The value should include the padding bytes\n     * at the end of each row, if any. If the parameter is missing (set to AUTO_STEP ), no padding is\n     * assumed and the actual step is calculated as cols*elemSize(). See Mat::elemSize.\n     */\n    constructor(size: Size, type: int, data: any, step?: size_t);\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param ndims Array dimensionality.\n     *\n     * @param sizes Array of integers specifying an n-dimensional array shape.\n     *\n     * @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),\n     * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.\n     *\n     * @param data Pointer to the user data. Matrix constructors that take data and step parameters do\n     * not allocate matrix data. Instead, they just initialize the matrix header that points to the\n     * specified data, which means that no data is copied. This operation is very efficient and can be used\n     * to process external data using OpenCV functions. The external data is not automatically deallocated,\n     * so you should take care of it.\n     *\n     * @param steps Array of ndims-1 steps in case of a multi-dimensional array (the last step is always\n     * set to the element size). If not specified, the matrix is assumed to be continuous.\n     */\n    constructor(ndims: int, sizes: any, type: int, data: any, steps?: any);\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param sizes Array of integers specifying an n-dimensional array shape.\n     *\n     * @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),\n     * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.\n     *\n     * @param data Pointer to the user data. Matrix constructors that take data and step parameters do\n     * not allocate matrix data. Instead, they just initialize the matrix header that points to the\n     * specified data, which means that no data is copied. This operation is very efficient and can be used\n     * to process external data using OpenCV functions. The external data is not automatically deallocated,\n     * so you should take care of it.\n     *\n     * @param steps Array of ndims-1 steps in case of a multi-dimensional array (the last step is always\n     * set to the element size). If not specified, the matrix is assumed to be continuous.\n     */\n    constructor(sizes: any, type: int, data: any, steps?: any);\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param m Array that (as a whole or partly) is assigned to the constructed matrix. No data is\n     * copied by these constructors. Instead, the header pointing to m data or its sub-array is constructed\n     * and associated with it. The reference counter, if any, is incremented. So, when you modify the\n     * matrix formed using such a constructor, you also modify the corresponding elements of m . If you\n     * want to have an independent copy of the sub-array, use Mat::clone() .\n     *\n     * @param rowRange Range of the m rows to take. As usual, the range start is inclusive and the range\n     * end is exclusive. Use Range::all() to take all the rows.\n     *\n     * @param colRange Range of the m columns to take. Use Range::all() to take all the columns.\n     */\n    constructor(m: Mat, rowRange: Range, colRange?: Range);\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param m Array that (as a whole or partly) is assigned to the constructed matrix. No data is\n     * copied by these constructors. Instead, the header pointing to m data or its sub-array is constructed\n     * and associated with it. The reference counter, if any, is incremented. So, when you modify the\n     * matrix formed using such a constructor, you also modify the corresponding elements of m . If you\n     * want to have an independent copy of the sub-array, use Mat::clone() .\n     *\n     * @param roi Region of interest.\n     */\n    constructor(m: Mat, roi: Rect);\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param m Array that (as a whole or partly) is assigned to the constructed matrix. No data is\n     * copied by these constructors. Instead, the header pointing to m data or its sub-array is constructed\n     * and associated with it. The reference counter, if any, is incremented. So, when you modify the\n     * matrix formed using such a constructor, you also modify the corresponding elements of m . If you\n     * want to have an independent copy of the sub-array, use Mat::clone() .\n     *\n     * @param ranges Array of selected ranges of m along each dimensionality.\n     */\n    constructor(m: Mat, ranges: Range);\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param m Array that (as a whole or partly) is assigned to the constructed matrix. No data is\n     * copied by these constructors. Instead, the header pointing to m data or its sub-array is constructed\n     * and associated with it. The reference counter, if any, is incremented. So, when you modify the\n     * matrix formed using such a constructor, you also modify the corresponding elements of m . If you\n     * want to have an independent copy of the sub-array, use Mat::clone() .\n     *\n     * @param ranges Array of selected ranges of m along each dimensionality.\n     */\n    constructor(m: Mat, ranges: Range);\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param vec STL vector whose elements form the matrix. The matrix has a single column and the\n     * number of rows equal to the number of vector elements. Type of the matrix matches the type of vector\n     * elements. The constructor can handle arbitrary types, for which there is a properly declared\n     * DataType . This means that the vector elements must be primitive numbers or uni-type numerical\n     * tuples of numbers. Mixed-type structures are not supported. The corresponding constructor is\n     * explicit. Since STL vectors are not automatically converted to Mat instances, you should write\n     * Mat(vec) explicitly. Unless you copy the data into the matrix ( copyData=true ), no new elements\n     * will be added to the vector because it can potentially yield vector data reallocation, and, thus,\n     * the matrix data pointer will be invalid.\n     *\n     * @param copyData Flag to specify whether the underlying data of the STL vector should be copied to\n     * (true) or shared with (false) the newly constructed matrix. When the data is copied, the allocated\n     * buffer is managed using Mat reference counting mechanism. While the data is shared, the reference\n     * counter is NULL, and you should not deallocate the data until the matrix is not destructed.\n     */\n    constructor(arg3: any, vec: any, copyData?: bool);\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     */\n    constructor(arg4: any, arg5?: typename, list?: any);\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     */\n    constructor(arg6: any, sizes: any, list: any);\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     */\n    constructor(arg7: any, _Nm: size_t, arr: any, copyData?: bool);\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     */\n    constructor(arg8: any, n: int, vec: Vec, copyData?: bool);\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     */\n    constructor(arg9: any, m: int, n: int, mtx: Matx, copyData?: bool);\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     */\n    constructor(arg10: any, pt: Point_, copyData?: bool);\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     */\n    constructor(arg11: any, pt: Point3_, copyData?: bool);\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     */\n    constructor(arg12: any, commaInitializer: MatCommaInitializer_);\n    constructor(m: any);\n    constructor(m: Mat);\n    /**\n     *   The method increments the reference counter associated with the matrix data. If the matrix header\n     * points to an external data set (see [Mat::Mat] ), the reference counter is NULL, and the method has\n     * no effect in this case. Normally, to avoid memory leaks, the method should not be called explicitly.\n     * It is called implicitly by the matrix assignment operator. The reference counter increment is an\n     * atomic operation on the platforms that support it. Thus, it is safe to operate on the same matrices\n     * asynchronously in different threads.\n     */\n    addref(): void;\n    /**\n     *   The method is complimentary to [Mat::locateROI] . The typical use of these functions is to\n     * determine the submatrix position within the parent matrix and then shift the position somehow.\n     * Typically, it can be required for filtering operations when pixels outside of the ROI should be\n     * taken into account. When all the method parameters are positive, the ROI needs to grow in all\n     * directions by the specified amount, for example:\n     *\n     *   ```cpp\n     *   A.adjustROI(2, 2, 2, 2);\n     *   ```\n     *\n     *    In this example, the matrix size is increased by 4 elements in each direction. The matrix is\n     * shifted by 2 elements to the left and 2 elements up, which brings in all the necessary pixels for\n     * the filtering with the 5x5 kernel.\n     *\n     *   adjustROI forces the adjusted ROI to be inside of the parent matrix that is boundaries of the\n     * adjusted ROI are constrained by boundaries of the parent matrix. For example, if the submatrix A is\n     * located in the first row of a parent matrix and you called A.adjustROI(2, 2, 2, 2) then A will not\n     * be increased in the upward direction.\n     *\n     *   The function is used internally by the OpenCV filtering functions, like filter2D , morphological\n     * operations, and so on.\n     *\n     *   [copyMakeBorder]\n     *\n     * @param dtop Shift of the top submatrix boundary upwards.\n     *\n     * @param dbottom Shift of the bottom submatrix boundary downwards.\n     *\n     * @param dleft Shift of the left submatrix boundary to the left.\n     *\n     * @param dright Shift of the right submatrix boundary to the right.\n     */\n    adjustROI(dtop: int, dbottom: int, dleft: int, dright: int): Mat;\n    /**\n     *   This is an internally used method called by the [MatrixExpressions] engine.\n     *\n     * @param m Destination array.\n     *\n     * @param type Desired destination array depth (or -1 if it should be the same as the source type).\n     */\n    assignTo(m: Mat, type?: int): Mat;\n    /**\n     *   The template methods return a reference to the specified array element. For the sake of higher\n     * performance, the index range checks are only performed in the Debug configuration.\n     *\n     *   Note that the variants with a single index (i) can be used to access elements of single-row or\n     * single-column 2-dimensional arrays. That is, if, for example, A is a 1 x N floating-point matrix and\n     * B is an M x 1 integer matrix, you can simply write `A.at<float>(k+4)` and `B.at<int>(2*i+1)` instead\n     * of `A.at<float>(0,k+4)` and `B.at<int>(2*i+1,0)`, respectively.\n     *\n     *   The example below initializes a Hilbert matrix:\n     *\n     *   ```cpp\n     *   Mat H(100, 100, CV_64F);\n     *   for(int i = 0; i < H.rows; i++)\n     *       for(int j = 0; j < H.cols; j++)\n     *           H.at<double>(i,j)=1./(i+j+1);\n     *   ```\n     *\n     *   Keep in mind that the size identifier used in the at operator cannot be chosen at random. It\n     * depends on the image from which you are trying to retrieve the data. The table below gives a better\n     * insight in this:\n     *\n     * If matrix is of type `CV_8U` then use `[Mat.at]<uchar>(y,x)`.\n     * If matrix is of type `CV_8S` then use `[Mat.at]<schar>(y,x)`.\n     * If matrix is of type `CV_16U` then use `[Mat.at]<ushort>(y,x)`.\n     * If matrix is of type `CV_16S` then use `[Mat.at]<short>(y,x)`.\n     * If matrix is of type `CV_32S` then use `[Mat.at]<int>(y,x)`.\n     * If matrix is of type `CV_32F` then use `[Mat.at]<float>(y,x)`.\n     * If matrix is of type `CV_64F` then use `[Mat.at]<double>(y,x)`.\n     *\n     * @param i0 Index along the dimension 0\n     */\n    at(arg13: any, i0?: int): any;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param i0 Index along the dimension 0\n     */\n    at(arg14: any, i0?: int): any;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param row Index along the dimension 0\n     *\n     * @param col Index along the dimension 1\n     */\n    at(arg15: any, row: int, col: int): any;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param row Index along the dimension 0\n     *\n     * @param col Index along the dimension 1\n     */\n    at(arg16: any, row: int, col: int): any;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param i0 Index along the dimension 0\n     *\n     * @param i1 Index along the dimension 1\n     *\n     * @param i2 Index along the dimension 2\n     */\n    at(arg17: any, i0: int, i1: int, i2: int): any;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param i0 Index along the dimension 0\n     *\n     * @param i1 Index along the dimension 1\n     *\n     * @param i2 Index along the dimension 2\n     */\n    at(arg18: any, i0: int, i1: int, i2: int): any;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param idx Array of Mat::dims indices.\n     */\n    at(arg19: any, idx: any): any;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param idx Array of Mat::dims indices.\n     */\n    at(arg20: any, idx: any): any;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     */\n    at(arg21: any, n: int, idx: Vec): Vec;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     */\n    at(arg22: any, n: int, idx: Vec): Vec;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts. special versions for 2D arrays (especially convenient\n     * for referencing image pixels)\n     *\n     * @param pt Element position specified as Point(j,i) .\n     */\n    at(arg23: any, pt: Point): Point;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts. special versions for 2D arrays (especially convenient\n     * for referencing image pixels)\n     *\n     * @param pt Element position specified as Point(j,i) .\n     */\n    at(arg24: any, pt: Point): Point;\n    /**\n     *   The methods return the matrix read-only or read-write iterators. The use of matrix iterators is\n     * very similar to the use of bi-directional STL iterators. In the example below, the alpha blending\n     * function is rewritten using the matrix iterators:\n     *\n     *   ```cpp\n     *   template<typename T>\n     *   void alphaBlendRGBA(const Mat& src1, const Mat& src2, Mat& dst)\n     *   {\n     *       typedef Vec<T, 4> VT;\n     *\n     *       const float alpha_scale = (float)std::numeric_limits<T>::max(),\n     *                   inv_scale = 1.f/alpha_scale;\n     *\n     *       CV_Assert( src1.type() == src2.type() &&\n     *                  src1.type() == traits::Type<VT>::value &&\n     *                  src1.size() == src2.size());\n     *       Size size = src1.size();\n     *       dst.create(size, src1.type());\n     *\n     *       MatConstIterator_<VT> it1 = src1.begin<VT>(), it1_end = src1.end<VT>();\n     *       MatConstIterator_<VT> it2 = src2.begin<VT>();\n     *       MatIterator_<VT> dst_it = dst.begin<VT>();\n     *\n     *       for( ; it1 != it1_end; ++it1, ++it2, ++dst_it )\n     *       {\n     *           VT pix1 = *it1, pix2 = *it2;\n     *           float alpha = pix1[3]*inv_scale, beta = pix2[3]*inv_scale;\n     * dst_it = VT(saturate_cast<T>(pix1[0]*alpha + pix2[0]*beta),\n     *                        saturate_cast<T>(pix1[1]*alpha + pix2[1]*beta),\n     *                        saturate_cast<T>(pix1[2]*alpha + pix2[2]*beta),\n     *                        saturate_cast<T>((1 - (1-alpha)*(1-beta))*alpha_scale));\n     *       }\n     *   }\n     *   ```\n     */\n    begin(arg25: any): MatIterator_;\n    begin(arg26: any): MatConstIterator_;\n    /**\n     *   The method returns the number of matrix channels.\n     */\n    channels(): int;\n    /**\n     *   -1 if the requirement is not satisfied. Otherwise, it returns the number of elements in the\n     * matrix. Note that an element may have multiple channels.\n     *   The following code demonstrates its usage for a 2-d matrix:\n     *\n     *   ```cpp\n     *       cv::Mat mat(20, 1, CV_32FC2);\n     *       int n = mat.checkVector(2);\n     *       CV_Assert(n == 20); // mat has 20 elements\n     *\n     *       mat.create(20, 2, CV_32FC1);\n     *       n = mat.checkVector(1);\n     *       CV_Assert(n == -1); // mat is neither a column nor a row vector\n     *\n     *       n = mat.checkVector(2);\n     *       CV_Assert(n == 20); // the 2 columns are considered as 1 element\n     *   ```\n     *\n     *    The following code demonstrates its usage for a 3-d matrix:\n     *\n     *   ```cpp\n     *       int dims[] = {1, 3, 5}; // 1 plane, every plane has 3 rows and 5 columns\n     *       mat.create(3, dims, CV_32FC1); // for 3-d mat, it MUST have only 1 channel\n     *       n = mat.checkVector(5); // the 5 columns are considered as 1 element\n     *       CV_Assert(n == 3);\n     *\n     *       int dims2[] = {3, 1, 5}; // 3 planes, every plane has 1 row and 5 columns\n     *       mat.create(3, dims2, CV_32FC1);\n     *       n = mat.checkVector(5); // the 5 columns are considered as 1 element\n     *       CV_Assert(n == 3);\n     *   ```\n     *\n     * @param elemChannels Number of channels or number of columns the matrix should have. For a 2-D\n     * matrix, when the matrix has only 1 column, then it should have elemChannels channels; When the\n     * matrix has only 1 channel, then it should have elemChannels columns. For a 3-D matrix, it should\n     * have only one channel. Furthermore, if the number of planes is not one, then the number of rows\n     * within every plane has to be 1; if the number of rows within every plane is not 1, then the number\n     * of planes has to be 1.\n     *\n     * @param depth The depth the matrix should have. Set it to -1 when any depth is fine.\n     *\n     * @param requireContinuous Set it to true to require the matrix to be continuous\n     */\n    checkVector(elemChannels: int, depth?: int, requireContinuous?: bool): int;\n    /**\n     *   The method creates a full copy of the array. The original step[] is not taken into account. So,\n     * the array copy is a continuous array occupying [total()]*elemSize() bytes.\n     */\n    clone(): Mat;\n    /**\n     *   The method makes a new header for the specified matrix column and returns it. This is an O(1)\n     * operation, regardless of the matrix size. The underlying data of the new matrix is shared with the\n     * original matrix. See also the [Mat::row] description.\n     *\n     * @param x A 0-based column index.\n     */\n    col(x: int): Mat;\n    /**\n     *   The method makes a new header for the specified column span of the matrix. Similarly to [Mat::row]\n     * and [Mat::col] , this is an O(1) operation.\n     *\n     * @param startcol An inclusive 0-based start index of the column span.\n     *\n     * @param endcol An exclusive 0-based ending index of the column span.\n     */\n    colRange(startcol: int, endcol: int): Mat;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param r Range structure containing both the start and the end indices.\n     */\n    colRange(r: Range): Mat;\n    /**\n     *   The method converts source pixel values to the target data type. saturate_cast<> is applied at the\n     * end to avoid possible overflows:\n     *\n     *   `\\\\[m(x,y) = saturate \\\\_ cast<rType>( \\\\alpha (*this)(x,y) + \\\\beta )\\\\]`\n     *\n     * @param m output matrix; if it does not have a proper size or type before the operation, it is\n     * reallocated.\n     *\n     * @param rtype desired output matrix type or, rather, the depth since the number of channels are the\n     * same as the input has; if rtype is negative, the output matrix will have the same type as the input.\n     *\n     * @param alpha optional scale factor.\n     *\n     * @param beta optional delta added to the scaled values.\n     */\n    convertTo(m: OutputArray, rtype: int, alpha?: double, beta?: double): OutputArray;\n    copySize(m: Mat): Mat;\n    /**\n     *   The method copies the matrix data to another matrix. Before copying the data, the method invokes :\n     *\n     *\n     *   ```cpp\n     *   m.create(this->size(), this->type());\n     *   ```\n     *\n     *    so that the destination matrix is reallocated if needed. While m.copyTo(m); works flawlessly, the\n     * function does not handle the case of a partial overlap between the source and the destination\n     * matrices.\n     *\n     *   When the operation mask is specified, if the [Mat::create] call shown above reallocates the\n     * matrix, the newly allocated matrix is initialized with all zeros before copying the data.\n     *\n     * @param m Destination matrix. If it does not have a proper size or type before the operation, it is\n     * reallocated.\n     */\n    copyTo(m: OutputArray): OutputArray;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param m Destination matrix. If it does not have a proper size or type before the operation, it is\n     * reallocated.\n     *\n     * @param mask Operation mask of the same size as *this. Its non-zero elements indicate which matrix\n     * elements need to be copied. The mask has to be of type CV_8U and can have 1 or multiple channels.\n     */\n    copyTo(m: OutputArray, mask: InputArray): OutputArray;\n    /**\n     *   This is one of the key [Mat] methods. Most new-style OpenCV functions and methods that produce\n     * arrays call this method for each output array. The method uses the following algorithm:\n     *\n     * If the current array shape and the type match the new ones, return immediately. Otherwise,\n     * de-reference the previous data by calling [Mat::release].\n     * Initialize the new header.\n     * Allocate the new data of [total()]*elemSize() bytes.\n     * Allocate the new, associated with the data, reference counter and set it to 1.\n     *\n     *   Such a scheme makes the memory management robust and efficient at the same time and helps avoid\n     * extra typing for you. This means that usually there is no need to explicitly allocate output arrays.\n     * That is, instead of writing:\n     *\n     *   ```cpp\n     *   Mat color;\n     *   ...\n     *   Mat gray(color.rows, color.cols, color.depth());\n     *   cvtColor(color, gray, COLOR_BGR2GRAY);\n     *   ```\n     *\n     *    you can simply write:\n     *\n     *   ```cpp\n     *   Mat color;\n     *   ...\n     *   Mat gray;\n     *   cvtColor(color, gray, COLOR_BGR2GRAY);\n     *   ```\n     *\n     *    because cvtColor, as well as the most of OpenCV functions, calls [Mat::create()] for the output\n     * array internally.\n     *\n     * @param rows New number of rows.\n     *\n     * @param cols New number of columns.\n     *\n     * @param type New matrix type.\n     */\n    create(rows: int, cols: int, type: int): void;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param size Alternative new matrix size specification: Size(cols, rows)\n     *\n     * @param type New matrix type.\n     */\n    create(size: Size, type: int): Size;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param ndims New array dimensionality.\n     *\n     * @param sizes Array of integers specifying a new array shape.\n     *\n     * @param type New matrix type.\n     */\n    create(ndims: int, sizes: any, type: int): void;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param sizes Array of integers specifying a new array shape.\n     *\n     * @param type New matrix type.\n     */\n    create(sizes: any, type: int): void;\n    /**\n     *   The method computes a cross-product of two 3-element vectors. The vectors must be 3-element\n     * floating-point vectors of the same shape and size. The result is another 3-element vector of the\n     * same shape and type as operands.\n     *\n     * @param m Another cross-product operand.\n     */\n    cross(m: InputArray): Mat;\n    deallocate(): void;\n    /**\n     *   The method returns the identifier of the matrix element depth (the type of each individual\n     * channel). For example, for a 16-bit signed element array, the method returns CV_16S . A complete\n     * list of matrix types contains the following values:\n     *\n     * CV_8U - 8-bit unsigned integers ( 0..255 )\n     * CV_8S - 8-bit signed integers ( -128..127 )\n     * CV_16U - 16-bit unsigned integers ( 0..65535 )\n     * CV_16S - 16-bit signed integers ( -32768..32767 )\n     * CV_32S - 32-bit signed integers ( -2147483648..2147483647 )\n     * CV_32F - 32-bit floating-point numbers ( -FLT_MAX..FLT_MAX, INF, NAN )\n     * CV_64F - 64-bit floating-point numbers ( -DBL_MAX..DBL_MAX, INF, NAN )\n     */\n    depth(): int;\n    /**\n     *   The method makes a new header for the specified matrix diagonal. The new matrix is represented as\n     * a single-column matrix. Similarly to [Mat::row] and [Mat::col], this is an O(1) operation.\n     *\n     * @param d index of the diagonal, with the following values:\n     *   d=0 is the main diagonal.d<0 is a diagonal from the lower half. For example, d=-1 means the\n     * diagonal is set immediately below the main one.d>0 is a diagonal from the upper half. For example,\n     * d=1 means the diagonal is set immediately above the main one. For example: Matm=(Mat_<int>(3,3)<<\n     *   1,2,3,\n     *   4,5,6,\n     *   7,8,9);\n     *   Matd0=m.diag(0);\n     *   Matd1=m.diag(1);\n     *   Matd_1=m.diag(-1);\n     *    The resulting matrices are d0=\n     *   [1;\n     *   5;\n     *   9]\n     *   d1=\n     *   [2;\n     *   6]\n     *   d_1=\n     *   [4;\n     *   8]\n     */\n    diag(d?: int): Mat;\n    /**\n     *   The method computes a dot-product of two matrices. If the matrices are not single-column or\n     * single-row vectors, the top-to-bottom left-to-right scan ordering is used to treat them as 1D\n     * vectors. The vectors must have the same size and type. If the matrices have more than one channel,\n     * the dot products from all the channels are summed together.\n     *\n     * @param m another dot-product operand.\n     */\n    dot(m: InputArray): InputArray;\n    /**\n     *   The method returns the matrix element size in bytes. For example, if the matrix type is CV_16SC3 ,\n     * the method returns 3*sizeof(short) or 6.\n     */\n    elemSize(): size_t;\n    /**\n     *   The method returns the matrix element channel size in bytes, that is, it ignores the number of\n     * channels. For example, if the matrix type is CV_16SC3 , the method returns sizeof(short) or 2.\n     */\n    elemSize1(): size_t;\n    /**\n     *   The method returns true if [Mat::total()] is 0 or if [Mat::data] is NULL. Because of [pop_back()]\n     * and [resize()] methods `[M.total()] == 0` does not imply that `M.data == NULL`.\n     */\n    empty(): bool;\n    /**\n     *   The methods return the matrix read-only or read-write iterators, set to the point following the\n     * last matrix element.\n     */\n    end(arg27: any): MatIterator_;\n    end(arg28: any): MatConstIterator_;\n    /**\n     *   The operation passed as argument has to be a function pointer, a function object or a\n     * lambda(C++11).\n     *\n     *   Example 1. All of the operations below put 0xFF the first channel of all matrix elements:\n     *\n     *   ```cpp\n     *   Mat image(1920, 1080, CV_8UC3);\n     *   typedef cv::Point3_<uint8_t> Pixel;\n     *\n     *   // first. raw pointer access.\n     *   for (int r = 0; r < image.rows; ++r) {\n     *       Pixel* ptr = image.ptr<Pixel>(r, 0);\n     *       const Pixel* ptr_end = ptr + image.cols;\n     *       for (; ptr != ptr_end; ++ptr) {\n     *           ptr->x = 255;\n     *       }\n     *   }\n     *\n     *   // Using MatIterator. (Simple but there are a Iterator's overhead)\n     *   for (Pixel &p : cv::Mat_<Pixel>(image)) {\n     *       p.x = 255;\n     *   }\n     *\n     *   // Parallel execution with function object.\n     *   struct Operator {\n     *       void operator ()(Pixel &pixel, const int * position) {\n     *           pixel.x = 255;\n     *       }\n     *   };\n     *   image.forEach<Pixel>(Operator());\n     *\n     *   // Parallel execution using C++11 lambda.\n     *   image.forEach<Pixel>([](Pixel &p, const int * position) -> void {\n     *       p.x = 255;\n     *   });\n     *   ```\n     *\n     *    Example 2. Using the pixel's position:\n     *\n     *   ```cpp\n     *   // Creating 3D matrix (255 x 255 x 255) typed uint8_t\n     *   // and initialize all elements by the value which equals elements position.\n     *   // i.e. pixels (x,y,z) = (1,2,3) is (b,g,r) = (1,2,3).\n     *\n     *   int sizes[] = { 255, 255, 255 };\n     *   typedef cv::Point3_<uint8_t> Pixel;\n     *\n     *   Mat_<Pixel> image = Mat::zeros(3, sizes, CV_8UC3);\n     *\n     *   image.forEach<Pixel>([&](Pixel& pixel, const int position[]) -> void {\n     *       pixel.x = position[0];\n     *       pixel.y = position[1];\n     *       pixel.z = position[2];\n     *   });\n     *   ```\n     */\n    forEach(arg29: any, arg30: any, operation: any): any;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     */\n    forEach(arg31: any, arg32: any, operation: any): any;\n    getUMat(accessFlags: AccessFlag, usageFlags?: UMatUsageFlags): UMat;\n    /**\n     *   The method performs a matrix inversion by means of matrix expressions. This means that a temporary\n     * matrix inversion object is returned by the method and can be used further as a part of more complex\n     * matrix expressions or can be assigned to a matrix.\n     *\n     * @param method Matrix inversion method. One of cv::DecompTypes\n     */\n    inv(method?: int): MatExpr;\n    /**\n     *   The method returns true if the matrix elements are stored continuously without gaps at the end of\n     * each row. Otherwise, it returns false. Obviously, 1x1 or 1xN matrices are always continuous.\n     * Matrices created with [Mat::create] are always continuous. But if you extract a part of the matrix\n     * using [Mat::col], [Mat::diag], and so on, or constructed a matrix header for externally allocated\n     * data, such matrices may no longer have this property.\n     *\n     *   The continuity flag is stored as a bit in the [Mat::flags] field and is computed automatically\n     * when you construct a matrix header. Thus, the continuity check is a very fast operation, though\n     * theoretically it could be done as follows:\n     *\n     *   ```cpp\n     *   // alternative implementation of Mat::isContinuous()\n     *   bool myCheckMatContinuity(const Mat& m)\n     *   {\n     *       //return (m.flags & Mat::CONTINUOUS_FLAG) != 0;\n     *       return m.rows == 1 || m.step == m.cols*m.elemSize();\n     *   }\n     *   ```\n     *\n     *    The method is used in quite a few of OpenCV functions. The point is that element-wise operations\n     * (such as arithmetic and logical operations, math functions, alpha blending, color space\n     * transformations, and others) do not depend on the image geometry. Thus, if all the input and output\n     * arrays are continuous, the functions can process them as very long single-row vectors. The example\n     * below illustrates how an alpha-blending function can be implemented:\n     *\n     *   ```cpp\n     *   template<typename T>\n     *   void alphaBlendRGBA(const Mat& src1, const Mat& src2, Mat& dst)\n     *   {\n     *       const float alpha_scale = (float)std::numeric_limits<T>::max(),\n     *                   inv_scale = 1.f/alpha_scale;\n     *\n     *       CV_Assert( src1.type() == src2.type() &&\n     *                  src1.type() == CV_MAKETYPE(traits::Depth<T>::value, 4) &&\n     *                  src1.size() == src2.size());\n     *       Size size = src1.size();\n     *       dst.create(size, src1.type());\n     *\n     *       // here is the idiom: check the arrays for continuity and,\n     *       // if this is the case,\n     *       // treat the arrays as 1D vectors\n     *       if( src1.isContinuous() && src2.isContinuous() && dst.isContinuous() )\n     *       {\n     *           size.width *= size.height;\n     *           size.height = 1;\n     *       }\n     *       size.width *= 4;\n     *\n     *       for( int i = 0; i < size.height; i++ )\n     *       {\n     *           // when the arrays are continuous,\n     *           // the outer loop is executed only once\n     *           const T* ptr1 = src1.ptr<T>(i);\n     *           const T* ptr2 = src2.ptr<T>(i);\n     *           T* dptr = dst.ptr<T>(i);\n     *\n     *           for( int j = 0; j < size.width; j += 4 )\n     *           {\n     *               float alpha = ptr1[j+3]*inv_scale, beta = ptr2[j+3]*inv_scale;\n     *               dptr[j] = saturate_cast<T>(ptr1[j]*alpha + ptr2[j]*beta);\n     *               dptr[j+1] = saturate_cast<T>(ptr1[j+1]*alpha + ptr2[j+1]*beta);\n     *               dptr[j+2] = saturate_cast<T>(ptr1[j+2]*alpha + ptr2[j+2]*beta);\n     *               dptr[j+3] = saturate_cast<T>((1 - (1-alpha)*(1-beta))*alpha_scale);\n     *           }\n     *       }\n     *   }\n     *   ```\n     *\n     *    This approach, while being very simple, can boost the performance of a simple element-operation\n     * by 10-20 percents, especially if the image is rather small and the operation is quite simple.\n     *\n     *   Another OpenCV idiom in this function, a call of [Mat::create] for the destination array, that\n     * allocates the destination array unless it already has the proper size and type. And while the newly\n     * allocated arrays are always continuous, you still need to check the destination array because\n     * [Mat::create] does not always allocate a new matrix.\n     */\n    isContinuous(): bool;\n    isSubmatrix(): bool;\n    /**\n     *   After you extracted a submatrix from a matrix using [Mat::row], [Mat::col], [Mat::rowRange],\n     * [Mat::colRange], and others, the resultant submatrix points just to the part of the original big\n     * matrix. However, each submatrix contains information (represented by datastart and dataend fields)\n     * that helps reconstruct the original matrix size and the position of the extracted submatrix within\n     * the original matrix. The method locateROI does exactly that.\n     *\n     * @param wholeSize Output parameter that contains the size of the whole matrix containing this as a\n     * part.\n     *\n     * @param ofs Output parameter that contains an offset of this inside the whole matrix.\n     */\n    locateROI(wholeSize: Size, ofs: Point): Size;\n    /**\n     *   The method returns a temporary object encoding per-element array multiplication, with optional\n     * scale. Note that this is not a matrix multiplication that corresponds to a simpler \"\\\\*\" operator.\n     *\n     *   Example:\n     *\n     *   ```cpp\n     *   Mat C = A.mul(5/B); // equivalent to divide(A, B, C, 5)\n     *   ```\n     *\n     * @param m Another array of the same type and the same size as *this, or a matrix expression.\n     *\n     * @param scale Optional scale factor.\n     */\n    mul(m: InputArray, scale?: double): MatExpr;\n    /**\n     *   The method removes one or more rows from the bottom of the matrix.\n     *\n     * @param nelems Number of removed rows. If it is greater than the total number of rows, an exception\n     * is thrown.\n     */\n    pop_back(nelems?: size_t): void;\n    /**\n     *   The methods return `uchar*` or typed pointer to the specified matrix row. See the sample in\n     * [Mat::isContinuous] to know how to use these methods.\n     *\n     * @param i0 A 0-based row index.\n     */\n    ptr(i0?: int): uchar;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     */\n    ptr(i0?: int): uchar;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param row Index along the dimension 0\n     *\n     * @param col Index along the dimension 1\n     */\n    ptr(row: int, col: int): uchar;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param row Index along the dimension 0\n     *\n     * @param col Index along the dimension 1\n     */\n    ptr(row: int, col: int): uchar;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     */\n    ptr(i0: int, i1: int, i2: int): uchar;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     */\n    ptr(i0: int, i1: int, i2: int): uchar;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     */\n    ptr(idx: any): uchar;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     */\n    ptr(idx: any): uchar;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     */\n    ptr(n: int, idx: Vec): uchar;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     */\n    ptr(n: int, idx: Vec): uchar;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     */\n    ptr(arg37: any, i0?: int): any;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     */\n    ptr(arg38: any, i0?: int): any;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param row Index along the dimension 0\n     *\n     * @param col Index along the dimension 1\n     */\n    ptr(arg39: any, row: int, col: int): any;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param row Index along the dimension 0\n     *\n     * @param col Index along the dimension 1\n     */\n    ptr(arg40: any, row: int, col: int): any;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     */\n    ptr(arg41: any, i0: int, i1: int, i2: int): any;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     */\n    ptr(arg42: any, i0: int, i1: int, i2: int): any;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     */\n    ptr(arg43: any, idx: any): any;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     */\n    ptr(arg44: any, idx: any): any;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     */\n    ptr(arg45: any, n: int, idx: Vec): Vec;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     */\n    ptr(arg46: any, n: int, idx: Vec): Vec;\n    /**\n     *   The methods add one or more elements to the bottom of the matrix. They emulate the corresponding\n     * method of the STL vector class. When elem is [Mat] , its type and the number of columns must be the\n     * same as in the container matrix.\n     *\n     * @param elem Added element(s).\n     */\n    push_back(arg47: any, elem: any): any;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param elem Added element(s).\n     */\n    push_back(arg48: any, elem: Mat_): Mat_;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param elem Added element(s).\n     */\n    push_back(arg49: any, elem: any): any;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param m Added line(s).\n     */\n    push_back(m: Mat): Mat;\n    push_back_(elem: any): void;\n    /**\n     *   The method decrements the reference counter associated with the matrix data. When the reference\n     * counter reaches 0, the matrix data is deallocated and the data and the reference counter pointers\n     * are set to NULL's. If the matrix header points to an external data set (see [Mat::Mat] ), the\n     * reference counter is NULL, and the method has no effect in this case.\n     *\n     *   This method can be called manually to force the matrix data deallocation. But since this method is\n     * automatically called in the destructor, or by any other method that changes the data pointer, it is\n     * usually not needed. The reference counter decrement and check for 0 is an atomic operation on the\n     * platforms that support it. Thus, it is safe to operate on the same matrices asynchronously in\n     * different threads.\n     */\n    release(): void;\n    /**\n     *   The method reserves space for sz rows. If the matrix already has enough space to store sz rows,\n     * nothing happens. If the matrix is reallocated, the first [Mat::rows] rows are preserved. The method\n     * emulates the corresponding method of the STL vector class.\n     *\n     * @param sz Number of rows.\n     */\n    reserve(sz: size_t): void;\n    /**\n     *   The method reserves space for sz bytes. If the matrix already has enough space to store sz bytes,\n     * nothing happens. If matrix has to be reallocated its previous content could be lost.\n     *\n     * @param sz Number of bytes.\n     */\n    reserveBuffer(sz: size_t): void;\n    /**\n     *   The method makes a new matrix header for *this elements. The new matrix may have a different size\n     * and/or different number of channels. Any combination is possible if:\n     *\n     * No extra elements are included into the new matrix and no elements are excluded. Consequently, the\n     * product rows*cols*channels() must stay the same after the transformation.\n     * No data is copied. That is, this is an O(1) operation. Consequently, if you change the number of\n     * rows, or the operation changes the indices of elements row in some other way, the matrix must be\n     * continuous. See [Mat::isContinuous] .\n     *\n     *   For example, if there is a set of 3D points stored as an STL vector, and you want to represent the\n     * points as a 3xN matrix, do the following:\n     *\n     *   ```cpp\n     *   std::vector<Point3f> vec;\n     *   ...\n     *   Mat pointMat = Mat(vec). // convert vector to Mat, O(1) operation\n     *                     reshape(1). // make Nx3 1-channel matrix out of Nx1 3-channel.\n     *                                 // Also, an O(1) operation\n     *                        t(); // finally, transpose the Nx3 matrix.\n     *                             // This involves copying all the elements\n     *   ```\n     *\n     * @param cn New number of channels. If the parameter is 0, the number of channels remains the same.\n     *\n     * @param rows New number of rows. If the parameter is 0, the number of rows remains the same.\n     */\n    reshape(cn: int, rows?: int): Mat;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     */\n    reshape(cn: int, newndims: int, newsz: any): Mat;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     */\n    reshape(cn: int, newshape: any): Mat;\n    /**\n     *   The methods change the number of matrix rows. If the matrix is reallocated, the first\n     * min(Mat::rows, sz) rows are preserved. The methods emulate the corresponding methods of the STL\n     * vector class.\n     *\n     * @param sz New number of rows.\n     */\n    resize(sz: size_t): void;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param sz New number of rows.\n     *\n     * @param s Value assigned to the newly added elements.\n     */\n    resize(sz: size_t, s: Scalar): Scalar;\n    /**\n     *   The method makes a new header for the specified matrix row and returns it. This is an O(1)\n     * operation, regardless of the matrix size. The underlying data of the new matrix is shared with the\n     * original matrix. Here is the example of one of the classical basic matrix processing operations,\n     * axpy, used by LU and many other algorithms:\n     *\n     *   ```cpp\n     *   inline void matrix_axpy(Mat& A, int i, int j, double alpha)\n     *   {\n     *       A.row(i) += A.row(j)*alpha;\n     *   }\n     *   ```\n     *\n     *   In the current implementation, the following code does not work as expected:\n     *\n     *   ```cpp\n     *   Mat A;\n     *   ...\n     *   A.row(i) = A.row(j); // will not work\n     *   ```\n     *\n     *    This happens because A.row(i) forms a temporary header that is further assigned to another\n     * header. Remember that each of these operations is O(1), that is, no data is copied. Thus, the above\n     * assignment is not true if you may have expected the j-th row to be copied to the i-th row. To\n     * achieve that, you should either turn this simple assignment into an expression or use the\n     * [Mat::copyTo] method:\n     *\n     *   ```cpp\n     *   Mat A;\n     *   ...\n     *   // works, but looks a bit obscure.\n     *   A.row(i) = A.row(j) + 0;\n     *   // this is a bit longer, but the recommended method.\n     *   A.row(j).copyTo(A.row(i));\n     *   ```\n     *\n     * @param y A 0-based row index.\n     */\n    row(y: int): Mat;\n    /**\n     *   The method makes a new header for the specified row span of the matrix. Similarly to [Mat::row]\n     * and [Mat::col] , this is an O(1) operation.\n     *\n     * @param startrow An inclusive 0-based start index of the row span.\n     *\n     * @param endrow An exclusive 0-based ending index of the row span.\n     */\n    rowRange(startrow: int, endrow: int): Mat;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param r Range structure containing both the start and the end indices.\n     */\n    rowRange(r: Range): Mat;\n    /**\n     *   This is an advanced variant of the [Mat::operator=(const Scalar& s)] operator.\n     *\n     * @param value Assigned scalar converted to the actual array type.\n     *\n     * @param mask Operation mask of the same size as *this. Its non-zero elements indicate which matrix\n     * elements need to be copied. The mask has to be of type CV_8U and can have 1 or multiple channels\n     */\n    setTo(value: InputArray, mask?: InputArray): Mat;\n    /**\n     *   The method returns a matrix step divided by [Mat::elemSize1()] . It can be useful to quickly\n     * access an arbitrary matrix element.\n     */\n    step1(i?: int): size_t;\n    /**\n     *   The method performs matrix transposition by means of matrix expressions. It does not perform the\n     * actual transposition but returns a temporary matrix transposition object that can be further used as\n     * a part of more complex matrix expressions or can be assigned to a matrix:\n     *\n     *   ```cpp\n     *   Mat A1 = A + Mat::eye(A.size(), A.type())*lambda;\n     *   Mat C = A1.t()*A1; // compute (A + lambda*I)^t * (A + lamda*I)\n     *   ```\n     */\n    t(): MatExpr;\n    /**\n     *   The method returns the number of array elements (a number of pixels if the array represents an\n     * image).\n     */\n    total(): size_t;\n    /**\n     *   The method returns the number of elements within a certain sub-array slice with startDim <= dim <\n     * endDim\n     */\n    total(startDim: int, endDim?: int): size_t;\n    /**\n     *   The method returns a matrix element type. This is an identifier compatible with the CvMat type\n     * system, like CV_16SC3 or 16-bit signed 3-channel array, and so on.\n     */\n    type(): int;\n    updateContinuityFlag(): void;\n    /**\n     *   The method creates a square diagonal matrix from specified main diagonal.\n     *\n     * @param d One-dimensional matrix that represents the main diagonal.\n     */\n    static diag(d: Mat): Mat;\n    /**\n     *   The method returns a Matlab-style identity matrix initializer, similarly to [Mat::zeros].\n     * Similarly to [Mat::ones], you can use a scale operation to create a scaled identity matrix\n     * efficiently:\n     *\n     *   ```cpp\n     *   // make a 4x4 diagonal matrix with 0.1's on the diagonal.\n     *   Mat A = Mat::eye(4, 4, CV_32F)*0.1;\n     *   ```\n     *\n     *   In case of multi-channels type, identity matrix will be initialized only for the first channel,\n     * the others will be set to 0's\n     *\n     * @param rows Number of rows.\n     *\n     * @param cols Number of columns.\n     *\n     * @param type Created matrix type.\n     */\n    static eye(rows: int, cols: int, type: int): MatExpr;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param size Alternative matrix size specification as Size(cols, rows) .\n     *\n     * @param type Created matrix type.\n     */\n    static eye(size: Size, type: int): MatExpr;\n    static getDefaultAllocator(): MatAllocator;\n    static getStdAllocator(): MatAllocator;\n    /**\n     *   The method returns a Matlab-style 1's array initializer, similarly to [Mat::zeros]. Note that\n     * using this method you can initialize an array with an arbitrary value, using the following Matlab\n     * idiom:\n     *\n     *   ```cpp\n     *   Mat A = Mat::ones(100, 100, CV_8U)*3; // make 100x100 matrix filled with 3.\n     *   ```\n     *\n     *    The above operation does not form a 100x100 matrix of 1's and then multiply it by 3. Instead, it\n     * just remembers the scale factor (3 in this case) and use it when actually invoking the matrix\n     * initializer.\n     *\n     *   In case of multi-channels type, only the first channel will be initialized with 1's, the others\n     * will be set to 0's.\n     *\n     * @param rows Number of rows.\n     *\n     * @param cols Number of columns.\n     *\n     * @param type Created matrix type.\n     */\n    static ones(rows: int, cols: int, type: int): MatExpr;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param size Alternative to the matrix size specification Size(cols, rows) .\n     *\n     * @param type Created matrix type.\n     */\n    static ones(size: Size, type: int): MatExpr;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param ndims Array dimensionality.\n     *\n     * @param sz Array of integers specifying the array shape.\n     *\n     * @param type Created matrix type.\n     */\n    static ones(ndims: int, sz: any, type: int): MatExpr;\n    static setDefaultAllocator(allocator: MatAllocator): MatAllocator;\n    /**\n     *   The method returns a Matlab-style zero array initializer. It can be used to quickly form a\n     * constant array as a function parameter, part of a matrix expression, or as a matrix initializer:\n     *\n     *   ```cpp\n     *   Mat A;\n     *   A = Mat::zeros(3, 3, CV_32F);\n     *   ```\n     *\n     *    In the example above, a new matrix is allocated only if A is not a 3x3 floating-point matrix.\n     * Otherwise, the existing matrix A is filled with zeros.\n     *\n     * @param rows Number of rows.\n     *\n     * @param cols Number of columns.\n     *\n     * @param type Created matrix type.\n     */\n    static zeros(rows: int, cols: int, type: int): MatExpr;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param size Alternative to the matrix size specification Size(cols, rows) .\n     *\n     * @param type Created matrix type.\n     */\n    static zeros(size: Size, type: int): MatExpr;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param ndims Array dimensionality.\n     *\n     * @param sz Array of integers specifying the array shape.\n     *\n     * @param type Created matrix type.\n     */\n    static zeros(ndims: int, sz: any, type: int): MatExpr;\n}\nexport declare const MAGIC_VAL: any;\nexport declare const AUTO_STEP: any;\nexport declare const CONTINUOUS_FLAG: any;\nexport declare const SUBMATRIX_FLAG: any;\nexport declare const MAGIC_MASK: any;\nexport declare const TYPE_MASK: any;\nexport declare const DEPTH_MASK: any;\n"},"node_modules_mirada_dist_src_types_opencv_PCA_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_PCA_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/PCA.d.ts","content":"import { double, FileNode, FileStorage, InputArray, int, Mat, OutputArray } from './_types';\n/**\n * The class is used to calculate a special basis for a set of vectors. The basis will consist of\n * eigenvectors of the covariance matrix calculated from the input set of vectors. The class PCA can\n * also transform vectors to/from the new coordinate space defined by the basis. Usually, in this new\n * coordinate system, each vector from the original set (and any linear combination of such vectors)\n * can be quite accurately approximated by taking its first few components, corresponding to the\n * eigenvectors of the largest eigenvalues of the covariance matrix. Geometrically it means that you\n * calculate a projection of the vector to a subspace formed by a few eigenvectors corresponding to the\n * dominant eigenvalues of the covariance matrix. And usually such a projection is very close to the\n * original vector. So, you can represent the original vector from a high-dimensional space with a much\n * shorter vector consisting of the projected vector's coordinates in the subspace. Such a\n * transformation is also known as Karhunen-Loeve Transform, or KLT. See\n *\n * The sample below is the function that takes two matrices. The first function stores a set of vectors\n * (a row per vector) that is used to calculate [PCA](#d3/d8d/classcv_1_1PCA}). The second function\n * stores another \"test\" set of vectors (a row per vector). First, these vectors are compressed with\n * [PCA](#d3/d8d/classcv_1_1PCA}), then reconstructed back, and then the reconstruction error norm is\n * computed and printed for each vector. :\n *\n * ```cpp\n * using namespace cv;\n *\n * PCA compressPCA(const Mat& pcaset, int maxComponents,\n *                 const Mat& testset, Mat& compressed)\n * {\n *     PCA pca(pcaset, // pass the data\n *             Mat(), // we do not have a pre-computed mean vector,\n *                    // so let the PCA engine to compute it\n *             PCA::DATA_AS_ROW, // indicate that the vectors\n *                                 // are stored as matrix rows\n *                                 // (use PCA::DATA_AS_COL if the vectors are\n *                                 // the matrix columns)\n *             maxComponents // specify, how many principal components to retain\n *             );\n *     // if there is no test data, just return the computed basis, ready-to-use\n *     if( !testset.data )\n *         return pca;\n *     CV_Assert( testset.cols == pcaset.cols );\n *\n *     compressed.create(testset.rows, maxComponents, testset.type());\n *\n *     Mat reconstructed;\n *     for( int i = 0; i < testset.rows; i++ )\n *     {\n *         Mat vec = testset.row(i), coeffs = compressed.row(i), reconstructed;\n *         // compress the vector, the result will be stored\n *         // in the i-th row of the output matrix\n *         pca.project(vec, coeffs);\n *         // and then reconstruct it\n *         pca.backProject(coeffs, reconstructed);\n *         // and measure the error\n *         printf(\"%d. diff = %g\\\\n\", i, norm(vec, reconstructed, NORM_L2));\n *     }\n *     return pca;\n * }\n * ```\n *\n * [calcCovarMatrix](#d2/de8/group__core__array_1gae6ffa9354633f984246945d52823165d}),\n * [mulTransposed](#d2/de8/group__core__array_1gadc4e49f8f7a155044e3be1b9e3b270ab}),\n * [SVD](#df/df7/classcv_1_1SVD}),\n * [dft](#d2/de8/group__core__array_1gadd6cf9baf2b8b704a11b5f04aaf4f39d}),\n * [dct](#d2/de8/group__core__array_1ga85aad4d668c01fbd64825f589e3696d4})\n *\n * Source:\n * [opencv2/core.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/core.hpp#L2393).\n *\n */\nexport declare class PCA {\n    eigenvalues: Mat;\n    eigenvectors: Mat;\n    mean: Mat;\n    /**\n     *   The default constructor initializes an empty PCA structure. The other constructors initialize the\n     * structure and call [PCA::operator()()].\n     */\n    constructor();\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param data input samples stored as matrix rows or matrix columns.\n     *\n     * @param mean optional mean value; if the matrix is empty (noArray()), the mean is computed from the\n     * data.\n     *\n     * @param flags operation flags; currently the parameter is only used to specify the data layout\n     * (PCA::Flags)\n     *\n     * @param maxComponents maximum number of components that PCA should retain; by default, all the\n     * components are retained.\n     */\n    constructor(data: InputArray, mean: InputArray, flags: int, maxComponents?: int);\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param data input samples stored as matrix rows or matrix columns.\n     *\n     * @param mean optional mean value; if the matrix is empty (noArray()), the mean is computed from the\n     * data.\n     *\n     * @param flags operation flags; currently the parameter is only used to specify the data layout\n     * (PCA::Flags)\n     *\n     * @param retainedVariance Percentage of variance that PCA should retain. Using this parameter will\n     * let the PCA decided how many components to retain but it will always keep at least 2.\n     */\n    constructor(data: InputArray, mean: InputArray, flags: int, retainedVariance: double);\n    /**\n     *   The methods are inverse operations to [PCA::project]. They take PC coordinates of projected\n     * vectors and reconstruct the original vectors. Unless all the principal components have been\n     * retained, the reconstructed vectors are different from the originals. But typically, the difference\n     * is small if the number of components is large enough (but still much smaller than the original\n     * vector dimensionality). As a result, [PCA] is used.\n     *\n     * @param vec coordinates of the vectors in the principal component subspace, the layout and size are\n     * the same as of PCA::project output vectors.\n     */\n    backProject(vec: InputArray): Mat;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param vec coordinates of the vectors in the principal component subspace, the layout and size are\n     * the same as of PCA::project output vectors.\n     *\n     * @param result reconstructed vectors; the layout and size are the same as of PCA::project input\n     * vectors.\n     */\n    backProject(vec: InputArray, result: OutputArray): InputArray;\n    /**\n     *   The methods project one or more vectors to the principal component subspace, where each vector\n     * projection is represented by coefficients in the principal component basis. The first form of the\n     * method returns the matrix that the second form writes to the result. So the first form can be used\n     * as a part of expression while the second form can be more efficient in a processing loop.\n     *\n     * @param vec input vector(s); must have the same dimensionality and the same layout as the input\n     * data used at PCA phase, that is, if DATA_AS_ROW are specified, then vec.cols==data.cols (vector\n     * dimensionality) and vec.rows is the number of vectors to project, and the same is true for the\n     * PCA::DATA_AS_COL case.\n     */\n    project(vec: InputArray): Mat;\n    /**\n     *   This is an overloaded member function, provided for convenience. It differs from the above\n     * function only in what argument(s) it accepts.\n     *\n     * @param vec input vector(s); must have the same dimensionality and the same layout as the input\n     * data used at PCA phase, that is, if DATA_AS_ROW are specified, then vec.cols==data.cols (vector\n     * dimensionality) and vec.rows is the number of vectors to project, and the same is true for the\n     * PCA::DATA_AS_COL case.\n     *\n     * @param result output vectors; in case of PCA::DATA_AS_COL, the output matrix has as many columns\n     * as the number of input vectors, this means that result.cols==vec.cols and the number of rows match\n     * the number of principal components (for example, maxComponents parameter passed to the constructor).\n     */\n    project(vec: InputArray, result: OutputArray): InputArray;\n    /**\n     *   Loads [eigenvalues] [eigenvectors] and [mean] from specified [FileNode]\n     */\n    read(fn: FileNode): FileNode;\n    /**\n     *   Writes [eigenvalues] [eigenvectors] and [mean] to specified [FileStorage]\n     */\n    write(fs: FileStorage): FileStorage;\n}\nexport declare const DATA_AS_ROW: Flags;\nexport declare const DATA_AS_COL: Flags;\nexport declare const USE_AVG: Flags;\nexport declare type Flags = any;\n"},"node_modules_mirada_dist_src_types_opencv_photo_inpaint_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_photo_inpaint_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/photo_inpaint.d.ts","content":"import { double, InputArray, int, OutputArray } from './_types';\n/**\n * The function reconstructs the selected image area from the pixel near the area boundary. The\n * function may be used to remove dust and scratches from a scanned photo, or to remove undesirable\n * objects from still images or video. See  for more details.\n *\n * An example using the inpainting technique can be found at opencv_source_code/samples/cpp/inpaint.cpp\n * (Python) An example using the inpainting technique can be found at\n * opencv_source_code/samples/python/inpaint.py\n *\n * @param src Input 8-bit, 16-bit unsigned or 32-bit float 1-channel or 8-bit 3-channel image.\n *\n * @param inpaintMask Inpainting mask, 8-bit 1-channel image. Non-zero pixels indicate the area that\n * needs to be inpainted.\n *\n * @param dst Output image with the same size and type as src .\n *\n * @param inpaintRadius Radius of a circular neighborhood of each point inpainted that is considered by\n * the algorithm.\n *\n * @param flags Inpainting method that could be cv::INPAINT_NS or cv::INPAINT_TELEA\n */\nexport declare function inpaint(src: InputArray, inpaintMask: InputArray, dst: OutputArray, inpaintRadius: double, flags: int): void;\nexport declare const INPAINT_NS: any;\nexport declare const INPAINT_TELEA: any;\n"},"node_modules_mirada_dist_src_types_opencv_RotatedRect_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_RotatedRect_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/RotatedRect.d.ts","content":"import { float, Point2f, Rect, Rect_, Size2f } from './_types';\n/**\n * Each rectangle is specified by the center point (mass center), length of each side (represented by\n * [Size2f](#dc/d84/group__core__basic_1gab34496d2466b5f69930ab74c70f117d4}) structure) and the\n * rotation angle in degrees.\n *\n * The sample below demonstrates how to use [RotatedRect](#db/dd6/classcv_1_1RotatedRect}):\n *\n * ```cpp\n *     Mat test_image(200, 200, CV_8UC3, Scalar(0));\n *     RotatedRect rRect = RotatedRect(Point2f(100,100), Size2f(100,50), 30);\n *\n *     Point2f vertices[4];\n *     rRect.points(vertices);\n *     for (int i = 0; i < 4; i++)\n *         line(test_image, vertices[i], vertices[(i+1)%4], Scalar(0,255,0), 2);\n *\n *     Rect brect = rRect.boundingRect();\n *     rectangle(test_image, brect, Scalar(255,0,0), 2);\n *\n *     imshow(\"rectangles\", test_image);\n *     waitKey(0);\n * ```\n *\n * [CamShift](#dc/d6b/group__video__track_1gaef2bd39c8356f423124f1fe7c44d54a1}),\n * [fitEllipse](#d3/dc0/group__imgproc__shape_1gaf259efaad93098103d6c27b9e4900ffa}),\n * [minAreaRect](#d3/dc0/group__imgproc__shape_1ga3d476a3417130ae5154aea421ca7ead9}), CvBox2D\n *\n * Source:\n * [opencv2/core/types.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/core/types.hpp#L534).\n *\n */\nexport declare class RotatedRect {\n    angle: float;\n    center: Point2f;\n    size: Size2f;\n    constructor();\n    /**\n     *   full constructor\n     *\n     * @param center The rectangle mass center.\n     *\n     * @param size Width and height of the rectangle.\n     *\n     * @param angle The rotation angle in a clockwise direction. When the angle is 0, 90, 180, 270 etc.,\n     * the rectangle becomes an up-right rectangle.\n     */\n    constructor(center: Point2f, size: Size2f, angle: float);\n    /**\n     *   Any 3 end points of the [RotatedRect]. They must be given in order (either clockwise or\n     * anticlockwise).\n     */\n    constructor(point1: Point2f, point2: Point2f, point3: Point2f);\n    boundingRect(): Rect;\n    boundingRect2f(): Rect_;\n    /**\n     *   returns 4 vertices of the rectangle\n     *\n     * @param pts The points array for storing rectangle vertices. The order is bottomLeft, topLeft,\n     * topRight, bottomRight.\n     */\n    points(pts: Point2f): Point2f;\n}\n"},"node_modules_mirada_dist_src_types_opencv_softdouble_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_softdouble_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/softdouble.d.ts","content":"import { bool, int, int32_t, int64_t, uint32_t, uint64_t } from './_types';\nexport declare class softdouble {\n    v: uint64_t;\n    constructor();\n    constructor(c: softdouble);\n    constructor(arg159: uint32_t);\n    constructor(arg160: uint64_t);\n    constructor(arg161: int32_t);\n    constructor(arg162: int64_t);\n    constructor(a: any);\n    getExp(): int;\n    /**\n     *   Returns a number 1 <= x < 2 with the same significand\n     */\n    getFrac(): softdouble;\n    getSign(): bool;\n    isInf(): bool;\n    isNaN(): bool;\n    isSubnormal(): bool;\n    setExp(e: int): softdouble;\n    /**\n     *   Constructs a copy of a number with significand taken from parameter\n     */\n    setFrac(s: softdouble): softdouble;\n    setSign(sign: bool): softdouble;\n    static eps(): softdouble;\n    /**\n     *   Builds new value from raw binary representation\n     */\n    static fromRaw(a: uint64_t): softdouble;\n    static inf(): softdouble;\n    static max(): softdouble;\n    static min(): softdouble;\n    static nan(): softdouble;\n    static one(): softdouble;\n    static pi(): softdouble;\n    static zero(): softdouble;\n}\n"},"node_modules_mirada_dist_src_types_opencv_softfloat_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_softfloat_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/softfloat.d.ts","content":"import { bool, int, int32_t, int64_t, uint32_t, uint64_t } from './_types';\nexport declare class softfloat {\n    v: uint32_t;\n    constructor();\n    constructor(c: softfloat);\n    constructor(arg174: uint32_t);\n    constructor(arg175: uint64_t);\n    constructor(arg176: int32_t);\n    constructor(arg177: int64_t);\n    constructor(a: any);\n    getExp(): int;\n    /**\n     *   Returns a number 1 <= x < 2 with the same significand\n     */\n    getFrac(): softfloat;\n    getSign(): bool;\n    isInf(): bool;\n    isNaN(): bool;\n    isSubnormal(): bool;\n    setExp(e: int): softfloat;\n    /**\n     *   Constructs a copy of a number with significand taken from parameter\n     */\n    setFrac(s: softfloat): softfloat;\n    setSign(sign: bool): softfloat;\n    static eps(): softfloat;\n    /**\n     *   Builds new value from raw binary representation\n     */\n    static fromRaw(a: uint32_t): softfloat;\n    static inf(): softfloat;\n    static max(): softfloat;\n    static min(): softfloat;\n    static nan(): softfloat;\n    static one(): softfloat;\n    static pi(): softfloat;\n    static zero(): softfloat;\n}\n"},"node_modules_mirada_dist_src_types_opencv_video_track_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_video_track_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/video_track.d.ts","content":"import { bool, double, InputArray, InputOutputArray, int, Mat, OutputArray, OutputArrayOfArrays, RotatedRect, Size, TermCriteria } from './_types';\n/**\n * number of levels in constructed pyramid. Can be less than maxLevel.\n *\n * @param img 8-bit input image.\n *\n * @param pyramid output pyramid.\n *\n * @param winSize window size of optical flow algorithm. Must be not less than winSize argument of\n * calcOpticalFlowPyrLK. It is needed to calculate required padding for pyramid levels.\n *\n * @param maxLevel 0-based maximal pyramid level number.\n *\n * @param withDerivatives set to precompute gradients for the every pyramid level. If pyramid is\n * constructed without the gradients then calcOpticalFlowPyrLK will calculate them internally.\n *\n * @param pyrBorder the border mode for pyramid layers.\n *\n * @param derivBorder the border mode for gradients.\n *\n * @param tryReuseInputImage put ROI of input image into the pyramid if possible. You can pass false to\n * force data copying.\n */\nexport declare function buildOpticalFlowPyramid(img: InputArray, pyramid: OutputArrayOfArrays, winSize: Size, maxLevel: int, withDerivatives?: bool, pyrBorder?: int, derivBorder?: int, tryReuseInputImage?: bool): int;\n/**\n * The function finds an optical flow for each prev pixel using the Farneback2003 algorithm so that\n *\n * `\\\\[\\\\texttt{prev} (y,x) \\\\sim \\\\texttt{next} ( y + \\\\texttt{flow} (y,x)[1], x + \\\\texttt{flow}\n * (y,x)[0])\\\\]`\n *\n * An example using the optical flow algorithm described by Gunnar Farneback can be found at\n * opencv_source_code/samples/cpp/fback.cpp\n * (Python) An example using the optical flow algorithm described by Gunnar Farneback can be found at\n * opencv_source_code/samples/python/opt_flow.py\n *\n * @param prev first 8-bit single-channel input image.\n *\n * @param next second input image of the same size and the same type as prev.\n *\n * @param flow computed flow image that has the same size as prev and type CV_32FC2.\n *\n * @param pyr_scale parameter, specifying the image scale (<1) to build pyramids for each image;\n * pyr_scale=0.5 means a classical pyramid, where each next layer is twice smaller than the previous\n * one.\n *\n * @param levels number of pyramid layers including the initial image; levels=1 means that no extra\n * layers are created and only the original images are used.\n *\n * @param winsize averaging window size; larger values increase the algorithm robustness to image noise\n * and give more chances for fast motion detection, but yield more blurred motion field.\n *\n * @param iterations number of iterations the algorithm does at each pyramid level.\n *\n * @param poly_n size of the pixel neighborhood used to find polynomial expansion in each pixel; larger\n * values mean that the image will be approximated with smoother surfaces, yielding more robust\n * algorithm and more blurred motion field, typically poly_n =5 or 7.\n *\n * @param poly_sigma standard deviation of the Gaussian that is used to smooth derivatives used as a\n * basis for the polynomial expansion; for poly_n=5, you can set poly_sigma=1.1, for poly_n=7, a good\n * value would be poly_sigma=1.5.\n *\n * @param flags operation flags that can be a combination of the following:\n * OPTFLOW_USE_INITIAL_FLOW uses the input flow as an initial flow\n * approximation.OPTFLOW_FARNEBACK_GAUSSIAN uses the Gaussian $\\texttt{winsize}\\times\\texttt{winsize}$\n * filter instead of a box filter of the same size for optical flow estimation; usually, this option\n * gives z more accurate flow than with a box filter, at the cost of lower speed; normally, winsize for\n * a Gaussian window should be set to a larger value to achieve the same level of robustness.\n */\nexport declare function calcOpticalFlowFarneback(prev: InputArray, next: InputArray, flow: InputOutputArray, pyr_scale: double, levels: int, winsize: int, iterations: int, poly_n: int, poly_sigma: double, flags: int): void;\n/**\n * The function implements a sparse iterative version of the Lucas-Kanade optical flow in pyramids. See\n * Bouguet00 . The function is parallelized with the TBB library.\n *\n * An example using the Lucas-Kanade optical flow algorithm can be found at\n * opencv_source_code/samples/cpp/lkdemo.cpp\n * (Python) An example using the Lucas-Kanade optical flow algorithm can be found at\n * opencv_source_code/samples/python/lk_track.py\n * (Python) An example using the Lucas-Kanade tracker for homography matching can be found at\n * opencv_source_code/samples/python/lk_homography.py\n *\n * @param prevImg first 8-bit input image or pyramid constructed by buildOpticalFlowPyramid.\n *\n * @param nextImg second input image or pyramid of the same size and the same type as prevImg.\n *\n * @param prevPts vector of 2D points for which the flow needs to be found; point coordinates must be\n * single-precision floating-point numbers.\n *\n * @param nextPts output vector of 2D points (with single-precision floating-point coordinates)\n * containing the calculated new positions of input features in the second image; when\n * OPTFLOW_USE_INITIAL_FLOW flag is passed, the vector must have the same size as in the input.\n *\n * @param status output status vector (of unsigned chars); each element of the vector is set to 1 if\n * the flow for the corresponding features has been found, otherwise, it is set to 0.\n *\n * @param err output vector of errors; each element of the vector is set to an error for the\n * corresponding feature, type of the error measure can be set in flags parameter; if the flow wasn't\n * found then the error is not defined (use the status parameter to find such cases).\n *\n * @param winSize size of the search window at each pyramid level.\n *\n * @param maxLevel 0-based maximal pyramid level number; if set to 0, pyramids are not used (single\n * level), if set to 1, two levels are used, and so on; if pyramids are passed to input then algorithm\n * will use as many levels as pyramids have but no more than maxLevel.\n *\n * @param criteria parameter, specifying the termination criteria of the iterative search algorithm\n * (after the specified maximum number of iterations criteria.maxCount or when the search window moves\n * by less than criteria.epsilon.\n *\n * @param flags operation flags:\n * OPTFLOW_USE_INITIAL_FLOW uses initial estimations, stored in nextPts; if the flag is not set, then\n * prevPts is copied to nextPts and is considered the initial estimate.OPTFLOW_LK_GET_MIN_EIGENVALS use\n * minimum eigen values as an error measure (see minEigThreshold description); if the flag is not set,\n * then L1 distance between patches around the original and a moved point, divided by number of pixels\n * in a window, is used as a error measure.\n *\n * @param minEigThreshold the algorithm calculates the minimum eigen value of a 2x2 normal matrix of\n * optical flow equations (this matrix is called a spatial gradient matrix in Bouguet00), divided by\n * number of pixels in a window; if this value is less than minEigThreshold, then a corresponding\n * feature is filtered out and its flow is not processed, so it allows to remove bad points and get a\n * performance boost.\n */\nexport declare function calcOpticalFlowPyrLK(prevImg: InputArray, nextImg: InputArray, prevPts: InputArray, nextPts: InputOutputArray, status: OutputArray, err: OutputArray, winSize?: Size, maxLevel?: int, criteria?: TermCriteria, flags?: int, minEigThreshold?: double): void;\n/**\n * See the OpenCV sample camshiftdemo.c that tracks colored objects.\n *\n * (Python) A sample explaining the camshift tracking algorithm can be found at\n * opencv_source_code/samples/python/camshift.py\n *\n * @param probImage Back projection of the object histogram. See calcBackProject.\n *\n * @param window Initial search window.\n *\n * @param criteria Stop criteria for the underlying meanShift. returns (in old interfaces) Number of\n * iterations CAMSHIFT took to converge The function implements the CAMSHIFT object tracking algorithm\n * Bradski98 . First, it finds an object center using meanShift and then adjusts the window size and\n * finds the optimal rotation. The function returns the rotated rectangle structure that includes the\n * object position, size, and orientation. The next position of the search window can be obtained with\n * RotatedRect::boundingRect()\n */\nexport declare function CamShift(probImage: InputArray, window: any, criteria: TermCriteria): RotatedRect;\n/**\n * [findTransformECC]\n *\n * @param templateImage single-channel template image; CV_8U or CV_32F array.\n *\n * @param inputImage single-channel input image to be warped to provide an image similar to\n * templateImage, same type as templateImage.\n *\n * @param inputMask An optional mask to indicate valid values of inputImage.\n */\nexport declare function computeECC(templateImage: InputArray, inputImage: InputArray, inputMask?: InputArray): double;\n/**\n * The function finds an optimal affine transform *[A|b]* (a 2 x 3 floating-point matrix) that\n * approximates best the affine transformation between:  In case of point sets, the problem is\n * formulated as follows: you need to find a 2x2 matrix *A* and 2x1 vector *b* so that:\n *\n * `\\\\[[A^*|b^*] = arg \\\\min _{[A|b]} \\\\sum _i \\\\| \\\\texttt{dst}[i] - A { \\\\texttt{src}[i]}^T - b \\\\|\n * ^2\\\\]` where src[i] and dst[i] are the i-th points in src and dst, respectively `$[A|b]$` can be\n * either arbitrary (when fullAffine=true ) or have a form of `\\\\[\\\\begin{bmatrix} a_{11} & a_{12} &\n * b_1 \\\\\\\\ -a_{12} & a_{11} & b_2 \\\\end{bmatrix}\\\\]` when fullAffine=false.\n *\n * [estimateAffine2D], [estimateAffinePartial2D], [getAffineTransform], [getPerspectiveTransform],\n * [findHomography]\n *\n * @param src First input 2D point set stored in std::vector or Mat, or an image stored in Mat.\n *\n * @param dst Second input 2D point set of the same size and the same type as A, or another image.\n *\n * @param fullAffine If true, the function finds an optimal affine transformation with no additional\n * restrictions (6 degrees of freedom). Otherwise, the class of transformations to choose from is\n * limited to combinations of translation, rotation, and uniform scaling (4 degrees of freedom).\n */\nexport declare function estimateRigidTransform(src: InputArray, dst: InputArray, fullAffine: bool): Mat;\n/**\n * The function estimates the optimum transformation (warpMatrix) with respect to ECC criterion (EP08),\n * that is\n *\n * `\\\\[\\\\texttt{warpMatrix} = \\\\texttt{warpMatrix} = \\\\arg\\\\max_{W}\n * \\\\texttt{ECC}(\\\\texttt{templateImage}(x,y),\\\\texttt{inputImage}(x',y'))\\\\]`\n *\n * where\n *\n * `\\\\[\\\\begin{bmatrix} x' \\\\\\\\ y' \\\\end{bmatrix} = W \\\\cdot \\\\begin{bmatrix} x \\\\\\\\ y \\\\\\\\ 1\n * \\\\end{bmatrix}\\\\]`\n *\n * (the equation holds with homogeneous coordinates for homography). It returns the final enhanced\n * correlation coefficient, that is the correlation coefficient between the template image and the\n * final warped input image. When a `$3\\\\times 3$` matrix is given with motionType =0, 1 or 2, the\n * third row is ignored.\n *\n * Unlike findHomography and estimateRigidTransform, the function findTransformECC implements an\n * area-based alignment that builds on intensity similarities. In essence, the function updates the\n * initial transformation that roughly aligns the images. If this information is missing, the identity\n * warp (unity matrix) is used as an initialization. Note that if images undergo strong\n * displacements/rotations, an initial transformation that roughly aligns the images is necessary\n * (e.g., a simple euclidean/similarity transform that allows for the images showing the same image\n * content approximately). Use inverse warping in the second image to take an image close to the first\n * one, i.e. use the flag WARP_INVERSE_MAP with warpAffine or warpPerspective. See also the OpenCV\n * sample image_alignment.cpp that demonstrates the use of the function. Note that the function throws\n * an exception if algorithm does not converges.\n *\n * [computeECC], [estimateAffine2D], [estimateAffinePartial2D], [findHomography]\n *\n * @param templateImage single-channel template image; CV_8U or CV_32F array.\n *\n * @param inputImage single-channel input image which should be warped with the final warpMatrix in\n * order to provide an image similar to templateImage, same type as templateImage.\n *\n * @param warpMatrix floating-point $2\\times 3$ or $3\\times 3$ mapping matrix (warp).\n *\n * @param motionType parameter, specifying the type of motion:\n * MOTION_TRANSLATION sets a translational motion model; warpMatrix is $2\\times 3$ with the first\n * $2\\times 2$ part being the unity matrix and the rest two parameters being estimated.MOTION_EUCLIDEAN\n * sets a Euclidean (rigid) transformation as motion model; three parameters are estimated; warpMatrix\n * is $2\\times 3$.MOTION_AFFINE sets an affine motion model (DEFAULT); six parameters are estimated;\n * warpMatrix is $2\\times 3$.MOTION_HOMOGRAPHY sets a homography as a motion model; eight parameters\n * are estimated;`warpMatrix` is $3\\times 3$.\n *\n * @param criteria parameter, specifying the termination criteria of the ECC algorithm;\n * criteria.epsilon defines the threshold of the increment in the correlation coefficient between two\n * iterations (a negative criteria.epsilon makes criteria.maxcount the only termination criterion).\n * Default values are shown in the declaration above.\n *\n * @param inputMask An optional mask to indicate valid values of inputImage.\n *\n * @param gaussFiltSize An optional value indicating size of gaussian blur filter; (DEFAULT: 5)\n */\nexport declare function findTransformECC(templateImage: InputArray, inputImage: InputArray, warpMatrix: InputOutputArray, motionType: int, criteria: TermCriteria, inputMask: InputArray, gaussFiltSize: int): double;\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function findTransformECC(templateImage: InputArray, inputImage: InputArray, warpMatrix: InputOutputArray, motionType?: int, criteria?: TermCriteria, inputMask?: InputArray): double;\n/**\n * @param probImage Back projection of the object histogram. See calcBackProject for details.\n *\n * @param window Initial search window.\n *\n * @param criteria Stop criteria for the iterative search algorithm. returns : Number of iterations\n * CAMSHIFT took to converge. The function implements the iterative object search algorithm. It takes\n * the input back projection of an object and the initial position. The mass center in window of the\n * back projection image is computed and the search window center shifts to the mass center. The\n * procedure is repeated until the specified number of iterations criteria.maxCount is done or until\n * the window center shifts by less than criteria.epsilon. The algorithm is used inside CamShift and,\n * unlike CamShift , the search window size or orientation do not change during the search. You can\n * simply pass the output of calcBackProject to this function. But better results can be obtained if\n * you pre-filter the back projection and remove the noise. For example, you can do this by retrieving\n * connected components with findContours , throwing away contours with small area ( contourArea ), and\n * rendering the remaining contours with drawContours.\n */\nexport declare function meanShift(probImage: InputArray, window: any, criteria: TermCriteria): int;\n/**\n * The function readOpticalFlow loads a flow field from a file and returns it as a single matrix.\n * Resulting [Mat] has a type CV_32FC2 - floating-point, 2-channel. First channel corresponds to the\n * flow in the horizontal direction (u), second - vertical (v).\n *\n * @param path Path to the file to be loaded\n */\nexport declare function readOpticalFlow(path: any): Mat;\n/**\n * The function stores a flow field in a file, returns true on success, false otherwise. The flow field\n * must be a 2-channel, floating-point matrix (CV_32FC2). First channel corresponds to the flow in the\n * horizontal direction (u), second - vertical (v).\n *\n * @param path Path to the file to be written\n *\n * @param flow Flow field to be stored\n */\nexport declare function writeOpticalFlow(path: any, flow: InputArray): bool;\nexport declare const OPTFLOW_USE_INITIAL_FLOW: any;\nexport declare const OPTFLOW_LK_GET_MIN_EIGENVALS: any;\nexport declare const OPTFLOW_FARNEBACK_GAUSSIAN: any;\nexport declare const MOTION_TRANSLATION: any;\nexport declare const MOTION_EUCLIDEAN: any;\nexport declare const MOTION_AFFINE: any;\nexport declare const MOTION_HOMOGRAPHY: any;\n"},"node_modules_mirada_dist_src_util_base64_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_util_base64_d_ts","originalFileName":"node_modules/mirada/dist/src/util/base64.d.ts","content":"export declare function dataToUrl(data: string, mimeType: string, fileName?: string): string;\nexport declare function dataToBase64(data: string): string;\n/**\n * Creates a DataUrl like `data:image/jpeg;name=hindenburg.jpg;base64,` using given base64 content, mimeType and fileName.\n */\nexport declare function base64ToUrl(base64: string, mimeType: string, fileName?: string): string;\nexport declare function urlToBase64(s: string): string;\nexport declare function urlToData(s: string): string;\nexport declare function isBase64(str: string): boolean;\n/**\n * Extracts the name of a data url like `data:image/jpeg;name=hindenburg.jpg;base64,`..., if any.\n */\nexport declare function getDataUrlFileName(url: string): string;\nexport declare function arrayBufferToBase64(buffer: ArrayBuffer): string;\nexport declare function arrayBufferToUrl(buffer: ArrayBuffer, mime: string, name?: string): string;\nexport declare function arrayBufferToString(buffer: ArrayBuffer): string;\n"},"node_modules_mirada_dist_src_util_fileUtil_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_util_fileUtil_d_ts","originalFileName":"node_modules/mirada/dist/src/util/fileUtil.d.ts","content":"/**\n * if given a file it ignores its contents and alwasys read again from FS\n */\nexport declare function readFile(f: string, FS?: import(\"..\").FS): ArrayBufferView;\n/**\n * Returns file name / path of given file relative to emscripten FS root  (in the context of emscripten FS)\n */\nexport declare function getFileName(path: string): string;\n/**\n * Returns absolute path of given file (in the context of emscripten FS)\n */\nexport declare function getFilePath(path: string): string;\nexport declare function writeFile(name: string, f: ArrayBufferView, FS?: import(\"..\").FS): void;\nexport declare function removeFile(f: string, FS?: import(\"..\").FS): void;\nexport declare function isDir(f: string, FS?: import(\"..\").FS): boolean;\nexport declare function isFile(f: string, FS?: import(\"..\").FS): boolean;\nexport declare const fileUtil: {\n    isDir: typeof isDir;\n    isFile: typeof isFile;\n    removeFile: typeof removeFile;\n    writeFile: typeof writeFile;\n    getFilePath: typeof getFilePath;\n    readFile: typeof readFile;\n    getFileName: typeof getFileName;\n};\nexport declare function loadDataFile(url: string, name?: string): Promise<string>;\n"},"node_modules_mirada_dist_src_util_imageUtil_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_util_imageUtil_d_ts","originalFileName":"node_modules/mirada/dist/src/util/imageUtil.d.ts","content":"import { Mat } from '../types/opencv';\n/**\n * Creates an CV ImageData object from given image.\n */\nexport declare function toImageData(img: Mat): {\n    data: Uint8ClampedArray;\n    width: any;\n    height: any;\n};\n/**\n * Returns a new image that is identical to given (1, 3 or 4 channels)\n * but has 4 RGBA channels.\n */\nexport declare function toRgba(mat: Mat): Mat;\nexport declare function fromFile(f: string): Promise<Mat>;\nexport declare function fromArrayBuffer(a: ArrayBuffer): Promise<Mat>;\nexport declare function fromUrl(f: string): Promise<Mat>;\n"},"node_modules_mirada_dist_src_util_index_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_util_index_d_ts","originalFileName":"node_modules/mirada/dist/src/util/index.d.ts","content":"export { fileUtil, loadDataFile } from './fileUtil';\nexport * from './imageUtil';\nexport { file };\nimport * as file from './fileUtil';\n"},"node_modules_mirada_dist_src_util_misc_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_util_misc_d_ts","originalFileName":"node_modules/mirada/dist/src/util/misc.d.ts","content":"export declare function buildError(e: any): Error;\nexport declare function resolveNodeModule(p: string): string;\n"}};
