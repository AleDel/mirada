export interface Node_modules_mirada_dist_src_file_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_format_canvasCodec_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_format_format_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_format_jimpCodec_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_format_index_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_index_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_opencvReady_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_tool_grabCut_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_tool_index_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_emscripten_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_mirada_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv__hacks_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv__types_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_Affine3_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_Algorithm_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_BFMatcher_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_AutoBuffer_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_BOWTrainer_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_CascadeClassifier_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_calib3d_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_core_cluster_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_core_hal_interface_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_core_array_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_core_utils_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_DescriptorMatcher_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_DynamicBitset_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_dnn_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_Exception_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_features2d_draw_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_FlannBasedMatcher_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_HOGDescriptor_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_imgproc_color_conversions_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_imgproc_draw_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_imgproc_hist_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_imgproc_filter_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_imgproc_object_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_imgproc_shape_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_imgproc_misc_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_index_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_imgproc_feature_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_Logger_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_LshTable_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_imgproc_transform_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_MatExpr_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_MatOp_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_Matx_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_Mat_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_objdetect_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_PCA_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_Node_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_photo_inpaint_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_RotatedRect_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_softdouble_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_softfloat_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_types_opencv_video_track_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_util_base64_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_util_browserImageUtil_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_util_fileUtil_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_util_imageUtil_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_util_index_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface Node_modules_mirada_dist_src_util_misc_d_t {
	isBinary: boolean;
	fileName: string;
	originalFileName: string;
	content: string;
}

export interface magica_definition {
	node_modules_mirada_dist_src_file_d_ts: Node_modules_mirada_dist_src_file_d_t;
	node_modules_mirada_dist_src_format_canvasCodec_d_ts: Node_modules_mirada_dist_src_format_canvasCodec_d_t;
	node_modules_mirada_dist_src_format_format_d_ts: Node_modules_mirada_dist_src_format_format_d_t;
	node_modules_mirada_dist_src_format_jimpCodec_d_ts: Node_modules_mirada_dist_src_format_jimpCodec_d_t;
	node_modules_mirada_dist_src_format_index_d_ts: Node_modules_mirada_dist_src_format_index_d_t;
	node_modules_mirada_dist_src_index_d_ts: Node_modules_mirada_dist_src_index_d_t;
	node_modules_mirada_dist_src_opencvReady_d_ts: Node_modules_mirada_dist_src_opencvReady_d_t;
	node_modules_mirada_dist_src_tool_grabCut_d_ts: Node_modules_mirada_dist_src_tool_grabCut_d_t;
	node_modules_mirada_dist_src_tool_index_d_ts: Node_modules_mirada_dist_src_tool_index_d_t;
	node_modules_mirada_dist_src_types_emscripten_d_ts: Node_modules_mirada_dist_src_types_emscripten_d_t;
	node_modules_mirada_dist_src_types_mirada_d_ts: Node_modules_mirada_dist_src_types_mirada_d_t;
	node_modules_mirada_dist_src_types_opencv__hacks_d_ts: Node_modules_mirada_dist_src_types_opencv__hacks_d_t;
	node_modules_mirada_dist_src_types_opencv__types_d_ts: Node_modules_mirada_dist_src_types_opencv__types_d_t;
	node_modules_mirada_dist_src_types_opencv_Affine3_d_ts: Node_modules_mirada_dist_src_types_opencv_Affine3_d_t;
	node_modules_mirada_dist_src_types_opencv_Algorithm_d_ts: Node_modules_mirada_dist_src_types_opencv_Algorithm_d_t;
	node_modules_mirada_dist_src_types_opencv_BFMatcher_d_ts: Node_modules_mirada_dist_src_types_opencv_BFMatcher_d_t;
	node_modules_mirada_dist_src_types_opencv_AutoBuffer_d_ts: Node_modules_mirada_dist_src_types_opencv_AutoBuffer_d_t;
	node_modules_mirada_dist_src_types_opencv_BOWTrainer_d_ts: Node_modules_mirada_dist_src_types_opencv_BOWTrainer_d_t;
	node_modules_mirada_dist_src_types_opencv_CascadeClassifier_d_ts: Node_modules_mirada_dist_src_types_opencv_CascadeClassifier_d_t;
	node_modules_mirada_dist_src_types_opencv_calib3d_d_ts: Node_modules_mirada_dist_src_types_opencv_calib3d_d_t;
	node_modules_mirada_dist_src_types_opencv_core_cluster_d_ts: Node_modules_mirada_dist_src_types_opencv_core_cluster_d_t;
	node_modules_mirada_dist_src_types_opencv_core_hal_interface_d_ts: Node_modules_mirada_dist_src_types_opencv_core_hal_interface_d_t;
	node_modules_mirada_dist_src_types_opencv_core_array_d_ts: Node_modules_mirada_dist_src_types_opencv_core_array_d_t;
	node_modules_mirada_dist_src_types_opencv_core_utils_d_ts: Node_modules_mirada_dist_src_types_opencv_core_utils_d_t;
	node_modules_mirada_dist_src_types_opencv_DescriptorMatcher_d_ts: Node_modules_mirada_dist_src_types_opencv_DescriptorMatcher_d_t;
	node_modules_mirada_dist_src_types_opencv_DynamicBitset_d_ts: Node_modules_mirada_dist_src_types_opencv_DynamicBitset_d_t;
	node_modules_mirada_dist_src_types_opencv_dnn_d_ts: Node_modules_mirada_dist_src_types_opencv_dnn_d_t;
	node_modules_mirada_dist_src_types_opencv_Exception_d_ts: Node_modules_mirada_dist_src_types_opencv_Exception_d_t;
	node_modules_mirada_dist_src_types_opencv_features2d_draw_d_ts: Node_modules_mirada_dist_src_types_opencv_features2d_draw_d_t;
	node_modules_mirada_dist_src_types_opencv_FlannBasedMatcher_d_ts: Node_modules_mirada_dist_src_types_opencv_FlannBasedMatcher_d_t;
	node_modules_mirada_dist_src_types_opencv_HOGDescriptor_d_ts: Node_modules_mirada_dist_src_types_opencv_HOGDescriptor_d_t;
	node_modules_mirada_dist_src_types_opencv_imgproc_color_conversions_d_ts: Node_modules_mirada_dist_src_types_opencv_imgproc_color_conversions_d_t;
	node_modules_mirada_dist_src_types_opencv_imgproc_draw_d_ts: Node_modules_mirada_dist_src_types_opencv_imgproc_draw_d_t;
	node_modules_mirada_dist_src_types_opencv_imgproc_hist_d_ts: Node_modules_mirada_dist_src_types_opencv_imgproc_hist_d_t;
	node_modules_mirada_dist_src_types_opencv_imgproc_filter_d_ts: Node_modules_mirada_dist_src_types_opencv_imgproc_filter_d_t;
	node_modules_mirada_dist_src_types_opencv_imgproc_object_d_ts: Node_modules_mirada_dist_src_types_opencv_imgproc_object_d_t;
	node_modules_mirada_dist_src_types_opencv_imgproc_shape_d_ts: Node_modules_mirada_dist_src_types_opencv_imgproc_shape_d_t;
	node_modules_mirada_dist_src_types_opencv_imgproc_misc_d_ts: Node_modules_mirada_dist_src_types_opencv_imgproc_misc_d_t;
	node_modules_mirada_dist_src_types_opencv_index_d_ts: Node_modules_mirada_dist_src_types_opencv_index_d_t;
	node_modules_mirada_dist_src_types_opencv_imgproc_feature_d_ts: Node_modules_mirada_dist_src_types_opencv_imgproc_feature_d_t;
	node_modules_mirada_dist_src_types_opencv_Logger_d_ts: Node_modules_mirada_dist_src_types_opencv_Logger_d_t;
	node_modules_mirada_dist_src_types_opencv_LshTable_d_ts: Node_modules_mirada_dist_src_types_opencv_LshTable_d_t;
	node_modules_mirada_dist_src_types_opencv_imgproc_transform_d_ts: Node_modules_mirada_dist_src_types_opencv_imgproc_transform_d_t;
	node_modules_mirada_dist_src_types_opencv_MatExpr_d_ts: Node_modules_mirada_dist_src_types_opencv_MatExpr_d_t;
	node_modules_mirada_dist_src_types_opencv_MatOp_d_ts: Node_modules_mirada_dist_src_types_opencv_MatOp_d_t;
	node_modules_mirada_dist_src_types_opencv_Matx_d_ts: Node_modules_mirada_dist_src_types_opencv_Matx_d_t;
	node_modules_mirada_dist_src_types_opencv_Mat_d_ts: Node_modules_mirada_dist_src_types_opencv_Mat_d_t;
	node_modules_mirada_dist_src_types_opencv_objdetect_d_ts: Node_modules_mirada_dist_src_types_opencv_objdetect_d_t;
	node_modules_mirada_dist_src_types_opencv_PCA_d_ts: Node_modules_mirada_dist_src_types_opencv_PCA_d_t;
	node_modules_mirada_dist_src_types_opencv_Node_d_ts: Node_modules_mirada_dist_src_types_opencv_Node_d_t;
	node_modules_mirada_dist_src_types_opencv_photo_inpaint_d_ts: Node_modules_mirada_dist_src_types_opencv_photo_inpaint_d_t;
	node_modules_mirada_dist_src_types_opencv_RotatedRect_d_ts: Node_modules_mirada_dist_src_types_opencv_RotatedRect_d_t;
	node_modules_mirada_dist_src_types_opencv_softdouble_d_ts: Node_modules_mirada_dist_src_types_opencv_softdouble_d_t;
	node_modules_mirada_dist_src_types_opencv_softfloat_d_ts: Node_modules_mirada_dist_src_types_opencv_softfloat_d_t;
	node_modules_mirada_dist_src_types_opencv_video_track_d_ts: Node_modules_mirada_dist_src_types_opencv_video_track_d_t;
	node_modules_mirada_dist_src_util_base64_d_ts: Node_modules_mirada_dist_src_util_base64_d_t;
	node_modules_mirada_dist_src_util_browserImageUtil_d_ts: Node_modules_mirada_dist_src_util_browserImageUtil_d_t;
	node_modules_mirada_dist_src_util_fileUtil_d_ts: Node_modules_mirada_dist_src_util_fileUtil_d_t;
	node_modules_mirada_dist_src_util_imageUtil_d_ts: Node_modules_mirada_dist_src_util_imageUtil_d_t;
	node_modules_mirada_dist_src_util_index_d_ts: Node_modules_mirada_dist_src_util_index_d_t;
	node_modules_mirada_dist_src_util_misc_d_ts: Node_modules_mirada_dist_src_util_misc_d_t;
}
export const magica: magica_definition = {"node_modules_mirada_dist_src_file_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_file_d_ts","originalFileName":"node_modules/mirada/dist/src/file.d.ts","content":"import { ImageData, Mat } from './types/opencv';\nimport fileType = require('file-type');\n/**\n * A thin layer on top of cv.Mat with lots of utilities to load, write, encode, etc.\n */\nexport declare class File {\n    readonly name: string;\n    protected mat: Mat;\n    constructor(name: string, mat: Mat);\n    size(): {\n        width: any;\n        height: any;\n    };\n    getMimeType(): string | undefined;\n    getExtension(): string;\n    asMat(): Mat;\n    asImageData(): ImageData;\n    asDataUrl(): string;\n    /**\n     * Returns an array buffer containing the image encoded in given format or inferring format from its name.\n     */\n    asArrayBuffer(format?: string): Promise<ArrayBuffer>;\n    /**\n     * Writes this image on given file path, encoded in given format (or inferred form current name).\n     */\n    write(path?: string, format?: string): Promise<void>;\n    /**\n     * Shows this image in given HTML canvas or image element.\n     */\n    show(el: HTMLCanvasElement | HTMLImageElement): void;\n    asBase64(format?: string): Promise<string>;\n    delete(): any;\n    /**\n     * Loads file from given base64 string containing an encoded image.\n    */\n    static fromBase64(base64: string, name?: string): Promise<File>;\n    /**\n     * Loads file from given array buffer containing an encoded image.\n     */\n    static fromArrayBuffer(buffer: ArrayBuffer, name?: string): Promise<File>;\n    /**\n     * Loads file from given array buffer view containing an encoded image.\n     */\n    static fromArrayBufferView(a: ArrayBufferView, name?: string): Promise<File>;\n    static getBufferFileType(a: ArrayBuffer): fileType.FileTypeResult;\n    static getBufferFileName(a: ArrayBuffer): string;\n    /**\n     * Loads file from given data url string containing an encoded image.\n    */\n    static fromDataUrl(dataUrl: string, name?: string): Promise<File>;\n    /**\n     * Loads files from files in html input element of type \"file\".\n     */\n    static fromHtmlFileInputElement(el: HTMLInputElement): Promise<File[]>;\n    /**\n     * Loads file form existing HTMLCanvasElement or HTMLImageElement\n     */\n    static fromCanvas(el: HTMLCanvasElement | HTMLImageElement | string): File;\n    /**\n     * Shortcut for [resolve] that returns the first result.\n     */\n    static resolveOne(files: string | File | undefined | (string | File | undefined)[]): Promise<File | undefined>;\n    /**\n     * Given paths, urls or files it will try to load them all and return a list of File for those succeed.\n     */\n    static resolve(files: string | File | undefined | (string | File | undefined)[]): Promise<File[]>;\n    static isFile(f: any): f is File;\n    static asPath(f: string | File): string;\n    static fromData(data: ImageData, name: string): File;\n    static fromMat(mat: Mat, name?: string): File;\n    static fromUrl(url: string, o?: RequestInit & FileOptions): Promise<File>;\n    static fromFile(path: string, o?: FileOptions): Promise<File>;\n}\ninterface FileOptions {\n    name?: string;\n}\nexport {};\n"},"node_modules_mirada_dist_src_format_canvasCodec_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_format_canvasCodec_d_ts","originalFileName":"node_modules/mirada/dist/src/format/canvasCodec.d.ts","content":"import { FormatCodec } from '../types/mirada';\n/**\n  Example of declaring a format codec that uses DOM canvas instance which must be provided by the user.\n  \n```ts\nimport * as Jimp from 'jimp'\nclass JimpProxy implements FormatProxyClass {\n  async create() {\n   return new JimpFormatCodec(Jimp)\n  }\n}\n```\n */\nexport declare class CanvasCodec implements FormatCodec {\n    constructor();\n    decode(buffer: ArrayBuffer): Promise<ImageData>;\n    encode(data: ImageData, format: string): Promise<ArrayBuffer>;\n}\n"},"node_modules_mirada_dist_src_format_format_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_format_format_d_ts","originalFileName":"node_modules/mirada/dist/src/format/format.d.ts","content":"import { FormatCodec, FormatProxy } from '../types/mirada';\n/**\n * Nor or opencv.js or this library implement any image format so users are\n * responsible of providing a FormatProxy using some library.\n *\n */\nexport declare function installFormatProxy(proxy: FormatProxy): Promise<void>;\n/**\n * @internal\n */\nexport declare function loadFormatProxies(): Promise<void>;\nexport declare function getDefaultCodec(): FormatCodec;\n"},"node_modules_mirada_dist_src_format_jimpCodec_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_format_jimpCodec_d_ts","originalFileName":"node_modules/mirada/dist/src/format/jimpCodec.d.ts","content":"import { FormatCodec } from '../types/mirada';\ndeclare type AnyConstructor = {\n    [a: string]: any;\n    new (...args: any[]): any;\n};\ndeclare type Jimp = AnyConstructor;\n/**\n  Example of declaring a Jimp proxy as a class\n  \n```ts\nimport * as Jimp from 'jimp'\nclass JimpProxy implements FormatProxyClass {\n  async create() {\n   return new JimpFormatCodec(Jimp)\n  }\n}\n```\n */\nexport declare class JimpCodec implements FormatCodec {\n    protected jimp: Jimp;\n    constructor(jimp: Jimp);\n    decode(buffer: ArrayBuffer): Promise<ImageData>;\n    encode(data: ImageData, format: string): Promise<ArrayBuffer>;\n}\nexport {};\n"},"node_modules_mirada_dist_src_format_index_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_format_index_d_ts","originalFileName":"node_modules/mirada/dist/src/format/index.d.ts","content":"export * from './canvasCodec';\nexport * from './format';\nexport * from './jimpCodec';\n"},"node_modules_mirada_dist_src_index_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_index_d_ts","originalFileName":"node_modules/mirada/dist/src/index.d.ts","content":"export { File } from './file';\nexport * from './format/';\nexport * from './tool/';\nexport * from './opencvReady';\nexport * from './types/mirada';\nexport * from './util';\n"},"node_modules_mirada_dist_src_opencvReady_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_opencvReady_d_ts","originalFileName":"node_modules/mirada/dist/src/opencvReady.d.ts","content":"import { FS } from './types/emscripten';\nexport declare const FS_ROOT = \"/work\";\n/**\n * gets the emscripten FS API\n */\nexport declare function getFS(): FS;\ninterface LoadOptions {\n    onloadCallback?: () => void;\n    opencvUrl?: string;\n    /**\n     * node.js : current working dir. By default is '.'\n     */\n    cwd?: string;\n}\n/**\n * Loads opencv.js file. It will do it only once no matter if called multiple times.\n * In the browser a new script element is created to load the file while in Node.js\n * the file is loaded using a require() call.\n *\n * Returns a promise resolved when the library is ready or rejected if there's a problem.\n *\n * Notice that among the options users can define the location of opencv.js file, which\n * in the case of the browser it could be in an external server.\n */\nexport declare function loadOpencv(options?: LoadOptions): Promise<void> | Promise<FS>;\nexport {};\n"},"node_modules_mirada_dist_src_tool_grabCut_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_tool_grabCut_d_ts","originalFileName":"node_modules/mirada/dist/src/tool/grabCut.d.ts","content":"import { File } from '..';\nimport { ImageData, Rect, Scalar } from '../types/opencv';\nexport interface GrabCutOptions extends Rect {\n    image: File;\n    /**\n     * If given a rectangle frame will be drawn on given coordinates with that color.\n     */\n    frameColor?: Scalar;\n}\ninterface GrabCutResult {\n    image: ImageData;\n}\nexport declare function grabCut(o: GrabCutOptions): Promise<GrabCutResult>;\nexport {};\n"},"node_modules_mirada_dist_src_tool_index_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_tool_index_d_ts","originalFileName":"node_modules/mirada/dist/src/tool/index.d.ts","content":"import { grabCut } from './grabCut';\nexport declare const tool: {\n    grabCut: typeof grabCut;\n};\n"},"node_modules_mirada_dist_src_types_emscripten_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_emscripten_d_ts","originalFileName":"node_modules/mirada/dist/src/types/emscripten.d.ts","content":"interface Lookup {\n    path: string;\n    node: FSNode;\n}\ninterface FSStream {\n}\ninterface FSNode {\n}\nexport interface FS {\n    lookupPath(path: string, opts: any): Lookup;\n    getPath(node: FSNode): string;\n    isFile(mode: number): boolean;\n    isDir(mode: number): boolean;\n    isLink(mode: number): boolean;\n    isChrdev(mode: number): boolean;\n    isBlkdev(mode: number): boolean;\n    isFIFO(mode: number): boolean;\n    isSocket(mode: number): boolean;\n    major(dev: number): number;\n    minor(dev: number): number;\n    makedev(ma: number, mi: number): number;\n    registerDevice(dev: number, ops: any): void;\n    syncfs(populate: boolean, callback: (e: any) => any): void;\n    syncfs(callback: (e: any) => any, populate?: boolean): void;\n    mount(type: any, opts: any, mountpoint: string): any;\n    unmount(mountpoint: string): void;\n    mkdir(path: string, mode?: number): any;\n    mkdev(path: string, mode?: number, dev?: number): any;\n    symlink(oldpath: string, newpath: string): any;\n    rename(old_path: string, new_path: string): void;\n    rmdir(path: string): void;\n    readdir(path: string): string[];\n    unlink(path: string): void;\n    readlink(path: string): string;\n    stat(path: string, dontFollow?: boolean): any;\n    lstat(path: string): any;\n    chmod(path: string, mode: number, dontFollow?: boolean): void;\n    lchmod(path: string, mode: number): void;\n    fchmod(fd: number, mode: number): void;\n    chown(path: string, uid: number, gid: number, dontFollow?: boolean): void;\n    lchown(path: string, uid: number, gid: number): void;\n    fchown(fd: number, uid: number, gid: number): void;\n    truncate(path: string, len: number): void;\n    ftruncate(fd: number, len: number): void;\n    utime(path: string, atime: number, mtime: number): void;\n    open(path: string, flags: string, mode?: number, fd_start?: number, fd_end?: number): FSStream;\n    close(stream: FSStream): void;\n    llseek(stream: FSStream, offset: number, whence: number): any;\n    read(stream: FSStream, buffer: ArrayBufferView, offset: number, length: number, position?: number): number;\n    write(stream: FSStream, buffer: ArrayBufferView, offset: number, length: number, position?: number, canOwn?: boolean): number;\n    allocate(stream: FSStream, offset: number, length: number): void;\n    mmap(stream: FSStream, buffer: ArrayBufferView, offset: number, length: number, position: number, prot: number, flags: number): any;\n    ioctl(stream: FSStream, cmd: any, arg: any): any;\n    readFile(path: string, opts?: {\n        encoding: string;\n        flags: string;\n    }): ArrayBufferView;\n    writeFile(path: string, data: ArrayBufferView, opts?: {\n        encoding: string;\n        flags: string;\n    }): void;\n    writeFile(path: string, data: string, opts?: {\n        encoding: string;\n        flags: string;\n    }): void;\n    analyzePath(p: string): any;\n    cwd(): string;\n    chdir(path: string): void;\n    init(input: () => number, output: (c: number) => any, error: (c: number) => any): void;\n    createLazyFile(parent: string, name: string, url: string, canRead: boolean, canWrite: boolean): FSNode;\n    createLazyFile(parent: FSNode, name: string, url: string, canRead: boolean, canWrite: boolean): FSNode;\n    createPreloadedFile(parent: string, name: string, url: string, canRead: boolean, canWrite: boolean, onload?: () => void, onerror?: () => void, dontCreateFile?: boolean, canOwn?: boolean): void;\n    createPreloadedFile(parent: FSNode, name: string, url: string, canRead: boolean, canWrite: boolean, onload?: () => void, onerror?: () => void, dontCreateFile?: boolean, canOwn?: boolean): void;\n    createDataFile(parent: string, name: string, data: ArrayBufferView, canRead: boolean, canWrite: boolean, canOwn: boolean): void;\n}\nexport interface EmscriptenModule {\n    print(str: string): void;\n    printErr(str: string): void;\n    arguments: string[];\n    environment: EnvironmentType;\n    preInit: Array<{\n        (): void;\n    }>;\n    preRun: Array<{\n        (): void;\n    }>;\n    postRun: Array<{\n        (): void;\n    }>;\n    onAbort: {\n        (what: any): void;\n    };\n    onRuntimeInitialized: {\n        (): void;\n    };\n    preinitializedWebGLContext: WebGLRenderingContext;\n    noInitialRun: boolean;\n    noExitRuntime: boolean;\n    logReadFiles: boolean;\n    filePackagePrefixURL: string;\n    wasmBinary: ArrayBuffer;\n    destroy(object: object): void;\n    getPreloadedPackage(remotePackageName: string, remotePackageSize: number): ArrayBuffer;\n    instantiateWasm(imports: WebAssemblyImports, successCallback: (module: WebAssemblyModule) => void): WebAssemblyExports;\n    locateFile(url: string): string;\n    onCustomMessage(event: MessageEvent): void;\n    Runtime: any;\n    ccall(ident: string, returnType: ValueType | null, argTypes: ValueType[], args: TypeCompatibleWithC[], opts?: CCallOpts): any;\n    cwrap(ident: string, returnType: ValueType | null, argTypes: ValueType[], opts?: CCallOpts): (...args: any[]) => any;\n    setValue(ptr: number, value: any, type: string, noSafe?: boolean): void;\n    getValue(ptr: number, type: string, noSafe?: boolean): number;\n    ALLOC_NORMAL: number;\n    ALLOC_STACK: number;\n    ALLOC_STATIC: number;\n    ALLOC_DYNAMIC: number;\n    ALLOC_NONE: number;\n    allocate(slab: any, types: string | string[], allocator: number, ptr: number): number;\n    HEAP: Int32Array;\n    IHEAP: Int32Array;\n    FHEAP: Float64Array;\n    HEAP8: Int8Array;\n    HEAP16: Int16Array;\n    HEAP32: Int32Array;\n    HEAPU8: Uint8Array;\n    HEAPU16: Uint16Array;\n    HEAPU32: Uint32Array;\n    HEAPF32: Float32Array;\n    HEAPF64: Float64Array;\n    TOTAL_STACK: number;\n    TOTAL_MEMORY: number;\n    FAST_MEMORY: number;\n    addOnPreRun(cb: () => any): void;\n    addOnInit(cb: () => any): void;\n    addOnPreMain(cb: () => any): void;\n    addOnExit(cb: () => any): void;\n    addOnPostRun(cb: () => any): void;\n    intArrayFromString(stringy: string, dontAddNull?: boolean, length?: number): number[];\n    intArrayToString(array: number[]): string;\n    writeStringToMemory(str: string, buffer: number, dontAddNull: boolean): void;\n    writeArrayToMemory(array: number[], buffer: number): void;\n    writeAsciiToMemory(str: string, buffer: number, dontAddNull: boolean): void;\n    addRunDependency(id: any): void;\n    removeRunDependency(id: any): void;\n    preloadedImages: any;\n    preloadedAudios: any;\n    _malloc(size: number): number;\n    _free(ptr: number): void;\n}\ndeclare type EnvironmentType = \"WEB\" | \"NODE\" | \"SHELL\" | \"WORKER\";\ndeclare type ValueType = \"number\" | \"string\" | \"array\" | \"boolean\";\ndeclare type TypeCompatibleWithC = number | string | any[] | boolean;\ndeclare type WebAssemblyImports = Array<{\n    name: string;\n    kind: string;\n}>;\ndeclare type WebAssemblyExports = Array<{\n    module: string;\n    name: string;\n    kind: string;\n}>;\ninterface CCallOpts {\n    async?: boolean;\n}\ninterface WebAssemblyModule {\n}\nexport {};\n"},"node_modules_mirada_dist_src_types_mirada_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_mirada_d_ts","originalFileName":"node_modules/mirada/dist/src/types/mirada.d.ts","content":"import { ImageData } from './opencv';\nimport { FS as _FS } from './emscripten';\nimport { CV } from './opencv';\ndeclare global {\n    var cv: CV & {\n        FS: _FS;\n    };\n}\nexport { cv, CV, _FS as FS };\n/**\n * User provided image formats encode/decode implementation. The proxy is responsible of creating codec instances\n *\n   * This is particularly useful in this library so it can actually contain the implementation of concrete Codecs (see [JimpCodec]) without actually being responsible of loading / instantiating the library which will have to be handled by a JimpProxy provided by a third party (test, playground/user) . In other words, mirada provides codecs implementations for several libraries and at the while keeping agnostic/independent\n   *\n   * This is probably called only once and after obtaining a codec the same instance is used by the manager.\n\n   *\n */\nexport declare type FormatProxy = FormatProxyClass | (() => FormatCodec) | (() => Promise<FormatCodec>);\n/**\n * a class-like representation for format proxy instead functions\n * */\nexport interface FormatProxyClass {\n    /**\n     * This is probably called only once and after obtaining a codec the same instance is used by the manager.\n     */\n    create(): Promise<FormatCodec>;\n}\n/**\n * Codec instances are created by format proxies and are responsible of encode and decode certain set of image formats. See\n * IMPORTANT: formats are lowercase and in general the common extension of files\n */\nexport interface FormatCodec {\n    /**\n   * Given an array buffer that contains the content of an encoded image it will return a\n   * decoded ImageData object. The format parameter could be needed by some poor decoders\n   * that don't support file type sniffing. For example, magica or jimp libraries don't need this.\n   */\n    decode(buffer: ArrayBuffer, format?: string): Promise<ImageData>;\n    /**\n     * given an image data representing an unencoded raw image it will return an array buffer containing the enconcoded image content in given format.\n     */\n    encode(data: ImageData, format: string): Promise<ArrayBuffer>;\n    /**\n     * if provided an error will be thrown in case users request to decode to a format not included in this list.\n     */\n    getSupportedDecodeFormats?(): string[];\n    /**\n     * if provided an error will be thrown in case users request to encode to a format not included in this list.\n     */\n    getSupportedEncodeFormats?(): string[];\n}\n"},"node_modules_mirada_dist_src_types_opencv__hacks_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv__hacks_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/_hacks.d.ts","content":"// Scalar, Point, Rect, etc are defined by opencv.js (helpers.js) and we need to declare them manually:\n\nexport declare class Range {\n  public start: number\n  public end: number\n  public constructor(start: number, end: number)\n}\n\nexport declare class Scalar extends Array<number> {\n  public static all(...v: number[]): Scalar;\n}\n// Hack: expose Mat super classes like Mat_, InputArray, Vector, OutputArray we make them alias of Mat to simplify and make it work\nexport { Mat as InputArray, Mat as InputArrayOfArrays, Mat as InputOutputArray, Mat as InputOutputArrayOfArrays, Mat as MatVector, Mat as OutputArray, Mat as OutputArrayOfArrays } from './Mat'\nexport { Scalar as GScalar }\nexport { Point as Point2f }\nexport { Point as KeyPoint }\nexport { Point as Point2l }\nexport { Size as Point2d }\nexport { Size as Size2d }\nexport { Size as Size2f }\nexport { Size as Size2l }\nexport { Rect as Rect_ }\n\nexport declare class Point {\n  public constructor(x: number, y: number);\n  public x: number;\n  public y: number;\n}\n\n\nexport declare class Size {\n  public constructor(width: number, height: number);\n  public width: number;\n  public height: number;\n}\n\n\nexport declare class Rect {\n  public constructor();\n  public constructor(point: Point, size: Size);\n  public constructor(x: number, y: number, width: number, height: number);\n  public x: number;\n  public y: number;\n  public width: number;\n  public height: number;\n}\n\n\n// declare class RotatedRect {\n//   public center: Point\n//   public size: Size\n//   angle: number\n//   public constructor()\n//   public constructor(center: Point, size: Size, angle: number)\n//   public static rotatedRectPoints(obj:any):any\n//   public static rotatedRectBoundingRect(obj:any):any\n//   public static rotatedRectBoundingRect2f(obj:any):any\n// }\n\nexport declare class TermCriteria {\n  public type: number\n  public maxCount: number\n  public epsilon: number\n  public constructor()\n  public constructor(type: number, maxCount: number, epsilon: number)\n}\n\nexport declare class MinMaxLoc {\n  public minVal: number\n  public maxVal: number\n  public minLoc: Point\n  public maxLoc: Point\n  public constructor()\n  public constructor(minVal: number, maxVal: number, minLoc: Point, maxLoc: Point)\n}\n\n// expose emscripten / opencv.js specifics\n\nexport declare function exceptionFromPtr(err: number): any\nexport declare function onRuntimeInitialized(): any\nexport declare function FS_createDataFile(arg0: string, path: string, data: Uint8Array, arg3: boolean, arg4: boolean, arg5: boolean): any\n\ndeclare class Vector<T> {\n  get(i: number): T\n  set(i: number, t: T): void\n  size(): number\n  push_back(n: T): any\n  resize(count: number, value?: T): void\n  delete(): void\n}\n\nexport declare class Vec3d extends Vector<any> { }\nexport declare class IntVector extends Vector<number> { }\nexport declare class FloatVector extends Vector<number> { }\nexport declare class DoubleVector extends Vector<number>{ }\nexport declare class PointVector extends Vector<Point> { }\nexport declare class KeyPointVector extends Vector<any> { }\nexport declare class DMatchVector extends Vector<any> { }\nexport declare class DMatchVectorVector extends Vector<Vector<any>> { }\n\nexport declare class RectVector extends Rect implements Vector<Rect>{\n  get(i: number): Rect\n  set(i: number, t: Rect): void\n  size(): number\n  push_back(n: Rect): void\n  resize(count: number, value?: Rect | undefined): void\n  delete(): void\n}\n\n\n\nimport { LineTypes, Mat, RotatedRect } from '.'\n\nexport declare function matFromImageData(imageData: ImageData): Mat\n\n\n/** since we don't support inheritance yet we force Mat to extend Mat_ which type defined here: */\nexport declare class Mat_ extends Vector<Mat> {\n  public delete(): void\n  public data: ImageData\n  public ucharPtr(i: any, j: any): any\n  public charPtr(i: any, j: any): any\n  public charPtr(i: any, j: any): any\n  public shortPtr(i: any, j: any): any\n  public ushortPtr(i: any, j: any): any\n  public intPtr(i: any, j: any): any\n  public floatPtr(i: any, j: any): any\n  public doublePtr(i: any, j: any): any\n  public intPtr(i: any, j: any): any\n  public roi(rect: Rect): Mat\n}\n\nexport declare class ImageData {\n  data: ArrayBufferView\n  width: number\n  height: number\n}\n\n// TODO this types should be exposed by the tool - want to make it work:\nexport declare const CV_8UC1: number\nexport declare const CV_8U: number\nexport declare const CV_16S: number\nexport declare const CV_8UC3: any\nexport declare const CV_32S: number\nexport declare const CV_8S: number\nexport declare const CV_8UC4: number\nexport declare const CV_32F: number\n\nexport declare function ellipse1(dst: Mat, rotatedRect: RotatedRect, ellipseColor: Scalar, arg0: number, line: LineTypes): void\nexport declare function imread(canvasOrImageHtmlElement: HTMLElement | string): Mat\nexport declare function imshow(canvasSource: HTMLElement | string, mat: Mat): void\n\nexport declare class VideoCapture {\n  public constructor(videoSource: HTMLVideoElement | string)\n}\n\n\n// Missing imports: \nexport type Mat4 = any\nexport type Mat3 = any\nexport type Vec3 = any\nexport type float_type = any\nexport type int = any\nexport type bool = any\nexport type FileNode = any\nexport type FileStorage = any\nexport type Ptr = any\nexport type size_t = any\nexport type double = any\nexport type DMatch = any\nexport type float = any\nexport type UMat = any\nexport type DetectionROI = any\nexport type Matrix = any\nexport type BucketKey = any\nexport type Bucket = any\nexport type LshStats = any\nexport type MatAllocator = any\nexport type uchar = any\nexport type MatSize = any\nexport type MatStep = any\nexport type UMatData = any\nexport type typename = any\nexport type Vec = any\nexport type Point_ = any\nexport type Point3_ = any\nexport type MatCommaInitializer_ = any\nexport type MatIterator_ = any\nexport type MatConstIterator_ = any\nexport type AccessFlag = any\nexport type UMatUsageFlags = any\nexport type _Tp = any\nexport type Matx_AddOp = any\nexport type Matx_SubOp = any\nexport type _T2 = any\nexport type Matx_ScaleOp = any\nexport type Matx_MulOp = any\nexport type Matx_DivOp = any\nexport type Matx_MatMulOp = any\nexport type Matx_TOp = any\nexport type diag_type = any\nexport type _EqPredicate = any\nexport type cvhalDFT = any\nexport type schar = any\nexport type ushort = any\nexport type short = any\nexport type int64 = any\nexport type ErrorCallback = any\nexport type unsigned = any\nexport type uint64 = any\nexport type float16_t = any\nexport type AsyncArray = any\nexport type Net = any\nexport type Moments = any\nexport type uint64_t = any\nexport type uint32_t = any\nexport type int32_t = any\nexport type int64_t = any\n"},"node_modules_mirada_dist_src_types_opencv__types_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv__types_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/_types.d.ts","content":"export * from './Affine3'\nexport * from './Algorithm'\nexport * from './AutoBuffer'\nexport * from './BFMatcher'\nexport * from './BOWTrainer'\nexport * from './calib3d'\nexport * from './CascadeClassifier'\nexport * from './core_array'\nexport * from './core_cluster'\nexport * from './core_hal_interface'\nexport * from './core_utils'\nexport * from './DescriptorMatcher'\nexport * from './dnn'\nexport * from './DynamicBitset'\nexport * from './Exception'\nexport * from './features2d_draw'\nexport * from './FlannBasedMatcher'\nexport * from './HOGDescriptor'\nexport * from './imgproc_color_conversions'\nexport * from './imgproc_draw'\nexport * from './imgproc_feature'\nexport * from './imgproc_filter'\nexport * from './imgproc_hist'\nexport * from './imgproc_misc'\nexport * from './imgproc_object'\nexport * from './imgproc_shape'\nexport * from './imgproc_transform'\nexport * from './Logger'\nexport * from './LshTable'\nexport * from './Mat'\nexport * from './MatExpr'\nexport * from './MatOp'\nexport * from './Matx'\nexport * from './Node'\nexport * from './objdetect'\nexport * from './PCA'\nexport * from './photo_inpaint'\nexport * from './RotatedRect'\nexport * from './softdouble'\nexport * from './softfloat'\nexport * from './video_track'\nexport * from './_hacks'\n"},"node_modules_mirada_dist_src_types_opencv_Affine3_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_Affine3_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/Affine3.d.ts","content":"\nimport { float_type, int, Mat, Mat3, Mat4, Vec3 } from './_types'\n\n/**\n * It represents a 4x4 homogeneous transformation matrix `$T$`\n * \n * `\\\\[T = \\\\begin{bmatrix} R & t\\\\\\\\ 0 & 1\\\\\\\\ \\\\end{bmatrix} \\\\]`\n * \n * where `$R$` is a 3x3 rotation matrix and `$t$` is a 3x1 translation vector.\n * \n * You can specify `$R$` either by a 3x3 rotation matrix or by a 3x1 rotation vector, which is\n * converted to a 3x3 rotation matrix by the Rodrigues formula.\n * \n * To construct a matrix `$T$` representing first rotation around the axis `$r$` with rotation angle\n * `$|r|$` in radian (right hand rule) and then translation by the vector `$t$`, you can use\n * \n * ```cpp\n * cv::Vec3f r, t;\n * cv::Affine3f T(r, t);\n * ```\n * \n * If you already have the rotation matrix `$R$`, then you can use\n * \n * ```cpp\n * cv::Matx33f R;\n * cv::Affine3f T(R, t);\n * ```\n * \n * To extract the rotation matrix `$R$` from `$T$`, use\n * \n * ```cpp\n * cv::Matx33f R = T.rotation();\n * ```\n * \n * To extract the translation vector `$t$` from `$T$`, use\n * \n * ```cpp\n * cv::Vec3f t = T.translation();\n * ```\n * \n * To extract the rotation vector `$r$` from `$T$`, use\n * \n * ```cpp\n * cv::Vec3f r = T.rvec();\n * ```\n * \n * Note that since the mapping from rotation vectors to rotation matrices is many to one. The returned\n * rotation vector is not necessarily the one you used before to set the matrix.\n * \n * If you have two transformations `$T = T_1 * T_2$`, use\n * \n * ```cpp\n * cv::Affine3f T, T1, T2;\n * T = T2.concatenate(T1);\n * ```\n * \n * To get the inverse transform of `$T$`, use\n * \n * ```cpp\n * cv::Affine3f T, T_inv;\n * T_inv = T.inv();\n * ```\n * \n * Source:\n * [opencv2/core/affine.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/core/affine.hpp#L129).\n * \n */\nexport declare class Affine3 {\n\n  public matrix: Mat4\n\n  public constructor()\n\n  public constructor(affine: Mat4)\n\n  /**\n   *   The resulting 4x4 matrix is\n   *   \n   *   `\\\\[ \\\\begin{bmatrix} R & t\\\\\\\\ 0 & 1\\\\\\\\ \\\\end{bmatrix} \\\\]`\n   *   \n   *   @param R 3x3 rotation matrix.\n   *   \n   *   @param t 3x1 translation vector.\n   */\n  public constructor(R: Mat3, t?: Vec3)\n\n  /**\n   *   Rodrigues vector.\n   *   \n   *   The last row of the current matrix is set to [0,0,0,1].\n   *   \n   *   @param rvec 3x1 rotation vector. Its direction indicates the rotation axis and its length\n   * indicates the rotation angle in radian (using right hand rule).\n   *   \n   *   @param t 3x1 translation vector.\n   */\n  public constructor(rvec: Vec3, t?: Vec3)\n\n  /**\n   *   Combines all constructors above. Supports 4x4, 3x4, 3x3, 1x3, 3x1 sizes of data matrix.\n   *   \n   *   The last row of the current matrix is set to [0,0,0,1] when data is not 4x4.\n   *   \n   *   @param data 1-channel matrix. when it is 4x4, it is copied to the current matrix and t is not\n   * used. When it is 3x4, it is copied to the upper part 3x4 of the current matrix and t is not used.\n   * When it is 3x3, it is copied to the upper left 3x3 part of the current matrix. When it is 3x1 or\n   * 1x3, it is treated as a rotation vector and the Rodrigues formula is used to compute a 3x3 rotation\n   * matrix.\n   *   \n   *   @param t 3x1 translation vector. It is used only when data is neither 4x4 nor 3x4.\n   */\n  public constructor(data: Mat, t?: Vec3)\n\n  public constructor(vals: float_type)\n\n  public cast(arg401: any): Affine3\n\n  public concatenate(affine: Affine3): Affine3\n\n  /**\n   *   the inverse of the current matrix.\n   */\n  public inv(method?: int): Affine3\n\n  /**\n   *   Copy the 3x3 matrix L to the upper left part of the current matrix\n   *   \n   *   It sets the upper left 3x3 part of the matrix. The remaining part is unaffected.\n   *   \n   *   @param L 3x3 matrix.\n   */\n  public linear(L: Mat3): Mat3\n\n  /**\n   *   the upper left 3x3 part\n   */\n  public linear(): Mat3\n\n  public rotate(R: Mat3): Affine3\n\n  public rotate(rvec: Vec3): Affine3\n\n  /**\n   *   Rotation matrix.\n   *   \n   *   Copy the rotation matrix to the upper left 3x3 part of the current matrix. The remaining elements\n   * of the current matrix are not changed.\n   *   \n   *   @param R 3x3 rotation matrix.\n   */\n  public rotation(R: Mat3): Mat3\n\n  /**\n   *   Rodrigues vector.\n   *   \n   *   It sets the upper left 3x3 part of the matrix. The remaining part is unaffected.\n   *   \n   *   @param rvec 3x1 rotation vector. The direction indicates the rotation axis and its length\n   * indicates the rotation angle in radian (using the right thumb convention).\n   */\n  public rotation(rvec: Vec3): Vec3\n\n  /**\n   *   Combines rotation methods above. Supports 3x3, 1x3, 3x1 sizes of data matrix.\n   *   \n   *   It sets the upper left 3x3 part of the matrix. The remaining part is unaffected.\n   *   \n   *   @param data 1-channel matrix. When it is a 3x3 matrix, it sets the upper left 3x3 part of the\n   * current matrix. When it is a 1x3 or 3x1 matrix, it is used as a rotation vector. The Rodrigues\n   * formula is used to compute the rotation matrix and sets the upper left 3x3 part of the current\n   * matrix.\n   */\n  public rotation(data: Mat): Mat\n\n  /**\n   *   the upper left 3x3 part\n   */\n  public rotation(): Mat3\n\n  /**\n   *   Rodrigues vector. \n   *   \n   *   a vector representing the upper left 3x3 rotation matrix of the current matrix. \n   *   \n   *   Since the mapping between rotation vectors and rotation matrices is many to one, this function\n   * returns only one rotation vector that represents the current rotation matrix, which is not\n   * necessarily the same one set by `[rotation(const Vec3&\n   * rvec)](#dd/d99/classcv_1_1Affine3_1acfe7474211770799f56deb8b81d829f5})`.\n   */\n  public rvec(): Vec3\n\n  public translate(t: Vec3): Affine3\n\n  /**\n   *   Copy t to the first three elements of the last column of the current matrix\n   *   \n   *   It sets the upper right 3x1 part of the matrix. The remaining part is unaffected.\n   *   \n   *   @param t 3x1 translation vector.\n   */\n  public translation(t: Vec3): Vec3\n\n  /**\n   *   the upper right 3x1 part\n   */\n  public translation(): Vec3\n\n  public static Identity(): Affine3\n}\n\n"},"node_modules_mirada_dist_src_types_opencv_Algorithm_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_Algorithm_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/Algorithm.d.ts","content":"\nimport { bool, FileNode, FileStorage, Ptr } from './_types'\n\n/**\n * especially for classes of algorithms, for which there can be multiple implementations. The examples\n * are stereo correspondence (for which there are algorithms like block matching, semi-global block\n * matching, graph-cut etc.), background subtraction (which can be done using mixture-of-gaussians\n * models, codebook-based algorithm etc.), optical flow (block matching, Lucas-Kanade, Horn-Schunck\n * etc.).\n * \n * Here is example of [SimpleBlobDetector](#d0/d7a/classcv_1_1SimpleBlobDetector}) use in your\n * application via [Algorithm](#d3/d46/classcv_1_1Algorithm}) interface: \n * \n * ```cpp\n *     Ptr<Feature2D> sbd = SimpleBlobDetector::create();\n *     FileStorage fs_read(\"SimpleBlobDetector_params.xml\", FileStorage::READ);\n * \n *     if (fs_read.isOpened()) // if we have file with parameters, read them\n *     {\n *         sbd->read(fs_read.root());\n *         fs_read.release();\n *     }\n *     else // else modify the parameters and store them; user can later edit the file to use different\n * parameters\n *     {\n *         fs_read.release();\n *         FileStorage fs_write(\"SimpleBlobDetector_params.xml\", FileStorage::WRITE);\n *         sbd->write(fs_write);\n *         fs_write.release();\n *     }\n * \n *     Mat result, image = imread(\"../data/detect_blob.png\", IMREAD_COLOR);\n *     vector<KeyPoint> keypoints;\n *     sbd->detect(image, keypoints, Mat());\n * \n *     drawKeypoints(image, keypoints, result);\n *     for (vector<KeyPoint>::iterator k = keypoints.begin(); k != keypoints.end(); ++k)\n *         circle(result, k->pt, (int)k->size, Scalar(0, 0, 255), 2);\n * \n *     imshow(\"result\", result);\n *     waitKey(0);\n * ```\n * \n * Source:\n * [opencv2/core.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/core.hpp#L3077).\n * \n */\nexport declare class Algorithm {\n\n  public constructor()\n\n  public clear(): void\n\n  public empty(): bool\n\n  /**\n   *   Returns the algorithm string identifier. This string is used as top level xml/yml node tag when\n   * the object is saved to a file or string.\n   */\n  public getDefaultName(): String\n\n  public read(fn: FileNode): FileNode\n\n  /**\n   *   Saves the algorithm to a file. In order to make this method work, the derived class must implement\n   * Algorithm::write(FileStorage& fs).\n   */\n  public save(filename: String): String\n\n  public write(fs: FileStorage): FileStorage\n\n  public write(fs: Ptr, name?: String): Ptr\n\n  /**\n   *   This is static template method of [Algorithm](#d3/d46/classcv_1_1Algorithm}). It's usage is\n   * following (in the case of SVM): \n   *   \n   *   ```cpp\n   *   Ptr<SVM> svm = Algorithm::load<SVM>(\"my_svm_model.xml\");\n   *   ```\n   *   \n   *    In order to make this method work, the derived class must overwrite\n   * [Algorithm::read](#d3/d46/classcv_1_1Algorithm_1aef2ad3f4145bd6e8c3664eb1c4b5e1e6})(const\n   * [FileNode](#de/dd9/classcv_1_1FileNode})& fn).\n   *   \n   *   @param filename Name of the file to read.\n   *   \n   *   @param objname The optional name of the node to read (if empty, the first top-level node will be\n   * used)\n   */\n  public static load(arg0: any, filename: String, objname?: String): Ptr\n\n  /**\n   *   This is static template method of [Algorithm](#d3/d46/classcv_1_1Algorithm}). It's usage is\n   * following (in the case of SVM): \n   *   \n   *   ```cpp\n   *   Ptr<SVM> svm = Algorithm::loadFromString<SVM>(myStringModel);\n   *   ```\n   *   \n   *   @param strModel The string variable containing the model you want to load.\n   *   \n   *   @param objname The optional name of the node to read (if empty, the first top-level node will be\n   * used)\n   */\n  public static loadFromString(arg1: any, strModel: String, objname?: String): Ptr\n\n  /**\n   *   This is static template method of [Algorithm](#d3/d46/classcv_1_1Algorithm}). It's usage is\n   * following (in the case of SVM): \n   *   \n   *   ```cpp\n   *   cv::FileStorage fsRead(\"example.xml\", FileStorage::READ);\n   *   Ptr<SVM> svm = Algorithm::read<SVM>(fsRead.root());\n   *   ```\n   *   \n   *    In order to make this method work, the derived class must overwrite\n   * [Algorithm::read](#d3/d46/classcv_1_1Algorithm_1aef2ad3f4145bd6e8c3664eb1c4b5e1e6})(const\n   * [FileNode](#de/dd9/classcv_1_1FileNode})& fn) and also have static create() method without\n   * parameters (or with all the optional parameters)\n   */\n  public static read(arg2: any, fn: FileNode): Ptr\n}\n\n"},"node_modules_mirada_dist_src_types_opencv_BFMatcher_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_BFMatcher_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/BFMatcher.d.ts","content":"\nimport { bool, int, Ptr } from './_types'\n\n/**\n * For each descriptor in the first set, this matcher finds the closest descriptor in the second set by\n * trying each one. This descriptor matcher supports masking permissible matches of descriptor sets.\n * \n * Source:\n * [opencv2/features2d.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/features2d.hpp#L1140).\n * \n */\nexport declare class BFMatcher {\n\n  public constructor(normType?: int, crossCheck?: bool)\n\n  /**\n   *   @param emptyTrainData If emptyTrainData is false, the method creates a deep copy of the object,\n   * that is, copies both parameters and train data. If emptyTrainData is true, the method creates an\n   * object copy with the current parameters but with empty train data.\n   */\n  public clone(emptyTrainData?: bool): Ptr\n\n  public isMaskSupported(): bool\n\n  /**\n   *   @param normType One of NORM_L1, NORM_L2, NORM_HAMMING, NORM_HAMMING2. L1 and L2 norms are\n   * preferable choices for SIFT and SURF descriptors, NORM_HAMMING should be used with ORB, BRISK and\n   * BRIEF, NORM_HAMMING2 should be used with ORB when WTA_K==3 or 4 (see ORB::ORB constructor\n   * description).\n   *   \n   *   @param crossCheck If it is false, this is will be default BFMatcher behaviour when it finds the k\n   * nearest neighbors for each query descriptor. If crossCheck==true, then the knnMatch() method with\n   * k=1 will only return pairs (i,j) such that for i-th query descriptor the j-th descriptor in the\n   * matcher's collection is the nearest and vice versa, i.e. the BFMatcher will only return consistent\n   * pairs. Such technique usually produces best results with minimal number of outliers when there are\n   * enough matches. This is alternative to the ratio test, used by D. Lowe in SIFT paper.\n   */\n  public static create(normType?: int, crossCheck?: bool): Ptr\n}\n\n"},"node_modules_mirada_dist_src_types_opencv_AutoBuffer_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_AutoBuffer_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/AutoBuffer.d.ts","content":"\nimport { size_t } from './_types'\n\n/**\n * The class is used for temporary buffers in functions and methods. If a temporary buffer is usually\n * small (a few K's of memory), but its size depends on the parameters, it makes sense to create a\n * small fixed-size array on stack and use it if it's large enough. If the required buffer size is\n * larger than the fixed size, another buffer of sufficient size is allocated dynamically and released\n * after the processing. Therefore, in typical cases, when the buffer size is small, there is no\n * overhead associated with malloc()/free(). At the same time, there is no limit on the size of\n * processed data.\n * \n * This is what [AutoBuffer](#d8/dd0/classcv_1_1AutoBuffer}) does. The template takes 2 parameters -\n * type of the buffer elements and the number of stack-allocated elements. Here is how the class is\n * used:\n * \n * ```cpp\n * void my_func(const cv::Mat& m)\n * {\n *    cv::AutoBuffer<float> buf(1000); // create automatic buffer containing 1000 floats\n * \n *    buf.allocate(m.rows); // if m.rows <= 1000, the pre-allocated buffer is used,\n *                          // otherwise the buffer of \"m.rows\" floats will be allocated\n *                          // dynamically and deallocated in cv::AutoBuffer destructor\n *    ...\n * }\n * ```\n * \n * Source:\n * [opencv2/core/utility.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/core/utility.hpp#L128).\n * \n */\nexport declare class AutoBuffer {\n\n  public constructor()\n\n  public constructor(_size: size_t)\n\n  public constructor(buf: AutoBuffer)\n\n  public allocate(_size: size_t): void\n\n  public data(): any\n\n  public data(): any\n\n  public deallocate(): void\n\n  public resize(_size: size_t): void\n\n  public size(): size_t\n}\n\n"},"node_modules_mirada_dist_src_types_opencv_BOWTrainer_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_BOWTrainer_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/BOWTrainer.d.ts","content":"\nimport { int, Mat } from './_types'\n\n/**\n * For details, see, for example, *Visual Categorization with Bags of Keypoints* by Gabriella Csurka,\n * Christopher R. Dance, Lixin Fan, Jutta Willamowski, Cedric Bray, 2004. :\n * \n * Source:\n * [opencv2/features2d.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/features2d.hpp#L1339).\n * \n */\nexport declare class BOWTrainer {\n\n  public constructor()\n\n  /**\n   *   The training set is clustered using clustermethod to construct the vocabulary.\n   *   \n   *   @param descriptors Descriptors to add to a training set. Each row of the descriptors matrix is a\n   * descriptor.\n   */\n  public add(descriptors: Mat): Mat\n\n  public clear(): void\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   */\n  public cluster(): Mat\n\n  /**\n   *   The vocabulary consists of cluster centers. So, this method returns the vocabulary. In the first\n   * variant of the method, train descriptors stored in the object are clustered. In the second variant,\n   * input descriptors are clustered.\n   *   \n   *   @param descriptors Descriptors to cluster. Each row of the descriptors matrix is a descriptor.\n   * Descriptors are not added to the inner train descriptor set.\n   */\n  public cluster(descriptors: Mat): Mat\n\n  public descriptorsCount(): int\n\n  public getDescriptors(): Mat\n}\n\n"},"node_modules_mirada_dist_src_types_opencv_CascadeClassifier_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_CascadeClassifier_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/CascadeClassifier.d.ts","content":"\nimport { bool, double, FileNode, InputArray, int, Mat, Ptr, Rect, Size } from './_types'\n\nexport declare class CascadeClassifier extends Mat {\n\n  public cc: Ptr\n\n  public constructor()\n\n  /**\n   *   @param filename Name of the file from which the classifier is loaded.\n   */\n  public constructor(filename: String)\n\n  /**\n   *   The function is parallelized with the TBB library.\n   *   \n   * (Python) A face detection example using cascade classifiers can be found at\n   * opencv_source_code/samples/python/facedetect.py\n   *   \n   *   @param image Matrix of the type CV_8U containing an image where objects are detected.\n   *   \n   *   @param objects Vector of rectangles where each rectangle contains the detected object, the\n   * rectangles may be partially outside the original image.\n   *   \n   *   @param scaleFactor Parameter specifying how much the image size is reduced at each image scale.\n   *   \n   *   @param minNeighbors Parameter specifying how many neighbors each candidate rectangle should have\n   * to retain it.\n   *   \n   *   @param flags Parameter with the same meaning for an old cascade as in the function\n   * cvHaarDetectObjects. It is not used for a new cascade.\n   *   \n   *   @param minSize Minimum possible object size. Objects smaller than that are ignored.\n   *   \n   *   @param maxSize Maximum possible object size. Objects larger than that are ignored. If maxSize ==\n   * minSize model is evaluated on single scale.\n   */\n  public detectMultiScale(image: InputArray, objects: Rect, scaleFactor?: double, minNeighbors?: int, flags?: int, minSize?: Size, maxSize?: Size): InputArray\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param image Matrix of the type CV_8U containing an image where objects are detected.\n   *   \n   *   @param objects Vector of rectangles where each rectangle contains the detected object, the\n   * rectangles may be partially outside the original image.\n   *   \n   *   @param numDetections Vector of detection numbers for the corresponding objects. An object's number\n   * of detections is the number of neighboring positively classified rectangles that were joined\n   * together to form the object.\n   *   \n   *   @param scaleFactor Parameter specifying how much the image size is reduced at each image scale.\n   *   \n   *   @param minNeighbors Parameter specifying how many neighbors each candidate rectangle should have\n   * to retain it.\n   *   \n   *   @param flags Parameter with the same meaning for an old cascade as in the function\n   * cvHaarDetectObjects. It is not used for a new cascade.\n   *   \n   *   @param minSize Minimum possible object size. Objects smaller than that are ignored.\n   *   \n   *   @param maxSize Maximum possible object size. Objects larger than that are ignored. If maxSize ==\n   * minSize model is evaluated on single scale.\n   */\n  public detectMultiScale(image: InputArray, objects: Rect, numDetections: any, scaleFactor?: double, minNeighbors?: int, flags?: int, minSize?: Size, maxSize?: Size): InputArray\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts. This function allows you to retrieve the final stage\n   * decision certainty of classification. For this, one needs to set `outputRejectLevels` on true and\n   * provide the `rejectLevels` and `levelWeights` parameter. For each resulting detection,\n   * `levelWeights` will then contain the certainty of classification at the final stage. This value can\n   * then be used to separate strong from weaker classifications.\n   *   \n   *   A code sample on how to use it efficiently can be found below: \n   *   \n   *   ```cpp\n   *   Mat img;\n   *   vector<double> weights;\n   *   vector<int> levels;\n   *   vector<Rect> detections;\n   *   CascadeClassifier model(\"/path/to/your/model.xml\");\n   *   model.detectMultiScale(img, detections, levels, weights, 1.1, 3, 0, Size(), Size(), true);\n   *   cerr << \"Detection \" << detections[0] << \" with weight \" << weights[0] << endl;\n   *   ```\n   */\n  public detectMultiScale(image: InputArray, objects: Rect, rejectLevels: any, levelWeights: any, scaleFactor?: double, minNeighbors?: int, flags?: int, minSize?: Size, maxSize?: Size, outputRejectLevels?: bool): InputArray\n\n  public empty(): bool\n\n  public getFeatureType(): int\n\n  public getMaskGenerator(): Ptr\n\n  public getOldCascade(): any\n\n  public getOriginalWindowSize(): Size\n\n  public isOldFormatCascade(): bool\n\n  /**\n   *   @param filename Name of the file from which the classifier is loaded. The file may contain an old\n   * HAAR classifier trained by the haartraining application or a new cascade classifier trained by the\n   * traincascade application.\n   */\n  public load(filename: String): String\n\n  /**\n   *   The file may contain a new cascade classifier (trained traincascade application) only.\n   */\n  public read(node: FileNode): FileNode\n\n  public setMaskGenerator(maskGenerator: Ptr): Ptr\n\n  public static convert(oldcascade: String, newcascade: String): String\n}\n\n"},"node_modules_mirada_dist_src_types_opencv_calib3d_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_calib3d_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/calib3d.d.ts","content":"\nimport { bool, double, float, InputArray, InputArrayOfArrays, InputOutputArray, int, Mat, OutputArray, OutputArrayOfArrays, Point2d, Rect, Size, size_t, TermCriteria, Vec3d } from './_types'\n/*\n * # Camera Calibration and 3D Reconstruction\n * The functions in this section use a so-called pinhole camera model. In this model, a scene view is formed by projecting 3D points into the image plane using a perspective transformation.\n * \n * `\\[s \\; m' = A [R|t] M'\\]`\n * \n * or\n * \n * `\\[s \\vecthree{u}{v}{1} = \\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_1 \\\\ r_{21} & r_{22} & r_{23} & t_2 \\\\ r_{31} & r_{32} & r_{33} & t_3 \\end{bmatrix} \\begin{bmatrix} X \\\\ Y \\\\ Z \\\\ 1 \\end{bmatrix}\\]`\n * \n * where:\n * \n * \n * \n * \n * \n *  * `$(X, Y, Z)$` are the coordinates of a 3D point in the world coordinate space\n *  * `$(u, v)$` are the coordinates of the projection point in pixels\n *  * `$A$` is a camera matrix, or a matrix of intrinsic parameters\n *  * `$(cx, cy)$` is a principal point that is usually at the image center\n *  * `$fx, fy$` are the focal lengths expressed in pixel units.\n * \n * \n * Thus, if an image from the camera is scaled by a factor, all of these parameters should be scaled (multiplied/divided, respectively) by the same factor. The matrix of intrinsic parameters does not depend on the scene viewed. So, once estimated, it can be re-used as long as the focal length is fixed (in case of zoom lens). The joint rotation-translation matrix `$[R|t]$` is called a matrix of extrinsic parameters. It is used to describe the camera motion around a static scene, or vice versa, rigid motion of an object in front of a still camera. That is, `$[R|t]$` translates coordinates of a point `$(X, Y, Z)$` to a coordinate system, fixed with respect to the camera. The transformation above is equivalent to the following (when `$z \\ne 0$` ):\n * \n * `\\[\\begin{array}{l} \\vecthree{x}{y}{z} = R \\vecthree{X}{Y}{Z} + t \\\\ x' = x/z \\\\ y' = y/z \\\\ u = f_x*x' + c_x \\\\ v = f_y*y' + c_y \\end{array}\\]`\n * \n * The following figure illustrates the pinhole camera model.\n * \n * \n *  Real lenses usually have some distortion, mostly radial distortion and slight tangential distortion. So, the above model is extended as:\n * \n * `\\[\\begin{array}{l} \\vecthree{x}{y}{z} = R \\vecthree{X}{Y}{Z} + t \\\\ x' = x/z \\\\ y' = y/z \\\\ x'' = x' \\frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + 2 p_1 x' y' + p_2(r^2 + 2 x'^2) + s_1 r^2 + s_2 r^4 \\\\ y'' = y' \\frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + p_1 (r^2 + 2 y'^2) + 2 p_2 x' y' + s_3 r^2 + s_4 r^4 \\\\ \\text{where} \\quad r^2 = x'^2 + y'^2 \\\\ u = f_x*x'' + c_x \\\\ v = f_y*y'' + c_y \\end{array}\\]`\n * \n * `$k_1$`, `$k_2$`, `$k_3$`, `$k_4$`, `$k_5$`, and `$k_6$` are radial distortion coefficients. `$p_1$` and `$p_2$` are tangential distortion coefficients. `$s_1$`, `$s_2$`, `$s_3$`, and `$s_4$`, are the thin prism distortion coefficients. Higher-order coefficients are not considered in OpenCV.\n * \n * The next figures show two common types of radial distortion: barrel distortion (typically `$ k_1 < 0 $`) and pincushion distortion (typically `$ k_1 > 0 $`).\n * \n * \n *  \n * \n * \n * In some cases the image sensor may be tilted in order to focus an oblique plane in front of the camera (Scheimpfug condition). This can be useful for particle image velocimetry (PIV) or triangulation with a laser fan. The tilt causes a perspective distortion of `$x''$` and `$y''$`. This distortion can be modelled in the following way, see e.g. Louhichi07.\n * \n * `\\[\\begin{array}{l} s\\vecthree{x'''}{y'''}{1} = \\vecthreethree{R_{33}(\\tau_x, \\tau_y)}{0}{-R_{13}(\\tau_x, \\tau_y)} {0}{R_{33}(\\tau_x, \\tau_y)}{-R_{23}(\\tau_x, \\tau_y)} {0}{0}{1} R(\\tau_x, \\tau_y) \\vecthree{x''}{y''}{1}\\\\ u = f_x*x''' + c_x \\\\ v = f_y*y''' + c_y \\end{array}\\]`\n * \n * where the matrix `$R(\\tau_x, \\tau_y)$` is defined by two rotations with angular parameter `$\\tau_x$` and `$\\tau_y$`, respectively,\n * \n * `\\[ R(\\tau_x, \\tau_y) = \\vecthreethree{\\cos(\\tau_y)}{0}{-\\sin(\\tau_y)}{0}{1}{0}{\\sin(\\tau_y)}{0}{\\cos(\\tau_y)} \\vecthreethree{1}{0}{0}{0}{\\cos(\\tau_x)}{\\sin(\\tau_x)}{0}{-\\sin(\\tau_x)}{\\cos(\\tau_x)} = \\vecthreethree{\\cos(\\tau_y)}{\\sin(\\tau_y)\\sin(\\tau_x)}{-\\sin(\\tau_y)\\cos(\\tau_x)} {0}{\\cos(\\tau_x)}{\\sin(\\tau_x)} {\\sin(\\tau_y)}{-\\cos(\\tau_y)\\sin(\\tau_x)}{\\cos(\\tau_y)\\cos(\\tau_x)}. \\]`\n * \n * In the functions below the coefficients are passed or returned as\n * \n * `\\[(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])\\]`\n * \n * vector. That is, if the vector contains four elements, it means that `$k_3=0$` . The distortion coefficients do not depend on the scene viewed. Thus, they also belong to the intrinsic camera parameters. And they remain the same regardless of the captured image resolution. If, for example, a camera has been calibrated on images of 320 x 240 resolution, absolutely the same distortion coefficients can be used for 640 x 480 images from the same camera while `$f_x$`, `$f_y$`, `$c_x$`, and `$c_y$` need to be scaled appropriately.\n * \n * The functions below use the above model to do the following:\n * \n * \n * \n * \n * \n *  * Project 3D points to the image plane given intrinsic and extrinsic parameters.\n *  * Compute extrinsic parameters given intrinsic parameters, a few 3D points, and their projections.\n *  * Estimate intrinsic and extrinsic camera parameters from several views of a known calibration pattern (every view is described by several 3D-2D point correspondences).\n *  * Estimate the relative position and orientation of the stereo camera \"heads\" and compute the rectification* transformation that makes the camera optical axes parallel.\n * \n * \n * \n * \n * \n * \n * \n * \n *  * A calibration sample for 3 cameras in horizontal position can be found at opencv_source_code/samples/cpp/3calibration.cpp\n *  * A calibration sample based on a sequence of images can be found at opencv_source_code/samples/cpp/calibration.cpp\n *  * A calibration sample in order to do 3D reconstruction can be found at opencv_source_code/samples/cpp/build3dmodel.cpp\n *  * A calibration example on stereo calibration can be found at opencv_source_code/samples/cpp/stereo_calib.cpp\n *  * A calibration example on stereo matching can be found at opencv_source_code/samples/cpp/stereo_match.cpp\n *  * (Python) A camera calibration sample can be found at opencv_source_code/samples/python/calibrate.py\n */\n/**\n * the overall RMS re-projection error.\n * The function estimates the intrinsic camera parameters and extrinsic parameters for each of the\n * views. The algorithm is based on Zhang2000 and BouguetMCT . The coordinates of 3D object points and\n * their corresponding 2D projections in each view must be specified. That may be achieved by using an\n * object with a known geometry and easily detectable feature points. Such an object is called a\n * calibration rig or calibration pattern, and OpenCV has built-in support for a chessboard as a\n * calibration rig (see findChessboardCorners ). Currently, initialization of intrinsic parameters\n * (when CALIB_USE_INTRINSIC_GUESS is not set) is only implemented for planar calibration patterns\n * (where Z-coordinates of the object points must be all zeros). 3D calibration rigs can also be used\n * as long as initial cameraMatrix is provided.\n * \n * The algorithm performs the following steps:\n * \n * Compute the initial intrinsic parameters (the option only available for planar calibration patterns)\n * or read them from the input parameters. The distortion coefficients are all set to zeros initially\n * unless some of CALIB_FIX_K? are specified.\n * Estimate the initial camera pose as if the intrinsic parameters have been already known. This is\n * done using solvePnP .\n * Run the global Levenberg-Marquardt optimization algorithm to minimize the reprojection error, that\n * is, the total sum of squared distances between the observed feature points imagePoints and the\n * projected (using the current estimates for camera parameters and the poses) object points\n * objectPoints. See projectPoints for details.\n * \n * If you use a non-square (=non-NxN) grid and findChessboardCorners for calibration, and\n * calibrateCamera returns bad values (zero distortion coefficients, an image center very far from\n * (w/2-0.5,h/2-0.5), and/or large differences between `$f_x$` and `$f_y$` (ratios of 10:1 or more)),\n * then you have probably used patternSize=cvSize(rows,cols) instead of using\n * patternSize=cvSize(cols,rows) in findChessboardCorners .\n * \n * [calibrateCameraRO](#d9/d0c/group__calib3d_1ga11eeb16e5a458e1ed382fb27f585b753}),\n * [findChessboardCorners](#d9/d0c/group__calib3d_1ga93efa9b0aa890de240ca32b11253dd4a}),\n * [solvePnP](#d9/d0c/group__calib3d_1ga549c2075fac14829ff4a58bc931c033d}),\n * [initCameraMatrix2D](#d9/d0c/group__calib3d_1ga8132c7dbbb61738cc3510bebbdffde55}),\n * [stereoCalibrate](#d9/d0c/group__calib3d_1ga91018d80e2a93ade37539f01e6f07de5}),\n * [undistort](#d9/d0c/group__calib3d_1ga69f2545a8b62a6b0fc2ee060dc30559d})\n * \n * @param objectPoints In the new interface it is a vector of vectors of calibration pattern points in\n * the calibration pattern coordinate space (e.g. std::vector<std::vector<cv::Vec3f>>). The outer\n * vector contains as many elements as the number of the pattern views. If the same calibration pattern\n * is shown in each view and it is fully visible, all the vectors will be the same. Although, it is\n * possible to use partially occluded patterns, or even different patterns in different views. Then,\n * the vectors will be different. The points are 3D, but since they are in a pattern coordinate system,\n * then, if the rig is planar, it may make sense to put the model to a XY coordinate plane so that\n * Z-coordinate of each input object point is 0. In the old interface all the vectors of object points\n * from different views are concatenated together.\n * \n * @param imagePoints In the new interface it is a vector of vectors of the projections of calibration\n * pattern points (e.g. std::vector<std::vector<cv::Vec2f>>). imagePoints.size() and\n * objectPoints.size() and imagePoints[i].size() must be equal to objectPoints[i].size() for each i. In\n * the old interface all the vectors of object points from different views are concatenated together.\n * \n * @param imageSize Size of the image used only to initialize the intrinsic camera matrix.\n * \n * @param cameraMatrix Output 3x3 floating-point camera matrix $A =\n * \\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}$ . If CV_CALIB_USE_INTRINSIC_GUESS and/or\n * CALIB_FIX_ASPECT_RATIO are specified, some or all of fx, fy, cx, cy must be initialized before\n * calling the function.\n * \n * @param distCoeffs Output vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5,\n * k_6 [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])$ of 4, 5, 8, 12 or 14 elements.\n * \n * @param rvecs Output vector of rotation vectors (see Rodrigues ) estimated for each pattern view\n * (e.g. std::vector<cv::Mat>>). That is, each k-th rotation vector together with the corresponding\n * k-th translation vector (see the next output parameter description) brings the calibration pattern\n * from the model coordinate space (in which object points are specified) to the world coordinate\n * space, that is, a real position of the calibration pattern in the k-th pattern view (k=0.. M -1).\n * \n * @param tvecs Output vector of translation vectors estimated for each pattern view.\n * \n * @param stdDeviationsIntrinsics Output vector of standard deviations estimated for intrinsic\n * parameters. Order of deviations values: $(f_x, f_y, c_x, c_y, k_1, k_2, p_1, p_2, k_3, k_4, k_5, k_6\n * , s_1, s_2, s_3, s_4, \\tau_x, \\tau_y)$ If one of parameters is not estimated, it's deviation is\n * equals to zero.\n * \n * @param stdDeviationsExtrinsics Output vector of standard deviations estimated for extrinsic\n * parameters. Order of deviations values: $(R_1, T_1, \\dotsc , R_M, T_M)$ where M is number of pattern\n * views, $R_i, T_i$ are concatenated 1x3 vectors.\n * \n * @param perViewErrors Output vector of the RMS re-projection error estimated for each pattern view.\n * \n * @param flags Different flags that may be zero or a combination of the following values:\n * CALIB_USE_INTRINSIC_GUESS cameraMatrix contains valid initial values of fx, fy, cx, cy that are\n * optimized further. Otherwise, (cx, cy) is initially set to the image center ( imageSize is used),\n * and focal distances are computed in a least-squares fashion. Note, that if intrinsic parameters are\n * known, there is no need to use this function just to estimate extrinsic parameters. Use solvePnP\n * instead.CALIB_FIX_PRINCIPAL_POINT The principal point is not changed during the global optimization.\n * It stays at the center or at a different location specified when CALIB_USE_INTRINSIC_GUESS is set\n * too.CALIB_FIX_ASPECT_RATIO The functions considers only fy as a free parameter. The ratio fx/fy\n * stays the same as in the input cameraMatrix . When CALIB_USE_INTRINSIC_GUESS is not set, the actual\n * input values of fx and fy are ignored, only their ratio is computed and used\n * further.CALIB_ZERO_TANGENT_DIST Tangential distortion coefficients $(p_1, p_2)$ are set to zeros and\n * stay zero.CALIB_FIX_K1,...,CALIB_FIX_K6 The corresponding radial distortion coefficient is not\n * changed during the optimization. If CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the\n * supplied distCoeffs matrix is used. Otherwise, it is set to 0.CALIB_RATIONAL_MODEL Coefficients k4,\n * k5, and k6 are enabled. To provide the backward compatibility, this extra flag should be explicitly\n * specified to make the calibration function use the rational model and return 8 coefficients. If the\n * flag is not set, the function computes and returns only 5 distortion\n * coefficients.CALIB_THIN_PRISM_MODEL Coefficients s1, s2, s3 and s4 are enabled. To provide the\n * backward compatibility, this extra flag should be explicitly specified to make the calibration\n * function use the thin prism model and return 12 coefficients. If the flag is not set, the function\n * computes and returns only 5 distortion coefficients.CALIB_FIX_S1_S2_S3_S4 The thin prism distortion\n * coefficients are not changed during the optimization. If CALIB_USE_INTRINSIC_GUESS is set, the\n * coefficient from the supplied distCoeffs matrix is used. Otherwise, it is set to\n * 0.CALIB_TILTED_MODEL Coefficients tauX and tauY are enabled. To provide the backward compatibility,\n * this extra flag should be explicitly specified to make the calibration function use the tilted\n * sensor model and return 14 coefficients. If the flag is not set, the function computes and returns\n * only 5 distortion coefficients.CALIB_FIX_TAUX_TAUY The coefficients of the tilted sensor model are\n * not changed during the optimization. If CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the\n * supplied distCoeffs matrix is used. Otherwise, it is set to 0.\n * \n * @param criteria Termination criteria for the iterative optimization algorithm.\n */\nexport declare function calibrateCamera(objectPoints: InputArrayOfArrays, imagePoints: InputArrayOfArrays, imageSize: Size, cameraMatrix: InputOutputArray, distCoeffs: InputOutputArray, rvecs: OutputArrayOfArrays, tvecs: OutputArrayOfArrays, stdDeviationsIntrinsics: OutputArray, stdDeviationsExtrinsics: OutputArray, perViewErrors: OutputArray, flags?: int, criteria?: TermCriteria): double\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function calibrateCamera(objectPoints: InputArrayOfArrays, imagePoints: InputArrayOfArrays, imageSize: Size, cameraMatrix: InputOutputArray, distCoeffs: InputOutputArray, rvecs: OutputArrayOfArrays, tvecs: OutputArrayOfArrays, flags?: int, criteria?: TermCriteria): double\n\n/**\n * This function is an extension of\n * [calibrateCamera()](#d9/d0c/group__calib3d_1ga3207604e4b1a1758aa66acb6ed5aa65d}) with the method of\n * releasing object which was proposed in strobl2011iccv. In many common cases with inaccurate,\n * unmeasured, roughly planar targets (calibration plates), this method can dramatically improve the\n * precision of the estimated camera parameters. Both the object-releasing method and standard method\n * are supported by this function. Use the parameter **iFixedPoint** for method selection. In the\n * internal implementation,\n * [calibrateCamera()](#d9/d0c/group__calib3d_1ga3207604e4b1a1758aa66acb6ed5aa65d}) is a wrapper for\n * this function.\n * \n * the overall RMS re-projection error.\n * The function estimates the intrinsic camera parameters and extrinsic parameters for each of the\n * views. The algorithm is based on Zhang2000, BouguetMCT and strobl2011iccv. See\n * [calibrateCamera()](#d9/d0c/group__calib3d_1ga3207604e4b1a1758aa66acb6ed5aa65d}) for other detailed\n * explanations. \n * \n * [calibrateCamera](#d9/d0c/group__calib3d_1ga3207604e4b1a1758aa66acb6ed5aa65d}),\n * [findChessboardCorners](#d9/d0c/group__calib3d_1ga93efa9b0aa890de240ca32b11253dd4a}),\n * [solvePnP](#d9/d0c/group__calib3d_1ga549c2075fac14829ff4a58bc931c033d}),\n * [initCameraMatrix2D](#d9/d0c/group__calib3d_1ga8132c7dbbb61738cc3510bebbdffde55}),\n * [stereoCalibrate](#d9/d0c/group__calib3d_1ga91018d80e2a93ade37539f01e6f07de5}),\n * [undistort](#d9/d0c/group__calib3d_1ga69f2545a8b62a6b0fc2ee060dc30559d})\n * \n * @param objectPoints Vector of vectors of calibration pattern points in the calibration pattern\n * coordinate space. See calibrateCamera() for details. If the method of releasing object to be used,\n * the identical calibration board must be used in each view and it must be fully visible, and all\n * objectPoints[i] must be the same and all points should be roughly close to a plane. The calibration\n * target has to be rigid, or at least static if the camera (rather than the calibration target) is\n * shifted for grabbing images.\n * \n * @param imagePoints Vector of vectors of the projections of calibration pattern points. See\n * calibrateCamera() for details.\n * \n * @param imageSize Size of the image used only to initialize the intrinsic camera matrix.\n * \n * @param iFixedPoint The index of the 3D object point in objectPoints[0] to be fixed. It also acts as\n * a switch for calibration method selection. If object-releasing method to be used, pass in the\n * parameter in the range of [1, objectPoints[0].size()-2], otherwise a value out of this range will\n * make standard calibration method selected. Usually the top-right corner point of the calibration\n * board grid is recommended to be fixed when object-releasing method being utilized. According to\n * strobl2011iccv, two other points are also fixed. In this implementation, objectPoints[0].front and\n * objectPoints[0].back.z are used. With object-releasing method, accurate rvecs, tvecs and\n * newObjPoints are only possible if coordinates of these three fixed points are accurate enough.\n * \n * @param cameraMatrix Output 3x3 floating-point camera matrix. See calibrateCamera() for details.\n * \n * @param distCoeffs Output vector of distortion coefficients. See calibrateCamera() for details.\n * \n * @param rvecs Output vector of rotation vectors estimated for each pattern view. See\n * calibrateCamera() for details.\n * \n * @param tvecs Output vector of translation vectors estimated for each pattern view.\n * \n * @param newObjPoints The updated output vector of calibration pattern points. The coordinates might\n * be scaled based on three fixed points. The returned coordinates are accurate only if the above\n * mentioned three fixed points are accurate. If not needed, noArray() can be passed in. This parameter\n * is ignored with standard calibration method.\n * \n * @param stdDeviationsIntrinsics Output vector of standard deviations estimated for intrinsic\n * parameters. See calibrateCamera() for details.\n * \n * @param stdDeviationsExtrinsics Output vector of standard deviations estimated for extrinsic\n * parameters. See calibrateCamera() for details.\n * \n * @param stdDeviationsObjPoints Output vector of standard deviations estimated for refined coordinates\n * of calibration pattern points. It has the same size and order as objectPoints[0] vector. This\n * parameter is ignored with standard calibration method.\n * \n * @param perViewErrors Output vector of the RMS re-projection error estimated for each pattern view.\n * \n * @param flags Different flags that may be zero or a combination of some predefined values. See\n * calibrateCamera() for details. If the method of releasing object is used, the calibration time may\n * be much longer. CALIB_USE_QR or CALIB_USE_LU could be used for faster calibration with potentially\n * less precise and less stable in some rare cases.\n * \n * @param criteria Termination criteria for the iterative optimization algorithm.\n */\nexport declare function calibrateCameraRO(objectPoints: InputArrayOfArrays, imagePoints: InputArrayOfArrays, imageSize: Size, iFixedPoint: int, cameraMatrix: InputOutputArray, distCoeffs: InputOutputArray, rvecs: OutputArrayOfArrays, tvecs: OutputArrayOfArrays, newObjPoints: OutputArray, stdDeviationsIntrinsics: OutputArray, stdDeviationsExtrinsics: OutputArray, stdDeviationsObjPoints: OutputArray, perViewErrors: OutputArray, flags?: int, criteria?: TermCriteria): double\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function calibrateCameraRO(objectPoints: InputArrayOfArrays, imagePoints: InputArrayOfArrays, imageSize: Size, iFixedPoint: int, cameraMatrix: InputOutputArray, distCoeffs: InputOutputArray, rvecs: OutputArrayOfArrays, tvecs: OutputArrayOfArrays, newObjPoints: OutputArray, flags?: int, criteria?: TermCriteria): double\n\n/**\n * The function performs the Hand-Eye calibration using various methods. One approach consists in\n * estimating the rotation then the translation (separable solutions) and the following methods are\n * implemented:\n * \n * R. Tsai, R. Lenz A New Technique for Fully Autonomous and Efficient 3D Robotics Hand/EyeCalibration\n * Tsai89\n * F. Park, B. Martin Robot Sensor Calibration: Solving AX = XB on the Euclidean Group Park94\n * R. Horaud, F. Dornaika Hand-Eye Calibration Horaud95\n * \n * Another approach consists in estimating simultaneously the rotation and the translation\n * (simultaneous solutions), with the following implemented method:\n * \n * N. Andreff, R. Horaud, B. Espiau On-line Hand-Eye Calibration Andreff99\n * K. Daniilidis Hand-Eye Calibration Using Dual Quaternions Daniilidis98\n * \n * The following picture describes the Hand-Eye calibration problem where the transformation between a\n * camera (\"eye\") mounted on a robot gripper (\"hand\") has to be estimated.\n * \n * The calibration procedure is the following:\n * \n * a static calibration pattern is used to estimate the transformation between the target frame and the\n * camera frame\n * the robot gripper is moved in order to acquire several poses\n * for each pose, the homogeneous transformation between the gripper frame and the robot base frame is\n * recorded using for instance the robot kinematics `\\\\[ \\\\begin{bmatrix} X_b\\\\\\\\ Y_b\\\\\\\\ Z_b\\\\\\\\ 1\n * \\\\end{bmatrix} = \\\\begin{bmatrix} _{}^{b}\\\\textrm{R}_g & _{}^{b}\\\\textrm{t}_g \\\\\\\\ 0_{1 \\\\times 3} &\n * 1 \\\\end{bmatrix} \\\\begin{bmatrix} X_g\\\\\\\\ Y_g\\\\\\\\ Z_g\\\\\\\\ 1 \\\\end{bmatrix} \\\\]`\n * for each pose, the homogeneous transformation between the calibration target frame and the camera\n * frame is recorded using for instance a pose estimation method (PnP) from 2D-3D point correspondences\n * `\\\\[ \\\\begin{bmatrix} X_c\\\\\\\\ Y_c\\\\\\\\ Z_c\\\\\\\\ 1 \\\\end{bmatrix} = \\\\begin{bmatrix}\n * _{}^{c}\\\\textrm{R}_t & _{}^{c}\\\\textrm{t}_t \\\\\\\\ 0_{1 \\\\times 3} & 1 \\\\end{bmatrix} \\\\begin{bmatrix}\n * X_t\\\\\\\\ Y_t\\\\\\\\ Z_t\\\\\\\\ 1 \\\\end{bmatrix} \\\\]`\n * \n * The Hand-Eye calibration procedure returns the following homogeneous transformation `\\\\[\n * \\\\begin{bmatrix} X_g\\\\\\\\ Y_g\\\\\\\\ Z_g\\\\\\\\ 1 \\\\end{bmatrix} = \\\\begin{bmatrix} _{}^{g}\\\\textrm{R}_c &\n * _{}^{g}\\\\textrm{t}_c \\\\\\\\ 0_{1 \\\\times 3} & 1 \\\\end{bmatrix} \\\\begin{bmatrix} X_c\\\\\\\\ Y_c\\\\\\\\\n * Z_c\\\\\\\\ 1 \\\\end{bmatrix} \\\\]`\n * \n * This problem is also known as solving the `$\\\\mathbf{A}\\\\mathbf{X}=\\\\mathbf{X}\\\\mathbf{B}$`\n * equation: `\\\\[ \\\\begin{align*} ^{b}{\\\\textrm{T}_g}^{(1)} \\\\hspace{0.2em} ^{g}\\\\textrm{T}_c\n * \\\\hspace{0.2em} ^{c}{\\\\textrm{T}_t}^{(1)} &= \\\\hspace{0.1em} ^{b}{\\\\textrm{T}_g}^{(2)}\n * \\\\hspace{0.2em} ^{g}\\\\textrm{T}_c \\\\hspace{0.2em} ^{c}{\\\\textrm{T}_t}^{(2)} \\\\\\\\\n * (^{b}{\\\\textrm{T}_g}^{(2)})^{-1} \\\\hspace{0.2em} ^{b}{\\\\textrm{T}_g}^{(1)} \\\\hspace{0.2em}\n * ^{g}\\\\textrm{T}_c &= \\\\hspace{0.1em} ^{g}\\\\textrm{T}_c \\\\hspace{0.2em} ^{c}{\\\\textrm{T}_t}^{(2)}\n * (^{c}{\\\\textrm{T}_t}^{(1)})^{-1} \\\\\\\\ \\\\textrm{A}_i \\\\textrm{X} &= \\\\textrm{X} \\\\textrm{B}_i \\\\\\\\\n * \\\\end{align*} \\\\]`\n * \n * Additional information can be found on this . \n * \n * A minimum of 2 motions with non parallel rotation axes are necessary to determine the hand-eye\n * transformation. So at least 3 different poses are required, but it is strongly recommended to use\n * many more poses.\n * \n * @param R_gripper2base Rotation part extracted from the homogeneous matrix that transforms a point\n * expressed in the gripper frame to the robot base frame ( $_{}^{b}\\textrm{T}_g$). This is a vector\n * (vector<Mat>) that contains the rotation matrices for all the transformations from gripper frame to\n * robot base frame.\n * \n * @param t_gripper2base Translation part extracted from the homogeneous matrix that transforms a point\n * expressed in the gripper frame to the robot base frame ( $_{}^{b}\\textrm{T}_g$). This is a vector\n * (vector<Mat>) that contains the translation vectors for all the transformations from gripper frame\n * to robot base frame.\n * \n * @param R_target2cam Rotation part extracted from the homogeneous matrix that transforms a point\n * expressed in the target frame to the camera frame ( $_{}^{c}\\textrm{T}_t$). This is a vector\n * (vector<Mat>) that contains the rotation matrices for all the transformations from calibration\n * target frame to camera frame.\n * \n * @param t_target2cam Rotation part extracted from the homogeneous matrix that transforms a point\n * expressed in the target frame to the camera frame ( $_{}^{c}\\textrm{T}_t$). This is a vector\n * (vector<Mat>) that contains the translation vectors for all the transformations from calibration\n * target frame to camera frame.\n * \n * @param R_cam2gripper Estimated rotation part extracted from the homogeneous matrix that transforms a\n * point expressed in the camera frame to the gripper frame ( $_{}^{g}\\textrm{T}_c$).\n * \n * @param t_cam2gripper Estimated translation part extracted from the homogeneous matrix that\n * transforms a point expressed in the camera frame to the gripper frame ( $_{}^{g}\\textrm{T}_c$).\n * \n * @param method One of the implemented Hand-Eye calibration method, see cv::HandEyeCalibrationMethod\n */\nexport declare function calibrateHandEye(R_gripper2base: InputArrayOfArrays, t_gripper2base: InputArrayOfArrays, R_target2cam: InputArrayOfArrays, t_target2cam: InputArrayOfArrays, R_cam2gripper: OutputArray, t_cam2gripper: OutputArray, method?: HandEyeCalibrationMethod): void\n\n/**\n * The function computes various useful camera characteristics from the previously estimated camera\n * matrix.\n * \n * Do keep in mind that the unity measure 'mm' stands for whatever unit of measure one chooses for the\n * chessboard pitch (it can thus be any value).\n * \n * @param cameraMatrix Input camera matrix that can be estimated by calibrateCamera or stereoCalibrate\n * .\n * \n * @param imageSize Input image size in pixels.\n * \n * @param apertureWidth Physical width in mm of the sensor.\n * \n * @param apertureHeight Physical height in mm of the sensor.\n * \n * @param fovx Output field of view in degrees along the horizontal sensor axis.\n * \n * @param fovy Output field of view in degrees along the vertical sensor axis.\n * \n * @param focalLength Focal length of the lens in mm.\n * \n * @param principalPoint Principal point in mm.\n * \n * @param aspectRatio $f_y/f_x$\n */\nexport declare function calibrationMatrixValues(cameraMatrix: InputArray, imageSize: Size, apertureWidth: double, apertureHeight: double, fovx: any, fovy: any, focalLength: any, principalPoint: any, aspectRatio: any): void\n\nexport declare function checkChessboard(img: InputArray, size: Size): bool\n\n/**\n * The functions compute:\n * \n * `\\\\[\\\\begin{array}{l} \\\\texttt{rvec3} = \\\\mathrm{rodrigues} ^{-1} \\\\left ( \\\\mathrm{rodrigues} (\n * \\\\texttt{rvec2} ) \\\\cdot \\\\mathrm{rodrigues} ( \\\\texttt{rvec1} ) \\\\right ) \\\\\\\\ \\\\texttt{tvec3} =\n * \\\\mathrm{rodrigues} ( \\\\texttt{rvec2} ) \\\\cdot \\\\texttt{tvec1} + \\\\texttt{tvec2} \\\\end{array} ,\\\\]`\n * \n * where `$\\\\mathrm{rodrigues}$` denotes a rotation vector to a rotation matrix transformation, and\n * `$\\\\mathrm{rodrigues}^{-1}$` denotes the inverse transformation. See Rodrigues for details.\n * \n * Also, the functions can compute the derivatives of the output vectors with regards to the input\n * vectors (see matMulDeriv ). The functions are used inside stereoCalibrate but can also be used in\n * your own code where Levenberg-Marquardt or another gradient-based solver is used to optimize a\n * function that contains a matrix multiplication.\n * \n * @param rvec1 First rotation vector.\n * \n * @param tvec1 First translation vector.\n * \n * @param rvec2 Second rotation vector.\n * \n * @param tvec2 Second translation vector.\n * \n * @param rvec3 Output rotation vector of the superposition.\n * \n * @param tvec3 Output translation vector of the superposition.\n * \n * @param dr3dr1 Optional output derivative of rvec3 with regard to rvec1\n * \n * @param dr3dt1 Optional output derivative of rvec3 with regard to tvec1\n * \n * @param dr3dr2 Optional output derivative of rvec3 with regard to rvec2\n * \n * @param dr3dt2 Optional output derivative of rvec3 with regard to tvec2\n * \n * @param dt3dr1 Optional output derivative of tvec3 with regard to rvec1\n * \n * @param dt3dt1 Optional output derivative of tvec3 with regard to tvec1\n * \n * @param dt3dr2 Optional output derivative of tvec3 with regard to rvec2\n * \n * @param dt3dt2 Optional output derivative of tvec3 with regard to tvec2\n */\nexport declare function composeRT(rvec1: InputArray, tvec1: InputArray, rvec2: InputArray, tvec2: InputArray, rvec3: OutputArray, tvec3: OutputArray, dr3dr1?: OutputArray, dr3dt1?: OutputArray, dr3dr2?: OutputArray, dr3dt2?: OutputArray, dt3dr1?: OutputArray, dt3dt1?: OutputArray, dt3dr2?: OutputArray, dt3dt2?: OutputArray): void\n\n/**\n * For every point in one of the two images of a stereo pair, the function finds the equation of the\n * corresponding epipolar line in the other image.\n * \n * From the fundamental matrix definition (see findFundamentalMat ), line `$l^{(2)}_i$` in the second\n * image for the point `$p^{(1)}_i$` in the first image (when whichImage=1 ) is computed as:\n * \n * `\\\\[l^{(2)}_i = F p^{(1)}_i\\\\]`\n * \n * And vice versa, when whichImage=2, `$l^{(1)}_i$` is computed from `$p^{(2)}_i$` as:\n * \n * `\\\\[l^{(1)}_i = F^T p^{(2)}_i\\\\]`\n * \n * Line coefficients are defined up to a scale. They are normalized so that `$a_i^2+b_i^2=1$` .\n * \n * @param points Input points. $N \\times 1$ or $1 \\times N$ matrix of type CV_32FC2 or vector<Point2f>\n * .\n * \n * @param whichImage Index of the image (1 or 2) that contains the points .\n * \n * @param F Fundamental matrix that can be estimated using findFundamentalMat or stereoRectify .\n * \n * @param lines Output vector of the epipolar lines corresponding to the points in the other image.\n * Each line $ax + by + c=0$ is encoded by 3 numbers $(a, b, c)$ .\n */\nexport declare function computeCorrespondEpilines(points: InputArray, whichImage: int, F: InputArray, lines: OutputArray): void\n\n/**\n * The function converts points homogeneous to Euclidean space using perspective projection. That is,\n * each point (x1, x2, ... x(n-1), xn) is converted to (x1/xn, x2/xn, ..., x(n-1)/xn). When xn=0, the\n * output point coordinates will be (0,0,0,...).\n * \n * @param src Input vector of N-dimensional points.\n * \n * @param dst Output vector of N-1-dimensional points.\n */\nexport declare function convertPointsFromHomogeneous(src: InputArray, dst: OutputArray): void\n\n/**\n * The function converts 2D or 3D points from/to homogeneous coordinates by calling either\n * convertPointsToHomogeneous or convertPointsFromHomogeneous.\n * \n * The function is obsolete. Use one of the previous two functions instead.\n * \n * @param src Input array or vector of 2D, 3D, or 4D points.\n * \n * @param dst Output vector of 2D, 3D, or 4D points.\n */\nexport declare function convertPointsHomogeneous(src: InputArray, dst: OutputArray): void\n\n/**\n * The function converts points from Euclidean to homogeneous space by appending 1's to the tuple of\n * point coordinates. That is, each point (x1, x2, ..., xn) is converted to (x1, x2, ..., xn, 1).\n * \n * @param src Input vector of N-dimensional points.\n * \n * @param dst Output vector of N+1-dimensional points.\n */\nexport declare function convertPointsToHomogeneous(src: InputArray, dst: OutputArray): void\n\n/**\n * The function implements the Optimal Triangulation Method (see Multiple View Geometry for details).\n * For each given point correspondence points1[i] <-> points2[i], and a fundamental matrix F, it\n * computes the corrected correspondences newPoints1[i] <-> newPoints2[i] that minimize the geometric\n * error `$d(points1[i], newPoints1[i])^2 + d(points2[i],newPoints2[i])^2$` (where `$d(a,b)$` is the\n * geometric distance between points `$a$` and `$b$` ) subject to the epipolar constraint\n * `$newPoints2^T * F * newPoints1 = 0$` .\n * \n * @param F 3x3 fundamental matrix.\n * \n * @param points1 1xN array containing the first set of points.\n * \n * @param points2 1xN array containing the second set of points.\n * \n * @param newPoints1 The optimized points1.\n * \n * @param newPoints2 The optimized points2.\n */\nexport declare function correctMatches(F: InputArray, points1: InputArray, points2: InputArray, newPoints1: OutputArray, newPoints2: OutputArray): void\n\n/**\n * This function decompose an essential matrix E using svd decomposition HartleyZ00 . Generally 4\n * possible poses exists for a given E. They are `$[R_1, t]$`, `$[R_1, -t]$`, `$[R_2, t]$`, `$[R_2,\n * -t]$`. By decomposing E, you can only get the direction of the translation, so the function returns\n * unit t.\n * \n * @param E The input essential matrix.\n * \n * @param R1 One possible rotation matrix.\n * \n * @param R2 Another possible rotation matrix.\n * \n * @param t One possible translation.\n */\nexport declare function decomposeEssentialMat(E: InputArray, R1: OutputArray, R2: OutputArray, t: OutputArray): void\n\n/**\n * This function extracts relative camera motion between two views observing a planar object from the\n * homography H induced by the plane. The intrinsic camera matrix K must also be provided. The function\n * may return up to four mathematical solution sets. At least two of the solutions may further be\n * invalidated if point correspondences are available by applying positive depth constraint (all points\n * must be in front of the camera). The decomposition method is described in detail in Malis .\n * \n * @param H The input homography matrix between two images.\n * \n * @param K The input intrinsic camera calibration matrix.\n * \n * @param rotations Array of rotation matrices.\n * \n * @param translations Array of translation matrices.\n * \n * @param normals Array of plane normal matrices.\n */\nexport declare function decomposeHomographyMat(H: InputArray, K: InputArray, rotations: OutputArrayOfArrays, translations: OutputArrayOfArrays, normals: OutputArrayOfArrays): int\n\n/**\n * The function computes a decomposition of a projection matrix into a calibration and a rotation\n * matrix and the position of a camera.\n * \n * It optionally returns three rotation matrices, one for each axis, and three Euler angles that could\n * be used in OpenGL. Note, there is always more than one sequence of rotations about the three\n * principal axes that results in the same orientation of an object, e.g. see Slabaugh . Returned tree\n * rotation matrices and corresponding three Euler angles are only one of the possible solutions.\n * \n * The function is based on RQDecomp3x3 .\n * \n * @param projMatrix 3x4 input projection matrix P.\n * \n * @param cameraMatrix Output 3x3 camera matrix K.\n * \n * @param rotMatrix Output 3x3 external rotation matrix R.\n * \n * @param transVect Output 4x1 translation vector T.\n * \n * @param rotMatrixX Optional 3x3 rotation matrix around x-axis.\n * \n * @param rotMatrixY Optional 3x3 rotation matrix around y-axis.\n * \n * @param rotMatrixZ Optional 3x3 rotation matrix around z-axis.\n * \n * @param eulerAngles Optional three-element vector containing three Euler angles of rotation in\n * degrees.\n */\nexport declare function decomposeProjectionMatrix(projMatrix: InputArray, cameraMatrix: OutputArray, rotMatrix: OutputArray, transVect: OutputArray, rotMatrixX?: OutputArray, rotMatrixY?: OutputArray, rotMatrixZ?: OutputArray, eulerAngles?: OutputArray): void\n\n/**\n * The function draws individual chessboard corners detected either as red circles if the board was not\n * found, or as colored corners connected with lines if the board was found.\n * \n * @param image Destination image. It must be an 8-bit color image.\n * \n * @param patternSize Number of inner corners per a chessboard row and column (patternSize =\n * cv::Size(points_per_row,points_per_column)).\n * \n * @param corners Array of detected corners, the output of findChessboardCorners.\n * \n * @param patternWasFound Parameter indicating whether the complete board was found or not. The return\n * value of findChessboardCorners should be passed here.\n */\nexport declare function drawChessboardCorners(image: InputOutputArray, patternSize: Size, corners: InputArray, patternWasFound: bool): void\n\n/**\n * [solvePnP](#d9/d0c/group__calib3d_1ga549c2075fac14829ff4a58bc931c033d})\n * \n * This function draws the axes of the world/object coordinate system w.r.t. to the camera frame. OX is\n * drawn in red, OY in green and OZ in blue.\n * \n * @param image Input/output image. It must have 1 or 3 channels. The number of channels is not\n * altered.\n * \n * @param cameraMatrix Input 3x3 floating-point matrix of camera intrinsic parameters. $A =\n * \\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}$\n * \n * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6\n * [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is empty,\n * the zero distortion coefficients are assumed.\n * \n * @param rvec Rotation vector (see Rodrigues ) that, together with tvec, brings points from the model\n * coordinate system to the camera coordinate system.\n * \n * @param tvec Translation vector.\n * \n * @param length Length of the painted axes in the same unit than tvec (usually in meters).\n * \n * @param thickness Line thickness of the painted axes.\n */\nexport declare function drawFrameAxes(image: InputOutputArray, cameraMatrix: InputArray, distCoeffs: InputArray, rvec: InputArray, tvec: InputArray, length: float, thickness?: int): void\n\n/**\n * It computes `\\\\[ \\\\begin{bmatrix} x\\\\\\\\ y\\\\\\\\ \\\\end{bmatrix} = \\\\begin{bmatrix} a_{11} & a_{12}\\\\\\\\\n * a_{21} & a_{22}\\\\\\\\ \\\\end{bmatrix} \\\\begin{bmatrix} X\\\\\\\\ Y\\\\\\\\ \\\\end{bmatrix} + \\\\begin{bmatrix}\n * b_1\\\\\\\\ b_2\\\\\\\\ \\\\end{bmatrix} \\\\]`\n * \n * Output 2D affine transformation matrix `$2 \\\\times 3$` or empty matrix if transformation could not\n * be estimated. The returned matrix has the following form: `\\\\[ \\\\begin{bmatrix} a_{11} & a_{12} &\n * b_1\\\\\\\\ a_{21} & a_{22} & b_2\\\\\\\\ \\\\end{bmatrix} \\\\]`\n * The function estimates an optimal 2D affine transformation between two 2D point sets using the\n * selected robust algorithm.\n * \n * The computed transformation is then refined further (using only inliers) with the\n * Levenberg-Marquardt method to reduce the re-projection error even more.\n * \n * The RANSAC method can handle practically any ratio of outliers but needs a threshold to distinguish\n * inliers from outliers. The method LMeDS does not need any threshold but it works correctly only when\n * there are more than 50% of inliers.\n * \n * [estimateAffinePartial2D](#d9/d0c/group__calib3d_1gad767faff73e9cbd8b9d92b955b50062d}),\n * [getAffineTransform](#da/d54/group__imgproc__transform_1ga8f6d378f9f8eebb5cb55cd3ae295a999})\n * \n * @param from First input 2D point set containing $(X,Y)$.\n * \n * @param to Second input 2D point set containing $(x,y)$.\n * \n * @param inliers Output vector indicating which points are inliers (1-inlier, 0-outlier).\n * \n * @param method Robust method used to compute transformation. The following methods are possible:\n * cv::RANSAC - RANSAC-based robust methodcv::LMEDS - Least-Median robust method RANSAC is the default\n * method.\n * \n * @param ransacReprojThreshold Maximum reprojection error in the RANSAC algorithm to consider a point\n * as an inlier. Applies only to RANSAC.\n * \n * @param maxIters The maximum number of robust method iterations.\n * \n * @param confidence Confidence level, between 0 and 1, for the estimated transformation. Anything\n * between 0.95 and 0.99 is usually good enough. Values too close to 1 can slow down the estimation\n * significantly. Values lower than 0.8-0.9 can result in an incorrectly estimated transformation.\n * \n * @param refineIters Maximum number of iterations of refining algorithm (Levenberg-Marquardt). Passing\n * 0 will disable refining, so the output matrix will be output of robust method.\n */\nexport declare function estimateAffine2D(from: InputArray, to: InputArray, inliers?: OutputArray, method?: int, ransacReprojThreshold?: double, maxIters?: size_t, confidence?: double, refineIters?: size_t): any\n\n/**\n * It computes `\\\\[ \\\\begin{bmatrix} x\\\\\\\\ y\\\\\\\\ z\\\\\\\\ \\\\end{bmatrix} = \\\\begin{bmatrix} a_{11} &\n * a_{12} & a_{13}\\\\\\\\ a_{21} & a_{22} & a_{23}\\\\\\\\ a_{31} & a_{32} & a_{33}\\\\\\\\ \\\\end{bmatrix}\n * \\\\begin{bmatrix} X\\\\\\\\ Y\\\\\\\\ Z\\\\\\\\ \\\\end{bmatrix} + \\\\begin{bmatrix} b_1\\\\\\\\ b_2\\\\\\\\ b_3\\\\\\\\\n * \\\\end{bmatrix} \\\\]`\n * \n * The function estimates an optimal 3D affine transformation between two 3D point sets using the\n * RANSAC algorithm.\n * \n * @param src First input 3D point set containing $(X,Y,Z)$.\n * \n * @param dst Second input 3D point set containing $(x,y,z)$.\n * \n * @param out Output 3D affine transformation matrix $3 \\times 4$ of the form \\[ \\begin{bmatrix} a_{11}\n * & a_{12} & a_{13} & b_1\\\\ a_{21} & a_{22} & a_{23} & b_2\\\\ a_{31} & a_{32} & a_{33} & b_3\\\\\n * \\end{bmatrix} \\]\n * \n * @param inliers Output vector indicating which points are inliers (1-inlier, 0-outlier).\n * \n * @param ransacThreshold Maximum reprojection error in the RANSAC algorithm to consider a point as an\n * inlier.\n * \n * @param confidence Confidence level, between 0 and 1, for the estimated transformation. Anything\n * between 0.95 and 0.99 is usually good enough. Values too close to 1 can slow down the estimation\n * significantly. Values lower than 0.8-0.9 can result in an incorrectly estimated transformation.\n */\nexport declare function estimateAffine3D(src: InputArray, dst: InputArray, out: OutputArray, inliers: OutputArray, ransacThreshold?: double, confidence?: double): int\n\n/**\n * Output 2D affine transformation (4 degrees of freedom) matrix `$2 \\\\times 3$` or empty matrix if\n * transformation could not be estimated.\n * The function estimates an optimal 2D affine transformation with 4 degrees of freedom limited to\n * combinations of translation, rotation, and uniform scaling. Uses the selected algorithm for robust\n * estimation.\n * \n * The computed transformation is then refined further (using only inliers) with the\n * Levenberg-Marquardt method to reduce the re-projection error even more.\n * \n * Estimated transformation matrix is: `\\\\[ \\\\begin{bmatrix} \\\\cos(\\\\theta) \\\\cdot s & -\\\\sin(\\\\theta)\n * \\\\cdot s & t_x \\\\\\\\ \\\\sin(\\\\theta) \\\\cdot s & \\\\cos(\\\\theta) \\\\cdot s & t_y \\\\end{bmatrix} \\\\]`\n * Where `$ \\\\theta $` is the rotation angle, `$ s $` the scaling factor and `$ t_x, t_y $` are\n * translations in `$ x, y $` axes respectively.\n * \n * The RANSAC method can handle practically any ratio of outliers but need a threshold to distinguish\n * inliers from outliers. The method LMeDS does not need any threshold but it works correctly only when\n * there are more than 50% of inliers.\n * \n * [estimateAffine2D](#d9/d0c/group__calib3d_1ga27865b1d26bac9ce91efaee83e94d4dd}),\n * [getAffineTransform](#da/d54/group__imgproc__transform_1ga8f6d378f9f8eebb5cb55cd3ae295a999})\n * \n * @param from First input 2D point set.\n * \n * @param to Second input 2D point set.\n * \n * @param inliers Output vector indicating which points are inliers.\n * \n * @param method Robust method used to compute transformation. The following methods are possible:\n * cv::RANSAC - RANSAC-based robust methodcv::LMEDS - Least-Median robust method RANSAC is the default\n * method.\n * \n * @param ransacReprojThreshold Maximum reprojection error in the RANSAC algorithm to consider a point\n * as an inlier. Applies only to RANSAC.\n * \n * @param maxIters The maximum number of robust method iterations.\n * \n * @param confidence Confidence level, between 0 and 1, for the estimated transformation. Anything\n * between 0.95 and 0.99 is usually good enough. Values too close to 1 can slow down the estimation\n * significantly. Values lower than 0.8-0.9 can result in an incorrectly estimated transformation.\n * \n * @param refineIters Maximum number of iterations of refining algorithm (Levenberg-Marquardt). Passing\n * 0 will disable refining, so the output matrix will be output of robust method.\n */\nexport declare function estimateAffinePartial2D(from: InputArray, to: InputArray, inliers?: OutputArray, method?: int, ransacReprojThreshold?: double, maxIters?: size_t, confidence?: double, refineIters?: size_t): any\n\n/**\n * This function is intended to filter the output of the decomposeHomographyMat based on additional\n * information as described in Malis . The summary of the method: the decomposeHomographyMat function\n * returns 2 unique solutions and their \"opposites\" for a total of 4 solutions. If we have access to\n * the sets of points visible in the camera frame before and after the homography transformation is\n * applied, we can determine which are the true potential solutions and which are the opposites by\n * verifying which homographies are consistent with all visible reference points being in front of the\n * camera. The inputs are left unchanged; the filtered solution set is returned as indices into the\n * existing one.\n * \n * @param rotations Vector of rotation matrices.\n * \n * @param normals Vector of plane normal matrices.\n * \n * @param beforePoints Vector of (rectified) visible reference points before the homography is applied\n * \n * @param afterPoints Vector of (rectified) visible reference points after the homography is applied\n * \n * @param possibleSolutions Vector of int indices representing the viable solution set after filtering\n * \n * @param pointsMask optional Mat/Vector of 8u type representing the mask for the inliers as given by\n * the findHomography function\n */\nexport declare function filterHomographyDecompByVisibleRefpoints(rotations: InputArrayOfArrays, normals: InputArrayOfArrays, beforePoints: InputArray, afterPoints: InputArray, possibleSolutions: OutputArray, pointsMask?: InputArray): void\n\n/**\n * @param img The input 16-bit signed disparity image\n * \n * @param newVal The disparity value used to paint-off the speckles\n * \n * @param maxSpeckleSize The maximum speckle size to consider it a speckle. Larger blobs are not\n * affected by the algorithm\n * \n * @param maxDiff Maximum difference between neighbor disparity pixels to put them into the same blob.\n * Note that since StereoBM, StereoSGBM and may be other algorithms return a fixed-point disparity map,\n * where disparity values are multiplied by 16, this scale factor should be taken into account when\n * specifying this parameter value.\n * \n * @param buf The optional temporary buffer to avoid memory allocation within the function.\n */\nexport declare function filterSpeckles(img: InputOutputArray, newVal: double, maxSpeckleSize: int, maxDiff: double, buf?: InputOutputArray): void\n\nexport declare function find4QuadCornerSubpix(img: InputArray, corners: InputOutputArray, region_size: Size): bool\n\n/**\n * The function attempts to determine whether the input image is a view of the chessboard pattern and\n * locate the internal chessboard corners. The function returns a non-zero value if all of the corners\n * are found and they are placed in a certain order (row by row, left to right in every row).\n * Otherwise, if the function fails to find all the corners or reorder them, it returns 0. For example,\n * a regular chessboard has 8 x 8 squares and 7 x 7 internal corners, that is, points where the black\n * squares touch each other. The detected coordinates are approximate, and to determine their positions\n * more accurately, the function calls cornerSubPix. You also may use the function cornerSubPix with\n * different parameters if returned coordinates are not accurate enough.\n * \n * Sample usage of detecting and drawing chessboard corners: : \n * \n * ```cpp\n * Size patternsize(8,6); //interior number of corners\n * Mat gray = ....; //source image\n * vector<Point2f> corners; //this will be filled by the detected corners\n * \n * //CALIB_CB_FAST_CHECK saves a lot of time on images\n * //that do not contain any chessboard corners\n * bool patternfound = findChessboardCorners(gray, patternsize, corners,\n *         CALIB_CB_ADAPTIVE_THRESH + CALIB_CB_NORMALIZE_IMAGE\n *         + CALIB_CB_FAST_CHECK);\n * \n * if(patternfound)\n *   cornerSubPix(gray, corners, Size(11, 11), Size(-1, -1),\n *     TermCriteria(CV_TERMCRIT_EPS + CV_TERMCRIT_ITER, 30, 0.1));\n * \n * drawChessboardCorners(img, patternsize, Mat(corners), patternfound);\n * ```\n * \n * The function requires white space (like a square-thick border, the wider the better) around the\n * board to make the detection more robust in various environments. Otherwise, if there is no border\n * and the background is dark, the outer black squares cannot be segmented properly and so the square\n * grouping and ordering algorithm fails.\n * \n * @param image Source chessboard view. It must be an 8-bit grayscale or color image.\n * \n * @param patternSize Number of inner corners per a chessboard row and column ( patternSize =\n * cv::Size(points_per_row,points_per_colum) = cv::Size(columns,rows) ).\n * \n * @param corners Output array of detected corners.\n * \n * @param flags Various operation flags that can be zero or a combination of the following values:\n * CALIB_CB_ADAPTIVE_THRESH Use adaptive thresholding to convert the image to black and white, rather\n * than a fixed threshold level (computed from the average image brightness).CALIB_CB_NORMALIZE_IMAGE\n * Normalize the image gamma with equalizeHist before applying fixed or adaptive\n * thresholding.CALIB_CB_FILTER_QUADS Use additional criteria (like contour area, perimeter,\n * square-like shape) to filter out false quads extracted at the contour retrieval\n * stage.CALIB_CB_FAST_CHECK Run a fast check on the image that looks for chessboard corners, and\n * shortcut the call if none is found. This can drastically speed up the call in the degenerate\n * condition when no chessboard is observed.\n */\nexport declare function findChessboardCorners(image: InputArray, patternSize: Size, corners: OutputArray, flags?: int): bool\n\n/**\n * The function is analog to findchessboardCorners but uses a localized radon transformation\n * approximated by box filters being more robust to all sort of noise, faster on larger images and is\n * able to directly return the sub-pixel position of the internal chessboard corners. The Method is\n * based on the paper duda2018 \"Accurate Detection and Localization of Checkerboard Corners for\n * Calibration\" demonstrating that the returned sub-pixel positions are more accurate than the one\n * returned by cornerSubPix allowing a precise camera calibration for demanding applications.\n * \n * The function requires a white boarder with roughly the same width as one of the checkerboard fields\n * around the whole board to improve the detection in various environments. In addition, because of the\n * localized radon transformation it is beneficial to use round corners for the field corners which are\n * located on the outside of the board. The following figure illustrates a sample checkerboard\n * optimized for the detection. However, any other checkerboard can be used as well.\n * \n * @param image Source chessboard view. It must be an 8-bit grayscale or color image.\n * \n * @param patternSize Number of inner corners per a chessboard row and column ( patternSize =\n * cv::Size(points_per_row,points_per_colum) = cv::Size(columns,rows) ).\n * \n * @param corners Output array of detected corners.\n * \n * @param flags Various operation flags that can be zero or a combination of the following values:\n * CALIB_CB_NORMALIZE_IMAGE Normalize the image gamma with equalizeHist before\n * detection.CALIB_CB_EXHAUSTIVE Run an exhaustive search to improve detection rate.CALIB_CB_ACCURACY\n * Up sample input image to improve sub-pixel accuracy due to aliasing effects. This should be used if\n * an accurate camera calibration is required.\n */\nexport declare function findChessboardCornersSB(image: InputArray, patternSize: Size, corners: OutputArray, flags?: int): bool\n\n/**\n * The function attempts to determine whether the input image contains a grid of circles. If it is, the\n * function locates centers of the circles. The function returns a non-zero value if all of the centers\n * have been found and they have been placed in a certain order (row by row, left to right in every\n * row). Otherwise, if the function fails to find all the corners or reorder them, it returns 0.\n * \n * Sample usage of detecting and drawing the centers of circles: : \n * \n * ```cpp\n * Size patternsize(7,7); //number of centers\n * Mat gray = ....; //source image\n * vector<Point2f> centers; //this will be filled by the detected centers\n * \n * bool patternfound = findCirclesGrid(gray, patternsize, centers);\n * \n * drawChessboardCorners(img, patternsize, Mat(centers), patternfound);\n * ```\n * \n * The function requires white space (like a square-thick border, the wider the better) around the\n * board to make the detection more robust in various environments.\n * \n * @param image grid view of input circles; it must be an 8-bit grayscale or color image.\n * \n * @param patternSize number of circles per row and column ( patternSize = Size(points_per_row,\n * points_per_colum) ).\n * \n * @param centers output array of detected centers.\n * \n * @param flags various operation flags that can be one of the following values:\n * CALIB_CB_SYMMETRIC_GRID uses symmetric pattern of circles.CALIB_CB_ASYMMETRIC_GRID uses asymmetric\n * pattern of circles.CALIB_CB_CLUSTERING uses a special algorithm for grid detection. It is more\n * robust to perspective distortions but much more sensitive to background clutter.\n * \n * @param blobDetector feature detector that finds blobs like dark circles on light background.\n * \n * @param parameters struct for finding circles in a grid pattern.\n */\nexport declare function findCirclesGrid(image: InputArray, patternSize: Size, centers: OutputArray, flags: int, blobDetector: any, parameters: any): bool\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function findCirclesGrid(image: InputArray, patternSize: Size, centers: OutputArray, flags?: int, blobDetector?: any): bool\n\n/**\n * This function estimates essential matrix based on the five-point algorithm solver in Nister03 .\n * SteweniusCFS is also a related. The epipolar geometry is described by the following equation:\n * \n * `\\\\[[p_2; 1]^T K^{-T} E K^{-1} [p_1; 1] = 0\\\\]`\n * \n * where `$E$` is an essential matrix, `$p_1$` and `$p_2$` are corresponding points in the first and\n * the second images, respectively. The result of this function may be passed further to\n * decomposeEssentialMat or recoverPose to recover the relative pose between cameras.\n * \n * @param points1 Array of N (N >= 5) 2D points from the first image. The point coordinates should be\n * floating-point (single or double precision).\n * \n * @param points2 Array of the second image points of the same size and format as points1 .\n * \n * @param cameraMatrix Camera matrix $K = \\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}$ . Note\n * that this function assumes that points1 and points2 are feature points from cameras with the same\n * camera matrix.\n * \n * @param method Method for computing an essential matrix.\n * RANSAC for the RANSAC algorithm.LMEDS for the LMedS algorithm.\n * \n * @param prob Parameter used for the RANSAC or LMedS methods only. It specifies a desirable level of\n * confidence (probability) that the estimated matrix is correct.\n * \n * @param threshold Parameter used for RANSAC. It is the maximum distance from a point to an epipolar\n * line in pixels, beyond which the point is considered an outlier and is not used for computing the\n * final fundamental matrix. It can be set to something like 1-3, depending on the accuracy of the\n * point localization, image resolution, and the image noise.\n * \n * @param mask Output array of N elements, every element of which is set to 0 for outliers and to 1 for\n * the other points. The array is computed only in the RANSAC and LMedS methods.\n */\nexport declare function findEssentialMat(points1: InputArray, points2: InputArray, cameraMatrix: InputArray, method?: int, prob?: double, threshold?: double, mask?: OutputArray): Mat\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts. \n * This function differs from the one above that it computes camera matrix from focal length and\n * principal point:\n * \n * `\\\\[K = \\\\begin{bmatrix} f & 0 & x_{pp} \\\\\\\\ 0 & f & y_{pp} \\\\\\\\ 0 & 0 & 1 \\\\end{bmatrix}\\\\]`\n * \n * @param points1 Array of N (N >= 5) 2D points from the first image. The point coordinates should be\n * floating-point (single or double precision).\n * \n * @param points2 Array of the second image points of the same size and format as points1 .\n * \n * @param focal focal length of the camera. Note that this function assumes that points1 and points2\n * are feature points from cameras with same focal length and principal point.\n * \n * @param pp principal point of the camera.\n * \n * @param method Method for computing a fundamental matrix.\n * RANSAC for the RANSAC algorithm.LMEDS for the LMedS algorithm.\n * \n * @param prob Parameter used for the RANSAC or LMedS methods only. It specifies a desirable level of\n * confidence (probability) that the estimated matrix is correct.\n * \n * @param threshold Parameter used for RANSAC. It is the maximum distance from a point to an epipolar\n * line in pixels, beyond which the point is considered an outlier and is not used for computing the\n * final fundamental matrix. It can be set to something like 1-3, depending on the accuracy of the\n * point localization, image resolution, and the image noise.\n * \n * @param mask Output array of N elements, every element of which is set to 0 for outliers and to 1 for\n * the other points. The array is computed only in the RANSAC and LMedS methods.\n */\nexport declare function findEssentialMat(points1: InputArray, points2: InputArray, focal?: double, pp?: Point2d, method?: int, prob?: double, threshold?: double, mask?: OutputArray): Mat\n\n/**\n * `\\\\[[p_2; 1]^T F [p_1; 1] = 0\\\\]`\n * \n * where `$F$` is a fundamental matrix, `$p_1$` and `$p_2$` are corresponding points in the first and\n * the second images, respectively.\n * \n * The function calculates the fundamental matrix using one of four methods listed above and returns\n * the found fundamental matrix. Normally just one matrix is found. But in case of the 7-point\n * algorithm, the function may return up to 3 solutions ( `$9 \\\\times 3$` matrix that stores all 3\n * matrices sequentially).\n * \n * The calculated fundamental matrix may be passed further to computeCorrespondEpilines that finds the\n * epipolar lines corresponding to the specified points. It can also be passed to\n * stereoRectifyUncalibrated to compute the rectification transformation. : \n * \n * ```cpp\n * // Example. Estimation of fundamental matrix using the RANSAC algorithm\n * int point_count = 100;\n * vector<Point2f> points1(point_count);\n * vector<Point2f> points2(point_count);\n * \n * // initialize the points here ...\n * for( int i = 0; i < point_count; i++ )\n * {\n *     points1[i] = ...;\n *     points2[i] = ...;\n * }\n * \n * Mat fundamental_matrix =\n *  findFundamentalMat(points1, points2, FM_RANSAC, 3, 0.99);\n * ```\n * \n * @param points1 Array of N points from the first image. The point coordinates should be\n * floating-point (single or double precision).\n * \n * @param points2 Array of the second image points of the same size and format as points1 .\n * \n * @param method Method for computing a fundamental matrix.\n * CV_FM_7POINT for a 7-point algorithm. $N = 7$CV_FM_8POINT for an 8-point algorithm. $N \\ge\n * 8$CV_FM_RANSAC for the RANSAC algorithm. $N \\ge 8$CV_FM_LMEDS for the LMedS algorithm. $N \\ge 8$\n * \n * @param ransacReprojThreshold Parameter used only for RANSAC. It is the maximum distance from a point\n * to an epipolar line in pixels, beyond which the point is considered an outlier and is not used for\n * computing the final fundamental matrix. It can be set to something like 1-3, depending on the\n * accuracy of the point localization, image resolution, and the image noise.\n * \n * @param confidence Parameter used for the RANSAC and LMedS methods only. It specifies a desirable\n * level of confidence (probability) that the estimated matrix is correct.\n * \n * @param mask The epipolar geometry is described by the following equation:\n */\nexport declare function findFundamentalMat(points1: InputArray, points2: InputArray, method?: int, ransacReprojThreshold?: double, confidence?: double, mask?: OutputArray): Mat\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function findFundamentalMat(points1: InputArray, points2: InputArray, mask: OutputArray, method?: int, ransacReprojThreshold?: double, confidence?: double): Mat\n\n/**\n * The function finds and returns the perspective transformation `$H$` between the source and the\n * destination planes:\n * \n * `\\\\[s_i \\\\vecthree{x'_i}{y'_i}{1} \\\\sim H \\\\vecthree{x_i}{y_i}{1}\\\\]`\n * \n * so that the back-projection error\n * \n * `\\\\[\\\\sum _i \\\\left ( x'_i- \\\\frac{h_{11} x_i + h_{12} y_i + h_{13}}{h_{31} x_i + h_{32} y_i +\n * h_{33}} \\\\right )^2+ \\\\left ( y'_i- \\\\frac{h_{21} x_i + h_{22} y_i + h_{23}}{h_{31} x_i + h_{32} y_i\n * + h_{33}} \\\\right )^2\\\\]`\n * \n * is minimized. If the parameter method is set to the default value 0, the function uses all the point\n * pairs to compute an initial homography estimate with a simple least-squares scheme.\n * \n * However, if not all of the point pairs ( `$srcPoints_i$`, `$dstPoints_i$` ) fit the rigid\n * perspective transformation (that is, there are some outliers), this initial estimate will be poor.\n * In this case, you can use one of the three robust methods. The methods RANSAC, LMeDS and RHO try\n * many different random subsets of the corresponding point pairs (of four pairs each, collinear pairs\n * are discarded), estimate the homography matrix using this subset and a simple least-squares\n * algorithm, and then compute the quality/goodness of the computed homography (which is the number of\n * inliers for RANSAC or the least median re-projection error for LMeDS). The best subset is then used\n * to produce the initial estimate of the homography matrix and the mask of inliers/outliers.\n * \n * Regardless of the method, robust or not, the computed homography matrix is refined further (using\n * inliers only in case of a robust method) with the Levenberg-Marquardt method to reduce the\n * re-projection error even more.\n * \n * The methods RANSAC and RHO can handle practically any ratio of outliers but need a threshold to\n * distinguish inliers from outliers. The method LMeDS does not need any threshold but it works\n * correctly only when there are more than 50% of inliers. Finally, if there are no outliers and the\n * noise is rather small, use the default method (method=0).\n * \n * The function is used to find initial intrinsic and extrinsic matrices. Homography matrix is\n * determined up to a scale. Thus, it is normalized so that `$h_{33}=1$`. Note that whenever an `$H$`\n * matrix cannot be estimated, an empty one will be returned.\n * \n * [getAffineTransform](#da/d54/group__imgproc__transform_1ga8f6d378f9f8eebb5cb55cd3ae295a999}),\n * [estimateAffine2D](#d9/d0c/group__calib3d_1ga27865b1d26bac9ce91efaee83e94d4dd}),\n * [estimateAffinePartial2D](#d9/d0c/group__calib3d_1gad767faff73e9cbd8b9d92b955b50062d}),\n * [getPerspectiveTransform](#da/d54/group__imgproc__transform_1ga20f62aa3235d869c9956436c870893ae}),\n * [warpPerspective](#da/d54/group__imgproc__transform_1gaf73673a7e8e18ec6963e3774e6a94b87}),\n * [perspectiveTransform](#d2/de8/group__core__array_1gad327659ac03e5fd6894b90025e6900a7})\n * \n * @param srcPoints Coordinates of the points in the original plane, a matrix of the type CV_32FC2 or\n * vector<Point2f> .\n * \n * @param dstPoints Coordinates of the points in the target plane, a matrix of the type CV_32FC2 or a\n * vector<Point2f> .\n * \n * @param method Method used to compute a homography matrix. The following methods are possible:\n * 0 - a regular method using all the points, i.e., the least squares methodRANSAC - RANSAC-based\n * robust methodLMEDS - Least-Median robust methodRHO - PROSAC-based robust method\n * \n * @param ransacReprojThreshold Maximum allowed reprojection error to treat a point pair as an inlier\n * (used in the RANSAC and RHO methods only). That is, if \\[\\| \\texttt{dstPoints} _i -\n * \\texttt{convertPointsHomogeneous} ( \\texttt{H} * \\texttt{srcPoints} _i) \\|_2 >\n * \\texttt{ransacReprojThreshold}\\] then the point $i$ is considered as an outlier. If srcPoints and\n * dstPoints are measured in pixels, it usually makes sense to set this parameter somewhere in the\n * range of 1 to 10.\n * \n * @param mask Optional output mask set by a robust method ( RANSAC or LMEDS ). Note that the input\n * mask values are ignored.\n * \n * @param maxIters The maximum number of RANSAC iterations.\n * \n * @param confidence Confidence level, between 0 and 1.\n */\nexport declare function findHomography(srcPoints: InputArray, dstPoints: InputArray, method?: int, ransacReprojThreshold?: double, mask?: OutputArray, maxIters?: any, confidence?: any): Mat\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function findHomography(srcPoints: InputArray, dstPoints: InputArray, mask: OutputArray, method?: int, ransacReprojThreshold?: double): Mat\n\n/**\n * The function returns the camera matrix that is either an exact copy of the input cameraMatrix (when\n * centerPrinicipalPoint=false ), or the modified one (when centerPrincipalPoint=true).\n * \n * In the latter case, the new camera matrix will be:\n * \n * `\\\\[\\\\begin{bmatrix} f_x && 0 && ( \\\\texttt{imgSize.width} -1)*0.5 \\\\\\\\ 0 && f_y && (\n * \\\\texttt{imgSize.height} -1)*0.5 \\\\\\\\ 0 && 0 && 1 \\\\end{bmatrix} ,\\\\]`\n * \n * where `$f_x$` and `$f_y$` are `$(0,0)$` and `$(1,1)$` elements of cameraMatrix, respectively.\n * \n * By default, the undistortion functions in OpenCV (see\n * [initUndistortRectifyMap](#d9/d0c/group__calib3d_1ga7dfb72c9cf9780a347fbe3d1c47e5d5a}),\n * [undistort](#d9/d0c/group__calib3d_1ga69f2545a8b62a6b0fc2ee060dc30559d})) do not move the principal\n * point. However, when you work with stereo, it is important to move the principal points in both\n * views to the same y-coordinate (which is required by most of stereo correspondence algorithms), and\n * may be to the same x-coordinate too. So, you can form the new camera matrix for each view where the\n * principal points are located at the center.\n * \n * @param cameraMatrix Input camera matrix.\n * \n * @param imgsize Camera view image size in pixels.\n * \n * @param centerPrincipalPoint Location of the principal point in the new camera matrix. The parameter\n * indicates whether this location should be at the image center or not.\n */\nexport declare function getDefaultNewCameraMatrix(cameraMatrix: InputArray, imgsize?: Size, centerPrincipalPoint?: bool): Mat\n\n/**\n * new_camera_matrix Output new camera matrix.\n * The function computes and returns the optimal new camera matrix based on the free scaling parameter.\n * By varying this parameter, you may retrieve only sensible pixels alpha=0 , keep all the original\n * image pixels if there is valuable information in the corners alpha=1 , or get something in between.\n * When alpha>0 , the undistorted result is likely to have some black pixels corresponding to \"virtual\"\n * pixels outside of the captured distorted image. The original camera matrix, distortion coefficients,\n * the computed new camera matrix, and newImageSize should be passed to initUndistortRectifyMap to\n * produce the maps for remap .\n * \n * @param cameraMatrix Input camera matrix.\n * \n * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6\n * [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is\n * NULL/empty, the zero distortion coefficients are assumed.\n * \n * @param imageSize Original image size.\n * \n * @param alpha Free scaling parameter between 0 (when all the pixels in the undistorted image are\n * valid) and 1 (when all the source image pixels are retained in the undistorted image). See\n * stereoRectify for details.\n * \n * @param newImgSize Image size after rectification. By default, it is set to imageSize .\n * \n * @param validPixROI Optional output rectangle that outlines all-good-pixels region in the undistorted\n * image. See roi1, roi2 description in stereoRectify .\n * \n * @param centerPrincipalPoint Optional flag that indicates whether in the new camera matrix the\n * principal point should be at the image center or not. By default, the principal point is chosen to\n * best fit a subset of the source image (determined by alpha) to the corrected image.\n */\nexport declare function getOptimalNewCameraMatrix(cameraMatrix: InputArray, distCoeffs: InputArray, imageSize: Size, alpha: double, newImgSize?: Size, validPixROI?: any, centerPrincipalPoint?: bool): Mat\n\nexport declare function getValidDisparityROI(roi1: Rect, roi2: Rect, minDisparity: int, numberOfDisparities: int, SADWindowSize: int): Rect\n\n/**\n * The function estimates and returns an initial camera matrix for the camera calibration process.\n * Currently, the function only supports planar calibration patterns, which are patterns where each\n * object point has z-coordinate =0.\n * \n * @param objectPoints Vector of vectors of the calibration pattern points in the calibration pattern\n * coordinate space. In the old interface all the per-view vectors are concatenated. See\n * calibrateCamera for details.\n * \n * @param imagePoints Vector of vectors of the projections of the calibration pattern points. In the\n * old interface all the per-view vectors are concatenated.\n * \n * @param imageSize Image size in pixels used to initialize the principal point.\n * \n * @param aspectRatio If it is zero or negative, both $f_x$ and $f_y$ are estimated independently.\n * Otherwise, $f_x = f_y * \\texttt{aspectRatio}$ .\n */\nexport declare function initCameraMatrix2D(objectPoints: InputArrayOfArrays, imagePoints: InputArrayOfArrays, imageSize: Size, aspectRatio?: double): Mat\n\n/**\n * The function computes the joint undistortion and rectification transformation and represents the\n * result in the form of maps for remap. The undistorted image looks like original, as if it is\n * captured with a camera using the camera matrix =newCameraMatrix and zero distortion. In case of a\n * monocular camera, newCameraMatrix is usually equal to cameraMatrix, or it can be computed by\n * [getOptimalNewCameraMatrix](#d9/d0c/group__calib3d_1ga7a6c4e032c97f03ba747966e6ad862b1}) for a\n * better control over scaling. In case of a stereo camera, newCameraMatrix is normally set to P1 or P2\n * computed by [stereoRectify](#d9/d0c/group__calib3d_1ga617b1685d4059c6040827800e72ad2b6}) .\n * \n * Also, this new camera is oriented differently in the coordinate space, according to R. That, for\n * example, helps to align two heads of a stereo camera so that the epipolar lines on both images\n * become horizontal and have the same y- coordinate (in case of a horizontally aligned stereo camera).\n * \n * The function actually builds the maps for the inverse mapping algorithm that is used by remap. That\n * is, for each pixel `$(u, v)$` in the destination (corrected and rectified) image, the function\n * computes the corresponding coordinates in the source image (that is, in the original image from\n * camera). The following process is applied: `\\\\[ \\\\begin{array}{l} x \\\\leftarrow (u - {c'}_x)/{f'}_x\n * \\\\\\\\ y \\\\leftarrow (v - {c'}_y)/{f'}_y \\\\\\\\ {[X\\\\,Y\\\\,W]} ^T \\\\leftarrow R^{-1}*[x \\\\, y \\\\, 1]^T\n * \\\\\\\\ x' \\\\leftarrow X/W \\\\\\\\ y' \\\\leftarrow Y/W \\\\\\\\ r^2 \\\\leftarrow x'^2 + y'^2 \\\\\\\\ x''\n * \\\\leftarrow x' \\\\frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + 2p_1 x' y'\n * + p_2(r^2 + 2 x'^2) + s_1 r^2 + s_2 r^4\\\\\\\\ y'' \\\\leftarrow y' \\\\frac{1 + k_1 r^2 + k_2 r^4 + k_3\n * r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + p_1 (r^2 + 2 y'^2) + 2 p_2 x' y' + s_3 r^2 + s_4 r^4 \\\\\\\\\n * s\\\\vecthree{x'''}{y'''}{1} = \\\\vecthreethree{R_{33}(\\\\tau_x, \\\\tau_y)}{0}{-R_{13}((\\\\tau_x,\n * \\\\tau_y)} {0}{R_{33}(\\\\tau_x, \\\\tau_y)}{-R_{23}(\\\\tau_x, \\\\tau_y)} {0}{0}{1} R(\\\\tau_x, \\\\tau_y)\n * \\\\vecthree{x''}{y''}{1}\\\\\\\\ map_x(u,v) \\\\leftarrow x''' f_x + c_x \\\\\\\\ map_y(u,v) \\\\leftarrow y'''\n * f_y + c_y \\\\end{array} \\\\]` where `$(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6[, s_1, s_2, s_3, s_4[,\n * \\\\tau_x, \\\\tau_y]]]])$` are the distortion coefficients.\n * \n * In case of a stereo camera, this function is called twice: once for each camera head, after\n * stereoRectify, which in its turn is called after\n * [stereoCalibrate](#d9/d0c/group__calib3d_1ga91018d80e2a93ade37539f01e6f07de5}). But if the stereo\n * camera was not calibrated, it is still possible to compute the rectification transformations\n * directly from the fundamental matrix using\n * [stereoRectifyUncalibrated](#d9/d0c/group__calib3d_1gaadc5b14471ddc004939471339294f052}). For each\n * camera, the function computes homography H as the rectification transformation in a pixel domain,\n * not a rotation matrix R in 3D space. R can be computed from H as `\\\\[\\\\texttt{R} =\n * \\\\texttt{cameraMatrix} ^{-1} \\\\cdot \\\\texttt{H} \\\\cdot \\\\texttt{cameraMatrix}\\\\]` where cameraMatrix\n * can be chosen arbitrarily.\n * \n * @param cameraMatrix Input camera matrix $A=\\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}$ .\n * \n * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5,\n * k_6[, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is\n * NULL/empty, the zero distortion coefficients are assumed.\n * \n * @param R Optional rectification transformation in the object space (3x3 matrix). R1 or R2 , computed\n * by stereoRectify can be passed here. If the matrix is empty, the identity transformation is assumed.\n * In cvInitUndistortMap R assumed to be an identity matrix.\n * \n * @param newCameraMatrix New camera matrix $A'=\\vecthreethree{f_x'}{0}{c_x'}{0}{f_y'}{c_y'}{0}{0}{1}$.\n * \n * @param size Undistorted image size.\n * \n * @param m1type Type of the first output map that can be CV_32FC1, CV_32FC2 or CV_16SC2, see\n * convertMaps\n * \n * @param map1 The first output map.\n * \n * @param map2 The second output map.\n */\nexport declare function initUndistortRectifyMap(cameraMatrix: InputArray, distCoeffs: InputArray, R: InputArray, newCameraMatrix: InputArray, size: Size, m1type: int, map1: OutputArray, map2: OutputArray): void\n\nexport declare function initWideAngleProjMap(cameraMatrix: InputArray, distCoeffs: InputArray, imageSize: Size, destImageWidth: int, m1type: int, map1: OutputArray, map2: OutputArray, projType?: any, alpha?: double): float\n\nexport declare function initWideAngleProjMap(cameraMatrix: InputArray, distCoeffs: InputArray, imageSize: Size, destImageWidth: int, m1type: int, map1: OutputArray, map2: OutputArray, projType: int, alpha?: double): float\n\n/**\n * The function computes partial derivatives of the elements of the matrix product `$A*B$` with regard\n * to the elements of each of the two input matrices. The function is used to compute the Jacobian\n * matrices in stereoCalibrate but can also be used in any other similar optimization function.\n * \n * @param A First multiplied matrix.\n * \n * @param B Second multiplied matrix.\n * \n * @param dABdA First output derivative matrix d(A*B)/dA of size $\\texttt{A.rows*B.cols} \\times\n * {A.rows*A.cols}$ .\n * \n * @param dABdB Second output derivative matrix d(A*B)/dB of size $\\texttt{A.rows*B.cols} \\times\n * {B.rows*B.cols}$ .\n */\nexport declare function matMulDeriv(A: InputArray, B: InputArray, dABdA: OutputArray, dABdB: OutputArray): void\n\n/**\n * The function computes projections of 3D points to the image plane given intrinsic and extrinsic\n * camera parameters. Optionally, the function computes Jacobians - matrices of partial derivatives of\n * image points coordinates (as functions of all the input parameters) with respect to the particular\n * parameters, intrinsic and/or extrinsic. The Jacobians are used during the global optimization in\n * calibrateCamera, solvePnP, and stereoCalibrate . The function itself can also be used to compute a\n * re-projection error given the current intrinsic and extrinsic parameters.\n * \n * By setting rvec=tvec=(0,0,0) or by setting cameraMatrix to a 3x3 identity matrix, or by passing zero\n * distortion coefficients, you can get various useful partial cases of the function. This means that\n * you can compute the distorted coordinates for a sparse set of points or apply a perspective\n * transformation (and also compute the derivatives) in the ideal zero-distortion setup.\n * \n * @param objectPoints Array of object points, 3xN/Nx3 1-channel or 1xN/Nx1 3-channel (or\n * vector<Point3f> ), where N is the number of points in the view.\n * \n * @param rvec Rotation vector. See Rodrigues for details.\n * \n * @param tvec Translation vector.\n * \n * @param cameraMatrix Camera matrix $A = \\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{_1}$ .\n * \n * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6\n * [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is empty,\n * the zero distortion coefficients are assumed.\n * \n * @param imagePoints Output array of image points, 1xN/Nx1 2-channel, or vector<Point2f> .\n * \n * @param jacobian Optional output 2Nx(10+<numDistCoeffs>) jacobian matrix of derivatives of image\n * points with respect to components of the rotation vector, translation vector, focal lengths,\n * coordinates of the principal point and the distortion coefficients. In the old interface different\n * components of the jacobian are returned via different output parameters.\n * \n * @param aspectRatio Optional \"fixed aspect ratio\" parameter. If the parameter is not 0, the function\n * assumes that the aspect ratio (fx/fy) is fixed and correspondingly adjusts the jacobian matrix.\n */\nexport declare function projectPoints(objectPoints: InputArray, rvec: InputArray, tvec: InputArray, cameraMatrix: InputArray, distCoeffs: InputArray, imagePoints: OutputArray, jacobian?: OutputArray, aspectRatio?: double): void\n\n/**\n * This function can be used to process output E and mask from findEssentialMat. In this scenario,\n * points1 and points2 are the same input for findEssentialMat. : \n * \n * ```cpp\n * // Example. Estimation of fundamental matrix using the RANSAC algorithm\n * int point_count = 100;\n * vector<Point2f> points1(point_count);\n * vector<Point2f> points2(point_count);\n * \n * // initialize the points here ...\n * for( int i = 0; i < point_count; i++ )\n * {\n *     points1[i] = ...;\n *     points2[i] = ...;\n * }\n * \n * // cametra matrix with both focal lengths = 1, and principal point = (0, 0)\n * Mat cameraMatrix = Mat::eye(3, 3, CV_64F);\n * \n * Mat E, R, t, mask;\n * \n * E = findEssentialMat(points1, points2, cameraMatrix, RANSAC, 0.999, 1.0, mask);\n * recoverPose(E, points1, points2, cameraMatrix, R, t, mask);\n * ```\n * \n * @param E The input essential matrix.\n * \n * @param points1 Array of N 2D points from the first image. The point coordinates should be\n * floating-point (single or double precision).\n * \n * @param points2 Array of the second image points of the same size and format as points1 .\n * \n * @param cameraMatrix Camera matrix $K = \\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}$ . Note\n * that this function assumes that points1 and points2 are feature points from cameras with the same\n * camera matrix.\n * \n * @param R Recovered relative rotation.\n * \n * @param t Recovered relative translation.\n * \n * @param mask Input/output mask for inliers in points1 and points2. : If it is not empty, then it\n * marks inliers in points1 and points2 for then given essential matrix E. Only these inliers will be\n * used to recover pose. In the output mask only inliers which pass the cheirality check. This function\n * decomposes an essential matrix using decomposeEssentialMat and then verifies possible pose\n * hypotheses by doing cheirality check. The cheirality check basically means that the triangulated 3D\n * points should have positive depth. Some details can be found in Nister03 .\n */\nexport declare function recoverPose(E: InputArray, points1: InputArray, points2: InputArray, cameraMatrix: InputArray, R: OutputArray, t: OutputArray, mask?: InputOutputArray): int\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts. \n * This function differs from the one above that it computes camera matrix from focal length and\n * principal point:\n * \n * `\\\\[K = \\\\begin{bmatrix} f & 0 & x_{pp} \\\\\\\\ 0 & f & y_{pp} \\\\\\\\ 0 & 0 & 1 \\\\end{bmatrix}\\\\]`\n * \n * @param E The input essential matrix.\n * \n * @param points1 Array of N 2D points from the first image. The point coordinates should be\n * floating-point (single or double precision).\n * \n * @param points2 Array of the second image points of the same size and format as points1 .\n * \n * @param R Recovered relative rotation.\n * \n * @param t Recovered relative translation.\n * \n * @param focal Focal length of the camera. Note that this function assumes that points1 and points2\n * are feature points from cameras with same focal length and principal point.\n * \n * @param pp principal point of the camera.\n * \n * @param mask Input/output mask for inliers in points1 and points2. : If it is not empty, then it\n * marks inliers in points1 and points2 for then given essential matrix E. Only these inliers will be\n * used to recover pose. In the output mask only inliers which pass the cheirality check.\n */\nexport declare function recoverPose(E: InputArray, points1: InputArray, points2: InputArray, R: OutputArray, t: OutputArray, focal?: double, pp?: Point2d, mask?: InputOutputArray): int\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n * \n * @param E The input essential matrix.\n * \n * @param points1 Array of N 2D points from the first image. The point coordinates should be\n * floating-point (single or double precision).\n * \n * @param points2 Array of the second image points of the same size and format as points1.\n * \n * @param cameraMatrix Camera matrix $K = \\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}$ . Note\n * that this function assumes that points1 and points2 are feature points from cameras with the same\n * camera matrix.\n * \n * @param R Recovered relative rotation.\n * \n * @param t Recovered relative translation.\n * \n * @param distanceThresh threshold distance which is used to filter out far away points (i.e. infinite\n * points).\n * \n * @param mask Input/output mask for inliers in points1 and points2. : If it is not empty, then it\n * marks inliers in points1 and points2 for then given essential matrix E. Only these inliers will be\n * used to recover pose. In the output mask only inliers which pass the cheirality check.\n * \n * @param triangulatedPoints 3d points which were reconstructed by triangulation.\n */\nexport declare function recoverPose(E: InputArray, points1: InputArray, points2: InputArray, cameraMatrix: InputArray, R: OutputArray, t: OutputArray, distanceThresh: double, mask?: InputOutputArray, triangulatedPoints?: OutputArray): int\n\nexport declare function rectify3Collinear(cameraMatrix1: InputArray, distCoeffs1: InputArray, cameraMatrix2: InputArray, distCoeffs2: InputArray, cameraMatrix3: InputArray, distCoeffs3: InputArray, imgpt1: InputArrayOfArrays, imgpt3: InputArrayOfArrays, imageSize: Size, R12: InputArray, T12: InputArray, R13: InputArray, T13: InputArray, R1: OutputArray, R2: OutputArray, R3: OutputArray, P1: OutputArray, P2: OutputArray, P3: OutputArray, Q: OutputArray, alpha: double, newImgSize: Size, roi1: any, roi2: any, flags: int): float\n\n/**\n * The function transforms a single-channel disparity map to a 3-channel image representing a 3D\n * surface. That is, for each pixel (x,y) and the corresponding disparity d=disparity(x,y) , it\n * computes:\n * \n * `\\\\[\\\\begin{array}{l} [X \\\\; Y \\\\; Z \\\\; W]^T = \\\\texttt{Q} *[x \\\\; y \\\\; \\\\texttt{disparity} (x,y)\n * \\\\; 1]^T \\\\\\\\ \\\\texttt{\\\\_3dImage} (x,y) = (X/W, \\\\; Y/W, \\\\; Z/W) \\\\end{array}\\\\]`\n * \n * The matrix Q can be an arbitrary `$4 \\\\times 4$` matrix (for example, the one computed by\n * stereoRectify). To reproject a sparse set of points {(x,y,d),...} to 3D space, use\n * perspectiveTransform .\n * \n * @param disparity Input single-channel 8-bit unsigned, 16-bit signed, 32-bit signed or 32-bit\n * floating-point disparity image. If 16-bit signed format is used, the values are assumed to have no\n * fractional bits.\n * \n * @param _3dImage Output 3-channel floating-point image of the same size as disparity . Each element\n * of _3dImage(x,y) contains 3D coordinates of the point (x,y) computed from the disparity map.\n * \n * @param Q $4 \\times 4$ perspective transformation matrix that can be obtained with stereoRectify.\n * \n * @param handleMissingValues Indicates, whether the function should handle missing values (i.e. points\n * where the disparity was not computed). If handleMissingValues=true, then pixels with the minimal\n * disparity that corresponds to the outliers (see StereoMatcher::compute ) are transformed to 3D\n * points with a very large Z value (currently set to 10000).\n * \n * @param ddepth The optional output array depth. If it is -1, the output image will have CV_32F depth.\n * ddepth can also be set to CV_16S, CV_32S or CV_32F.\n */\nexport declare function reprojectImageTo3D(disparity: InputArray, _3dImage: OutputArray, Q: InputArray, handleMissingValues?: bool, ddepth?: int): void\n\n/**\n * `\\\\[\\\\begin{array}{l} \\\\theta \\\\leftarrow norm(r) \\\\\\\\ r \\\\leftarrow r/ \\\\theta \\\\\\\\ R =\n * \\\\cos{\\\\theta} I + (1- \\\\cos{\\\\theta} ) r r^T + \\\\sin{\\\\theta}\n * \\\\vecthreethree{0}{-r_z}{r_y}{r_z}{0}{-r_x}{-r_y}{r_x}{0} \\\\end{array}\\\\]`\n * \n * Inverse transformation can be also done easily, since\n * \n * `\\\\[\\\\sin ( \\\\theta ) \\\\vecthreethree{0}{-r_z}{r_y}{r_z}{0}{-r_x}{-r_y}{r_x}{0} = \\\\frac{R -\n * R^T}{2}\\\\]`\n * \n * A rotation vector is a convenient and most compact representation of a rotation matrix (since any\n * rotation matrix has just 3 degrees of freedom). The representation is used in the global 3D geometry\n * optimization procedures like calibrateCamera, stereoCalibrate, or solvePnP .\n * \n * @param src Input rotation vector (3x1 or 1x3) or rotation matrix (3x3).\n * \n * @param dst Output rotation matrix (3x3) or rotation vector (3x1 or 1x3), respectively.\n * \n * @param jacobian Optional output Jacobian matrix, 3x9 or 9x3, which is a matrix of partial\n * derivatives of the output array components with respect to the input array components.\n */\nexport declare function Rodrigues(src: InputArray, dst: OutputArray, jacobian?: OutputArray): void\n\n/**\n * The function computes a RQ decomposition using the given rotations. This function is used in\n * decomposeProjectionMatrix to decompose the left 3x3 submatrix of a projection matrix into a camera\n * and a rotation matrix.\n * \n * It optionally returns three rotation matrices, one for each axis, and the three Euler angles in\n * degrees (as the return value) that could be used in OpenGL. Note, there is always more than one\n * sequence of rotations about the three principal axes that results in the same orientation of an\n * object, e.g. see Slabaugh . Returned tree rotation matrices and corresponding three Euler angles are\n * only one of the possible solutions.\n * \n * @param src 3x3 input matrix.\n * \n * @param mtxR Output 3x3 upper-triangular matrix.\n * \n * @param mtxQ Output 3x3 orthogonal matrix.\n * \n * @param Qx Optional output 3x3 rotation matrix around x-axis.\n * \n * @param Qy Optional output 3x3 rotation matrix around y-axis.\n * \n * @param Qz Optional output 3x3 rotation matrix around z-axis.\n */\nexport declare function RQDecomp3x3(src: InputArray, mtxR: OutputArray, mtxQ: OutputArray, Qx?: OutputArray, Qy?: OutputArray, Qz?: OutputArray): Vec3d\n\n/**\n * The function [cv::sampsonDistance](#d9/d0c/group__calib3d_1gacbba2ee98258ca81d352a31faa15a021})\n * calculates and returns the first order approximation of the geometric error as: `\\\\[ sd(\n * \\\\texttt{pt1} , \\\\texttt{pt2} )= \\\\frac{(\\\\texttt{pt2}^t \\\\cdot \\\\texttt{F} \\\\cdot \\\\texttt{pt1})^2}\n * {((\\\\texttt{F} \\\\cdot \\\\texttt{pt1})(0))^2 + ((\\\\texttt{F} \\\\cdot \\\\texttt{pt1})(1))^2 +\n * ((\\\\texttt{F}^t \\\\cdot \\\\texttt{pt2})(0))^2 + ((\\\\texttt{F}^t \\\\cdot \\\\texttt{pt2})(1))^2} \\\\]` The\n * fundamental matrix may be calculated using the\n * [cv::findFundamentalMat](#d9/d0c/group__calib3d_1gae420abc34eaa03d0c6a67359609d8429}) function. See\n * HartleyZ00 11.4.3 for details. \n * \n * The computed Sampson distance.\n * \n * @param pt1 first homogeneous 2d point\n * \n * @param pt2 second homogeneous 2d point\n * \n * @param F fundamental matrix\n */\nexport declare function sampsonDistance(pt1: InputArray, pt2: InputArray, F: InputArray): double\n\n/**\n * The function estimates the object pose given 3 object points, their corresponding image projections,\n * as well as the camera matrix and the distortion coefficients.\n * \n * The solutions are sorted by reprojection errors (lowest to highest).\n * \n * @param objectPoints Array of object points in the object coordinate space, 3x3 1-channel or 1x3/3x1\n * 3-channel. vector<Point3f> can be also passed here.\n * \n * @param imagePoints Array of corresponding image points, 3x2 1-channel or 1x3/3x1 2-channel.\n * vector<Point2f> can be also passed here.\n * \n * @param cameraMatrix Input camera matrix $A = \\vecthreethree{fx}{0}{cx}{0}{fy}{cy}{0}{0}{1}$ .\n * \n * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6\n * [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is\n * NULL/empty, the zero distortion coefficients are assumed.\n * \n * @param rvecs Output rotation vectors (see Rodrigues ) that, together with tvecs, brings points from\n * the model coordinate system to the camera coordinate system. A P3P problem has up to 4 solutions.\n * \n * @param tvecs Output translation vectors.\n * \n * @param flags Method for solving a P3P problem:\n * SOLVEPNP_P3P Method is based on the paper of X.S. Gao, X.-R. Hou, J. Tang, H.-F. Chang \"Complete\n * Solution Classification for the Perspective-Three-Point Problem\" (gao2003complete).SOLVEPNP_AP3P\n * Method is based on the paper of T. Ke and S. Roumeliotis. \"An Efficient Algebraic Solution to the\n * Perspective-Three-Point Problem\" (Ke17).\n */\nexport declare function solveP3P(objectPoints: InputArray, imagePoints: InputArray, cameraMatrix: InputArray, distCoeffs: InputArray, rvecs: OutputArrayOfArrays, tvecs: OutputArrayOfArrays, flags: int): int\n\n/**\n * P3P methods\n * ([SOLVEPNP_P3P](#d9/d0c/group__calib3d_1gga357634492a94efe8858d0ce1509da869af33a85ca698777ff9bd1de916bf5959a}),\n * [SOLVEPNP_AP3P](#d9/d0c/group__calib3d_1gga357634492a94efe8858d0ce1509da869a8d48dece2da6492d91fa2de0a04679f9})):\n * need 4 input points to return a unique solution.\n * [SOLVEPNP_IPPE](#d9/d0c/group__calib3d_1gga357634492a94efe8858d0ce1509da869acbd7d9f9513a22a59412661a9d31ca3d})\n * Input points must be >= 4 and object points must be coplanar.\n * [SOLVEPNP_IPPE_SQUARE](#d9/d0c/group__calib3d_1gga357634492a94efe8858d0ce1509da869ac5d28b2805d3ac32fd477eee4479406f})\n * Special case suitable for marker pose estimation. Number of input points must be 4. Object points\n * must be defined in the following order:\n * \n * point 0: [-squareLength / 2, squareLength / 2, 0]\n * point 1: [ squareLength / 2, squareLength / 2, 0]\n * point 2: [ squareLength / 2, -squareLength / 2, 0]\n * point 3: [-squareLength / 2, -squareLength / 2, 0]\n * \n * for all the other flags, number of input points must be >= 4 and object points can be in any\n * configuration.\n * \n * The function estimates the object pose given a set of object points, their corresponding image\n * projections, as well as the camera matrix and the distortion coefficients, see the figure below\n * (more precisely, the X-axis of the camera frame is pointing to the right, the Y-axis downward and\n * the Z-axis forward).\n * \n * Points expressed in the world frame `$ \\\\bf{X}_w $` are projected into the image plane `$ \\\\left[ u,\n * v \\\\right] $` using the perspective projection model `$ \\\\Pi $` and the camera intrinsic parameters\n * matrix `$ \\\\bf{A} $`:\n * \n * `\\\\[ \\\\begin{align*} \\\\begin{bmatrix} u \\\\\\\\ v \\\\\\\\ 1 \\\\end{bmatrix} &= \\\\bf{A} \\\\hspace{0.1em} \\\\Pi\n * \\\\hspace{0.2em} ^{c}\\\\bf{M}_w \\\\begin{bmatrix} X_{w} \\\\\\\\ Y_{w} \\\\\\\\ Z_{w} \\\\\\\\ 1 \\\\end{bmatrix}\n * \\\\\\\\ \\\\begin{bmatrix} u \\\\\\\\ v \\\\\\\\ 1 \\\\end{bmatrix} &= \\\\begin{bmatrix} f_x & 0 & c_x \\\\\\\\ 0 & f_y\n * & c_y \\\\\\\\ 0 & 0 & 1 \\\\end{bmatrix} \\\\begin{bmatrix} 1 & 0 & 0 & 0 \\\\\\\\ 0 & 1 & 0 & 0 \\\\\\\\ 0 & 0 & 1\n * & 0 \\\\end{bmatrix} \\\\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\\\\\ r_{21} & r_{22} & r_{23} &\n * t_y \\\\\\\\ r_{31} & r_{32} & r_{33} & t_z \\\\\\\\ 0 & 0 & 0 & 1 \\\\end{bmatrix} \\\\begin{bmatrix} X_{w}\n * \\\\\\\\ Y_{w} \\\\\\\\ Z_{w} \\\\\\\\ 1 \\\\end{bmatrix} \\\\end{align*} \\\\]`\n * \n * The estimated pose is thus the rotation (`rvec`) and the translation (`tvec`) vectors that allow\n * transforming a 3D point expressed in the world frame into the camera frame:\n * \n * `\\\\[ \\\\begin{align*} \\\\begin{bmatrix} X_c \\\\\\\\ Y_c \\\\\\\\ Z_c \\\\\\\\ 1 \\\\end{bmatrix} &= \\\\hspace{0.2em}\n * ^{c}\\\\bf{M}_w \\\\begin{bmatrix} X_{w} \\\\\\\\ Y_{w} \\\\\\\\ Z_{w} \\\\\\\\ 1 \\\\end{bmatrix} \\\\\\\\\n * \\\\begin{bmatrix} X_c \\\\\\\\ Y_c \\\\\\\\ Z_c \\\\\\\\ 1 \\\\end{bmatrix} &= \\\\begin{bmatrix} r_{11} & r_{12} &\n * r_{13} & t_x \\\\\\\\ r_{21} & r_{22} & r_{23} & t_y \\\\\\\\ r_{31} & r_{32} & r_{33} & t_z \\\\\\\\ 0 & 0 & 0\n * & 1 \\\\end{bmatrix} \\\\begin{bmatrix} X_{w} \\\\\\\\ Y_{w} \\\\\\\\ Z_{w} \\\\\\\\ 1 \\\\end{bmatrix} \\\\end{align*}\n * \\\\]`\n * \n * An example of how to use solvePnP for planar augmented reality can be found at\n * opencv_source_code/samples/python/plane_ar.py\n * If you are using Python:\n * \n * Numpy array slices won't work as input because solvePnP requires contiguous arrays (enforced by the\n * assertion using [cv::Mat::checkVector()](#d3/d63/classcv_1_1Mat_1a167a8e0a3a3d86e84b70e33483af4466})\n * around line 55 of modules/calib3d/src/solvepnp.cpp version 2.4.9)\n * The P3P algorithm requires image points to be in an array of shape (N,1,2) due to its calling of\n * [cv::undistortPoints](#d9/d0c/group__calib3d_1ga55c716492470bfe86b0ee9bf3a1f0f7e}) (around line 75\n * of modules/calib3d/src/solvepnp.cpp version 2.4.9) which requires 2-channel information.\n * Thus, given some data D = np.array(...) where D.shape = (N,M), in order to use a subset of it as,\n * e.g., imagePoints, one must effectively copy it into a new array: imagePoints =\n * np.ascontiguousarray(D[:,:2]).reshape((N,1,2))\n * \n * The methods **SOLVEPNP_DLS** and **SOLVEPNP_UPNP** cannot be used as the current implementations are\n * unstable and sometimes give completely wrong results. If you pass one of these two flags,\n * **SOLVEPNP_EPNP** method will be used instead.\n * The minimum number of points is 4 in the general case. In the case of **SOLVEPNP_P3P** and\n * **SOLVEPNP_AP3P** methods, it is required to use exactly 4 points (the first 3 points are used to\n * estimate all the solutions of the P3P problem, the last one is used to retain the best solution that\n * minimizes the reprojection error).\n * With **SOLVEPNP_ITERATIVE** method and `useExtrinsicGuess=true`, the minimum number of points is 3\n * (3 points are sufficient to compute a pose but there are up to 4 solutions). The initial solution\n * should be close to the global solution to converge.\n * With **SOLVEPNP_IPPE** input points must be >= 4 and object points must be coplanar.\n * With **SOLVEPNP_IPPE_SQUARE** this is a special case suitable for marker pose estimation. Number of\n * input points must be 4. Object points must be defined in the following order:\n * \n * point 0: [-squareLength / 2, squareLength / 2, 0]\n * point 1: [ squareLength / 2, squareLength / 2, 0]\n * point 2: [ squareLength / 2, -squareLength / 2, 0]\n * point 3: [-squareLength / 2, -squareLength / 2, 0]\n * \n * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or 1xN/Nx1\n * 3-channel, where N is the number of points. vector<Point3f> can be also passed here.\n * \n * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel, where N\n * is the number of points. vector<Point2f> can be also passed here.\n * \n * @param cameraMatrix Input camera matrix $A = \\vecthreethree{fx}{0}{cx}{0}{fy}{cy}{0}{0}{1}$ .\n * \n * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6\n * [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is\n * NULL/empty, the zero distortion coefficients are assumed.\n * \n * @param rvec Output rotation vector (see Rodrigues ) that, together with tvec, brings points from the\n * model coordinate system to the camera coordinate system.\n * \n * @param tvec Output translation vector.\n * \n * @param useExtrinsicGuess Parameter used for SOLVEPNP_ITERATIVE. If true (1), the function uses the\n * provided rvec and tvec values as initial approximations of the rotation and translation vectors,\n * respectively, and further optimizes them.\n * \n * @param flags Method for solving a PnP problem:\n * SOLVEPNP_ITERATIVE Iterative method is based on a Levenberg-Marquardt optimization. In this case the\n * function finds such a pose that minimizes reprojection error, that is the sum of squared distances\n * between the observed projections imagePoints and the projected (using projectPoints ) objectPoints\n * .SOLVEPNP_P3P Method is based on the paper of X.S. Gao, X.-R. Hou, J. Tang, H.-F. Chang \"Complete\n * Solution Classification for the Perspective-Three-Point Problem\" (gao2003complete). In this case the\n * function requires exactly four object and image points.SOLVEPNP_AP3P Method is based on the paper of\n * T. Ke, S. Roumeliotis \"An Efficient Algebraic Solution to the Perspective-Three-Point Problem\"\n * (Ke17). In this case the function requires exactly four object and image points.SOLVEPNP_EPNP Method\n * has been introduced by F. Moreno-Noguer, V. Lepetit and P. Fua in the paper \"EPnP: Efficient\n * Perspective-n-Point Camera Pose Estimation\" (lepetit2009epnp).SOLVEPNP_DLS Method is based on the\n * paper of J. Hesch and S. Roumeliotis. \"A Direct Least-Squares (DLS) Method for PnP\"\n * (hesch2011direct).SOLVEPNP_UPNP Method is based on the paper of A. Penate-Sanchez, J. Andrade-Cetto,\n * F. Moreno-Noguer. \"Exhaustive Linearization for Robust Camera Pose and Focal Length\n * Estimation\" (penate2013exhaustive). In this case the function also estimates the parameters $f_x$\n * and $f_y$ assuming that both have the same value. Then the cameraMatrix is updated with the\n * estimated focal length.SOLVEPNP_IPPE Method is based on the paper of T. Collins and A. Bartoli.\n * \"Infinitesimal Plane-Based Pose Estimation\" (Collins14). This method requires coplanar object\n * points.SOLVEPNP_IPPE_SQUARE Method is based on the paper of Toby Collins and Adrien Bartoli.\n * \"Infinitesimal Plane-Based Pose Estimation\" (Collins14). This method is suitable for marker pose\n * estimation. It requires 4 coplanar object points defined in the following order:\n * point 0: [-squareLength / 2, squareLength / 2, 0]point 1: [ squareLength / 2, squareLength / 2,\n * 0]point 2: [ squareLength / 2, -squareLength / 2, 0]point 3: [-squareLength / 2, -squareLength / 2,\n * 0]\n */\nexport declare function solvePnP(objectPoints: InputArray, imagePoints: InputArray, cameraMatrix: InputArray, distCoeffs: InputArray, rvec: OutputArray, tvec: OutputArray, useExtrinsicGuess?: bool, flags?: int): bool\n\n/**\n * P3P methods\n * ([SOLVEPNP_P3P](#d9/d0c/group__calib3d_1gga357634492a94efe8858d0ce1509da869af33a85ca698777ff9bd1de916bf5959a}),\n * [SOLVEPNP_AP3P](#d9/d0c/group__calib3d_1gga357634492a94efe8858d0ce1509da869a8d48dece2da6492d91fa2de0a04679f9})):\n * 3 or 4 input points. Number of returned solutions can be between 0 and 4 with 3 input points.\n * [SOLVEPNP_IPPE](#d9/d0c/group__calib3d_1gga357634492a94efe8858d0ce1509da869acbd7d9f9513a22a59412661a9d31ca3d})\n * Input points must be >= 4 and object points must be coplanar. Returns 2 solutions.\n * [SOLVEPNP_IPPE_SQUARE](#d9/d0c/group__calib3d_1gga357634492a94efe8858d0ce1509da869ac5d28b2805d3ac32fd477eee4479406f})\n * Special case suitable for marker pose estimation. Number of input points must be 4 and 2 solutions\n * are returned. Object points must be defined in the following order:\n * \n * point 0: [-squareLength / 2, squareLength / 2, 0]\n * point 1: [ squareLength / 2, squareLength / 2, 0]\n * point 2: [ squareLength / 2, -squareLength / 2, 0]\n * point 3: [-squareLength / 2, -squareLength / 2, 0]\n * \n * for all the other flags, number of input points must be >= 4 and object points can be in any\n * configuration. Only 1 solution is returned.\n * \n * The function estimates the object pose given a set of object points, their corresponding image\n * projections, as well as the camera matrix and the distortion coefficients, see the figure below\n * (more precisely, the X-axis of the camera frame is pointing to the right, the Y-axis downward and\n * the Z-axis forward).\n * \n * Points expressed in the world frame `$ \\\\bf{X}_w $` are projected into the image plane `$ \\\\left[ u,\n * v \\\\right] $` using the perspective projection model `$ \\\\Pi $` and the camera intrinsic parameters\n * matrix `$ \\\\bf{A} $`:\n * \n * `\\\\[ \\\\begin{align*} \\\\begin{bmatrix} u \\\\\\\\ v \\\\\\\\ 1 \\\\end{bmatrix} &= \\\\bf{A} \\\\hspace{0.1em} \\\\Pi\n * \\\\hspace{0.2em} ^{c}\\\\bf{M}_w \\\\begin{bmatrix} X_{w} \\\\\\\\ Y_{w} \\\\\\\\ Z_{w} \\\\\\\\ 1 \\\\end{bmatrix}\n * \\\\\\\\ \\\\begin{bmatrix} u \\\\\\\\ v \\\\\\\\ 1 \\\\end{bmatrix} &= \\\\begin{bmatrix} f_x & 0 & c_x \\\\\\\\ 0 & f_y\n * & c_y \\\\\\\\ 0 & 0 & 1 \\\\end{bmatrix} \\\\begin{bmatrix} 1 & 0 & 0 & 0 \\\\\\\\ 0 & 1 & 0 & 0 \\\\\\\\ 0 & 0 & 1\n * & 0 \\\\end{bmatrix} \\\\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\\\\\ r_{21} & r_{22} & r_{23} &\n * t_y \\\\\\\\ r_{31} & r_{32} & r_{33} & t_z \\\\\\\\ 0 & 0 & 0 & 1 \\\\end{bmatrix} \\\\begin{bmatrix} X_{w}\n * \\\\\\\\ Y_{w} \\\\\\\\ Z_{w} \\\\\\\\ 1 \\\\end{bmatrix} \\\\end{align*} \\\\]`\n * \n * The estimated pose is thus the rotation (`rvec`) and the translation (`tvec`) vectors that allow\n * transforming a 3D point expressed in the world frame into the camera frame:\n * \n * `\\\\[ \\\\begin{align*} \\\\begin{bmatrix} X_c \\\\\\\\ Y_c \\\\\\\\ Z_c \\\\\\\\ 1 \\\\end{bmatrix} &= \\\\hspace{0.2em}\n * ^{c}\\\\bf{M}_w \\\\begin{bmatrix} X_{w} \\\\\\\\ Y_{w} \\\\\\\\ Z_{w} \\\\\\\\ 1 \\\\end{bmatrix} \\\\\\\\\n * \\\\begin{bmatrix} X_c \\\\\\\\ Y_c \\\\\\\\ Z_c \\\\\\\\ 1 \\\\end{bmatrix} &= \\\\begin{bmatrix} r_{11} & r_{12} &\n * r_{13} & t_x \\\\\\\\ r_{21} & r_{22} & r_{23} & t_y \\\\\\\\ r_{31} & r_{32} & r_{33} & t_z \\\\\\\\ 0 & 0 & 0\n * & 1 \\\\end{bmatrix} \\\\begin{bmatrix} X_{w} \\\\\\\\ Y_{w} \\\\\\\\ Z_{w} \\\\\\\\ 1 \\\\end{bmatrix} \\\\end{align*}\n * \\\\]`\n * \n * An example of how to use solvePnP for planar augmented reality can be found at\n * opencv_source_code/samples/python/plane_ar.py\n * If you are using Python:\n * \n * Numpy array slices won't work as input because solvePnP requires contiguous arrays (enforced by the\n * assertion using [cv::Mat::checkVector()](#d3/d63/classcv_1_1Mat_1a167a8e0a3a3d86e84b70e33483af4466})\n * around line 55 of modules/calib3d/src/solvepnp.cpp version 2.4.9)\n * The P3P algorithm requires image points to be in an array of shape (N,1,2) due to its calling of\n * [cv::undistortPoints](#d9/d0c/group__calib3d_1ga55c716492470bfe86b0ee9bf3a1f0f7e}) (around line 75\n * of modules/calib3d/src/solvepnp.cpp version 2.4.9) which requires 2-channel information.\n * Thus, given some data D = np.array(...) where D.shape = (N,M), in order to use a subset of it as,\n * e.g., imagePoints, one must effectively copy it into a new array: imagePoints =\n * np.ascontiguousarray(D[:,:2]).reshape((N,1,2))\n * \n * The methods **SOLVEPNP_DLS** and **SOLVEPNP_UPNP** cannot be used as the current implementations are\n * unstable and sometimes give completely wrong results. If you pass one of these two flags,\n * **SOLVEPNP_EPNP** method will be used instead.\n * The minimum number of points is 4 in the general case. In the case of **SOLVEPNP_P3P** and\n * **SOLVEPNP_AP3P** methods, it is required to use exactly 4 points (the first 3 points are used to\n * estimate all the solutions of the P3P problem, the last one is used to retain the best solution that\n * minimizes the reprojection error).\n * With **SOLVEPNP_ITERATIVE** method and `useExtrinsicGuess=true`, the minimum number of points is 3\n * (3 points are sufficient to compute a pose but there are up to 4 solutions). The initial solution\n * should be close to the global solution to converge.\n * With **SOLVEPNP_IPPE** input points must be >= 4 and object points must be coplanar.\n * With **SOLVEPNP_IPPE_SQUARE** this is a special case suitable for marker pose estimation. Number of\n * input points must be 4. Object points must be defined in the following order:\n * \n * point 0: [-squareLength / 2, squareLength / 2, 0]\n * point 1: [ squareLength / 2, squareLength / 2, 0]\n * point 2: [ squareLength / 2, -squareLength / 2, 0]\n * point 3: [-squareLength / 2, -squareLength / 2, 0]\n * \n * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or 1xN/Nx1\n * 3-channel, where N is the number of points. vector<Point3f> can be also passed here.\n * \n * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel, where N\n * is the number of points. vector<Point2f> can be also passed here.\n * \n * @param cameraMatrix Input camera matrix $A = \\vecthreethree{fx}{0}{cx}{0}{fy}{cy}{0}{0}{1}$ .\n * \n * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6\n * [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is\n * NULL/empty, the zero distortion coefficients are assumed.\n * \n * @param rvecs Vector of output rotation vectors (see Rodrigues ) that, together with tvecs, brings\n * points from the model coordinate system to the camera coordinate system.\n * \n * @param tvecs Vector of output translation vectors.\n * \n * @param useExtrinsicGuess Parameter used for SOLVEPNP_ITERATIVE. If true (1), the function uses the\n * provided rvec and tvec values as initial approximations of the rotation and translation vectors,\n * respectively, and further optimizes them.\n * \n * @param flags Method for solving a PnP problem:\n * SOLVEPNP_ITERATIVE Iterative method is based on a Levenberg-Marquardt optimization. In this case the\n * function finds such a pose that minimizes reprojection error, that is the sum of squared distances\n * between the observed projections imagePoints and the projected (using projectPoints ) objectPoints\n * .SOLVEPNP_P3P Method is based on the paper of X.S. Gao, X.-R. Hou, J. Tang, H.-F. Chang \"Complete\n * Solution Classification for the Perspective-Three-Point Problem\" (gao2003complete). In this case the\n * function requires exactly four object and image points.SOLVEPNP_AP3P Method is based on the paper of\n * T. Ke, S. Roumeliotis \"An Efficient Algebraic Solution to the Perspective-Three-Point Problem\"\n * (Ke17). In this case the function requires exactly four object and image points.SOLVEPNP_EPNP Method\n * has been introduced by F.Moreno-Noguer, V.Lepetit and P.Fua in the paper \"EPnP: Efficient\n * Perspective-n-Point Camera Pose Estimation\" (lepetit2009epnp).SOLVEPNP_DLS Method is based on the\n * paper of Joel A. Hesch and Stergios I. Roumeliotis. \"A Direct Least-Squares (DLS) Method for PnP\"\n * (hesch2011direct).SOLVEPNP_UPNP Method is based on the paper of A.Penate-Sanchez, J.Andrade-Cetto,\n * F.Moreno-Noguer. \"Exhaustive Linearization for Robust Camera Pose and Focal Length\n * Estimation\" (penate2013exhaustive). In this case the function also estimates the parameters $f_x$\n * and $f_y$ assuming that both have the same value. Then the cameraMatrix is updated with the\n * estimated focal length.SOLVEPNP_IPPE Method is based on the paper of T. Collins and A. Bartoli.\n * \"Infinitesimal Plane-Based Pose Estimation\" (Collins14). This method requires coplanar object\n * points.SOLVEPNP_IPPE_SQUARE Method is based on the paper of Toby Collins and Adrien Bartoli.\n * \"Infinitesimal Plane-Based Pose Estimation\" (Collins14). This method is suitable for marker pose\n * estimation. It requires 4 coplanar object points defined in the following order:\n * point 0: [-squareLength / 2, squareLength / 2, 0]point 1: [ squareLength / 2, squareLength / 2,\n * 0]point 2: [ squareLength / 2, -squareLength / 2, 0]point 3: [-squareLength / 2, -squareLength / 2,\n * 0]\n * \n * @param rvec Rotation vector used to initialize an iterative PnP refinement algorithm, when flag is\n * SOLVEPNP_ITERATIVE and useExtrinsicGuess is set to true.\n * \n * @param tvec Translation vector used to initialize an iterative PnP refinement algorithm, when flag\n * is SOLVEPNP_ITERATIVE and useExtrinsicGuess is set to true.\n * \n * @param reprojectionError Optional vector of reprojection error, that is the RMS error ( $\n * \\text{RMSE} = \\sqrt{\\frac{\\sum_{i}^{N} \\left ( \\hat{y_i} - y_i \\right )^2}{N}} $) between the input\n * image points and the 3D object points projected with the estimated pose.\n */\nexport declare function solvePnPGeneric(objectPoints: InputArray, imagePoints: InputArray, cameraMatrix: InputArray, distCoeffs: InputArray, rvecs: OutputArrayOfArrays, tvecs: OutputArrayOfArrays, useExtrinsicGuess?: bool, flags?: SolvePnPMethod, rvec?: InputArray, tvec?: InputArray, reprojectionError?: OutputArray): int\n\n/**\n * The function estimates an object pose given a set of object points, their corresponding image\n * projections, as well as the camera matrix and the distortion coefficients. This function finds such\n * a pose that minimizes reprojection error, that is, the sum of squared distances between the observed\n * projections imagePoints and the projected (using\n * [projectPoints](#d9/d0c/group__calib3d_1ga1019495a2c8d1743ed5cc23fa0daff8c}) ) objectPoints. The use\n * of RANSAC makes the function resistant to outliers.\n * \n * An example of how to use solvePNPRansac for object detection can be found at\n * opencv_source_code/samples/cpp/tutorial_code/calib3d/real_time_pose_estimation/\n * The default method used to estimate the camera pose for the Minimal Sample Sets step is\n * [SOLVEPNP_EPNP](#d9/d0c/group__calib3d_1gga357634492a94efe8858d0ce1509da869ae25763f5155defc67ef0f68b74d6c074}).\n * Exceptions are:\n * \n * if you choose\n * [SOLVEPNP_P3P](#d9/d0c/group__calib3d_1gga357634492a94efe8858d0ce1509da869af33a85ca698777ff9bd1de916bf5959a})\n * or\n * [SOLVEPNP_AP3P](#d9/d0c/group__calib3d_1gga357634492a94efe8858d0ce1509da869a8d48dece2da6492d91fa2de0a04679f9}),\n * these methods will be used.\n * if the number of input points is equal to 4,\n * [SOLVEPNP_P3P](#d9/d0c/group__calib3d_1gga357634492a94efe8858d0ce1509da869af33a85ca698777ff9bd1de916bf5959a})\n * is used.\n * \n * The method used to estimate the camera pose using all the inliers is defined by the flags parameters\n * unless it is equal to\n * [SOLVEPNP_P3P](#d9/d0c/group__calib3d_1gga357634492a94efe8858d0ce1509da869af33a85ca698777ff9bd1de916bf5959a})\n * or\n * [SOLVEPNP_AP3P](#d9/d0c/group__calib3d_1gga357634492a94efe8858d0ce1509da869a8d48dece2da6492d91fa2de0a04679f9}).\n * In this case, the method\n * [SOLVEPNP_EPNP](#d9/d0c/group__calib3d_1gga357634492a94efe8858d0ce1509da869ae25763f5155defc67ef0f68b74d6c074})\n * will be used instead.\n * \n * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or 1xN/Nx1\n * 3-channel, where N is the number of points. vector<Point3f> can be also passed here.\n * \n * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel, where N\n * is the number of points. vector<Point2f> can be also passed here.\n * \n * @param cameraMatrix Input camera matrix $A = \\vecthreethree{fx}{0}{cx}{0}{fy}{cy}{0}{0}{1}$ .\n * \n * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6\n * [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is\n * NULL/empty, the zero distortion coefficients are assumed.\n * \n * @param rvec Output rotation vector (see Rodrigues ) that, together with tvec, brings points from the\n * model coordinate system to the camera coordinate system.\n * \n * @param tvec Output translation vector.\n * \n * @param useExtrinsicGuess Parameter used for SOLVEPNP_ITERATIVE. If true (1), the function uses the\n * provided rvec and tvec values as initial approximations of the rotation and translation vectors,\n * respectively, and further optimizes them.\n * \n * @param iterationsCount Number of iterations.\n * \n * @param reprojectionError Inlier threshold value used by the RANSAC procedure. The parameter value is\n * the maximum allowed distance between the observed and computed point projections to consider it an\n * inlier.\n * \n * @param confidence The probability that the algorithm produces a useful result.\n * \n * @param inliers Output vector that contains indices of inliers in objectPoints and imagePoints .\n * \n * @param flags Method for solving a PnP problem (see solvePnP ).\n */\nexport declare function solvePnPRansac(objectPoints: InputArray, imagePoints: InputArray, cameraMatrix: InputArray, distCoeffs: InputArray, rvec: OutputArray, tvec: OutputArray, useExtrinsicGuess?: bool, iterationsCount?: int, reprojectionError?: float, confidence?: double, inliers?: OutputArray, flags?: int): bool\n\n/**\n * The function refines the object pose given at least 3 object points, their corresponding image\n * projections, an initial solution for the rotation and translation vector, as well as the camera\n * matrix and the distortion coefficients. The function minimizes the projection error with respect to\n * the rotation and the translation vectors, according to a Levenberg-Marquardt iterative minimization\n * Madsen04 Eade13 process.\n * \n * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or 1xN/Nx1\n * 3-channel, where N is the number of points. vector<Point3f> can also be passed here.\n * \n * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel, where N\n * is the number of points. vector<Point2f> can also be passed here.\n * \n * @param cameraMatrix Input camera matrix $A = \\vecthreethree{fx}{0}{cx}{0}{fy}{cy}{0}{0}{1}$ .\n * \n * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6\n * [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is\n * NULL/empty, the zero distortion coefficients are assumed.\n * \n * @param rvec Input/Output rotation vector (see Rodrigues ) that, together with tvec, brings points\n * from the model coordinate system to the camera coordinate system. Input values are used as an\n * initial solution.\n * \n * @param tvec Input/Output translation vector. Input values are used as an initial solution.\n * \n * @param criteria Criteria when to stop the Levenberg-Marquard iterative algorithm.\n */\nexport declare function solvePnPRefineLM(objectPoints: InputArray, imagePoints: InputArray, cameraMatrix: InputArray, distCoeffs: InputArray, rvec: InputOutputArray, tvec: InputOutputArray, criteria?: TermCriteria): void\n\n/**\n * The function refines the object pose given at least 3 object points, their corresponding image\n * projections, an initial solution for the rotation and translation vector, as well as the camera\n * matrix and the distortion coefficients. The function minimizes the projection error with respect to\n * the rotation and the translation vectors, using a virtual visual servoing (VVS) Chaumette06\n * Marchand16 scheme.\n * \n * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or 1xN/Nx1\n * 3-channel, where N is the number of points. vector<Point3f> can also be passed here.\n * \n * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel, where N\n * is the number of points. vector<Point2f> can also be passed here.\n * \n * @param cameraMatrix Input camera matrix $A = \\vecthreethree{fx}{0}{cx}{0}{fy}{cy}{0}{0}{1}$ .\n * \n * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6\n * [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is\n * NULL/empty, the zero distortion coefficients are assumed.\n * \n * @param rvec Input/Output rotation vector (see Rodrigues ) that, together with tvec, brings points\n * from the model coordinate system to the camera coordinate system. Input values are used as an\n * initial solution.\n * \n * @param tvec Input/Output translation vector. Input values are used as an initial solution.\n * \n * @param criteria Criteria when to stop the Levenberg-Marquard iterative algorithm.\n * \n * @param VVSlambda Gain for the virtual visual servoing control law, equivalent to the $\\alpha$ gain\n * in the Damped Gauss-Newton formulation.\n */\nexport declare function solvePnPRefineVVS(objectPoints: InputArray, imagePoints: InputArray, cameraMatrix: InputArray, distCoeffs: InputArray, rvec: InputOutputArray, tvec: InputOutputArray, criteria?: TermCriteria, VVSlambda?: double): void\n\n/**\n * The function estimates transformation between two cameras making a stereo pair. If you have a stereo\n * camera where the relative position and orientation of two cameras is fixed, and if you computed\n * poses of an object relative to the first camera and to the second camera, (R1, T1) and (R2, T2),\n * respectively (this can be done with solvePnP ), then those poses definitely relate to each other.\n * This means that, given ( `$R_1$`, `$T_1$` ), it should be possible to compute ( `$R_2$`, `$T_2$` ).\n * You only need to know the position and orientation of the second camera relative to the first\n * camera. This is what the described function does. It computes ( `$R$`, `$T$` ) so that:\n * \n * `\\\\[R_2=R*R_1\\\\]` `\\\\[T_2=R*T_1 + T,\\\\]`\n * \n * Optionally, it computes the essential matrix E:\n * \n * `\\\\[E= \\\\vecthreethree{0}{-T_2}{T_1}{T_2}{0}{-T_0}{-T_1}{T_0}{0} *R\\\\]`\n * \n * where `$T_i$` are components of the translation vector `$T$` : `$T=[T_0, T_1, T_2]^T$` . And the\n * function can also compute the fundamental matrix F:\n * \n * `\\\\[F = cameraMatrix2^{-T} E cameraMatrix1^{-1}\\\\]`\n * \n * Besides the stereo-related information, the function can also perform a full calibration of each of\n * two cameras. However, due to the high dimensionality of the parameter space and noise in the input\n * data, the function can diverge from the correct solution. If the intrinsic parameters can be\n * estimated with high accuracy for each of the cameras individually (for example, using\n * calibrateCamera ), you are recommended to do so and then pass CALIB_FIX_INTRINSIC flag to the\n * function along with the computed intrinsic parameters. Otherwise, if all the parameters are\n * estimated at once, it makes sense to restrict some parameters, for example, pass\n * CALIB_SAME_FOCAL_LENGTH and CALIB_ZERO_TANGENT_DIST flags, which is usually a reasonable assumption.\n * \n * Similarly to calibrateCamera , the function minimizes the total re-projection error for all the\n * points in all the available views from both cameras. The function returns the final value of the\n * re-projection error.\n * \n * @param objectPoints Vector of vectors of the calibration pattern points.\n * \n * @param imagePoints1 Vector of vectors of the projections of the calibration pattern points, observed\n * by the first camera.\n * \n * @param imagePoints2 Vector of vectors of the projections of the calibration pattern points, observed\n * by the second camera.\n * \n * @param cameraMatrix1 Input/output first camera matrix:\n * $\\vecthreethree{f_x^{(j)}}{0}{c_x^{(j)}}{0}{f_y^{(j)}}{c_y^{(j)}}{0}{0}{1}$ , $j = 0,\\, 1$ . If any\n * of CALIB_USE_INTRINSIC_GUESS , CALIB_FIX_ASPECT_RATIO , CALIB_FIX_INTRINSIC , or\n * CALIB_FIX_FOCAL_LENGTH are specified, some or all of the matrix components must be initialized. See\n * the flags description for details.\n * \n * @param distCoeffs1 Input/output vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4,\n * k_5, k_6 [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. The output\n * vector length depends on the flags.\n * \n * @param cameraMatrix2 Input/output second camera matrix. The parameter is similar to cameraMatrix1\n * \n * @param distCoeffs2 Input/output lens distortion coefficients for the second camera. The parameter is\n * similar to distCoeffs1 .\n * \n * @param imageSize Size of the image used only to initialize intrinsic camera matrix.\n * \n * @param R Output rotation matrix between the 1st and the 2nd camera coordinate systems.\n * \n * @param T Output translation vector between the coordinate systems of the cameras.\n * \n * @param E Output essential matrix.\n * \n * @param F Output fundamental matrix.\n * \n * @param perViewErrors Output vector of the RMS re-projection error estimated for each pattern view.\n * \n * @param flags Different flags that may be zero or a combination of the following values:\n * CALIB_FIX_INTRINSIC Fix cameraMatrix? and distCoeffs? so that only R, T, E , and F matrices are\n * estimated.CALIB_USE_INTRINSIC_GUESS Optimize some or all of the intrinsic parameters according to\n * the specified flags. Initial values are provided by the user.CALIB_USE_EXTRINSIC_GUESS R, T contain\n * valid initial values that are optimized further. Otherwise R, T are initialized to the median value\n * of the pattern views (each dimension separately).CALIB_FIX_PRINCIPAL_POINT Fix the principal points\n * during the optimization.CALIB_FIX_FOCAL_LENGTH Fix $f^{(j)}_x$ and $f^{(j)}_y$\n * .CALIB_FIX_ASPECT_RATIO Optimize $f^{(j)}_y$ . Fix the ratio $f^{(j)}_x/f^{(j)}_y$\n * \n * CALIB_SAME_FOCAL_LENGTH Enforce $f^{(0)}_x=f^{(1)}_x$ and $f^{(0)}_y=f^{(1)}_y$\n * .CALIB_ZERO_TANGENT_DIST Set tangential distortion coefficients for each camera to zeros and fix\n * there.CALIB_FIX_K1,...,CALIB_FIX_K6 Do not change the corresponding radial distortion coefficient\n * during the optimization. If CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the supplied\n * distCoeffs matrix is used. Otherwise, it is set to 0.CALIB_RATIONAL_MODEL Enable coefficients k4,\n * k5, and k6. To provide the backward compatibility, this extra flag should be explicitly specified to\n * make the calibration function use the rational model and return 8 coefficients. If the flag is not\n * set, the function computes and returns only 5 distortion coefficients.CALIB_THIN_PRISM_MODEL\n * Coefficients s1, s2, s3 and s4 are enabled. To provide the backward compatibility, this extra flag\n * should be explicitly specified to make the calibration function use the thin prism model and return\n * 12 coefficients. If the flag is not set, the function computes and returns only 5 distortion\n * coefficients.CALIB_FIX_S1_S2_S3_S4 The thin prism distortion coefficients are not changed during the\n * optimization. If CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the supplied distCoeffs\n * matrix is used. Otherwise, it is set to 0.CALIB_TILTED_MODEL Coefficients tauX and tauY are enabled.\n * To provide the backward compatibility, this extra flag should be explicitly specified to make the\n * calibration function use the tilted sensor model and return 14 coefficients. If the flag is not set,\n * the function computes and returns only 5 distortion coefficients.CALIB_FIX_TAUX_TAUY The\n * coefficients of the tilted sensor model are not changed during the optimization. If\n * CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the supplied distCoeffs matrix is used.\n * Otherwise, it is set to 0.\n * \n * @param criteria Termination criteria for the iterative optimization algorithm.\n */\nexport declare function stereoCalibrate(objectPoints: InputArrayOfArrays, imagePoints1: InputArrayOfArrays, imagePoints2: InputArrayOfArrays, cameraMatrix1: InputOutputArray, distCoeffs1: InputOutputArray, cameraMatrix2: InputOutputArray, distCoeffs2: InputOutputArray, imageSize: Size, R: InputOutputArray, T: InputOutputArray, E: OutputArray, F: OutputArray, perViewErrors: OutputArray, flags?: int, criteria?: TermCriteria): double\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function stereoCalibrate(objectPoints: InputArrayOfArrays, imagePoints1: InputArrayOfArrays, imagePoints2: InputArrayOfArrays, cameraMatrix1: InputOutputArray, distCoeffs1: InputOutputArray, cameraMatrix2: InputOutputArray, distCoeffs2: InputOutputArray, imageSize: Size, R: OutputArray, T: OutputArray, E: OutputArray, F: OutputArray, flags?: int, criteria?: TermCriteria): double\n\n/**\n * The function computes the rotation matrices for each camera that (virtually) make both camera image\n * planes the same plane. Consequently, this makes all the epipolar lines parallel and thus simplifies\n * the dense stereo correspondence problem. The function takes the matrices computed by stereoCalibrate\n * as input. As output, it provides two rotation matrices and also two projection matrices in the new\n * coordinates. The function distinguishes the following two cases:\n * \n * **Horizontal stereo**: the first and the second camera views are shifted relative to each other\n * mainly along the x axis (with possible small vertical shift). In the rectified images, the\n * corresponding epipolar lines in the left and right cameras are horizontal and have the same\n * y-coordinate. P1 and P2 look like:`\\\\[\\\\texttt{P1} = \\\\begin{bmatrix} f & 0 & cx_1 & 0 \\\\\\\\ 0 & f &\n * cy & 0 \\\\\\\\ 0 & 0 & 1 & 0 \\\\end{bmatrix}\\\\]``\\\\[\\\\texttt{P2} = \\\\begin{bmatrix} f & 0 & cx_2 & T_x*f\n * \\\\\\\\ 0 & f & cy & 0 \\\\\\\\ 0 & 0 & 1 & 0 \\\\end{bmatrix} ,\\\\]`where `$T_x$` is a horizontal shift\n * between the cameras and `$cx_1=cx_2$` if CALIB_ZERO_DISPARITY is set.\n * **Vertical stereo**: the first and the second camera views are shifted relative to each other mainly\n * in vertical direction (and probably a bit in the horizontal direction too). The epipolar lines in\n * the rectified images are vertical and have the same x-coordinate. P1 and P2 look\n * like:`\\\\[\\\\texttt{P1} = \\\\begin{bmatrix} f & 0 & cx & 0 \\\\\\\\ 0 & f & cy_1 & 0 \\\\\\\\ 0 & 0 & 1 & 0\n * \\\\end{bmatrix}\\\\]``\\\\[\\\\texttt{P2} = \\\\begin{bmatrix} f & 0 & cx & 0 \\\\\\\\ 0 & f & cy_2 & T_y*f \\\\\\\\\n * 0 & 0 & 1 & 0 \\\\end{bmatrix} ,\\\\]`where `$T_y$` is a vertical shift between the cameras and\n * `$cy_1=cy_2$` if CALIB_ZERO_DISPARITY is set.\n * \n * As you can see, the first three columns of P1 and P2 will effectively be the new \"rectified\" camera\n * matrices. The matrices, together with R1 and R2 , can then be passed to initUndistortRectifyMap to\n * initialize the rectification map for each camera.\n * \n * See below the screenshot from the stereo_calib.cpp sample. Some red horizontal lines pass through\n * the corresponding image regions. This means that the images are well rectified, which is what most\n * stereo correspondence algorithms rely on. The green rectangles are roi1 and roi2 . You see that\n * their interiors are all valid pixels.\n * \n * @param cameraMatrix1 First camera matrix.\n * \n * @param distCoeffs1 First camera distortion parameters.\n * \n * @param cameraMatrix2 Second camera matrix.\n * \n * @param distCoeffs2 Second camera distortion parameters.\n * \n * @param imageSize Size of the image used for stereo calibration.\n * \n * @param R Rotation matrix between the coordinate systems of the first and the second cameras.\n * \n * @param T Translation vector between coordinate systems of the cameras.\n * \n * @param R1 Output 3x3 rectification transform (rotation matrix) for the first camera.\n * \n * @param R2 Output 3x3 rectification transform (rotation matrix) for the second camera.\n * \n * @param P1 Output 3x4 projection matrix in the new (rectified) coordinate systems for the first\n * camera.\n * \n * @param P2 Output 3x4 projection matrix in the new (rectified) coordinate systems for the second\n * camera.\n * \n * @param Q Output $4 \\times 4$ disparity-to-depth mapping matrix (see reprojectImageTo3D ).\n * \n * @param flags Operation flags that may be zero or CALIB_ZERO_DISPARITY . If the flag is set, the\n * function makes the principal points of each camera have the same pixel coordinates in the rectified\n * views. And if the flag is not set, the function may still shift the images in the horizontal or\n * vertical direction (depending on the orientation of epipolar lines) to maximize the useful image\n * area.\n * \n * @param alpha Free scaling parameter. If it is -1 or absent, the function performs the default\n * scaling. Otherwise, the parameter should be between 0 and 1. alpha=0 means that the rectified images\n * are zoomed and shifted so that only valid pixels are visible (no black areas after rectification).\n * alpha=1 means that the rectified image is decimated and shifted so that all the pixels from the\n * original images from the cameras are retained in the rectified images (no source image pixels are\n * lost). Obviously, any intermediate value yields an intermediate result between those two extreme\n * cases.\n * \n * @param newImageSize New image resolution after rectification. The same size should be passed to\n * initUndistortRectifyMap (see the stereo_calib.cpp sample in OpenCV samples directory). When (0,0) is\n * passed (default), it is set to the original imageSize . Setting it to larger value can help you\n * preserve details in the original image, especially when there is a big radial distortion.\n * \n * @param validPixROI1 Optional output rectangles inside the rectified images where all the pixels are\n * valid. If alpha=0 , the ROIs cover the whole images. Otherwise, they are likely to be smaller (see\n * the picture below).\n * \n * @param validPixROI2 Optional output rectangles inside the rectified images where all the pixels are\n * valid. If alpha=0 , the ROIs cover the whole images. Otherwise, they are likely to be smaller (see\n * the picture below).\n */\nexport declare function stereoRectify(cameraMatrix1: InputArray, distCoeffs1: InputArray, cameraMatrix2: InputArray, distCoeffs2: InputArray, imageSize: Size, R: InputArray, T: InputArray, R1: OutputArray, R2: OutputArray, P1: OutputArray, P2: OutputArray, Q: OutputArray, flags?: int, alpha?: double, newImageSize?: Size, validPixROI1?: any, validPixROI2?: any): void\n\n/**\n * The function computes the rectification transformations without knowing intrinsic parameters of the\n * cameras and their relative position in the space, which explains the suffix \"uncalibrated\". Another\n * related difference from stereoRectify is that the function outputs not the rectification\n * transformations in the object (3D) space, but the planar perspective transformations encoded by the\n * homography matrices H1 and H2 . The function implements the algorithm Hartley99 .\n * \n * While the algorithm does not need to know the intrinsic parameters of the cameras, it heavily\n * depends on the epipolar geometry. Therefore, if the camera lenses have a significant distortion, it\n * would be better to correct it before computing the fundamental matrix and calling this function. For\n * example, distortion coefficients can be estimated for each head of stereo camera separately by using\n * calibrateCamera . Then, the images can be corrected using undistort , or just the point coordinates\n * can be corrected with undistortPoints .\n * \n * @param points1 Array of feature points in the first image.\n * \n * @param points2 The corresponding points in the second image. The same formats as in\n * findFundamentalMat are supported.\n * \n * @param F Input fundamental matrix. It can be computed from the same set of point pairs using\n * findFundamentalMat .\n * \n * @param imgSize Size of the image.\n * \n * @param H1 Output rectification homography matrix for the first image.\n * \n * @param H2 Output rectification homography matrix for the second image.\n * \n * @param threshold Optional threshold used to filter out the outliers. If the parameter is greater\n * than zero, all the point pairs that do not comply with the epipolar geometry (that is, the points\n * for which $|\\texttt{points2[i]}^T*\\texttt{F}*\\texttt{points1[i]}|>\\texttt{threshold}$ ) are rejected\n * prior to computing the homographies. Otherwise, all the points are considered inliers.\n */\nexport declare function stereoRectifyUncalibrated(points1: InputArray, points2: InputArray, F: InputArray, imgSize: Size, H1: OutputArray, H2: OutputArray, threshold?: double): bool\n\n/**\n * The function reconstructs 3-dimensional points (in homogeneous coordinates) by using their\n * observations with a stereo camera. Projections matrices can be obtained from stereoRectify.\n * \n * Keep in mind that all input data should be of float type in order for this function to work.\n * \n * [reprojectImageTo3D](#d9/d0c/group__calib3d_1ga1bc1152bd57d63bc524204f21fde6e02})\n * \n * @param projMatr1 3x4 projection matrix of the first camera.\n * \n * @param projMatr2 3x4 projection matrix of the second camera.\n * \n * @param projPoints1 2xN array of feature points in the first image. In case of c++ version it can be\n * also a vector of feature points or two-channel matrix of size 1xN or Nx1.\n * \n * @param projPoints2 2xN array of corresponding points in the second image. In case of c++ version it\n * can be also a vector of feature points or two-channel matrix of size 1xN or Nx1.\n * \n * @param points4D 4xN array of reconstructed points in homogeneous coordinates.\n */\nexport declare function triangulatePoints(projMatr1: InputArray, projMatr2: InputArray, projPoints1: InputArray, projPoints2: InputArray, points4D: OutputArray): void\n\n/**\n * The function transforms an image to compensate radial and tangential lens distortion.\n * \n * The function is simply a combination of\n * [initUndistortRectifyMap](#d9/d0c/group__calib3d_1ga7dfb72c9cf9780a347fbe3d1c47e5d5a}) (with unity R\n * ) and [remap](#da/d54/group__imgproc__transform_1gab75ef31ce5cdfb5c44b6da5f3b908ea4}) (with bilinear\n * interpolation). See the former function for details of the transformation being performed.\n * \n * Those pixels in the destination image, for which there is no correspondent pixels in the source\n * image, are filled with zeros (black color).\n * \n * A particular subset of the source image that will be visible in the corrected image can be regulated\n * by newCameraMatrix. You can use\n * [getOptimalNewCameraMatrix](#d9/d0c/group__calib3d_1ga7a6c4e032c97f03ba747966e6ad862b1}) to compute\n * the appropriate newCameraMatrix depending on your requirements.\n * \n * The camera matrix and the distortion parameters can be determined using\n * [calibrateCamera](#d9/d0c/group__calib3d_1ga3207604e4b1a1758aa66acb6ed5aa65d}). If the resolution of\n * images is different from the resolution used at the calibration stage, `$f_x, f_y, c_x$` and `$c_y$`\n * need to be scaled accordingly, while the distortion coefficients remain the same.\n * \n * @param src Input (distorted) image.\n * \n * @param dst Output (corrected) image that has the same size and type as src .\n * \n * @param cameraMatrix Input camera matrix $A = \\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}$ .\n * \n * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5,\n * k_6[, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is\n * NULL/empty, the zero distortion coefficients are assumed.\n * \n * @param newCameraMatrix Camera matrix of the distorted image. By default, it is the same as\n * cameraMatrix but you may additionally scale and shift the result by using a different matrix.\n */\nexport declare function undistort(src: InputArray, dst: OutputArray, cameraMatrix: InputArray, distCoeffs: InputArray, newCameraMatrix?: InputArray): void\n\n/**\n * The function is similar to [undistort](#d9/d0c/group__calib3d_1ga69f2545a8b62a6b0fc2ee060dc30559d})\n * and [initUndistortRectifyMap](#d9/d0c/group__calib3d_1ga7dfb72c9cf9780a347fbe3d1c47e5d5a}) but it\n * operates on a sparse set of points instead of a raster image. Also the function performs a reverse\n * transformation to projectPoints. In case of a 3D object, it does not reconstruct its 3D coordinates,\n * but for a planar object, it does, up to a translation vector, if the proper R is specified.\n * \n * For each observed point coordinate `$(u, v)$` the function computes: `\\\\[ \\\\begin{array}{l} x^{\"}\n * \\\\leftarrow (u - c_x)/f_x \\\\\\\\ y^{\"} \\\\leftarrow (v - c_y)/f_y \\\\\\\\ (x',y') = undistort(x^{\"},y^{\"},\n * \\\\texttt{distCoeffs}) \\\\\\\\ {[X\\\\,Y\\\\,W]} ^T \\\\leftarrow R*[x' \\\\, y' \\\\, 1]^T \\\\\\\\ x \\\\leftarrow X/W\n * \\\\\\\\ y \\\\leftarrow Y/W \\\\\\\\ \\\\text{only performed if P is specified:} \\\\\\\\ u' \\\\leftarrow x {f'}_x +\n * {c'}_x \\\\\\\\ v' \\\\leftarrow y {f'}_y + {c'}_y \\\\end{array} \\\\]`\n * \n * where *undistort* is an approximate iterative algorithm that estimates the normalized original point\n * coordinates out of the normalized distorted point coordinates (\"normalized\" means that the\n * coordinates do not depend on the camera matrix).\n * \n * The function can be used for both a stereo camera head or a monocular camera (when R is empty).\n * \n * @param src Observed point coordinates, 2xN/Nx2 1-channel or 1xN/Nx1 2-channel (CV_32FC2 or CV_64FC2)\n * (or vector<Point2f> ).\n * \n * @param dst Output ideal point coordinates (1xN/Nx1 2-channel or vector<Point2f> ) after undistortion\n * and reverse perspective transformation. If matrix P is identity or omitted, dst will contain\n * normalized point coordinates.\n * \n * @param cameraMatrix Camera matrix $\\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}$ .\n * \n * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5,\n * k_6[, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is\n * NULL/empty, the zero distortion coefficients are assumed.\n * \n * @param R Rectification transformation in the object space (3x3 matrix). R1 or R2 computed by\n * stereoRectify can be passed here. If the matrix is empty, the identity transformation is used.\n * \n * @param P New camera matrix (3x3) or new projection matrix (3x4) $\\begin{bmatrix} {f'}_x & 0 & {c'}_x\n * & t_x \\\\ 0 & {f'}_y & {c'}_y & t_y \\\\ 0 & 0 & 1 & t_z \\end{bmatrix}$. P1 or P2 computed by\n * stereoRectify can be passed here. If the matrix is empty, the identity new camera matrix is used.\n */\nexport declare function undistortPoints(src: InputArray, dst: OutputArray, cameraMatrix: InputArray, distCoeffs: InputArray, R?: InputArray, P?: InputArray): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts. \n * \n * Default version of [undistortPoints](#d9/d0c/group__calib3d_1ga55c716492470bfe86b0ee9bf3a1f0f7e})\n * does 5 iterations to compute undistorted points.\n */\nexport declare function undistortPoints(src: InputArray, dst: OutputArray, cameraMatrix: InputArray, distCoeffs: InputArray, R: InputArray, P: InputArray, criteria: TermCriteria): void\n\nexport declare function validateDisparity(disparity: InputOutputArray, cost: InputArray, minDisparity: int, numberOfDisparities: int, disp12MaxDisp?: int): void\n\nexport declare const LMEDS: any // initializer: = 4\n\nexport declare const RANSAC: any // initializer: = 8\n\nexport declare const RHO: any // initializer: = 16\n\nexport declare const CALIB_CB_ADAPTIVE_THRESH: any // initializer: = 1\n\nexport declare const CALIB_CB_NORMALIZE_IMAGE: any // initializer: = 2\n\nexport declare const CALIB_CB_FILTER_QUADS: any // initializer: = 4\n\nexport declare const CALIB_CB_FAST_CHECK: any // initializer: = 8\n\nexport declare const CALIB_CB_EXHAUSTIVE: any // initializer: = 16\n\nexport declare const CALIB_CB_ACCURACY: any // initializer: = 32\n\nexport declare const CALIB_CB_SYMMETRIC_GRID: any // initializer: = 1\n\nexport declare const CALIB_CB_ASYMMETRIC_GRID: any // initializer: = 2\n\nexport declare const CALIB_CB_CLUSTERING: any // initializer: = 4\n\nexport declare const CALIB_NINTRINSIC: any // initializer: = 18\n\nexport declare const CALIB_USE_INTRINSIC_GUESS: any // initializer: = 0x00001\n\nexport declare const CALIB_FIX_ASPECT_RATIO: any // initializer: = 0x00002\n\nexport declare const CALIB_FIX_PRINCIPAL_POINT: any // initializer: = 0x00004\n\nexport declare const CALIB_ZERO_TANGENT_DIST: any // initializer: = 0x00008\n\nexport declare const CALIB_FIX_FOCAL_LENGTH: any // initializer: = 0x00010\n\nexport declare const CALIB_FIX_K1: any // initializer: = 0x00020\n\nexport declare const CALIB_FIX_K2: any // initializer: = 0x00040\n\nexport declare const CALIB_FIX_K3: any // initializer: = 0x00080\n\nexport declare const CALIB_FIX_K4: any // initializer: = 0x00800\n\nexport declare const CALIB_FIX_K5: any // initializer: = 0x01000\n\nexport declare const CALIB_FIX_K6: any // initializer: = 0x02000\n\nexport declare const CALIB_RATIONAL_MODEL: any // initializer: = 0x04000\n\nexport declare const CALIB_THIN_PRISM_MODEL: any // initializer: = 0x08000\n\nexport declare const CALIB_FIX_S1_S2_S3_S4: any // initializer: = 0x10000\n\nexport declare const CALIB_TILTED_MODEL: any // initializer: = 0x40000\n\nexport declare const CALIB_FIX_TAUX_TAUY: any // initializer: = 0x80000\n\nexport declare const CALIB_USE_QR: any // initializer: = 0x100000\n\nexport declare const CALIB_FIX_TANGENT_DIST: any // initializer: = 0x200000\n\nexport declare const CALIB_FIX_INTRINSIC: any // initializer: = 0x00100\n\nexport declare const CALIB_SAME_FOCAL_LENGTH: any // initializer: = 0x00200\n\nexport declare const CALIB_ZERO_DISPARITY: any // initializer: = 0x00400\n\nexport declare const CALIB_USE_LU: any // initializer: = (1 << 17)\n\nexport declare const CALIB_USE_EXTRINSIC_GUESS: any // initializer: = (1 << 22)\n\nexport declare const FM_7POINT: any // initializer: = 1\n\nexport declare const FM_8POINT: any // initializer: = 2\n\nexport declare const FM_LMEDS: any // initializer: = 4\n\nexport declare const FM_RANSAC: any // initializer: = 8\n\nexport declare const CALIB_HAND_EYE_TSAI: HandEyeCalibrationMethod // initializer: = 0\n\nexport declare const CALIB_HAND_EYE_PARK: HandEyeCalibrationMethod // initializer: = 1\n\nexport declare const CALIB_HAND_EYE_HORAUD: HandEyeCalibrationMethod // initializer: = 2\n\nexport declare const CALIB_HAND_EYE_ANDREFF: HandEyeCalibrationMethod // initializer: = 3\n\nexport declare const CALIB_HAND_EYE_DANIILIDIS: HandEyeCalibrationMethod // initializer: = 4\n\nexport declare const SOLVEPNP_ITERATIVE: SolvePnPMethod // initializer: = 0\n\nexport declare const SOLVEPNP_EPNP: SolvePnPMethod // initializer: = 1\n\nexport declare const SOLVEPNP_P3P: SolvePnPMethod // initializer: = 2\n\nexport declare const SOLVEPNP_DLS: SolvePnPMethod // initializer: = 3\n\nexport declare const SOLVEPNP_UPNP: SolvePnPMethod // initializer: = 4\n\nexport declare const SOLVEPNP_AP3P: SolvePnPMethod // initializer: = 5\n\n/**\n * Infinitesimal Plane-Based Pose Estimation Collins14 \n *  Object points must be coplanar.\n * \n */\nexport declare const SOLVEPNP_IPPE: SolvePnPMethod // initializer: = 6\n\n/**\n * Infinitesimal Plane-Based Pose Estimation Collins14 \n *  This is a special case suitable for marker pose estimation.\n *  4 coplanar object points must be defined in the following order:\n * \n * point 0: [-squareLength / 2, squareLength / 2, 0]\n * point 1: [ squareLength / 2, squareLength / 2, 0]\n * point 2: [ squareLength / 2, -squareLength / 2, 0]\n * point 3: [-squareLength / 2, -squareLength / 2, 0]\n * \n */\nexport declare const SOLVEPNP_IPPE_SQUARE: SolvePnPMethod // initializer: = 7\n\nexport declare const PROJ_SPHERICAL_ORTHO: UndistortTypes // initializer: = 0\n\nexport declare const PROJ_SPHERICAL_EQRECT: UndistortTypes // initializer: = 1\n\nexport type HandEyeCalibrationMethod = any\n\nexport type SolvePnPMethod = any\n\nexport type UndistortTypes = any\n\n"},"node_modules_mirada_dist_src_types_opencv_core_cluster_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_core_cluster_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/core_cluster.d.ts","content":"\nimport { double, InputArray, InputOutputArray, int, OutputArray, TermCriteria, _EqPredicate } from './_types'\n/*\n * # Clustering\n * \n */\n/**\n * The function kmeans implements a k-means algorithm that finds the centers of cluster_count clusters\n * and groups the input samples around the clusters. As an output, `$\\\\texttt{bestLabels}_i$` contains\n * a 0-based cluster index for the sample stored in the `$i^{th}$` row of the samples matrix.\n * \n * (Python) An example on K-means clustering can be found at\n * opencv_source_code/samples/python/kmeans.py \n * \n * The function returns the compactness measure that is computed as `\\\\[\\\\sum _i \\\\| \\\\texttt{samples}\n * _i - \\\\texttt{centers} _{ \\\\texttt{labels} _i} \\\\| ^2\\\\]` after every attempt. The best (minimum)\n * value is chosen and the corresponding labels and the compactness value are returned by the function.\n * Basically, you can use only the core of the function, set the number of attempts to 1, initialize\n * labels each time using a custom algorithm, pass them with the ( flags =\n * [KMEANS_USE_INITIAL_LABELS](#d0/de1/group__core_1gga276000efe55ee2756e0c471c7b270949a40625baa3d28c780813f9634b960b366})\n * ) flag, and then choose the best (most-compact) clustering.\n * \n * @param data Data for clustering. An array of N-Dimensional points with float coordinates is needed.\n * Examples of this array can be:\n * Mat points(count, 2, CV_32F);Mat points(count, 1, CV_32FC2);Mat points(1, count,\n * CV_32FC2);std::vector<cv::Point2f> points(sampleCount);\n * \n * @param K Number of clusters to split the set by.\n * \n * @param bestLabels Input/output integer array that stores the cluster indices for every sample.\n * \n * @param criteria The algorithm termination criteria, that is, the maximum number of iterations and/or\n * the desired accuracy. The accuracy is specified as criteria.epsilon. As soon as each of the cluster\n * centers moves by less than criteria.epsilon on some iteration, the algorithm stops.\n * \n * @param attempts Flag to specify the number of times the algorithm is executed using different\n * initial labellings. The algorithm returns the labels that yield the best compactness (see the last\n * function parameter).\n * \n * @param flags Flag that can take values of cv::KmeansFlags\n * \n * @param centers Output matrix of the cluster centers, one row per each cluster center.\n */\nexport declare function kmeans(data: InputArray, K: int, bestLabels: InputOutputArray, criteria: TermCriteria, attempts: int, flags: int, centers?: OutputArray): double\n\n/**\n * The generic function partition implements an `$O(N^2)$` algorithm for splitting a set of `$N$`\n * elements into one or more equivalency classes, as described in  . The function returns the number of\n * equivalency classes.\n * \n * @param _vec Set of elements stored as a vector.\n * \n * @param labels Output vector of labels. It contains as many elements as vec. Each label labels[i] is\n * a 0-based cluster index of vec[i].\n * \n * @param predicate Equivalence predicate (pointer to a boolean function of two arguments or an\n * instance of the class that has the method bool operator()(const _Tp& a, const _Tp& b) ). The\n * predicate returns true when the elements are certainly in the same class, and returns false if they\n * may or may not be in the same class.\n */\nexport declare function partition(arg119: any, arg120: any, _vec: any, labels: any, predicate?: _EqPredicate): any\n\n"},"node_modules_mirada_dist_src_types_opencv_core_hal_interface_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_core_hal_interface_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/core_hal_interface.d.ts","content":"\nimport { cvhalDFT, int, size_t, uchar } from './_types'\n/*\n * # Interface\n * Define your functions to override default implementations: \n * \n * ```cpp\n * #undef hal_add8u\n * #define hal_add8u my_add8u\n * ```\n */\n/**\n * @param context pointer to context storing all necessary data\n * \n * @param src_data source image data and step\n * \n * @param dst_data destination image data and step\n */\nexport declare function hal_ni_dct2D(context: cvhalDFT, src_data: uchar, src_step: size_t, dst_data: uchar, dst_step: size_t): cvhalDFT\n\n/**\n * @param context pointer to context storing all necessary data\n */\nexport declare function hal_ni_dctFree2D(context: cvhalDFT): cvhalDFT\n\n/**\n * @param context double pointer to context storing all necessary data\n * \n * @param width image dimensions\n * \n * @param depth image type (CV_32F or CV64F)\n * \n * @param flags algorithm options (combination of CV_HAL_DFT_INVERSE, ...)\n */\nexport declare function hal_ni_dctInit2D(context: cvhalDFT, width: int, height: int, depth: int, flags: int): cvhalDFT\n\n/**\n * @param context pointer to context storing all necessary data\n * \n * @param src source data\n * \n * @param dst destination data\n */\nexport declare function hal_ni_dft1D(context: cvhalDFT, src: uchar, dst: uchar): cvhalDFT\n\n/**\n * @param context pointer to context storing all necessary data\n * \n * @param src_data source image data and step\n * \n * @param dst_data destination image data and step\n */\nexport declare function hal_ni_dft2D(context: cvhalDFT, src_data: uchar, src_step: size_t, dst_data: uchar, dst_step: size_t): cvhalDFT\n\n/**\n * @param context pointer to context storing all necessary data\n */\nexport declare function hal_ni_dftFree1D(context: cvhalDFT): cvhalDFT\n\n/**\n * @param context pointer to context storing all necessary data\n */\nexport declare function hal_ni_dftFree2D(context: cvhalDFT): cvhalDFT\n\n/**\n * @param context double pointer to context storing all necessary data\n * \n * @param len transformed array length\n * \n * @param count estimated transformation count\n * \n * @param depth array type (CV_32F or CV_64F)\n * \n * @param flags algorithm options (combination of CV_HAL_DFT_INVERSE, CV_HAL_DFT_SCALE, ...)\n * \n * @param needBuffer pointer to boolean variable, if valid pointer provided, then variable value should\n * be set to true to signal that additional memory buffer is needed for operations\n */\nexport declare function hal_ni_dftInit1D(context: cvhalDFT, len: int, count: int, depth: int, flags: int, needBuffer: any): cvhalDFT\n\n/**\n * @param context double pointer to context storing all necessary data\n * \n * @param width image dimensions\n * \n * @param depth image type (CV_32F or CV64F)\n * \n * @param src_channels number of channels in input image\n * \n * @param dst_channels number of channels in output image\n * \n * @param flags algorithm options (combination of CV_HAL_DFT_INVERSE, ...)\n * \n * @param nonzero_rows number of nonzero rows in image, can be used for optimization\n */\nexport declare function hal_ni_dftInit2D(context: cvhalDFT, width: int, height: int, depth: int, src_channels: int, dst_channels: int, flags: int, nonzero_rows: int): cvhalDFT\n\n/**\n * @param src_data Source image\n * \n * @param width Source image dimensions\n * \n * @param depth Depth of source image\n * \n * @param minVal Pointer to the returned global minimum and maximum in an array.\n * \n * @param minIdx Pointer to the returned minimum and maximum location.\n * \n * @param mask Specified array region.\n */\nexport declare function hal_ni_minMaxIdx(src_data: uchar, src_step: size_t, width: int, height: int, depth: int, minVal: any, maxVal: any, minIdx: any, maxIdx: any, mask: uchar): uchar\n\n"},"node_modules_mirada_dist_src_types_opencv_core_array_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_core_array_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/core_array.d.ts","content":"\nimport { bool, double, InputArray, InputArrayOfArrays, InputOutputArray, InputOutputArrayOfArrays, int, Mat, OutputArray, OutputArrayOfArrays, Scalar, size_t } from './_types'\n/*\n * # Operations on arrays\n * \n */\n/**\n * The function [cv::absdiff](#d2/de8/group__core__array_1ga6fef31bc8c4071cbc114a758a2b79c14})\n * calculates: Absolute difference between two arrays when they have the same size and type:\n * `\\\\[\\\\texttt{dst}(I) = \\\\texttt{saturate} (| \\\\texttt{src1}(I) - \\\\texttt{src2}(I)|)\\\\]` Absolute\n * difference between an array and a scalar when the second array is constructed from Scalar or has as\n * many elements as the number of channels in `src1`: `\\\\[\\\\texttt{dst}(I) = \\\\texttt{saturate} (|\n * \\\\texttt{src1}(I) - \\\\texttt{src2} |)\\\\]` Absolute difference between a scalar and an array when the\n * first array is constructed from Scalar or has as many elements as the number of channels in `src2`:\n * `\\\\[\\\\texttt{dst}(I) = \\\\texttt{saturate} (| \\\\texttt{src1} - \\\\texttt{src2}(I) |)\\\\]` where I is a\n * multi-dimensional index of array elements. In case of multi-channel arrays, each channel is\n * processed independently. \n * \n * Saturation is not applied when the arrays have the depth CV_32S. You may even get a negative value\n * in the case of overflow. \n * \n * cv::abs(const Mat&)\n * \n * @param src1 first input array or a scalar.\n * \n * @param src2 second input array or a scalar.\n * \n * @param dst output array that has the same size and type as input arrays.\n */\nexport declare function absdiff(src1: InputArray, src2: InputArray, dst: OutputArray): void\n\n/**\n * The function add calculates:\n * \n * Sum of two arrays when both input arrays have the same size and the same number of channels:\n * `\\\\[\\\\texttt{dst}(I) = \\\\texttt{saturate} ( \\\\texttt{src1}(I) + \\\\texttt{src2}(I)) \\\\quad\n * \\\\texttt{if mask}(I) \\\\ne0\\\\]`\n * Sum of an array and a scalar when src2 is constructed from Scalar or has the same number of elements\n * as `src1.channels()`: `\\\\[\\\\texttt{dst}(I) = \\\\texttt{saturate} ( \\\\texttt{src1}(I) + \\\\texttt{src2}\n * ) \\\\quad \\\\texttt{if mask}(I) \\\\ne0\\\\]`\n * Sum of a scalar and an array when src1 is constructed from Scalar or has the same number of elements\n * as `src2.channels()`: `\\\\[\\\\texttt{dst}(I) = \\\\texttt{saturate} ( \\\\texttt{src1} + \\\\texttt{src2}(I)\n * ) \\\\quad \\\\texttt{if mask}(I) \\\\ne0\\\\]` where `I` is a multi-dimensional index of array elements. In\n * case of multi-channel arrays, each channel is processed independently.\n * \n * The first function in the list above can be replaced with matrix expressions: \n * \n * ```cpp\n * dst = src1 + src2;\n * dst += src1; // equivalent to add(dst, src1, dst);\n * ```\n * \n *  The input arrays and the output array can all have the same or different depths. For example, you\n * can add a 16-bit unsigned array to a 8-bit signed array and store the sum as a 32-bit floating-point\n * array. Depth of the output array is determined by the dtype parameter. In the second and third cases\n * above, as well as in the first case, when src1.depth() == src2.depth(), dtype can be set to the\n * default -1. In this case, the output array will have the same depth as the input array, be it src1,\n * src2 or both. \n * \n * Saturation is not applied when the output array has the depth CV_32S. You may even get result of an\n * incorrect sign in the case of overflow. \n * \n * [subtract](#d2/de8/group__core__array_1gaa0f00d98b4b5edeaeb7b8333b2de353b}),\n * [addWeighted](#d2/de8/group__core__array_1gafafb2513349db3bcff51f54ee5592a19}),\n * [scaleAdd](#d2/de8/group__core__array_1ga9e0845db4135f55dcf20227402f00d98}),\n * [Mat::convertTo](#d3/d63/classcv_1_1Mat_1adf88c60c5b4980e05bb556080916978b})\n * \n * @param src1 first input array or a scalar.\n * \n * @param src2 second input array or a scalar.\n * \n * @param dst output array that has the same size and number of channels as the input array(s); the\n * depth is defined by dtype or src1/src2.\n * \n * @param mask optional operation mask - 8-bit single channel array, that specifies elements of the\n * output array to be changed.\n * \n * @param dtype optional depth of the output array (see the discussion below).\n */\nexport declare function add(src1: InputArray, src2: InputArray, dst: OutputArray, mask?: InputArray, dtype?: int): void\n\n/**\n * The function addWeighted calculates the weighted sum of two arrays as follows: `\\\\[\\\\texttt{dst}\n * (I)= \\\\texttt{saturate} ( \\\\texttt{src1} (I)* \\\\texttt{alpha} + \\\\texttt{src2} (I)* \\\\texttt{beta} +\n * \\\\texttt{gamma} )\\\\]` where I is a multi-dimensional index of array elements. In case of\n * multi-channel arrays, each channel is processed independently. The function can be replaced with a\n * matrix expression: \n * \n * ```cpp\n * dst = src1*alpha + src2*beta + gamma;\n * ```\n * \n * Saturation is not applied when the output array has the depth CV_32S. You may even get result of an\n * incorrect sign in the case of overflow. \n * \n * [add](#d2/de8/group__core__array_1ga10ac1bfb180e2cfda1701d06c24fdbd6}),\n * [subtract](#d2/de8/group__core__array_1gaa0f00d98b4b5edeaeb7b8333b2de353b}),\n * [scaleAdd](#d2/de8/group__core__array_1ga9e0845db4135f55dcf20227402f00d98}),\n * [Mat::convertTo](#d3/d63/classcv_1_1Mat_1adf88c60c5b4980e05bb556080916978b})\n * \n * @param src1 first input array.\n * \n * @param alpha weight of the first array elements.\n * \n * @param src2 second input array of the same size and channel number as src1.\n * \n * @param beta weight of the second array elements.\n * \n * @param gamma scalar added to each sum.\n * \n * @param dst output array that has the same size and number of channels as the input arrays.\n * \n * @param dtype optional depth of the output array; when both input arrays have the same depth, dtype\n * can be set to -1, which will be equivalent to src1.depth().\n */\nexport declare function addWeighted(src1: InputArray, alpha: double, src2: InputArray, beta: double, gamma: double, dst: OutputArray, dtype?: int): void\n\n/**\n * see\n */\nexport declare function batchDistance(src1: InputArray, src2: InputArray, dist: OutputArray, dtype: int, nidx: OutputArray, normType?: int, K?: int, mask?: InputArray, update?: int, crosscheck?: bool): void\n\n/**\n * The function [cv::bitwise_and](#d2/de8/group__core__array_1ga60b4d04b251ba5eb1392c34425497e14})\n * calculates the per-element bit-wise logical conjunction for: Two arrays when src1 and src2 have the\n * same size: `\\\\[\\\\texttt{dst} (I) = \\\\texttt{src1} (I) \\\\wedge \\\\texttt{src2} (I) \\\\quad \\\\texttt{if\n * mask} (I) \\\\ne0\\\\]` An array and a scalar when src2 is constructed from Scalar or has the same\n * number of elements as `src1.channels()`: `\\\\[\\\\texttt{dst} (I) = \\\\texttt{src1} (I) \\\\wedge\n * \\\\texttt{src2} \\\\quad \\\\texttt{if mask} (I) \\\\ne0\\\\]` A scalar and an array when src1 is constructed\n * from Scalar or has the same number of elements as `src2.channels()`: `\\\\[\\\\texttt{dst} (I) =\n * \\\\texttt{src1} \\\\wedge \\\\texttt{src2} (I) \\\\quad \\\\texttt{if mask} (I) \\\\ne0\\\\]` In case of\n * floating-point arrays, their machine-specific bit representations (usually IEEE754-compliant) are\n * used for the operation. In case of multi-channel arrays, each channel is processed independently. In\n * the second and third cases above, the scalar is first converted to the array type.\n * \n * @param src1 first input array or a scalar.\n * \n * @param src2 second input array or a scalar.\n * \n * @param dst output array that has the same size and type as the input arrays.\n * \n * @param mask optional operation mask, 8-bit single channel array, that specifies elements of the\n * output array to be changed.\n */\nexport declare function bitwise_and(src1: InputArray, src2: InputArray, dst: OutputArray, mask?: InputArray): void\n\n/**\n * The function [cv::bitwise_not](#d2/de8/group__core__array_1ga0002cf8b418479f4cb49a75442baee2f})\n * calculates per-element bit-wise inversion of the input array: `\\\\[\\\\texttt{dst} (I) = \\\\neg\n * \\\\texttt{src} (I)\\\\]` In case of a floating-point input array, its machine-specific bit\n * representation (usually IEEE754-compliant) is used for the operation. In case of multi-channel\n * arrays, each channel is processed independently.\n * \n * @param src input array.\n * \n * @param dst output array that has the same size and type as the input array.\n * \n * @param mask optional operation mask, 8-bit single channel array, that specifies elements of the\n * output array to be changed.\n */\nexport declare function bitwise_not(src: InputArray, dst: OutputArray, mask?: InputArray): void\n\n/**\n * The function [cv::bitwise_or](#d2/de8/group__core__array_1gab85523db362a4e26ff0c703793a719b4})\n * calculates the per-element bit-wise logical disjunction for: Two arrays when src1 and src2 have the\n * same size: `\\\\[\\\\texttt{dst} (I) = \\\\texttt{src1} (I) \\\\vee \\\\texttt{src2} (I) \\\\quad \\\\texttt{if\n * mask} (I) \\\\ne0\\\\]` An array and a scalar when src2 is constructed from Scalar or has the same\n * number of elements as `src1.channels()`: `\\\\[\\\\texttt{dst} (I) = \\\\texttt{src1} (I) \\\\vee\n * \\\\texttt{src2} \\\\quad \\\\texttt{if mask} (I) \\\\ne0\\\\]` A scalar and an array when src1 is constructed\n * from Scalar or has the same number of elements as `src2.channels()`: `\\\\[\\\\texttt{dst} (I) =\n * \\\\texttt{src1} \\\\vee \\\\texttt{src2} (I) \\\\quad \\\\texttt{if mask} (I) \\\\ne0\\\\]` In case of\n * floating-point arrays, their machine-specific bit representations (usually IEEE754-compliant) are\n * used for the operation. In case of multi-channel arrays, each channel is processed independently. In\n * the second and third cases above, the scalar is first converted to the array type.\n * \n * @param src1 first input array or a scalar.\n * \n * @param src2 second input array or a scalar.\n * \n * @param dst output array that has the same size and type as the input arrays.\n * \n * @param mask optional operation mask, 8-bit single channel array, that specifies elements of the\n * output array to be changed.\n */\nexport declare function bitwise_or(src1: InputArray, src2: InputArray, dst: OutputArray, mask?: InputArray): void\n\n/**\n * The function [cv::bitwise_xor](#d2/de8/group__core__array_1ga84b2d8188ce506593dcc3f8cd00e8e2c})\n * calculates the per-element bit-wise logical \"exclusive-or\" operation for: Two arrays when src1 and\n * src2 have the same size: `\\\\[\\\\texttt{dst} (I) = \\\\texttt{src1} (I) \\\\oplus \\\\texttt{src2} (I)\n * \\\\quad \\\\texttt{if mask} (I) \\\\ne0\\\\]` An array and a scalar when src2 is constructed from Scalar or\n * has the same number of elements as `src1.channels()`: `\\\\[\\\\texttt{dst} (I) = \\\\texttt{src1} (I)\n * \\\\oplus \\\\texttt{src2} \\\\quad \\\\texttt{if mask} (I) \\\\ne0\\\\]` A scalar and an array when src1 is\n * constructed from Scalar or has the same number of elements as `src2.channels()`: `\\\\[\\\\texttt{dst}\n * (I) = \\\\texttt{src1} \\\\oplus \\\\texttt{src2} (I) \\\\quad \\\\texttt{if mask} (I) \\\\ne0\\\\]` In case of\n * floating-point arrays, their machine-specific bit representations (usually IEEE754-compliant) are\n * used for the operation. In case of multi-channel arrays, each channel is processed independently. In\n * the 2nd and 3rd cases above, the scalar is first converted to the array type.\n * \n * @param src1 first input array or a scalar.\n * \n * @param src2 second input array or a scalar.\n * \n * @param dst output array that has the same size and type as the input arrays.\n * \n * @param mask optional operation mask, 8-bit single channel array, that specifies elements of the\n * output array to be changed.\n */\nexport declare function bitwise_xor(src1: InputArray, src2: InputArray, dst: OutputArray, mask?: InputArray): void\n\n/**\n * The function computes and returns the coordinate of a donor pixel corresponding to the specified\n * extrapolated pixel when using the specified extrapolation border mode. For example, if you use\n * [cv::BORDER_WRAP](#d2/de8/group__core__array_1gga209f2f4869e304c82d07739337eae7c5a697c1b011884a7c2bdc0e5caf7955661})\n * mode in the horizontal direction,\n * [cv::BORDER_REFLECT_101](#d2/de8/group__core__array_1gga209f2f4869e304c82d07739337eae7c5ab3c5a6143d8120b95005fa7105a10bb4})\n * in the vertical direction and want to compute value of the \"virtual\" pixel Point(-5, 100) in a\n * floating-point image img , it looks like: \n * \n * ```cpp\n * float val = img.at<float>(borderInterpolate(100, img.rows, cv::BORDER_REFLECT_101),\n *                           borderInterpolate(-5, img.cols, cv::BORDER_WRAP));\n * ```\n * \n *  Normally, the function is not called directly. It is used inside filtering functions and also in\n * copyMakeBorder. \n * \n * [copyMakeBorder](#d2/de8/group__core__array_1ga2ac1049c2c3dd25c2b41bffe17658a36})\n * \n * @param p 0-based coordinate of the extrapolated pixel along one of the axes, likely <0 or >= len\n * \n * @param len Length of the array along the corresponding axis.\n * \n * @param borderType Border type, one of the BorderTypes, except for BORDER_TRANSPARENT and\n * BORDER_ISOLATED . When borderType==BORDER_CONSTANT , the function always returns -1, regardless of p\n * and len.\n */\nexport declare function borderInterpolate(p: int, len: int, borderType: int): int\n\n/**\n * The function [cv::calcCovarMatrix](#d2/de8/group__core__array_1gae6ffa9354633f984246945d52823165d})\n * calculates the covariance matrix and, optionally, the mean vector of the set of input vectors. \n * \n * [PCA](#d3/d8d/classcv_1_1PCA}),\n * [mulTransposed](#d2/de8/group__core__array_1gadc4e49f8f7a155044e3be1b9e3b270ab}),\n * [Mahalanobis](#d2/de8/group__core__array_1ga4493aee129179459cbfc6064f051aa7d})\n * \n * @param samples samples stored as separate matrices\n * \n * @param nsamples number of samples\n * \n * @param covar output covariance matrix of the type ctype and square size.\n * \n * @param mean input or output (depending on the flags) array as the average value of the input\n * vectors.\n * \n * @param flags operation flags as a combination of CovarFlags\n * \n * @param ctype type of the matrixl; it equals 'CV_64F' by default.\n */\nexport declare function calcCovarMatrix(samples: any, nsamples: int, covar: any, mean: any, flags: int, ctype?: int): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts. \n * \n * use\n * [COVAR_ROWS](#d0/de1/group__core_1gga719ebd4a73f30f4fab258ab7616d0f0fadbac775ac8245aad5bfef994731c635f})\n * or\n * [COVAR_COLS](#d0/de1/group__core_1gga719ebd4a73f30f4fab258ab7616d0f0fac8cc5a80914e18d6100184a2829aa3c0})\n * flag\n * \n * @param samples samples stored as rows/columns of a single matrix.\n * \n * @param covar output covariance matrix of the type ctype and square size.\n * \n * @param mean input or output (depending on the flags) array as the average value of the input\n * vectors.\n * \n * @param flags operation flags as a combination of CovarFlags\n * \n * @param ctype type of the matrixl; it equals 'CV_64F' by default.\n */\nexport declare function calcCovarMatrix(samples: InputArray, covar: OutputArray, mean: InputOutputArray, flags: int, ctype?: int): void\n\n/**\n * The function [cv::cartToPolar](#d2/de8/group__core__array_1gac5f92f48ec32cacf5275969c33ee837d})\n * calculates either the magnitude, angle, or both for every 2D vector (x(I),y(I)):\n * `\\\\[\\\\begin{array}{l} \\\\texttt{magnitude} (I)= \\\\sqrt{\\\\texttt{x}(I)^2+\\\\texttt{y}(I)^2} , \\\\\\\\\n * \\\\texttt{angle} (I)= \\\\texttt{atan2} ( \\\\texttt{y} (I), \\\\texttt{x} (I))[ \\\\cdot180 / \\\\pi ]\n * \\\\end{array}\\\\]`\n * \n * The angles are calculated with accuracy about 0.3 degrees. For the point (0,0), the angle is set to\n * 0. \n * \n * [Sobel](#d4/d86/group__imgproc__filter_1gacea54f142e81b6758cb6f375ce782c8d}),\n * [Scharr](#d4/d86/group__imgproc__filter_1gaa13106761eedf14798f37aa2d60404c9})\n * \n * @param x array of x-coordinates; this must be a single-precision or double-precision floating-point\n * array.\n * \n * @param y array of y-coordinates, that must have the same size and same type as x.\n * \n * @param magnitude output array of magnitudes of the same size and type as x.\n * \n * @param angle output array of angles that has the same size and type as x; the angles are measured in\n * radians (from 0 to 2*Pi) or in degrees (0 to 360 degrees).\n * \n * @param angleInDegrees a flag, indicating whether the angles are measured in radians (which is by\n * default), or in degrees.\n */\nexport declare function cartToPolar(x: InputArray, y: InputArray, magnitude: OutputArray, angle: OutputArray, angleInDegrees?: bool): void\n\n/**\n * The function [cv::checkRange](#d2/de8/group__core__array_1ga2bd19d89cae59361416736f87e3c7a64})\n * checks that every array element is neither NaN nor infinite. When minVal > -DBL_MAX and maxVal <\n * DBL_MAX, the function also checks that each value is between minVal and maxVal. In case of\n * multi-channel arrays, each channel is processed independently. If some values are out of range,\n * position of the first outlier is stored in pos (when pos != NULL). Then, the function either returns\n * false (when quiet=true) or throws an exception.\n * \n * @param a input array.\n * \n * @param quiet a flag, indicating whether the functions quietly return false when the array elements\n * are out of range or they throw an exception.\n * \n * @param pos optional output parameter, when not NULL, must be a pointer to array of src.dims\n * elements.\n * \n * @param minVal inclusive lower boundary of valid values range.\n * \n * @param maxVal exclusive upper boundary of valid values range.\n */\nexport declare function checkRange(a: InputArray, quiet?: bool, pos?: any, minVal?: double, maxVal?: double): bool\n\n/**\n * The function compares: Elements of two arrays when src1 and src2 have the same size:\n * `\\\\[\\\\texttt{dst} (I) = \\\\texttt{src1} (I) \\\\,\\\\texttt{cmpop}\\\\, \\\\texttt{src2} (I)\\\\]` Elements of\n * src1 with a scalar src2 when src2 is constructed from Scalar or has a single element:\n * `\\\\[\\\\texttt{dst} (I) = \\\\texttt{src1}(I) \\\\,\\\\texttt{cmpop}\\\\, \\\\texttt{src2}\\\\]` src1 with\n * elements of src2 when src1 is constructed from Scalar or has a single element: `\\\\[\\\\texttt{dst} (I)\n * = \\\\texttt{src1} \\\\,\\\\texttt{cmpop}\\\\, \\\\texttt{src2} (I)\\\\]` When the comparison result is true,\n * the corresponding element of output array is set to 255. The comparison operations can be replaced\n * with the equivalent matrix expressions: \n * \n * ```cpp\n * Mat dst1 = src1 >= src2;\n * Mat dst2 = src1 < 8;\n * ...\n * ```\n * \n * [checkRange](#d2/de8/group__core__array_1ga2bd19d89cae59361416736f87e3c7a64}),\n * [min](#d7/dcc/group__core__utils__softfloat_1gac48df53b8fd34b87e7b121fa8fd4c379}),\n * [max](#d7/dcc/group__core__utils__softfloat_1ga78f988f6cfa6223610298cbd4f86ec66}),\n * [threshold](#d7/d1b/group__imgproc__misc_1gae8a4a146d1ca78c626a53577199e9c57})\n * \n * @param src1 first input array or a scalar; when it is an array, it must have a single channel.\n * \n * @param src2 second input array or a scalar; when it is an array, it must have a single channel.\n * \n * @param dst output array of type ref CV_8U that has the same size and the same number of channels as\n * the input arrays.\n * \n * @param cmpop a flag, that specifies correspondence between the arrays (cv::CmpTypes)\n */\nexport declare function compare(src1: InputArray, src2: InputArray, dst: OutputArray, cmpop: int): void\n\n/**\n * The function [cv::completeSymm](#d2/de8/group__core__array_1ga6847337c0c55769e115a70e0f011b5ca})\n * copies the lower or the upper half of a square matrix to its another half. The matrix diagonal\n * remains unchanged:\n * \n * `$\\\\texttt{m}_{ij}=\\\\texttt{m}_{ji}$` for `$i > j$` if lowerToUpper=false\n * `$\\\\texttt{m}_{ij}=\\\\texttt{m}_{ji}$` for `$i < j$` if lowerToUpper=true\n * \n * [flip](#d2/de8/group__core__array_1gaca7be533e3dac7feb70fc60635adf441}),\n * [transpose](#d2/de8/group__core__array_1ga46630ed6c0ea6254a35f447289bd7404})\n * \n * @param m input-output floating-point square matrix.\n * \n * @param lowerToUpper operation flag; if true, the lower half is copied to the upper half. Otherwise,\n * the upper half is copied to the lower half.\n */\nexport declare function completeSymm(m: InputOutputArray, lowerToUpper?: bool): void\n\n/**\n * This function converts FP32 (single precision floating point) from/to FP16 (half precision floating\n * point). CV_16S format is used to represent FP16 data. There are two use modes (src -> dst): CV_32F\n * -> CV_16S and CV_16S -> CV_32F. The input array has to have type of CV_32F or CV_16S to represent\n * the bit depth. If the input array is neither of them, the function will raise an error. The format\n * of half precision floating point is defined in IEEE 754-2008.\n * \n * @param src input array.\n * \n * @param dst output array.\n */\nexport declare function convertFp16(src: InputArray, dst: OutputArray): void\n\n/**\n * On each element of the input array, the function convertScaleAbs performs three operations\n * sequentially: scaling, taking an absolute value, conversion to an unsigned 8-bit type:\n * `\\\\[\\\\texttt{dst} (I)= \\\\texttt{saturate\\\\_cast<uchar>} (| \\\\texttt{src} (I)* \\\\texttt{alpha} +\n * \\\\texttt{beta} |)\\\\]` In case of multi-channel arrays, the function processes each channel\n * independently. When the output is not 8-bit, the operation can be emulated by calling the\n * [Mat::convertTo](#d3/d63/classcv_1_1Mat_1adf88c60c5b4980e05bb556080916978b}) method (or by using\n * matrix expressions) and then by calculating an absolute value of the result. For example: \n * \n * ```cpp\n * Mat_<float> A(30,30);\n * randu(A, Scalar(-100), Scalar(100));\n * Mat_<float> B = A*5 + 3;\n * B = abs(B);\n * // Mat_<float> B = abs(A*5+3) will also do the job,\n * // but it will allocate a temporary matrix\n * ```\n * \n * [Mat::convertTo](#d3/d63/classcv_1_1Mat_1adf88c60c5b4980e05bb556080916978b}), cv::abs(const Mat&)\n * \n * @param src input array.\n * \n * @param dst output array.\n * \n * @param alpha optional scale factor.\n * \n * @param beta optional delta added to the scaled values.\n */\nexport declare function convertScaleAbs(src: InputArray, dst: OutputArray, alpha?: double, beta?: double): void\n\n/**\n * The function copies the source image into the middle of the destination image. The areas to the\n * left, to the right, above and below the copied source image will be filled with extrapolated pixels.\n * This is not what filtering functions based on it do (they extrapolate pixels on-fly), but what other\n * more complex functions, including your own, may do to simplify image boundary handling.\n * \n * The function supports the mode when src is already in the middle of dst . In this case, the function\n * does not copy src itself but simply constructs the border, for example:\n * \n * ```cpp\n * // let border be the same in all directions\n * int border=2;\n * // constructs a larger image to fit both the image and the border\n * Mat gray_buf(rgb.rows + border*2, rgb.cols + border*2, rgb.depth());\n * // select the middle part of it w/o copying data\n * Mat gray(gray_canvas, Rect(border, border, rgb.cols, rgb.rows));\n * // convert image from RGB to grayscale\n * cvtColor(rgb, gray, COLOR_RGB2GRAY);\n * // form a border in-place\n * copyMakeBorder(gray, gray_buf, border, border,\n *                border, border, BORDER_REPLICATE);\n * // now do some custom filtering ...\n * ...\n * ```\n * \n * When the source image is a part (ROI) of a bigger image, the function will try to use the pixels\n * outside of the ROI to form a border. To disable this feature and always do extrapolation, as if src\n * was not a ROI, use borderType |\n * [BORDER_ISOLATED](#d2/de8/group__core__array_1gga209f2f4869e304c82d07739337eae7c5a4fcb77ae62e1e1336c1c2b24a441995c}).\n * \n * [borderInterpolate](#d2/de8/group__core__array_1ga247f571aa6244827d3d798f13892da58})\n * \n * @param src Source image.\n * \n * @param dst Destination image of the same type as src and the size Size(src.cols+left+right,\n * src.rows+top+bottom) .\n * \n * @param top the top pixels\n * \n * @param bottom the bottom pixels\n * \n * @param left the left pixels\n * \n * @param right Parameter specifying how many pixels in each direction from the source image rectangle\n * to extrapolate. For example, top=1, bottom=1, left=1, right=1 mean that 1 pixel-wide border needs to\n * be built.\n * \n * @param borderType Border type. See borderInterpolate for details.\n * \n * @param value Border value if borderType==BORDER_CONSTANT .\n */\nexport declare function copyMakeBorder(src: InputArray, dst: OutputArray, top: int, bottom: int, left: int, right: int, borderType: int, value?: any): void\n\n/**\n * @param src source matrix.\n * \n * @param dst Destination matrix. If it does not have a proper size or type before the operation, it is\n * reallocated.\n * \n * @param mask Operation mask of the same size as *this. Its non-zero elements indicate which matrix\n * elements need to be copied. The mask has to be of type CV_8U and can have 1 or multiple channels.\n */\nexport declare function copyTo(src: InputArray, dst: OutputArray, mask: InputArray): void\n\n/**\n * The function returns the number of non-zero elements in src : `\\\\[\\\\sum _{I: \\\\; \\\\texttt{src} (I)\n * \\\\ne0 } 1\\\\]` \n * \n * [mean](#d2/de8/group__core__array_1ga191389f8a0e58180bb13a727782cd461}),\n * [meanStdDev](#d2/de8/group__core__array_1ga846c858f4004d59493d7c6a4354b301d}),\n * [norm](#dc/d84/group__core__basic_1ga4e556cb8ad35a643a1ea66e035711bb9}),\n * [minMaxLoc](#d2/de8/group__core__array_1gab473bf2eb6d14ff97e89b355dac20707}),\n * [calcCovarMatrix](#d2/de8/group__core__array_1gae6ffa9354633f984246945d52823165d})\n * \n * @param src single-channel array.\n */\nexport declare function countNonZero(src: InputArray): int\n\n/**\n * The function [cv::dct](#d2/de8/group__core__array_1ga85aad4d668c01fbd64825f589e3696d4}) performs a\n * forward or inverse discrete Cosine transform (DCT) of a 1D or 2D floating-point array:\n * \n * Forward Cosine transform of a 1D vector of N elements: `\\\\[Y = C^{(N)} \\\\cdot X\\\\]` where\n * `\\\\[C^{(N)}_{jk}= \\\\sqrt{\\\\alpha_j/N} \\\\cos \\\\left ( \\\\frac{\\\\pi(2k+1)j}{2N} \\\\right )\\\\]` and\n * `$\\\\alpha_0=1$`, `$\\\\alpha_j=2$` for *j > 0*.\n * Inverse Cosine transform of a 1D vector of N elements: `\\\\[X = \\\\left (C^{(N)} \\\\right )^{-1} \\\\cdot\n * Y = \\\\left (C^{(N)} \\\\right )^T \\\\cdot Y\\\\]` (since `$C^{(N)}$` is an orthogonal matrix, `$C^{(N)}\n * \\\\cdot \\\\left(C^{(N)}\\\\right)^T = I$` )\n * Forward 2D Cosine transform of M x N matrix: `\\\\[Y = C^{(N)} \\\\cdot X \\\\cdot \\\\left (C^{(N)} \\\\right\n * )^T\\\\]`\n * Inverse 2D Cosine transform of M x N matrix: `\\\\[X = \\\\left (C^{(N)} \\\\right )^T \\\\cdot X \\\\cdot\n * C^{(N)}\\\\]`\n * \n * The function chooses the mode of operation by looking at the flags and size of the input array:\n * \n * If (flags &\n * [DCT_INVERSE](#d2/de8/group__core__array_1ggaf4dde112b483b38175621befedda1f1ca7d18108cbce9d52e6496633c713587da}))\n * == 0 , the function does a forward 1D or 2D transform. Otherwise, it is an inverse 1D or 2D\n * transform.\n * If (flags &\n * [DCT_ROWS](#d2/de8/group__core__array_1ggaf4dde112b483b38175621befedda1f1ca49bc8de8aedbe7fabb8960445133e494}))\n * != 0 , the function performs a 1D transform of each row.\n * If the array is a single column or a single row, the function performs a 1D transform.\n * If none of the above is true, the function performs a 2D transform.\n * \n * Currently dct supports even-size arrays (2, 4, 6 ...). For data analysis and approximation, you can\n * pad the array when necessary. Also, the function performance depends very much, and not\n * monotonically, on the array size (see getOptimalDFTSize ). In the current implementation DCT of a\n * vector of size N is calculated via DFT of a vector of size N/2 . Thus, the optimal DCT size N1 >= N\n * can be calculated as: \n * \n * ```cpp\n * size_t getOptimalDCTSize(size_t N) { return 2*getOptimalDFTSize((N+1)/2); }\n * N1 = getOptimalDCTSize(N);\n * ```\n * \n * [dft](#d2/de8/group__core__array_1gadd6cf9baf2b8b704a11b5f04aaf4f39d}) ,\n * [getOptimalDFTSize](#d2/de8/group__core__array_1ga6577a2e59968936ae02eb2edde5de299}) ,\n * [idct](#d2/de8/group__core__array_1ga77b168d84e564c50228b69730a227ef2})\n * \n * @param src input floating-point array.\n * \n * @param dst output array of the same size and type as src .\n * \n * @param flags transformation flags as a combination of cv::DftFlags (DCT_*)\n */\nexport declare function dct(src: InputArray, dst: OutputArray, flags?: int): void\n\n/**\n * The function [cv::determinant](#dc/d84/group__core__basic_1ga06b8ec936c3cbc9502d76c7818053b41})\n * calculates and returns the determinant of the specified matrix. For small matrices (\n * mtx.cols=mtx.rows<=3 ), the direct method is used. For larger matrices, the function uses LU\n * factorization with partial pivoting.\n * \n * For symmetric positively-determined matrices, it is also possible to use eigen decomposition to\n * calculate the determinant. \n * \n * [trace](#dc/d84/group__core__basic_1ga36ad18631177b097a38198c4e83c6e2b}),\n * [invert](#d2/de8/group__core__array_1gad278044679d4ecf20f7622cc151aaaa2}),\n * [solve](#d2/de8/group__core__array_1ga12b43690dbd31fed96f213eefead2373}),\n * [eigen](#d2/de8/group__core__array_1ga9fa0d58657f60eaa6c71f6fbb40456e3}),\n * [MatrixExpressions](#d1/d10/classcv_1_1MatExpr_1MatrixExpressions})\n * \n * @param mtx input matrix that must have CV_32FC1 or CV_64FC1 type and square size.\n */\nexport declare function determinant(mtx: InputArray): double\n\n/**\n * The function [cv::dft](#d2/de8/group__core__array_1gadd6cf9baf2b8b704a11b5f04aaf4f39d}) performs one\n * of the following:\n * \n * Forward the Fourier transform of a 1D vector of N elements: `\\\\[Y = F^{(N)} \\\\cdot X,\\\\]` where\n * `$F^{(N)}_{jk}=\\\\exp(-2\\\\pi i j k/N)$` and `$i=\\\\sqrt{-1}$`\n * Inverse the Fourier transform of a 1D vector of N elements: `\\\\[\\\\begin{array}{l} X'= \\\\left\n * (F^{(N)} \\\\right )^{-1} \\\\cdot Y = \\\\left (F^{(N)} \\\\right )^* \\\\cdot y \\\\\\\\ X = (1/N) \\\\cdot X,\n * \\\\end{array}\\\\]` where `$F^*=\\\\left(\\\\textrm{Re}(F^{(N)})-\\\\textrm{Im}(F^{(N)})\\\\right)^T$`\n * Forward the 2D Fourier transform of a M x N matrix: `\\\\[Y = F^{(M)} \\\\cdot X \\\\cdot F^{(N)}\\\\]`\n * Inverse the 2D Fourier transform of a M x N matrix: `\\\\[\\\\begin{array}{l} X'= \\\\left (F^{(M)}\n * \\\\right )^* \\\\cdot Y \\\\cdot \\\\left (F^{(N)} \\\\right )^* \\\\\\\\ X = \\\\frac{1}{M \\\\cdot N} \\\\cdot X'\n * \\\\end{array}\\\\]`\n * \n * In case of real (single-channel) data, the output spectrum of the forward Fourier transform or input\n * spectrum of the inverse Fourier transform can be represented in a packed format called *CCS*\n * (complex-conjugate-symmetrical). It was borrowed from IPL (Intel* Image Processing Library). Here is\n * how 2D *CCS* spectrum looks: `\\\\[\\\\begin{bmatrix} Re Y_{0,0} & Re Y_{0,1} & Im Y_{0,1} & Re Y_{0,2}\n * & Im Y_{0,2} & \\\\cdots & Re Y_{0,N/2-1} & Im Y_{0,N/2-1} & Re Y_{0,N/2} \\\\\\\\ Re Y_{1,0} & Re Y_{1,1}\n * & Im Y_{1,1} & Re Y_{1,2} & Im Y_{1,2} & \\\\cdots & Re Y_{1,N/2-1} & Im Y_{1,N/2-1} & Re Y_{1,N/2}\n * \\\\\\\\ Im Y_{1,0} & Re Y_{2,1} & Im Y_{2,1} & Re Y_{2,2} & Im Y_{2,2} & \\\\cdots & Re Y_{2,N/2-1} & Im\n * Y_{2,N/2-1} & Im Y_{1,N/2} \\\\\\\\ \\\\hdotsfor{9} \\\\\\\\ Re Y_{M/2-1,0} & Re Y_{M-3,1} & Im Y_{M-3,1} &\n * \\\\hdotsfor{3} & Re Y_{M-3,N/2-1} & Im Y_{M-3,N/2-1}& Re Y_{M/2-1,N/2} \\\\\\\\ Im Y_{M/2-1,0} & Re\n * Y_{M-2,1} & Im Y_{M-2,1} & \\\\hdotsfor{3} & Re Y_{M-2,N/2-1} & Im Y_{M-2,N/2-1}& Im Y_{M/2-1,N/2}\n * \\\\\\\\ Re Y_{M/2,0} & Re Y_{M-1,1} & Im Y_{M-1,1} & \\\\hdotsfor{3} & Re Y_{M-1,N/2-1} & Im\n * Y_{M-1,N/2-1}& Re Y_{M/2,N/2} \\\\end{bmatrix}\\\\]`\n * \n * In case of 1D transform of a real vector, the output looks like the first row of the matrix above.\n * \n * So, the function chooses an operation mode depending on the flags and size of the input array:\n * \n * If\n * [DFT_ROWS](#d2/de8/group__core__array_1ggaf4dde112b483b38175621befedda1f1ca1744dc1cf1249944bc841e78c1565b7f})\n * is set or the input array has a single row or single column, the function performs a 1D forward or\n * inverse transform of each row of a matrix when\n * [DFT_ROWS](#d2/de8/group__core__array_1ggaf4dde112b483b38175621befedda1f1ca1744dc1cf1249944bc841e78c1565b7f})\n * is set. Otherwise, it performs a 2D transform.\n * If the input array is real and\n * [DFT_INVERSE](#d2/de8/group__core__array_1ggaf4dde112b483b38175621befedda1f1ca4e01d7e91cae1dbb68a26767d7b636be})\n * is not set, the function performs a forward 1D or 2D transform:\n * \n * When\n * [DFT_COMPLEX_OUTPUT](#d2/de8/group__core__array_1ggaf4dde112b483b38175621befedda1f1ca07b45079b38d60e7837dfb666a55299b})\n * is set, the output is a complex matrix of the same size as input.\n * When\n * [DFT_COMPLEX_OUTPUT](#d2/de8/group__core__array_1ggaf4dde112b483b38175621befedda1f1ca07b45079b38d60e7837dfb666a55299b})\n * is not set, the output is a real matrix of the same size as input. In case of 2D transform, it uses\n * the packed format as shown above. In case of a single 1D transform, it looks like the first row of\n * the matrix above. In case of multiple 1D transforms (when using the\n * [DFT_ROWS](#d2/de8/group__core__array_1ggaf4dde112b483b38175621befedda1f1ca1744dc1cf1249944bc841e78c1565b7f})\n * flag), each row of the output matrix looks like the first row of the matrix above.\n * \n * If the input array is complex and either\n * [DFT_INVERSE](#d2/de8/group__core__array_1ggaf4dde112b483b38175621befedda1f1ca4e01d7e91cae1dbb68a26767d7b636be})\n * or\n * [DFT_REAL_OUTPUT](#d2/de8/group__core__array_1ggaf4dde112b483b38175621befedda1f1ca28347c7846e5eaed83e019cd003e8e03})\n * are not set, the output is a complex array of the same size as input. The function performs a\n * forward or inverse 1D or 2D transform of the whole input array or each row of the input array\n * independently, depending on the flags DFT_INVERSE and DFT_ROWS.\n * When\n * [DFT_INVERSE](#d2/de8/group__core__array_1ggaf4dde112b483b38175621befedda1f1ca4e01d7e91cae1dbb68a26767d7b636be})\n * is set and the input array is real, or it is complex but\n * [DFT_REAL_OUTPUT](#d2/de8/group__core__array_1ggaf4dde112b483b38175621befedda1f1ca28347c7846e5eaed83e019cd003e8e03})\n * is set, the output is a real array of the same size as input. The function performs a 1D or 2D\n * inverse transformation of the whole input array or each individual row, depending on the flags\n * [DFT_INVERSE](#d2/de8/group__core__array_1ggaf4dde112b483b38175621befedda1f1ca4e01d7e91cae1dbb68a26767d7b636be})\n * and\n * [DFT_ROWS](#d2/de8/group__core__array_1ggaf4dde112b483b38175621befedda1f1ca1744dc1cf1249944bc841e78c1565b7f}).\n * \n * If\n * [DFT_SCALE](#d2/de8/group__core__array_1ggaf4dde112b483b38175621befedda1f1ca74746fb171aa4bfc08ace28d73f52375})\n * is set, the scaling is done after the transformation.\n * \n * Unlike dct , the function supports arrays of arbitrary size. But only those arrays are processed\n * efficiently, whose sizes can be factorized in a product of small prime numbers (2, 3, and 5 in the\n * current implementation). Such an efficient DFT size can be calculated using the getOptimalDFTSize\n * method.\n * \n * The sample below illustrates how to calculate a DFT-based convolution of two 2D real arrays: \n * \n * ```cpp\n * void convolveDFT(InputArray A, InputArray B, OutputArray C)\n * {\n *     // reallocate the output array if needed\n *     C.create(abs(A.rows - B.rows)+1, abs(A.cols - B.cols)+1, A.type());\n *     Size dftSize;\n *     // calculate the size of DFT transform\n *     dftSize.width = getOptimalDFTSize(A.cols + B.cols - 1);\n *     dftSize.height = getOptimalDFTSize(A.rows + B.rows - 1);\n * \n *     // allocate temporary buffers and initialize them with 0's\n *     Mat tempA(dftSize, A.type(), Scalar::all(0));\n *     Mat tempB(dftSize, B.type(), Scalar::all(0));\n * \n *     // copy A and B to the top-left corners of tempA and tempB, respectively\n *     Mat roiA(tempA, Rect(0,0,A.cols,A.rows));\n *     A.copyTo(roiA);\n *     Mat roiB(tempB, Rect(0,0,B.cols,B.rows));\n *     B.copyTo(roiB);\n * \n *     // now transform the padded A & B in-place;\n *     // use \"nonzeroRows\" hint for faster processing\n *     dft(tempA, tempA, 0, A.rows);\n *     dft(tempB, tempB, 0, B.rows);\n * \n *     // multiply the spectrums;\n *     // the function handles packed spectrum representations well\n *     mulSpectrums(tempA, tempB, tempA);\n * \n *     // transform the product back from the frequency domain.\n *     // Even though all the result rows will be non-zero,\n *     // you need only the first C.rows of them, and thus you\n *     // pass nonzeroRows == C.rows\n *     dft(tempA, tempA, DFT_INVERSE + DFT_SCALE, C.rows);\n * \n *     // now copy the result back to C.\n *     tempA(Rect(0, 0, C.cols, C.rows)).copyTo(C);\n * \n *     // all the temporary buffers will be deallocated automatically\n * }\n * ```\n * \n *  To optimize this sample, consider the following approaches:\n * \n * Since nonzeroRows != 0 is passed to the forward transform calls and since A and B are copied to the\n * top-left corners of tempA and tempB, respectively, it is not necessary to clear the whole tempA and\n * tempB. It is only necessary to clear the tempA.cols - A.cols ( tempB.cols - B.cols) rightmost\n * columns of the matrices.\n * This DFT-based convolution does not have to be applied to the whole big arrays, especially if B is\n * significantly smaller than A or vice versa. Instead, you can calculate convolution by parts. To do\n * this, you need to split the output array C into multiple tiles. For each tile, estimate which parts\n * of A and B are required to calculate convolution in this tile. If the tiles in C are too small, the\n * speed will decrease a lot because of repeated work. In the ultimate case, when each tile in C is a\n * single pixel, the algorithm becomes equivalent to the naive convolution algorithm. If the tiles are\n * too big, the temporary arrays tempA and tempB become too big and there is also a slowdown because of\n * bad cache locality. So, there is an optimal tile size somewhere in the middle.\n * If different tiles in C can be calculated in parallel and, thus, the convolution is done by parts,\n * the loop can be threaded.\n * \n * All of the above improvements have been implemented in\n * [matchTemplate](#df/dfb/group__imgproc__object_1ga586ebfb0a7fb604b35a23d85391329be}) and\n * [filter2D](#d4/d86/group__imgproc__filter_1ga27c049795ce870216ddfb366086b5a04}) . Therefore, by\n * using them, you can get the performance even better than with the above theoretically optimal\n * implementation. Though, those two functions actually calculate cross-correlation, not convolution,\n * so you need to \"flip\" the second convolution operand B vertically and horizontally using flip . \n * \n * An example using the discrete fourier transform can be found at\n * opencv_source_code/samples/cpp/dft.cpp\n * (Python) An example using the dft functionality to perform Wiener deconvolution can be found at\n * opencv_source/samples/python/deconvolution.py\n * (Python) An example rearranging the quadrants of a Fourier image can be found at\n * opencv_source/samples/python/dft.py \n * \n * [dct](#d2/de8/group__core__array_1ga85aad4d668c01fbd64825f589e3696d4}) ,\n * [getOptimalDFTSize](#d2/de8/group__core__array_1ga6577a2e59968936ae02eb2edde5de299}) ,\n * [mulSpectrums](#d2/de8/group__core__array_1ga3ab38646463c59bf0ce962a9d51db64f}),\n * [filter2D](#d4/d86/group__imgproc__filter_1ga27c049795ce870216ddfb366086b5a04}) ,\n * [matchTemplate](#df/dfb/group__imgproc__object_1ga586ebfb0a7fb604b35a23d85391329be}) ,\n * [flip](#d2/de8/group__core__array_1gaca7be533e3dac7feb70fc60635adf441}) ,\n * [cartToPolar](#d2/de8/group__core__array_1gac5f92f48ec32cacf5275969c33ee837d}) ,\n * [magnitude](#d2/de8/group__core__array_1ga6d3b097586bca4409873d64a90fe64c3}) ,\n * [phase](#d2/de8/group__core__array_1ga9db9ca9b4d81c3bde5677b8f64dc0137})\n * \n * @param src input array that could be real or complex.\n * \n * @param dst output array whose size and type depends on the flags .\n * \n * @param flags transformation flags, representing a combination of the DftFlags\n * \n * @param nonzeroRows when the parameter is not zero, the function assumes that only the first\n * nonzeroRows rows of the input array (DFT_INVERSE is not set) or only the first nonzeroRows of the\n * output array (DFT_INVERSE is set) contain non-zeros, thus, the function can handle the rest of the\n * rows more efficiently and save some time; this technique is very useful for calculating array\n * cross-correlation or convolution using DFT.\n */\nexport declare function dft(src: InputArray, dst: OutputArray, flags?: int, nonzeroRows?: int): void\n\n/**\n * The function [cv::divide](#d2/de8/group__core__array_1ga6db555d30115642fedae0cda05604874}) divides\n * one array by another: `\\\\[\\\\texttt{dst(I) = saturate(src1(I)*scale/src2(I))}\\\\]` or a scalar by an\n * array when there is no src1 : `\\\\[\\\\texttt{dst(I) = saturate(scale/src2(I))}\\\\]`\n * \n * Different channels of multi-channel arrays are processed independently.\n * \n * For integer types when src2(I) is zero, dst(I) will also be zero.\n * \n * In case of floating point data there is no special defined behavior for zero src2(I) values. Regular\n * floating-point division is used. Expect correct IEEE-754 behaviour for floating-point data (with\n * NaN, Inf result values).\n * \n * Saturation is not applied when the output array has the depth CV_32S. You may even get result of an\n * incorrect sign in the case of overflow. \n * \n * [multiply](#d2/de8/group__core__array_1ga979d898a58d7f61c53003e162e7ad89f}),\n * [add](#d2/de8/group__core__array_1ga10ac1bfb180e2cfda1701d06c24fdbd6}),\n * [subtract](#d2/de8/group__core__array_1gaa0f00d98b4b5edeaeb7b8333b2de353b})\n * \n * @param src1 first input array.\n * \n * @param src2 second input array of the same size and type as src1.\n * \n * @param dst output array of the same size and type as src2.\n * \n * @param scale scalar factor.\n * \n * @param dtype optional depth of the output array; if -1, dst will have depth src2.depth(), but in\n * case of an array-by-array division, you can only pass -1 when src1.depth()==src2.depth().\n */\nexport declare function divide(src1: InputArray, src2: InputArray, dst: OutputArray, scale?: double, dtype?: int): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function divide(scale: double, src2: InputArray, dst: OutputArray, dtype?: int): void\n\n/**\n * The function [cv::eigen](#d2/de8/group__core__array_1ga9fa0d58657f60eaa6c71f6fbb40456e3}) calculates\n * just eigenvalues, or eigenvalues and eigenvectors of the symmetric matrix src: \n * \n * ```cpp\n * src*eigenvectors.row(i).t() = eigenvalues.at<srcType>(i)*eigenvectors.row(i).t()\n * ```\n * \n * Use [cv::eigenNonSymmetric](#d2/de8/group__core__array_1gaf51987e03cac8d171fbd2b327cf966f6}) for\n * calculation of real eigenvalues and eigenvectors of non-symmetric matrix.\n * \n * [eigenNonSymmetric](#d2/de8/group__core__array_1gaf51987e03cac8d171fbd2b327cf966f6}),\n * [completeSymm](#d2/de8/group__core__array_1ga6847337c0c55769e115a70e0f011b5ca}) ,\n * [PCA](#d3/d8d/classcv_1_1PCA})\n * \n * @param src input matrix that must have CV_32FC1 or CV_64FC1 type, square size and be symmetrical\n * (src ^T^ == src).\n * \n * @param eigenvalues output vector of eigenvalues of the same type as src; the eigenvalues are stored\n * in the descending order.\n * \n * @param eigenvectors output matrix of eigenvectors; it has the same size and type as src; the\n * eigenvectors are stored as subsequent matrix rows, in the same order as the corresponding\n * eigenvalues.\n */\nexport declare function eigen(src: InputArray, eigenvalues: OutputArray, eigenvectors?: OutputArray): bool\n\n/**\n * Assumes real eigenvalues.\n * The function calculates eigenvalues and eigenvectors (optional) of the square matrix src: \n * \n * ```cpp\n * src*eigenvectors.row(i).t() = eigenvalues.at<srcType>(i)*eigenvectors.row(i).t()\n * ```\n * \n * [eigen](#d2/de8/group__core__array_1ga9fa0d58657f60eaa6c71f6fbb40456e3})\n * \n * @param src input matrix (CV_32FC1 or CV_64FC1 type).\n * \n * @param eigenvalues output vector of eigenvalues (type is the same type as src).\n * \n * @param eigenvectors output matrix of eigenvectors (type is the same type as src). The eigenvectors\n * are stored as subsequent matrix rows, in the same order as the corresponding eigenvalues.\n */\nexport declare function eigenNonSymmetric(src: InputArray, eigenvalues: OutputArray, eigenvectors: OutputArray): void\n\n/**\n * The function [cv::exp](#d7/dcc/group__core__utils__softfloat_1ga1eb07a682abff20e0864104599c06fbc})\n * calculates the exponent of every element of the input array: `\\\\[\\\\texttt{dst} [I] = e^{ src(I)\n * }\\\\]`\n * \n * The maximum relative error is about 7e-6 for single-precision input and less than 1e-10 for\n * double-precision input. Currently, the function converts denormalized values to zeros on output.\n * Special values (NaN, Inf) are not handled. \n * \n * [log](#d7/dcc/group__core__utils__softfloat_1gae5de78ee278fe88405c6dbc38502f7c1}) ,\n * [cartToPolar](#d2/de8/group__core__array_1gac5f92f48ec32cacf5275969c33ee837d}) ,\n * [polarToCart](#d2/de8/group__core__array_1ga581ff9d44201de2dd1b40a50db93d665}) ,\n * [phase](#d2/de8/group__core__array_1ga9db9ca9b4d81c3bde5677b8f64dc0137}) ,\n * [pow](#d7/dcc/group__core__utils__softfloat_1ga8bc36646a43b82baa15f151a973fb0c5}) ,\n * [sqrt](#d7/dcc/group__core__utils__softfloat_1ga682082a1892db64a2856403ec17ba297}) ,\n * [magnitude](#d2/de8/group__core__array_1ga6d3b097586bca4409873d64a90fe64c3})\n * \n * @param src input array.\n * \n * @param dst output array of the same size and type as src.\n */\nexport declare function exp(src: InputArray, dst: OutputArray): void\n\n/**\n * [mixChannels](#d2/de8/group__core__array_1ga51d768c270a1cdd3497255017c4504be}),\n * [split](#d2/de8/group__core__array_1ga0547c7fed86152d7e9d0096029c8518a})\n * \n * @param src input array\n * \n * @param dst output array\n * \n * @param coi index of channel to extract\n */\nexport declare function extractChannel(src: InputArray, dst: OutputArray, coi: int): void\n\n/**\n * Given a binary matrix (likely returned from an operation such as\n * [threshold()](#d7/d1b/group__imgproc__misc_1gae8a4a146d1ca78c626a53577199e9c57}),\n * [compare()](#d2/de8/group__core__array_1ga303cfb72acf8cbb36d884650c09a3a97}), >, ==, etc, return all\n * of the non-zero indices as a [cv::Mat](#d3/d63/classcv_1_1Mat}) or std::vector<cv::Point> (x,y) For\n * example: \n * \n * ```cpp\n * cv::Mat binaryImage; // input, binary image\n * cv::Mat locations;   // output, locations of non-zero pixels\n * cv::findNonZero(binaryImage, locations);\n * \n * // access pixel coordinates\n * Point pnt = locations.at<Point>(i);\n * ```\n * \n *  or \n * \n * ```cpp\n * cv::Mat binaryImage; // input, binary image\n * vector<Point> locations;   // output, locations of non-zero pixels\n * cv::findNonZero(binaryImage, locations);\n * \n * // access pixel coordinates\n * Point pnt = locations[i];\n * ```\n * \n * @param src single-channel array\n * \n * @param idx the output array, type of cv::Mat or std::vector<Point>, corresponding to non-zero\n * indices in the input\n */\nexport declare function findNonZero(src: InputArray, idx: OutputArray): void\n\n/**\n * The function [cv::flip](#d2/de8/group__core__array_1gaca7be533e3dac7feb70fc60635adf441}) flips the\n * array in one of three different ways (row and column indices are 0-based): `\\\\[\\\\texttt{dst} _{ij} =\n * \\\\left\\\\{ \\\\begin{array}{l l} \\\\texttt{src} _{\\\\texttt{src.rows}-i-1,j} & if\\\\; \\\\texttt{flipCode} =\n * 0 \\\\\\\\ \\\\texttt{src} _{i, \\\\texttt{src.cols} -j-1} & if\\\\; \\\\texttt{flipCode} > 0 \\\\\\\\ \\\\texttt{src}\n * _{ \\\\texttt{src.rows} -i-1, \\\\texttt{src.cols} -j-1} & if\\\\; \\\\texttt{flipCode} < 0 \\\\\\\\\n * \\\\end{array} \\\\right.\\\\]` The example scenarios of using the function are the following: Vertical\n * flipping of the image (flipCode == 0) to switch between top-left and bottom-left image origin. This\n * is a typical operation in video processing on Microsoft Windows* OS. Horizontal flipping of the\n * image with the subsequent horizontal shift and absolute difference calculation to check for a\n * vertical-axis symmetry (flipCode > 0). Simultaneous horizontal and vertical flipping of the image\n * with the subsequent shift and absolute difference calculation to check for a central symmetry\n * (flipCode < 0). Reversing the order of point arrays (flipCode > 0 or flipCode == 0). \n * \n * [transpose](#d2/de8/group__core__array_1ga46630ed6c0ea6254a35f447289bd7404}) ,\n * [repeat](#d2/de8/group__core__array_1ga496c3860f3ac44c40b48811333cfda2d}) ,\n * [completeSymm](#d2/de8/group__core__array_1ga6847337c0c55769e115a70e0f011b5ca})\n * \n * @param src input array.\n * \n * @param dst output array of the same size and type as src.\n * \n * @param flipCode a flag to specify how to flip the array; 0 means flipping around the x-axis and\n * positive value (for example, 1) means flipping around y-axis. Negative value (for example, -1) means\n * flipping around both axes.\n */\nexport declare function flip(src: InputArray, dst: OutputArray, flipCode: int): void\n\n/**\n * The function [cv::gemm](#d2/de8/group__core__array_1gacb6e64071dffe36434e1e7ee79e7cb35}) performs\n * generalized matrix multiplication similar to the gemm functions in BLAS level 3. For example,\n * `gemm(src1, src2, alpha, src3, beta, dst, GEMM_1_T + GEMM_3_T)` corresponds to `\\\\[\\\\texttt{dst} =\n * \\\\texttt{alpha} \\\\cdot \\\\texttt{src1} ^T \\\\cdot \\\\texttt{src2} + \\\\texttt{beta} \\\\cdot\n * \\\\texttt{src3} ^T\\\\]`\n * \n * In case of complex (two-channel) data, performed a complex matrix multiplication.\n * \n * The function can be replaced with a matrix expression. For example, the above call can be replaced\n * with: \n * \n * ```cpp\n * dst = alpha*src1.t()*src2 + beta*src3.t();\n * ```\n * \n * [mulTransposed](#d2/de8/group__core__array_1gadc4e49f8f7a155044e3be1b9e3b270ab}) ,\n * [transform](#d2/de8/group__core__array_1ga393164aa54bb9169ce0a8cc44e08ff22})\n * \n * @param src1 first multiplied input matrix that could be real(CV_32FC1, CV_64FC1) or\n * complex(CV_32FC2, CV_64FC2).\n * \n * @param src2 second multiplied input matrix of the same type as src1.\n * \n * @param alpha weight of the matrix product.\n * \n * @param src3 third optional delta matrix added to the matrix product; it should have the same type as\n * src1 and src2.\n * \n * @param beta weight of src3.\n * \n * @param dst output matrix; it has the proper size and the same type as input matrices.\n * \n * @param flags operation flags (cv::GemmFlags)\n */\nexport declare function gemm(src1: InputArray, src2: InputArray, alpha: double, src3: InputArray, beta: double, dst: OutputArray, flags?: int): void\n\n/**\n * DFT performance is not a monotonic function of a vector size. Therefore, when you calculate\n * convolution of two arrays or perform the spectral analysis of an array, it usually makes sense to\n * pad the input data with zeros to get a bit larger array that can be transformed much faster than the\n * original one. Arrays whose size is a power-of-two (2, 4, 8, 16, 32, ...) are the fastest to process.\n * Though, the arrays whose size is a product of 2's, 3's, and 5's (for example, 300 = 5*5*3*2*2) are\n * also processed quite efficiently.\n * \n * The function\n * [cv::getOptimalDFTSize](#d2/de8/group__core__array_1ga6577a2e59968936ae02eb2edde5de299}) returns the\n * minimum number N that is greater than or equal to vecsize so that the DFT of a vector of size N can\n * be processed efficiently. In the current implementation N = 2 ^p^ * 3 ^q^ * 5 ^r^ for some integer\n * p, q, r.\n * \n * The function returns a negative number if vecsize is too large (very close to INT_MAX ).\n * \n * While the function cannot be used directly to estimate the optimal vector size for DCT transform\n * (since the current DCT implementation supports only even-size vectors), it can be easily processed\n * as getOptimalDFTSize((vecsize+1)/2)*2. \n * \n * [dft](#d2/de8/group__core__array_1gadd6cf9baf2b8b704a11b5f04aaf4f39d}) ,\n * [dct](#d2/de8/group__core__array_1ga85aad4d668c01fbd64825f589e3696d4}) ,\n * [idft](#d2/de8/group__core__array_1gaa708aa2d2e57a508f968eb0f69aa5ff1}) ,\n * [idct](#d2/de8/group__core__array_1ga77b168d84e564c50228b69730a227ef2}) ,\n * [mulSpectrums](#d2/de8/group__core__array_1ga3ab38646463c59bf0ce962a9d51db64f})\n * \n * @param vecsize vector size.\n */\nexport declare function getOptimalDFTSize(vecsize: int): int\n\n/**\n * The function horizontally concatenates two or more [cv::Mat](#d3/d63/classcv_1_1Mat}) matrices (with\n * the same number of rows). \n * \n * ```cpp\n * cv::Mat matArray[] = { cv::Mat(4, 1, CV_8UC1, cv::Scalar(1)),\n *                        cv::Mat(4, 1, CV_8UC1, cv::Scalar(2)),\n *                        cv::Mat(4, 1, CV_8UC1, cv::Scalar(3)),};\n * \n * cv::Mat out;\n * cv::hconcat( matArray, 3, out );\n * //out:\n * //[1, 2, 3;\n * // 1, 2, 3;\n * // 1, 2, 3;\n * // 1, 2, 3]\n * ```\n * \n * [cv::vconcat(const Mat*, size_t,\n * OutputArray)](#d2/de8/group__core__array_1ga744f53b69f6e4f12156cdde4e76aed27}), \n * \n * [cv::vconcat(InputArrayOfArrays,\n * OutputArray)](#d2/de8/group__core__array_1ga558e169e15adcc46b8cdcc6cd215070f}) and \n * \n * [cv::vconcat(InputArray, InputArray,\n * OutputArray)](#d2/de8/group__core__array_1gaad07cede730cdde64b90e987aad179b8})\n * \n * @param src input array or vector of matrices. all of the matrices must have the same number of rows\n * and the same depth.\n * \n * @param nsrc number of matrices in src.\n * \n * @param dst output array. It has the same number of rows and depth as the src, and the sum of cols of\n * the src.\n */\nexport declare function hconcat(src: any, nsrc: size_t, dst: OutputArray): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts. \n * \n * ```cpp\n * cv::Mat_<float> A = (cv::Mat_<float>(3, 2) << 1, 4,\n *                                               2, 5,\n *                                               3, 6);\n * cv::Mat_<float> B = (cv::Mat_<float>(3, 2) << 7, 10,\n *                                               8, 11,\n *                                               9, 12);\n * \n * cv::Mat C;\n * cv::hconcat(A, B, C);\n * //C:\n * //[1, 4, 7, 10;\n * // 2, 5, 8, 11;\n * // 3, 6, 9, 12]\n * ```\n * \n * @param src1 first input array to be considered for horizontal concatenation.\n * \n * @param src2 second input array to be considered for horizontal concatenation.\n * \n * @param dst output array. It has the same number of rows and depth as the src1 and src2, and the sum\n * of cols of the src1 and src2.\n */\nexport declare function hconcat(src1: InputArray, src2: InputArray, dst: OutputArray): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts. \n * \n * ```cpp\n * std::vector<cv::Mat> matrices = { cv::Mat(4, 1, CV_8UC1, cv::Scalar(1)),\n *                                   cv::Mat(4, 1, CV_8UC1, cv::Scalar(2)),\n *                                   cv::Mat(4, 1, CV_8UC1, cv::Scalar(3)),};\n * \n * cv::Mat out;\n * cv::hconcat( matrices, out );\n * //out:\n * //[1, 2, 3;\n * // 1, 2, 3;\n * // 1, 2, 3;\n * // 1, 2, 3]\n * ```\n * \n * @param src input array or vector of matrices. all of the matrices must have the same number of rows\n * and the same depth.\n * \n * @param dst output array. It has the same number of rows and depth as the src, and the sum of cols of\n * the src. same depth.\n */\nexport declare function hconcat(src: InputArrayOfArrays, dst: OutputArray): void\n\n/**\n * idct(src, dst, flags) is equivalent to dct(src, dst, flags | DCT_INVERSE). \n * \n * [dct](#d2/de8/group__core__array_1ga85aad4d668c01fbd64825f589e3696d4}),\n * [dft](#d2/de8/group__core__array_1gadd6cf9baf2b8b704a11b5f04aaf4f39d}),\n * [idft](#d2/de8/group__core__array_1gaa708aa2d2e57a508f968eb0f69aa5ff1}),\n * [getOptimalDFTSize](#d2/de8/group__core__array_1ga6577a2e59968936ae02eb2edde5de299})\n * \n * @param src input floating-point single-channel array.\n * \n * @param dst output array of the same size and type as src.\n * \n * @param flags operation flags.\n */\nexport declare function idct(src: InputArray, dst: OutputArray, flags?: int): void\n\n/**\n * idft(src, dst, flags) is equivalent to dft(src, dst, flags |\n * [DFT_INVERSE](#d2/de8/group__core__array_1ggaf4dde112b483b38175621befedda1f1ca4e01d7e91cae1dbb68a26767d7b636be}))\n * . \n * \n * None of dft and idft scales the result by default. So, you should pass\n * [DFT_SCALE](#d2/de8/group__core__array_1ggaf4dde112b483b38175621befedda1f1ca74746fb171aa4bfc08ace28d73f52375})\n * to one of dft or idft explicitly to make these transforms mutually inverse. \n * \n * [dft](#d2/de8/group__core__array_1gadd6cf9baf2b8b704a11b5f04aaf4f39d}),\n * [dct](#d2/de8/group__core__array_1ga85aad4d668c01fbd64825f589e3696d4}),\n * [idct](#d2/de8/group__core__array_1ga77b168d84e564c50228b69730a227ef2}),\n * [mulSpectrums](#d2/de8/group__core__array_1ga3ab38646463c59bf0ce962a9d51db64f}),\n * [getOptimalDFTSize](#d2/de8/group__core__array_1ga6577a2e59968936ae02eb2edde5de299})\n * \n * @param src input floating-point real or complex array.\n * \n * @param dst output array whose size and type depend on the flags.\n * \n * @param flags operation flags (see dft and DftFlags).\n * \n * @param nonzeroRows number of dst rows to process; the rest of the rows have undefined content (see\n * the convolution sample in dft description.\n */\nexport declare function idft(src: InputArray, dst: OutputArray, flags?: int, nonzeroRows?: int): void\n\n/**\n * The function checks the range as follows:\n * \n * For every element of a single-channel input array: `\\\\[\\\\texttt{dst} (I)= \\\\texttt{lowerb} (I)_0\n * \\\\leq \\\\texttt{src} (I)_0 \\\\leq \\\\texttt{upperb} (I)_0\\\\]`\n * For two-channel arrays: `\\\\[\\\\texttt{dst} (I)= \\\\texttt{lowerb} (I)_0 \\\\leq \\\\texttt{src} (I)_0\n * \\\\leq \\\\texttt{upperb} (I)_0 \\\\land \\\\texttt{lowerb} (I)_1 \\\\leq \\\\texttt{src} (I)_1 \\\\leq\n * \\\\texttt{upperb} (I)_1\\\\]`\n * and so forth.\n * \n * That is, dst (I) is set to 255 (all 1 -bits) if src (I) is within the specified 1D, 2D, 3D, ... box\n * and 0 otherwise.\n * \n * When the lower and/or upper boundary parameters are scalars, the indexes (I) at lowerb and upperb in\n * the above formulas should be omitted.\n * \n * @param src first input array.\n * \n * @param lowerb inclusive lower boundary array or a scalar.\n * \n * @param upperb inclusive upper boundary array or a scalar.\n * \n * @param dst output array of the same size as src and CV_8U type.\n */\nexport declare function inRange(src: InputArray, lowerb: InputArray, upperb: InputArray, dst: OutputArray): void\n\n/**\n * [mixChannels](#d2/de8/group__core__array_1ga51d768c270a1cdd3497255017c4504be}),\n * [merge](#d2/de8/group__core__array_1ga7d7b4d6c6ee504b30a20b1680029c7b4})\n * \n * @param src input array\n * \n * @param dst output array\n * \n * @param coi index of channel for insertion\n */\nexport declare function insertChannel(src: InputArray, dst: InputOutputArray, coi: int): void\n\n/**\n * The function [cv::invert](#d2/de8/group__core__array_1gad278044679d4ecf20f7622cc151aaaa2}) inverts\n * the matrix src and stores the result in dst . When the matrix src is singular or non-square, the\n * function calculates the pseudo-inverse matrix (the dst matrix) so that norm(src*dst - I) is minimal,\n * where I is an identity matrix.\n * \n * In case of the\n * [DECOMP_LU](#d2/de8/group__core__array_1ggaaf9ea5dcc392d5ae04eacb9920b9674ca247a3455cd64973152e17e26999dc024})\n * method, the function returns non-zero value if the inverse has been successfully calculated and 0 if\n * src is singular.\n * \n * In case of the\n * [DECOMP_SVD](#d2/de8/group__core__array_1ggaaf9ea5dcc392d5ae04eacb9920b9674ca523b676c90c7a1d2841b1267ba9ba614})\n * method, the function returns the inverse condition number of src (the ratio of the smallest singular\n * value to the largest singular value) and 0 if src is singular. The [SVD](#df/df7/classcv_1_1SVD})\n * method calculates a pseudo-inverse matrix if src is singular.\n * \n * Similarly to\n * [DECOMP_LU](#d2/de8/group__core__array_1ggaaf9ea5dcc392d5ae04eacb9920b9674ca247a3455cd64973152e17e26999dc024}),\n * the method\n * [DECOMP_CHOLESKY](#d2/de8/group__core__array_1ggaaf9ea5dcc392d5ae04eacb9920b9674ca33cf860f98004310374a81d2c01715da})\n * works only with non-singular square matrices that should also be symmetrical and positively defined.\n * In this case, the function stores the inverted matrix in dst and returns non-zero. Otherwise, it\n * returns 0.\n * \n * [solve](#d2/de8/group__core__array_1ga12b43690dbd31fed96f213eefead2373}),\n * [SVD](#df/df7/classcv_1_1SVD})\n * \n * @param src input floating-point M x N matrix.\n * \n * @param dst output matrix of N x M size and the same type as src.\n * \n * @param flags inversion method (cv::DecompTypes)\n */\nexport declare function invert(src: InputArray, dst: OutputArray, flags?: int): double\n\n/**\n * The function [cv::log](#d7/dcc/group__core__utils__softfloat_1gae5de78ee278fe88405c6dbc38502f7c1})\n * calculates the natural logarithm of every element of the input array: `\\\\[\\\\texttt{dst} (I) = \\\\log\n * (\\\\texttt{src}(I)) \\\\]`\n * \n * Output on zero, negative and special (NaN, Inf) values is undefined.\n * \n * [exp](#d7/dcc/group__core__utils__softfloat_1ga1eb07a682abff20e0864104599c06fbc}),\n * [cartToPolar](#d2/de8/group__core__array_1gac5f92f48ec32cacf5275969c33ee837d}),\n * [polarToCart](#d2/de8/group__core__array_1ga581ff9d44201de2dd1b40a50db93d665}),\n * [phase](#d2/de8/group__core__array_1ga9db9ca9b4d81c3bde5677b8f64dc0137}),\n * [pow](#d7/dcc/group__core__utils__softfloat_1ga8bc36646a43b82baa15f151a973fb0c5}),\n * [sqrt](#d7/dcc/group__core__utils__softfloat_1ga682082a1892db64a2856403ec17ba297}),\n * [magnitude](#d2/de8/group__core__array_1ga6d3b097586bca4409873d64a90fe64c3})\n * \n * @param src input array.\n * \n * @param dst output array of the same size and type as src .\n */\nexport declare function log(src: InputArray, dst: OutputArray): void\n\n/**\n * The function LUT fills the output array with values from the look-up table. Indices of the entries\n * are taken from the input array. That is, the function processes each element of src as follows:\n * `\\\\[\\\\texttt{dst} (I) \\\\leftarrow \\\\texttt{lut(src(I) + d)}\\\\]` where `\\\\[d = \\\\fork{0}{if\n * \\\\(\\\\texttt{src}\\\\) has depth \\\\(\\\\texttt{CV_8U}\\\\)}{128}{if \\\\(\\\\texttt{src}\\\\) has depth\n * \\\\(\\\\texttt{CV_8S}\\\\)}\\\\]` \n * \n * [convertScaleAbs](#d2/de8/group__core__array_1ga3460e9c9f37b563ab9dd550c4d8c4e7d}),\n * [Mat::convertTo](#d3/d63/classcv_1_1Mat_1adf88c60c5b4980e05bb556080916978b})\n * \n * @param src input array of 8-bit elements.\n * \n * @param lut look-up table of 256 elements; in case of multi-channel input array, the table should\n * either have a single channel (in this case the same table is used for all channels) or the same\n * number of channels as in the input array.\n * \n * @param dst output array of the same size and number of channels as src, and the same depth as lut.\n */\nexport declare function LUT(src: InputArray, lut: InputArray, dst: OutputArray): void\n\n/**\n * The function [cv::magnitude](#d2/de8/group__core__array_1ga6d3b097586bca4409873d64a90fe64c3})\n * calculates the magnitude of 2D vectors formed from the corresponding elements of x and y arrays:\n * `\\\\[\\\\texttt{dst} (I) = \\\\sqrt{\\\\texttt{x}(I)^2 + \\\\texttt{y}(I)^2}\\\\]` \n * \n * [cartToPolar](#d2/de8/group__core__array_1gac5f92f48ec32cacf5275969c33ee837d}),\n * [polarToCart](#d2/de8/group__core__array_1ga581ff9d44201de2dd1b40a50db93d665}),\n * [phase](#d2/de8/group__core__array_1ga9db9ca9b4d81c3bde5677b8f64dc0137}),\n * [sqrt](#d7/dcc/group__core__utils__softfloat_1ga682082a1892db64a2856403ec17ba297})\n * \n * @param x floating-point array of x-coordinates of the vectors.\n * \n * @param y floating-point array of y-coordinates of the vectors; it must have the same size as x.\n * \n * @param magnitude output array of the same size and type as x.\n */\nexport declare function magnitude(x: InputArray, y: InputArray, magnitude: OutputArray): void\n\n/**\n * The function [cv::Mahalanobis](#d2/de8/group__core__array_1ga4493aee129179459cbfc6064f051aa7d})\n * calculates and returns the weighted distance between two vectors: `\\\\[d( \\\\texttt{vec1} ,\n * \\\\texttt{vec2} )=\n * \\\\sqrt{\\\\sum_{i,j}{\\\\texttt{icovar(i,j)}\\\\cdot(\\\\texttt{vec1}(I)-\\\\texttt{vec2}(I))\\\\cdot(\\\\texttt{vec1(j)}-\\\\texttt{vec2(j)})}\n * }\\\\]` The covariance matrix may be calculated using the\n * [calcCovarMatrix](#d2/de8/group__core__array_1gae6ffa9354633f984246945d52823165d}) function and then\n * inverted using the invert function (preferably using the\n * [DECOMP_SVD](#d2/de8/group__core__array_1ggaaf9ea5dcc392d5ae04eacb9920b9674ca523b676c90c7a1d2841b1267ba9ba614})\n * method, as the most accurate).\n * \n * @param v1 first 1D input vector.\n * \n * @param v2 second 1D input vector.\n * \n * @param icovar inverse covariance matrix.\n */\nexport declare function Mahalanobis(v1: InputArray, v2: InputArray, icovar: InputArray): double\n\n/**\n * The function [cv::max](#d7/dcc/group__core__utils__softfloat_1ga78f988f6cfa6223610298cbd4f86ec66})\n * calculates the per-element maximum of two arrays: `\\\\[\\\\texttt{dst} (I)= \\\\max ( \\\\texttt{src1} (I),\n * \\\\texttt{src2} (I))\\\\]` or array and a scalar: `\\\\[\\\\texttt{dst} (I)= \\\\max ( \\\\texttt{src1} (I),\n * \\\\texttt{value} )\\\\]` \n * \n * [min](#d7/dcc/group__core__utils__softfloat_1gac48df53b8fd34b87e7b121fa8fd4c379}),\n * [compare](#d2/de8/group__core__array_1ga303cfb72acf8cbb36d884650c09a3a97}),\n * [inRange](#d2/de8/group__core__array_1ga48af0ab51e36436c5d04340e036ce981}),\n * [minMaxLoc](#d2/de8/group__core__array_1gab473bf2eb6d14ff97e89b355dac20707}),\n * [MatrixExpressions](#d1/d10/classcv_1_1MatExpr_1MatrixExpressions})\n * \n * @param src1 first input array.\n * \n * @param src2 second input array of the same size and type as src1 .\n * \n * @param dst output array of the same size and type as src1.\n */\nexport declare function max(src1: InputArray, src2: InputArray, dst: OutputArray): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts. needed to avoid conflicts with const _Tp& std::min(const _Tp&,\n * const _Tp&, _Compare)\n */\nexport declare function max(src1: any, src2: any, dst: any): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts. needed to avoid conflicts with const _Tp& std::min(const _Tp&,\n * const _Tp&, _Compare)\n */\nexport declare function max(src1: any, src2: any, dst: any): void\n\n/**\n * The function [cv::mean](#d2/de8/group__core__array_1ga191389f8a0e58180bb13a727782cd461}) calculates\n * the mean value M of array elements, independently for each channel, and return it:\n * `\\\\[\\\\begin{array}{l} N = \\\\sum _{I: \\\\; \\\\texttt{mask} (I) \\\\ne 0} 1 \\\\\\\\ M_c = \\\\left ( \\\\sum _{I:\n * \\\\; \\\\texttt{mask} (I) \\\\ne 0}{ \\\\texttt{mtx} (I)_c} \\\\right )/N \\\\end{array}\\\\]` When all the mask\n * elements are 0's, the function returns Scalar::all(0) \n * \n * [countNonZero](#d2/de8/group__core__array_1gaa4b89393263bb4d604e0fe5986723914}),\n * [meanStdDev](#d2/de8/group__core__array_1ga846c858f4004d59493d7c6a4354b301d}),\n * [norm](#dc/d84/group__core__basic_1ga4e556cb8ad35a643a1ea66e035711bb9}),\n * [minMaxLoc](#d2/de8/group__core__array_1gab473bf2eb6d14ff97e89b355dac20707})\n * \n * @param src input array that should have from 1 to 4 channels so that the result can be stored in\n * Scalar_ .\n * \n * @param mask optional operation mask.\n */\nexport declare function mean(src: InputArray, mask?: InputArray): Scalar\n\n/**\n * Calculates a mean and standard deviation of array elements.\n * \n * The function [cv::meanStdDev](#d2/de8/group__core__array_1ga846c858f4004d59493d7c6a4354b301d})\n * calculates the mean and the standard deviation M of array elements independently for each channel\n * and returns it via the output parameters: `\\\\[\\\\begin{array}{l} N = \\\\sum _{I, \\\\texttt{mask} (I)\n * \\\\ne 0} 1 \\\\\\\\ \\\\texttt{mean} _c = \\\\frac{\\\\sum_{ I: \\\\; \\\\texttt{mask}(I) \\\\ne 0} \\\\texttt{src}\n * (I)_c}{N} \\\\\\\\ \\\\texttt{stddev} _c = \\\\sqrt{\\\\frac{\\\\sum_{ I: \\\\; \\\\texttt{mask}(I) \\\\ne 0} \\\\left (\n * \\\\texttt{src} (I)_c - \\\\texttt{mean} _c \\\\right )^2}{N}} \\\\end{array}\\\\]` When all the mask elements\n * are 0's, the function returns mean=stddev=Scalar::all(0). \n * \n * The calculated standard deviation is only the diagonal of the complete normalized covariance matrix.\n * If the full matrix is needed, you can reshape the multi-channel array M x N to the single-channel\n * array M*N x mtx.channels() (only possible when the matrix is continuous) and then pass the matrix to\n * calcCovarMatrix . \n * \n * [countNonZero](#d2/de8/group__core__array_1gaa4b89393263bb4d604e0fe5986723914}),\n * [mean](#d2/de8/group__core__array_1ga191389f8a0e58180bb13a727782cd461}),\n * [norm](#dc/d84/group__core__basic_1ga4e556cb8ad35a643a1ea66e035711bb9}),\n * [minMaxLoc](#d2/de8/group__core__array_1gab473bf2eb6d14ff97e89b355dac20707}),\n * [calcCovarMatrix](#d2/de8/group__core__array_1gae6ffa9354633f984246945d52823165d})\n * \n * @param src input array that should have from 1 to 4 channels so that the results can be stored in\n * Scalar_ 's.\n * \n * @param mean output parameter: calculated mean value.\n * \n * @param stddev output parameter: calculated standard deviation.\n * \n * @param mask optional operation mask.\n */\nexport declare function meanStdDev(src: InputArray, mean: OutputArray, stddev: OutputArray, mask?: InputArray): void\n\n/**\n * The function [cv::merge](#d2/de8/group__core__array_1ga7d7b4d6c6ee504b30a20b1680029c7b4}) merges\n * several arrays to make a single multi-channel array. That is, each element of the output array will\n * be a concatenation of the elements of the input arrays, where elements of i-th input array are\n * treated as mv[i].channels()-element vectors.\n * \n * The function [cv::split](#d2/de8/group__core__array_1ga0547c7fed86152d7e9d0096029c8518a}) does the\n * reverse operation. If you need to shuffle channels in some other advanced way, use\n * [cv::mixChannels](#d2/de8/group__core__array_1ga51d768c270a1cdd3497255017c4504be}).\n * \n * The following example shows how to merge 3 single channel matrices into a single 3-channel matrix. \n * \n * ```cpp\n *     Mat m1 = (Mat_<uchar>(2,2) << 1,4,7,10);\n *     Mat m2 = (Mat_<uchar>(2,2) << 2,5,8,11);\n *     Mat m3 = (Mat_<uchar>(2,2) << 3,6,9,12);\n * \n *     Mat channels[3] = {m1, m2, m3};\n *     Mat m;\n *     merge(channels, 3, m);\n *     /*\n *     m =\n *     [  1,   2,   3,   4,   5,   6;\n *        7,   8,   9,  10,  11,  12]\n *     m.channels() = 3\n * \\/\n * ```\n * \n * [mixChannels](#d2/de8/group__core__array_1ga51d768c270a1cdd3497255017c4504be}),\n * [split](#d2/de8/group__core__array_1ga0547c7fed86152d7e9d0096029c8518a}),\n * [Mat::reshape](#d3/d63/classcv_1_1Mat_1a4eb96e3251417fa88b78e2abd6cfd7d8})\n * \n * @param mv input array of matrices to be merged; all the matrices in mv must have the same size and\n * the same depth.\n * \n * @param count number of input matrices when mv is a plain C array; it must be greater than zero.\n * \n * @param dst output array of the same size and the same depth as mv[0]; The number of channels will be\n * equal to the parameter count.\n */\nexport declare function merge(mv: any, count: size_t, dst: OutputArray): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n * \n * @param mv input vector of matrices to be merged; all the matrices in mv must have the same size and\n * the same depth.\n * \n * @param dst output array of the same size and the same depth as mv[0]; The number of channels will be\n * the total number of channels in the matrix array.\n */\nexport declare function merge(mv: InputArrayOfArrays, dst: OutputArray): void\n\n/**\n * The function [cv::min](#d7/dcc/group__core__utils__softfloat_1gac48df53b8fd34b87e7b121fa8fd4c379})\n * calculates the per-element minimum of two arrays: `\\\\[\\\\texttt{dst} (I)= \\\\min ( \\\\texttt{src1} (I),\n * \\\\texttt{src2} (I))\\\\]` or array and a scalar: `\\\\[\\\\texttt{dst} (I)= \\\\min ( \\\\texttt{src1} (I),\n * \\\\texttt{value} )\\\\]` \n * \n * [max](#d7/dcc/group__core__utils__softfloat_1ga78f988f6cfa6223610298cbd4f86ec66}),\n * [compare](#d2/de8/group__core__array_1ga303cfb72acf8cbb36d884650c09a3a97}),\n * [inRange](#d2/de8/group__core__array_1ga48af0ab51e36436c5d04340e036ce981}),\n * [minMaxLoc](#d2/de8/group__core__array_1gab473bf2eb6d14ff97e89b355dac20707})\n * \n * @param src1 first input array.\n * \n * @param src2 second input array of the same size and type as src1.\n * \n * @param dst output array of the same size and type as src1.\n */\nexport declare function min(src1: InputArray, src2: InputArray, dst: OutputArray): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts. needed to avoid conflicts with const _Tp& std::min(const _Tp&,\n * const _Tp&, _Compare)\n */\nexport declare function min(src1: any, src2: any, dst: any): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts. needed to avoid conflicts with const _Tp& std::min(const _Tp&,\n * const _Tp&, _Compare)\n */\nexport declare function min(src1: any, src2: any, dst: any): void\n\n/**\n * The function [cv::minMaxIdx](#d2/de8/group__core__array_1ga7622c466c628a75d9ed008b42250a73f}) finds\n * the minimum and maximum element values and their positions. The extremums are searched across the\n * whole array or, if mask is not an empty array, in the specified array region. The function does not\n * work with multi-channel arrays. If you need to find minimum or maximum elements across all the\n * channels, use [Mat::reshape](#d3/d63/classcv_1_1Mat_1a4eb96e3251417fa88b78e2abd6cfd7d8}) first to\n * reinterpret the array as single-channel. Or you may extract the particular channel using either\n * extractImageCOI , or mixChannels , or split . In case of a sparse matrix, the minimum is found among\n * non-zero elements only. \n * \n * When minIdx is not NULL, it must have at least 2 elements (as well as maxIdx), even if src is a\n * single-row or single-column matrix. In OpenCV (following MATLAB) each array has at least 2\n * dimensions, i.e. single-column matrix is Mx1 matrix (and therefore minIdx/maxIdx will be\n * (i1,0)/(i2,0)) and single-row matrix is 1xN matrix (and therefore minIdx/maxIdx will be\n * (0,j1)/(0,j2)).\n * \n * @param src input single-channel array.\n * \n * @param minVal pointer to the returned minimum value; NULL is used if not required.\n * \n * @param maxVal pointer to the returned maximum value; NULL is used if not required.\n * \n * @param minIdx pointer to the returned minimum location (in nD case); NULL is used if not required;\n * Otherwise, it must point to an array of src.dims elements, the coordinates of the minimum element in\n * each dimension are stored there sequentially.\n * \n * @param maxIdx pointer to the returned maximum location (in nD case). NULL is used if not required.\n * \n * @param mask specified array region\n */\nexport declare function minMaxIdx(src: InputArray, minVal: any, maxVal?: any, minIdx?: any, maxIdx?: any, mask?: InputArray): void\n\n/**\n * The function [cv::minMaxLoc](#d2/de8/group__core__array_1gab473bf2eb6d14ff97e89b355dac20707}) finds\n * the minimum and maximum element values and their positions. The extremums are searched across the\n * whole array or, if mask is not an empty array, in the specified array region.\n * \n * The function do not work with multi-channel arrays. If you need to find minimum or maximum elements\n * across all the channels, use\n * [Mat::reshape](#d3/d63/classcv_1_1Mat_1a4eb96e3251417fa88b78e2abd6cfd7d8}) first to reinterpret the\n * array as single-channel. Or you may extract the particular channel using either extractImageCOI , or\n * mixChannels , or split . \n * \n * [max](#d7/dcc/group__core__utils__softfloat_1ga78f988f6cfa6223610298cbd4f86ec66}),\n * [min](#d7/dcc/group__core__utils__softfloat_1gac48df53b8fd34b87e7b121fa8fd4c379}),\n * [compare](#d2/de8/group__core__array_1ga303cfb72acf8cbb36d884650c09a3a97}),\n * [inRange](#d2/de8/group__core__array_1ga48af0ab51e36436c5d04340e036ce981}), extractImageCOI,\n * [mixChannels](#d2/de8/group__core__array_1ga51d768c270a1cdd3497255017c4504be}),\n * [split](#d2/de8/group__core__array_1ga0547c7fed86152d7e9d0096029c8518a}),\n * [Mat::reshape](#d3/d63/classcv_1_1Mat_1a4eb96e3251417fa88b78e2abd6cfd7d8})\n * \n * @param src input single-channel array.\n * \n * @param minVal pointer to the returned minimum value; NULL is used if not required.\n * \n * @param maxVal pointer to the returned maximum value; NULL is used if not required.\n * \n * @param minLoc pointer to the returned minimum location (in 2D case); NULL is used if not required.\n * \n * @param maxLoc pointer to the returned maximum location (in 2D case); NULL is used if not required.\n * \n * @param mask optional mask used to select a sub-array.\n */\nexport declare function minMaxLoc(src: InputArray, minVal: any, maxVal?: any, minLoc?: any, maxLoc?: any, mask?: InputArray): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n * \n * @param a input single-channel array.\n * \n * @param minVal pointer to the returned minimum value; NULL is used if not required.\n * \n * @param maxVal pointer to the returned maximum value; NULL is used if not required.\n * \n * @param minIdx pointer to the returned minimum location (in nD case); NULL is used if not required;\n * Otherwise, it must point to an array of src.dims elements, the coordinates of the minimum element in\n * each dimension are stored there sequentially.\n * \n * @param maxIdx pointer to the returned maximum location (in nD case). NULL is used if not required.\n */\nexport declare function minMaxLoc(a: any, minVal: any, maxVal: any, minIdx?: any, maxIdx?: any): void\n\n/**\n * The function [cv::mixChannels](#d2/de8/group__core__array_1ga51d768c270a1cdd3497255017c4504be})\n * provides an advanced mechanism for shuffling image channels.\n * \n * [cv::split](#d2/de8/group__core__array_1ga0547c7fed86152d7e9d0096029c8518a}),[cv::merge](#d2/de8/group__core__array_1ga7d7b4d6c6ee504b30a20b1680029c7b4}),[cv::extractChannel](#d2/de8/group__core__array_1gacc6158574aa1f0281878c955bcf35642}),[cv::insertChannel](#d2/de8/group__core__array_1ga1d4bd886d35b00ec0b764cb4ce6eb515})\n * and some forms of\n * [cv::cvtColor](#d8/d01/group__imgproc__color__conversions_1ga397ae87e1288a81d2363b61574eb8cab}) are\n * partial cases of [cv::mixChannels](#d2/de8/group__core__array_1ga51d768c270a1cdd3497255017c4504be}).\n * \n * In the example below, the code splits a 4-channel BGRA image into a 3-channel BGR (with B and R\n * channels swapped) and a separate alpha-channel image: \n * \n * ```cpp\n * Mat bgra( 100, 100, CV_8UC4, Scalar(255,0,0,255) );\n * Mat bgr( bgra.rows, bgra.cols, CV_8UC3 );\n * Mat alpha( bgra.rows, bgra.cols, CV_8UC1 );\n * \n * // forming an array of matrices is a quite efficient operation,\n * // because the matrix data is not copied, only the headers\n * Mat out[] = { bgr, alpha };\n * // bgra[0] -> bgr[2], bgra[1] -> bgr[1],\n * // bgra[2] -> bgr[0], bgra[3] -> alpha[0]\n * int from_to[] = { 0,2, 1,1, 2,0, 3,3 };\n * mixChannels( &bgra, 1, out, 2, from_to, 4 );\n * ```\n * \n * Unlike many other new-style C++ functions in OpenCV (see the introduction section and\n * [Mat::create](#d3/d63/classcv_1_1Mat_1a55ced2c8d844d683ea9a725c60037ad0}) ),\n * [cv::mixChannels](#d2/de8/group__core__array_1ga51d768c270a1cdd3497255017c4504be}) requires the\n * output arrays to be pre-allocated before calling the function. \n * \n * [split](#d2/de8/group__core__array_1ga0547c7fed86152d7e9d0096029c8518a}),\n * [merge](#d2/de8/group__core__array_1ga7d7b4d6c6ee504b30a20b1680029c7b4}),\n * [extractChannel](#d2/de8/group__core__array_1gacc6158574aa1f0281878c955bcf35642}),\n * [insertChannel](#d2/de8/group__core__array_1ga1d4bd886d35b00ec0b764cb4ce6eb515}),\n * [cvtColor](#d8/d01/group__imgproc__color__conversions_1ga397ae87e1288a81d2363b61574eb8cab})\n * \n * @param src input array or vector of matrices; all of the matrices must have the same size and the\n * same depth.\n * \n * @param nsrcs number of matrices in src.\n * \n * @param dst output array or vector of matrices; all the matrices must be allocated; their size and\n * depth must be the same as in src[0].\n * \n * @param ndsts number of matrices in dst.\n * \n * @param fromTo array of index pairs specifying which channels are copied and where; fromTo[k*2] is a\n * 0-based index of the input channel in src, fromTo[k*2+1] is an index of the output channel in dst;\n * the continuous channel numbering is used: the first input image channels are indexed from 0 to\n * src[0].channels()-1, the second input image channels are indexed from src[0].channels() to\n * src[0].channels() + src[1].channels()-1, and so on, the same scheme is used for the output image\n * channels; as a special case, when fromTo[k*2] is negative, the corresponding output channel is\n * filled with zero .\n * \n * @param npairs number of index pairs in fromTo.\n */\nexport declare function mixChannels(src: any, nsrcs: size_t, dst: any, ndsts: size_t, fromTo: any, npairs: size_t): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n * \n * @param src input array or vector of matrices; all of the matrices must have the same size and the\n * same depth.\n * \n * @param dst output array or vector of matrices; all the matrices must be allocated; their size and\n * depth must be the same as in src[0].\n * \n * @param fromTo array of index pairs specifying which channels are copied and where; fromTo[k*2] is a\n * 0-based index of the input channel in src, fromTo[k*2+1] is an index of the output channel in dst;\n * the continuous channel numbering is used: the first input image channels are indexed from 0 to\n * src[0].channels()-1, the second input image channels are indexed from src[0].channels() to\n * src[0].channels() + src[1].channels()-1, and so on, the same scheme is used for the output image\n * channels; as a special case, when fromTo[k*2] is negative, the corresponding output channel is\n * filled with zero .\n * \n * @param npairs number of index pairs in fromTo.\n */\nexport declare function mixChannels(src: InputArrayOfArrays, dst: InputOutputArrayOfArrays, fromTo: any, npairs: size_t): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n * \n * @param src input array or vector of matrices; all of the matrices must have the same size and the\n * same depth.\n * \n * @param dst output array or vector of matrices; all the matrices must be allocated; their size and\n * depth must be the same as in src[0].\n * \n * @param fromTo array of index pairs specifying which channels are copied and where; fromTo[k*2] is a\n * 0-based index of the input channel in src, fromTo[k*2+1] is an index of the output channel in dst;\n * the continuous channel numbering is used: the first input image channels are indexed from 0 to\n * src[0].channels()-1, the second input image channels are indexed from src[0].channels() to\n * src[0].channels() + src[1].channels()-1, and so on, the same scheme is used for the output image\n * channels; as a special case, when fromTo[k*2] is negative, the corresponding output channel is\n * filled with zero .\n */\nexport declare function mixChannels(src: InputArrayOfArrays, dst: InputOutputArrayOfArrays, fromTo: any): void\n\n/**\n * The function [cv::mulSpectrums](#d2/de8/group__core__array_1ga3ab38646463c59bf0ce962a9d51db64f})\n * performs the per-element multiplication of the two CCS-packed or complex matrices that are results\n * of a real or complex Fourier transform.\n * \n * The function, together with dft and idft , may be used to calculate convolution (pass conjB=false )\n * or correlation (pass conjB=true ) of two arrays rapidly. When the arrays are complex, they are\n * simply multiplied (per element) with an optional conjugation of the second-array elements. When the\n * arrays are real, they are assumed to be CCS-packed (see dft for details).\n * \n * @param a first input array.\n * \n * @param b second input array of the same size and type as src1 .\n * \n * @param c output array of the same size and type as src1 .\n * \n * @param flags operation flags; currently, the only supported flag is cv::DFT_ROWS, which indicates\n * that each row of src1 and src2 is an independent 1D Fourier spectrum. If you do not want to use this\n * flag, then simply add a 0 as value.\n * \n * @param conjB optional flag that conjugates the second input array before the multiplication (true)\n * or not (false).\n */\nexport declare function mulSpectrums(a: InputArray, b: InputArray, c: OutputArray, flags: int, conjB?: bool): void\n\n/**\n * The function multiply calculates the per-element product of two arrays:\n * \n * `\\\\[\\\\texttt{dst} (I)= \\\\texttt{saturate} ( \\\\texttt{scale} \\\\cdot \\\\texttt{src1} (I) \\\\cdot\n * \\\\texttt{src2} (I))\\\\]`\n * \n * There is also a [MatrixExpressions](#d1/d10/classcv_1_1MatExpr_1MatrixExpressions}) -friendly\n * variant of the first function. See\n * [Mat::mul](#d3/d63/classcv_1_1Mat_1a385c09827713dc3e6d713bfad8460706}) .\n * \n * For a not-per-element matrix product, see gemm .\n * \n * Saturation is not applied when the output array has the depth CV_32S. You may even get result of an\n * incorrect sign in the case of overflow. \n * \n * [add](#d2/de8/group__core__array_1ga10ac1bfb180e2cfda1701d06c24fdbd6}),\n * [subtract](#d2/de8/group__core__array_1gaa0f00d98b4b5edeaeb7b8333b2de353b}),\n * [divide](#d2/de8/group__core__array_1ga6db555d30115642fedae0cda05604874}),\n * [scaleAdd](#d2/de8/group__core__array_1ga9e0845db4135f55dcf20227402f00d98}),\n * [addWeighted](#d2/de8/group__core__array_1gafafb2513349db3bcff51f54ee5592a19}),\n * [accumulate](#d7/df3/group__imgproc__motion_1ga1a567a79901513811ff3b9976923b199}),\n * [accumulateProduct](#d7/df3/group__imgproc__motion_1ga82518a940ecfda49460f66117ac82520}),\n * [accumulateSquare](#d7/df3/group__imgproc__motion_1gacb75e7ffb573227088cef9ceaf80be8c}),\n * [Mat::convertTo](#d3/d63/classcv_1_1Mat_1adf88c60c5b4980e05bb556080916978b})\n * \n * @param src1 first input array.\n * \n * @param src2 second input array of the same size and the same type as src1.\n * \n * @param dst output array of the same size and type as src1.\n * \n * @param scale optional scale factor.\n * \n * @param dtype optional depth of the output array\n */\nexport declare function multiply(src1: InputArray, src2: InputArray, dst: OutputArray, scale?: double, dtype?: int): void\n\n/**\n * The function [cv::mulTransposed](#d2/de8/group__core__array_1gadc4e49f8f7a155044e3be1b9e3b270ab})\n * calculates the product of src and its transposition: `\\\\[\\\\texttt{dst} = \\\\texttt{scale} (\n * \\\\texttt{src} - \\\\texttt{delta} )^T ( \\\\texttt{src} - \\\\texttt{delta} )\\\\]` if aTa=true , and\n * `\\\\[\\\\texttt{dst} = \\\\texttt{scale} ( \\\\texttt{src} - \\\\texttt{delta} ) ( \\\\texttt{src} -\n * \\\\texttt{delta} )^T\\\\]` otherwise. The function is used to calculate the covariance matrix. With\n * zero delta, it can be used as a faster substitute for general matrix product A*B when B=A' \n * \n * [calcCovarMatrix](#d2/de8/group__core__array_1gae6ffa9354633f984246945d52823165d}),\n * [gemm](#d2/de8/group__core__array_1gacb6e64071dffe36434e1e7ee79e7cb35}),\n * [repeat](#d2/de8/group__core__array_1ga496c3860f3ac44c40b48811333cfda2d}),\n * [reduce](#d2/de8/group__core__array_1ga4b78072a303f29d9031d56e5638da78e})\n * \n * @param src input single-channel matrix. Note that unlike gemm, the function can multiply not only\n * floating-point matrices.\n * \n * @param dst output square matrix.\n * \n * @param aTa Flag specifying the multiplication ordering. See the description below.\n * \n * @param delta Optional delta matrix subtracted from src before the multiplication. When the matrix is\n * empty ( delta=noArray() ), it is assumed to be zero, that is, nothing is subtracted. If it has the\n * same size as src , it is simply subtracted. Otherwise, it is \"repeated\" (see repeat ) to cover the\n * full src and then subtracted. Type of the delta matrix, when it is not empty, must be the same as\n * the type of created output matrix. See the dtype parameter description below.\n * \n * @param scale Optional scale factor for the matrix product.\n * \n * @param dtype Optional type of the output matrix. When it is negative, the output matrix will have\n * the same type as src . Otherwise, it will be type=CV_MAT_DEPTH(dtype) that should be either CV_32F\n * or CV_64F .\n */\nexport declare function mulTransposed(src: InputArray, dst: OutputArray, aTa: bool, delta?: InputArray, scale?: double, dtype?: int): void\n\n/**\n * This version of [norm](#dc/d84/group__core__basic_1ga4e556cb8ad35a643a1ea66e035711bb9}) calculates\n * the absolute norm of src1. The type of norm to calculate is specified using\n * [NormTypes](#d2/de8/group__core__array_1gad12cefbcb5291cf958a85b4b67b6149f}).\n * \n * As example for one array consider the function `$r(x)= \\\\begin{pmatrix} x \\\\\\\\ 1-x \\\\end{pmatrix}, x\n * \\\\in [-1;1]$`. The `$ L_{1}, L_{2} $` and `$ L_{\\\\infty} $` norm for the sample value `$r(-1) =\n * \\\\begin{pmatrix} -1 \\\\\\\\ 2 \\\\end{pmatrix}$` is calculated as follows `\\\\begin{align*} \\\\| r(-1)\n * \\\\|_{L_1} &= |-1| + |2| = 3 \\\\\\\\ \\\\| r(-1) \\\\|_{L_2} &= \\\\sqrt{(-1)^{2} + (2)^{2}} = \\\\sqrt{5} \\\\\\\\\n * \\\\| r(-1) \\\\|_{L_\\\\infty} &= \\\\max(|-1|,|2|) = 2 \\\\end{align*}` and for `$r(0.5) = \\\\begin{pmatrix}\n * 0.5 \\\\\\\\ 0.5 \\\\end{pmatrix}$` the calculation is `\\\\begin{align*} \\\\| r(0.5) \\\\|_{L_1} &= |0.5| +\n * |0.5| = 1 \\\\\\\\ \\\\| r(0.5) \\\\|_{L_2} &= \\\\sqrt{(0.5)^{2} + (0.5)^{2}} = \\\\sqrt{0.5} \\\\\\\\ \\\\| r(0.5)\n * \\\\|_{L_\\\\infty} &= \\\\max(|0.5|,|0.5|) = 0.5. \\\\end{align*}` The following graphic shows all values\n * for the three norm functions `$\\\\| r(x) \\\\|_{L_1}, \\\\| r(x) \\\\|_{L_2}$` and `$\\\\| r(x)\n * \\\\|_{L_\\\\infty}$`. It is notable that the `$ L_{1} $` norm forms the upper and the `$ L_{\\\\infty} $`\n * norm forms the lower border for the example function `$ r(x) $`. \n *  When the mask parameter is specified and it is not empty, the norm is\n * \n * If normType is not specified,\n * [NORM_L2](#d2/de8/group__core__array_1ggad12cefbcb5291cf958a85b4b67b6149fa7bacbe84d400336a8f26297d8e80e3a2})\n * is used. calculated only over the region specified by the mask.\n * \n * Multi-channel input arrays are treated as single-channel arrays, that is, the results for all\n * channels are combined.\n * \n * [Hamming](#d3/d59/structcv_1_1Hamming}) norms can only be calculated with CV_8U depth arrays.\n * \n * @param src1 first input array.\n * \n * @param normType type of the norm (see NormTypes).\n * \n * @param mask optional operation mask; it must have the same size as src1 and CV_8UC1 type.\n */\nexport declare function norm(src1: InputArray, normType?: int, mask?: InputArray): double\n\n/**\n * This version of [cv::norm](#dc/d84/group__core__basic_1ga4e556cb8ad35a643a1ea66e035711bb9})\n * calculates the absolute difference norm or the relative difference norm of arrays src1 and src2. The\n * type of norm to calculate is specified using\n * [NormTypes](#d2/de8/group__core__array_1gad12cefbcb5291cf958a85b4b67b6149f}).\n * \n * @param src1 first input array.\n * \n * @param src2 second input array of the same size and the same type as src1.\n * \n * @param normType type of the norm (see NormTypes).\n * \n * @param mask optional operation mask; it must have the same size as src1 and CV_8UC1 type.\n */\nexport declare function norm(src1: InputArray, src2: InputArray, normType?: int, mask?: InputArray): double\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n * \n * @param src first input array.\n * \n * @param normType type of the norm (see NormTypes).\n */\nexport declare function norm(src: any, normType: int): double\n\n/**\n * The function [cv::normalize](#dc/d84/group__core__basic_1ga1b6a396a456c8b6c6e4afd8591560d80})\n * normalizes scale and shift the input array elements so that `\\\\[\\\\| \\\\texttt{dst} \\\\| _{L_p}=\n * \\\\texttt{alpha}\\\\]` (where p=Inf, 1 or 2) when normType=NORM_INF, NORM_L1, or NORM_L2, respectively;\n * or so that `\\\\[\\\\min _I \\\\texttt{dst} (I)= \\\\texttt{alpha} , \\\\, \\\\, \\\\max _I \\\\texttt{dst} (I)=\n * \\\\texttt{beta}\\\\]`\n * \n * when normType=NORM_MINMAX (for dense arrays only). The optional mask specifies a sub-array to be\n * normalized. This means that the norm or min-n-max are calculated over the sub-array, and then this\n * sub-array is modified to be normalized. If you want to only use the mask to calculate the norm or\n * min-max but modify the whole array, you can use norm and\n * [Mat::convertTo](#d3/d63/classcv_1_1Mat_1adf88c60c5b4980e05bb556080916978b}).\n * \n * In case of sparse matrices, only the non-zero values are analyzed and transformed. Because of this,\n * the range transformation for sparse matrices is not allowed since it can shift the zero level.\n * \n * Possible usage with some positive example data: \n * \n * ```cpp\n * vector<double> positiveData = { 2.0, 8.0, 10.0 };\n * vector<double> normalizedData_l1, normalizedData_l2, normalizedData_inf, normalizedData_minmax;\n * \n * // Norm to probability (total count)\n * // sum(numbers) = 20.0\n * // 2.0      0.1     (2.0/20.0)\n * // 8.0      0.4     (8.0/20.0)\n * // 10.0     0.5     (10.0/20.0)\n * normalize(positiveData, normalizedData_l1, 1.0, 0.0, NORM_L1);\n * \n * // Norm to unit vector: ||positiveData|| = 1.0\n * // 2.0      0.15\n * // 8.0      0.62\n * // 10.0     0.77\n * normalize(positiveData, normalizedData_l2, 1.0, 0.0, NORM_L2);\n * \n * // Norm to max element\n * // 2.0      0.2     (2.0/10.0)\n * // 8.0      0.8     (8.0/10.0)\n * // 10.0     1.0     (10.0/10.0)\n * normalize(positiveData, normalizedData_inf, 1.0, 0.0, NORM_INF);\n * \n * // Norm to range [0.0;1.0]\n * // 2.0      0.0     (shift to left border)\n * // 8.0      0.75    (6.0/8.0)\n * // 10.0     1.0     (shift to right border)\n * normalize(positiveData, normalizedData_minmax, 1.0, 0.0, NORM_MINMAX);\n * ```\n * \n * [norm](#dc/d84/group__core__basic_1ga4e556cb8ad35a643a1ea66e035711bb9}),\n * [Mat::convertTo](#d3/d63/classcv_1_1Mat_1adf88c60c5b4980e05bb556080916978b}),\n * [SparseMat::convertTo](#dd/da9/classcv_1_1SparseMat_1a577ea9bbc02ffcf195df6d96f9c9650c})\n * \n * @param src input array.\n * \n * @param dst output array of the same size as src .\n * \n * @param alpha norm value to normalize to or the lower range boundary in case of the range\n * normalization.\n * \n * @param beta upper range boundary in case of the range normalization; it is not used for the norm\n * normalization.\n * \n * @param norm_type normalization type (see cv::NormTypes).\n * \n * @param dtype when negative, the output array has the same type as src; otherwise, it has the same\n * number of channels as src and the depth =CV_MAT_DEPTH(dtype).\n * \n * @param mask optional operation mask.\n */\nexport declare function normalize(src: InputArray, dst: InputOutputArray, alpha?: double, beta?: double, norm_type?: int, dtype?: int, mask?: InputArray): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n * \n * @param src input array.\n * \n * @param dst output array of the same size as src .\n * \n * @param alpha norm value to normalize to or the lower range boundary in case of the range\n * normalization.\n * \n * @param normType normalization type (see cv::NormTypes).\n */\nexport declare function normalize(src: any, dst: any, alpha: double, normType: int): void\n\nexport declare function patchNaNs(a: InputOutputArray, val?: double): void\n\n/**\n * wrap [PCA::backProject](#d3/d8d/classcv_1_1PCA_1a5f84cfbdb25b9833cc1bfb5bd484ea79})\n */\nexport declare function PCABackProject(data: InputArray, mean: InputArray, eigenvectors: InputArray, result: OutputArray): void\n\n/**\n * wrap PCA::operator()\n */\nexport declare function PCACompute(data: InputArray, mean: InputOutputArray, eigenvectors: OutputArray, maxComponents?: int): void\n\n/**\n * wrap PCA::operator() and add eigenvalues output parameter\n */\nexport declare function PCACompute(data: InputArray, mean: InputOutputArray, eigenvectors: OutputArray, eigenvalues: OutputArray, maxComponents?: int): void\n\n/**\n * wrap PCA::operator()\n */\nexport declare function PCACompute(data: InputArray, mean: InputOutputArray, eigenvectors: OutputArray, retainedVariance: double): void\n\n/**\n * wrap PCA::operator() and add eigenvalues output parameter\n */\nexport declare function PCACompute(data: InputArray, mean: InputOutputArray, eigenvectors: OutputArray, eigenvalues: OutputArray, retainedVariance: double): void\n\n/**\n * wrap [PCA::project](#d3/d8d/classcv_1_1PCA_1a67c9a3f8fe804f40be58c88a3ae73f41})\n */\nexport declare function PCAProject(data: InputArray, mean: InputArray, eigenvectors: InputArray, result: OutputArray): void\n\n/**\n * The function\n * [cv::perspectiveTransform](#d2/de8/group__core__array_1gad327659ac03e5fd6894b90025e6900a7})\n * transforms every element of src by treating it as a 2D or 3D vector, in the following way: `\\\\[(x,\n * y, z) \\\\rightarrow (x'/w, y'/w, z'/w)\\\\]` where `\\\\[(x', y', z', w') = \\\\texttt{mat} \\\\cdot\n * \\\\begin{bmatrix} x & y & z & 1 \\\\end{bmatrix}\\\\]` and `\\\\[w = \\\\fork{w'}{if \\\\(w' \\\\ne\n * 0\\\\)}{\\\\infty}{otherwise}\\\\]`\n * \n * Here a 3D vector transformation is shown. In case of a 2D vector transformation, the z component is\n * omitted.\n * \n * The function transforms a sparse set of 2D or 3D vectors. If you want to transform an image using\n * perspective transformation, use warpPerspective . If you have an inverse problem, that is, you want\n * to compute the most probable perspective transformation out of several pairs of corresponding\n * points, you can use getPerspectiveTransform or findHomography . \n * \n * [transform](#d2/de8/group__core__array_1ga393164aa54bb9169ce0a8cc44e08ff22}),\n * [warpPerspective](#da/d54/group__imgproc__transform_1gaf73673a7e8e18ec6963e3774e6a94b87}),\n * [getPerspectiveTransform](#da/d54/group__imgproc__transform_1ga20f62aa3235d869c9956436c870893ae}),\n * [findHomography](#d9/d0c/group__calib3d_1ga4abc2ece9fab9398f2e560d53c8c9780})\n * \n * @param src input two-channel or three-channel floating-point array; each element is a 2D/3D vector\n * to be transformed.\n * \n * @param dst output array of the same size and type as src.\n * \n * @param m 3x3 or 4x4 floating-point transformation matrix.\n */\nexport declare function perspectiveTransform(src: InputArray, dst: OutputArray, m: InputArray): void\n\n/**\n * The function [cv::phase](#d2/de8/group__core__array_1ga9db9ca9b4d81c3bde5677b8f64dc0137}) calculates\n * the rotation angle of each 2D vector that is formed from the corresponding elements of x and y :\n * `\\\\[\\\\texttt{angle} (I) = \\\\texttt{atan2} ( \\\\texttt{y} (I), \\\\texttt{x} (I))\\\\]`\n * \n * The angle estimation accuracy is about 0.3 degrees. When x(I)=y(I)=0 , the corresponding angle(I) is\n * set to 0.\n * \n * @param x input floating-point array of x-coordinates of 2D vectors.\n * \n * @param y input array of y-coordinates of 2D vectors; it must have the same size and the same type as\n * x.\n * \n * @param angle output array of vector angles; it has the same size and same type as x .\n * \n * @param angleInDegrees when true, the function calculates the angle in degrees, otherwise, they are\n * measured in radians.\n */\nexport declare function phase(x: InputArray, y: InputArray, angle: OutputArray, angleInDegrees?: bool): void\n\n/**\n * The function [cv::polarToCart](#d2/de8/group__core__array_1ga581ff9d44201de2dd1b40a50db93d665})\n * calculates the Cartesian coordinates of each 2D vector represented by the corresponding elements of\n * magnitude and angle: `\\\\[\\\\begin{array}{l} \\\\texttt{x} (I) = \\\\texttt{magnitude} (I) \\\\cos (\n * \\\\texttt{angle} (I)) \\\\\\\\ \\\\texttt{y} (I) = \\\\texttt{magnitude} (I) \\\\sin ( \\\\texttt{angle} (I))\n * \\\\\\\\ \\\\end{array}\\\\]`\n * \n * The relative accuracy of the estimated coordinates is about 1e-6. \n * \n * [cartToPolar](#d2/de8/group__core__array_1gac5f92f48ec32cacf5275969c33ee837d}),\n * [magnitude](#d2/de8/group__core__array_1ga6d3b097586bca4409873d64a90fe64c3}),\n * [phase](#d2/de8/group__core__array_1ga9db9ca9b4d81c3bde5677b8f64dc0137}),\n * [exp](#d7/dcc/group__core__utils__softfloat_1ga1eb07a682abff20e0864104599c06fbc}),\n * [log](#d7/dcc/group__core__utils__softfloat_1gae5de78ee278fe88405c6dbc38502f7c1}),\n * [pow](#d7/dcc/group__core__utils__softfloat_1ga8bc36646a43b82baa15f151a973fb0c5}),\n * [sqrt](#d7/dcc/group__core__utils__softfloat_1ga682082a1892db64a2856403ec17ba297})\n * \n * @param magnitude input floating-point array of magnitudes of 2D vectors; it can be an empty matrix\n * (=Mat()), in this case, the function assumes that all the magnitudes are =1; if it is not empty, it\n * must have the same size and type as angle.\n * \n * @param angle input floating-point array of angles of 2D vectors.\n * \n * @param x output array of x-coordinates of 2D vectors; it has the same size and type as angle.\n * \n * @param y output array of y-coordinates of 2D vectors; it has the same size and type as angle.\n * \n * @param angleInDegrees when true, the input angles are measured in degrees, otherwise, they are\n * measured in radians.\n */\nexport declare function polarToCart(magnitude: InputArray, angle: InputArray, x: OutputArray, y: OutputArray, angleInDegrees?: bool): void\n\n/**\n * The function [cv::pow](#d7/dcc/group__core__utils__softfloat_1ga8bc36646a43b82baa15f151a973fb0c5})\n * raises every element of the input array to power : `\\\\[\\\\texttt{dst} (I) =\n * \\\\fork{\\\\texttt{src}(I)^{power}}{if \\\\(\\\\texttt{power}\\\\) is\n * integer}{|\\\\texttt{src}(I)|^{power}}{otherwise}\\\\]`\n * \n * So, for a non-integer power exponent, the absolute values of input array elements are used. However,\n * it is possible to get true values for negative values using some extra operations. In the example\n * below, computing the 5th root of array src shows: \n * \n * ```cpp\n * Mat mask = src < 0;\n * pow(src, 1./5, dst);\n * subtract(Scalar::all(0), dst, dst, mask);\n * ```\n * \n *  For some values of power, such as integer values, 0.5 and -0.5, specialized faster algorithms are\n * used.\n * \n * Special values (NaN, Inf) are not handled. \n * \n * [sqrt](#d7/dcc/group__core__utils__softfloat_1ga682082a1892db64a2856403ec17ba297}),\n * [exp](#d7/dcc/group__core__utils__softfloat_1ga1eb07a682abff20e0864104599c06fbc}),\n * [log](#d7/dcc/group__core__utils__softfloat_1gae5de78ee278fe88405c6dbc38502f7c1}),\n * [cartToPolar](#d2/de8/group__core__array_1gac5f92f48ec32cacf5275969c33ee837d}),\n * [polarToCart](#d2/de8/group__core__array_1ga581ff9d44201de2dd1b40a50db93d665})\n * \n * @param src input array.\n * \n * @param power exponent of power.\n * \n * @param dst output array of the same size and type as src.\n */\nexport declare function pow(src: InputArray, power: double, dst: OutputArray): void\n\n/**\n * This function calculates the Peak Signal-to-Noise Ratio (PSNR) image quality metric in decibels\n * (dB), between two input arrays src1 and src2. The arrays must have the same type.\n * \n * The PSNR is calculated as follows:\n * \n * `\\\\[ \\\\texttt{PSNR} = 10 \\\\cdot \\\\log_{10}{\\\\left( \\\\frac{R^2}{MSE} \\\\right) } \\\\]`\n * \n * where R is the maximum integer value of depth (e.g. 255 in the case of CV_8U data) and MSE is the\n * mean squared error between the two arrays.\n * \n * @param src1 first input array.\n * \n * @param src2 second input array of the same size as src1.\n * \n * @param R the maximum pixel value (255 by default)\n */\nexport declare function PSNR(src1: InputArray, src2: InputArray, R?: double): double\n\n/**\n * The function [cv::randn](#d2/de8/group__core__array_1gaeff1f61e972d133a04ce3a5f81cf6808}) fills the\n * matrix dst with normally distributed random numbers with the specified mean vector and the standard\n * deviation matrix. The generated random numbers are clipped to fit the value range of the output\n * array data type. \n * \n * [RNG](#d1/dd6/classcv_1_1RNG}),\n * [randu](#d2/de8/group__core__array_1ga1ba1026dca0807b27057ba6a49d258c0})\n * \n * @param dst output array of random numbers; the array must be pre-allocated and have 1 to 4 channels.\n * \n * @param mean mean value (expectation) of the generated random numbers.\n * \n * @param stddev standard deviation of the generated random numbers; it can be either a vector (in\n * which case a diagonal standard deviation matrix is assumed) or a square matrix.\n */\nexport declare function randn(dst: InputOutputArray, mean: InputArray, stddev: InputArray): void\n\n/**\n * The function [cv::randShuffle](#d2/de8/group__core__array_1ga6a789c8a5cb56c6dd62506179808f763})\n * shuffles the specified 1D array by randomly choosing pairs of elements and swapping them. The number\n * of such swap operations will be dst.rows*dst.cols*iterFactor . \n * \n * [RNG](#d1/dd6/classcv_1_1RNG}),\n * [sort](#d2/de8/group__core__array_1ga45dd56da289494ce874be2324856898f})\n * \n * @param dst input/output numerical 1D array.\n * \n * @param iterFactor scale factor that determines the number of random swap operations (see the details\n * below).\n * \n * @param rng optional random number generator used for shuffling; if it is zero, theRNG () is used\n * instead.\n */\nexport declare function randShuffle(dst: InputOutputArray, iterFactor?: double, rng?: any): void\n\n/**\n * Non-template variant of the function fills the matrix dst with uniformly-distributed random numbers\n * from the specified range: `\\\\[\\\\texttt{low} _c \\\\leq \\\\texttt{dst} (I)_c < \\\\texttt{high} _c\\\\]` \n * \n * [RNG](#d1/dd6/classcv_1_1RNG}),\n * [randn](#d2/de8/group__core__array_1gaeff1f61e972d133a04ce3a5f81cf6808}),\n * [theRNG](#d2/de8/group__core__array_1ga75843061d150ad6564b5447e38e57722})\n * \n * @param dst output array of random numbers; the array must be pre-allocated.\n * \n * @param low inclusive lower boundary of the generated random numbers.\n * \n * @param high exclusive upper boundary of the generated random numbers.\n */\nexport declare function randu(dst: InputOutputArray, low: InputArray, high: InputArray): void\n\n/**\n * The function [reduce](#d2/de8/group__core__array_1ga4b78072a303f29d9031d56e5638da78e}) reduces the\n * matrix to a vector by treating the matrix rows/columns as a set of 1D vectors and performing the\n * specified operation on the vectors until a single row/column is obtained. For example, the function\n * can be used to compute horizontal and vertical projections of a raster image. In case of\n * [REDUCE_MAX](#d0/de1/group__core_1gga14cdedf2933367eb9395ec16798af994a928b4c3eb0a038ea41b61d122c0495ee})\n * and\n * [REDUCE_MIN](#d0/de1/group__core_1gga14cdedf2933367eb9395ec16798af994a1f40a2ed66c8a8b8198186da47ec7b76})\n * , the output image should have the same type as the source one. In case of\n * [REDUCE_SUM](#d0/de1/group__core_1gga14cdedf2933367eb9395ec16798af994a101441e283ed69f20cfc5468114f9867})\n * and\n * [REDUCE_AVG](#d0/de1/group__core_1gga14cdedf2933367eb9395ec16798af994a85f039992a454ca367bc190529766c7e})\n * , the output may have a larger element bit-depth to preserve accuracy. And multi-channel arrays are\n * also supported in these two reduction modes.\n * \n * The following code demonstrates its usage for a single channel matrix. \n * \n * ```cpp\n *         Mat m = (Mat_<uchar>(3,2) << 1,2,3,4,5,6);\n *         Mat col_sum, row_sum;\n * \n *         reduce(m, col_sum, 0, REDUCE_SUM, CV_32F);\n *         reduce(m, row_sum, 1, REDUCE_SUM, CV_32F);\n *         /*\n *         m =\n *         [  1,   2;\n *            3,   4;\n *            5,   6]\n *         col_sum =\n *         [9, 12]\n *         row_sum =\n *         [3;\n *          7;\n *          11]\n * \\/\n * ```\n * \n *  And the following code demonstrates its usage for a two-channel matrix. \n * \n * ```cpp\n *         // two channels\n *         char d[] = {1,2,3,4,5,6};\n *         Mat m(3, 1, CV_8UC2, d);\n *         Mat col_sum_per_channel;\n *         reduce(m, col_sum_per_channel, 0, REDUCE_SUM, CV_32F);\n *         /*\n *         col_sum_per_channel =\n *         [9, 12]\n * \\/\n * ```\n * \n * [repeat](#d2/de8/group__core__array_1ga496c3860f3ac44c40b48811333cfda2d})\n * \n * @param src input 2D matrix.\n * \n * @param dst output vector. Its size and type is defined by dim and dtype parameters.\n * \n * @param dim dimension index along which the matrix is reduced. 0 means that the matrix is reduced to\n * a single row. 1 means that the matrix is reduced to a single column.\n * \n * @param rtype reduction operation that could be one of ReduceTypes\n * \n * @param dtype when negative, the output vector will have the same type as the input matrix,\n * otherwise, its type will be CV_MAKE_TYPE(CV_MAT_DEPTH(dtype), src.channels()).\n */\nexport declare function reduce(src: InputArray, dst: OutputArray, dim: int, rtype: int, dtype?: int): void\n\n/**\n * The function [cv::repeat](#d2/de8/group__core__array_1ga496c3860f3ac44c40b48811333cfda2d})\n * duplicates the input array one or more times along each of the two axes: `\\\\[\\\\texttt{dst} _{ij}=\n * \\\\texttt{src} _{i\\\\mod src.rows, \\\\; j\\\\mod src.cols }\\\\]` The second variant of the function is\n * more convenient to use with [MatrixExpressions](#d1/d10/classcv_1_1MatExpr_1MatrixExpressions}). \n * \n * [cv::reduce](#d2/de8/group__core__array_1ga4b78072a303f29d9031d56e5638da78e})\n * \n * @param src input array to replicate.\n * \n * @param ny Flag to specify how many times the src is repeated along the vertical axis.\n * \n * @param nx Flag to specify how many times the src is repeated along the horizontal axis.\n * \n * @param dst output array of the same type as src.\n */\nexport declare function repeat(src: InputArray, ny: int, nx: int, dst: OutputArray): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n * \n * @param src input array to replicate.\n * \n * @param ny Flag to specify how many times the src is repeated along the vertical axis.\n * \n * @param nx Flag to specify how many times the src is repeated along the horizontal axis.\n */\nexport declare function repeat(src: any, ny: int, nx: int): Mat\n\n/**\n * [transpose](#d2/de8/group__core__array_1ga46630ed6c0ea6254a35f447289bd7404}) ,\n * [repeat](#d2/de8/group__core__array_1ga496c3860f3ac44c40b48811333cfda2d}) ,\n * [completeSymm](#d2/de8/group__core__array_1ga6847337c0c55769e115a70e0f011b5ca}),\n * [flip](#d2/de8/group__core__array_1gaca7be533e3dac7feb70fc60635adf441}),\n * [RotateFlags](#d2/de8/group__core__array_1ga6f45d55c0b1cc9d97f5353a7c8a7aac2})\n * \n * @param src input array.\n * \n * @param dst output array of the same type as src. The size is the same with ROTATE_180, and the rows\n * and cols are switched for ROTATE_90_CLOCKWISE and ROTATE_90_COUNTERCLOCKWISE.\n * \n * @param rotateCode an enum to specify how to rotate the array; see the enum RotateFlags\n */\nexport declare function rotate(src: InputArray, dst: OutputArray, rotateCode: int): void\n\n/**\n * The function scaleAdd is one of the classical primitive linear algebra operations, known as DAXPY or\n * SAXPY in . It calculates the sum of a scaled array and another array: `\\\\[\\\\texttt{dst} (I)=\n * \\\\texttt{scale} \\\\cdot \\\\texttt{src1} (I) + \\\\texttt{src2} (I)\\\\]` The function can also be emulated\n * with a matrix expression, for example: \n * \n * ```cpp\n * Mat A(3, 3, CV_64F);\n * ...\n * A.row(0) = A.row(1)*2 + A.row(2);\n * ```\n * \n * [add](#d2/de8/group__core__array_1ga10ac1bfb180e2cfda1701d06c24fdbd6}),\n * [addWeighted](#d2/de8/group__core__array_1gafafb2513349db3bcff51f54ee5592a19}),\n * [subtract](#d2/de8/group__core__array_1gaa0f00d98b4b5edeaeb7b8333b2de353b}),\n * [Mat::dot](#d3/d63/classcv_1_1Mat_1a0f683eab191eeece33dfc64ae299a9cb}),\n * [Mat::convertTo](#d3/d63/classcv_1_1Mat_1adf88c60c5b4980e05bb556080916978b})\n * \n * @param src1 first input array.\n * \n * @param alpha scale factor for the first array.\n * \n * @param src2 second input array of the same size and type as src1.\n * \n * @param dst output array of the same size and type as src1.\n */\nexport declare function scaleAdd(src1: InputArray, alpha: double, src2: InputArray, dst: OutputArray): void\n\n/**\n * The function [cv::setIdentity](#d2/de8/group__core__array_1ga388d7575224a4a277ceb98ccaa327c99})\n * initializes a scaled identity matrix: `\\\\[\\\\texttt{mtx} (i,j)= \\\\fork{\\\\texttt{value}}{ if\n * \\\\(i=j\\\\)}{0}{otherwise}\\\\]`\n * \n * The function can also be emulated using the matrix initializers and the matrix expressions: \n * \n * ```cpp\n * Mat A = Mat::eye(4, 3, CV_32F)*5;\n * // A will be set to [[5, 0, 0], [0, 5, 0], [0, 0, 5], [0, 0, 0]]\n * ```\n * \n * [Mat::zeros](#d3/d63/classcv_1_1Mat_1a0b57b6a326c8876d944d188a46e0f556}),\n * [Mat::ones](#d3/d63/classcv_1_1Mat_1a69ae0402d116fc9c71908d8508dc2f09}),\n * [Mat::setTo](#d3/d63/classcv_1_1Mat_1a0440e2a164c0b0d8462fb1e487be9876}),\n * [Mat::operator=](#d3/d63/classcv_1_1Mat_1aed1f81fe7efaacc2bd95149cdfa34302})\n * \n * @param mtx matrix to initialize (not necessarily square).\n * \n * @param s value to assign to diagonal elements.\n */\nexport declare function setIdentity(mtx: InputOutputArray, s?: any): void\n\n/**\n * The function [cv::setRNGSeed](#d2/de8/group__core__array_1ga757e657c037410d9e19e819569e7de0f}) sets\n * state of default random number generator to custom value. \n * \n * [RNG](#d1/dd6/classcv_1_1RNG}),\n * [randu](#d2/de8/group__core__array_1ga1ba1026dca0807b27057ba6a49d258c0}),\n * [randn](#d2/de8/group__core__array_1gaeff1f61e972d133a04ce3a5f81cf6808})\n * \n * @param seed new state for default random number generator\n */\nexport declare function setRNGSeed(seed: int): void\n\n/**\n * The function [cv::solve](#d2/de8/group__core__array_1ga12b43690dbd31fed96f213eefead2373}) solves a\n * linear system or least-squares problem (the latter is possible with [SVD](#df/df7/classcv_1_1SVD})\n * or QR methods, or by specifying the flag\n * [DECOMP_NORMAL](#d2/de8/group__core__array_1ggaaf9ea5dcc392d5ae04eacb9920b9674ca13eaae0241295166140291223db12166})\n * ): `\\\\[\\\\texttt{dst} = \\\\arg \\\\min _X \\\\| \\\\texttt{src1} \\\\cdot \\\\texttt{X} - \\\\texttt{src2} \\\\|\\\\]`\n * \n * If\n * [DECOMP_LU](#d2/de8/group__core__array_1ggaaf9ea5dcc392d5ae04eacb9920b9674ca247a3455cd64973152e17e26999dc024})\n * or\n * [DECOMP_CHOLESKY](#d2/de8/group__core__array_1ggaaf9ea5dcc392d5ae04eacb9920b9674ca33cf860f98004310374a81d2c01715da})\n * method is used, the function returns 1 if src1 (or `$\\\\texttt{src1}^T\\\\texttt{src1}$` ) is\n * non-singular. Otherwise, it returns 0. In the latter case, dst is not valid. Other methods find a\n * pseudo-solution in case of a singular left-hand side part.\n * \n * If you want to find a unity-norm solution of an under-defined singular system\n * `$\\\\texttt{src1}\\\\cdot\\\\texttt{dst}=0$` , the function solve will not do the work. Use\n * [SVD::solveZ](#df/df7/classcv_1_1SVD_1ab255cd24a882ab993fb2f7377ef2774a}) instead.\n * \n * [invert](#d2/de8/group__core__array_1gad278044679d4ecf20f7622cc151aaaa2}),\n * [SVD](#df/df7/classcv_1_1SVD}),\n * [eigen](#d2/de8/group__core__array_1ga9fa0d58657f60eaa6c71f6fbb40456e3})\n * \n * @param src1 input matrix on the left-hand side of the system.\n * \n * @param src2 input matrix on the right-hand side of the system.\n * \n * @param dst output solution.\n * \n * @param flags solution (matrix inversion) method (DecompTypes)\n */\nexport declare function solve(src1: InputArray, src2: InputArray, dst: OutputArray, flags?: int): bool\n\n/**\n * The function solveCubic finds the real roots of a cubic equation:\n * \n * if coeffs is a 4-element vector: `\\\\[\\\\texttt{coeffs} [0] x^3 + \\\\texttt{coeffs} [1] x^2 +\n * \\\\texttt{coeffs} [2] x + \\\\texttt{coeffs} [3] = 0\\\\]`\n * if coeffs is a 3-element vector: `\\\\[x^3 + \\\\texttt{coeffs} [0] x^2 + \\\\texttt{coeffs} [1] x +\n * \\\\texttt{coeffs} [2] = 0\\\\]`\n * \n * The roots are stored in the roots array. \n * \n * number of real roots. It can be 0, 1 or 2.\n * \n * @param coeffs equation coefficients, an array of 3 or 4 elements.\n * \n * @param roots output array of real roots that has 1 or 3 elements.\n */\nexport declare function solveCubic(coeffs: InputArray, roots: OutputArray): int\n\n/**\n * The function [cv::solvePoly](#d2/de8/group__core__array_1gac2f5e953016fabcdf793d762f4ec5dce}) finds\n * real and complex roots of a polynomial equation: `\\\\[\\\\texttt{coeffs} [n] x^{n} + \\\\texttt{coeffs}\n * [n-1] x^{n-1} + ... + \\\\texttt{coeffs} [1] x + \\\\texttt{coeffs} [0] = 0\\\\]`\n * \n * @param coeffs array of polynomial coefficients.\n * \n * @param roots output (complex) array of roots.\n * \n * @param maxIters maximum number of iterations the algorithm does.\n */\nexport declare function solvePoly(coeffs: InputArray, roots: OutputArray, maxIters?: int): double\n\n/**\n * The function [cv::sort](#d2/de8/group__core__array_1ga45dd56da289494ce874be2324856898f}) sorts each\n * matrix row or each matrix column in ascending or descending order. So you should pass two operation\n * flags to get desired behaviour. If you want to sort matrix rows or columns lexicographically, you\n * can use STL std::sort generic function with the proper comparison predicate.\n * \n * [sortIdx](#d2/de8/group__core__array_1gadf35157cbf97f3cb85a545380e383506}),\n * [randShuffle](#d2/de8/group__core__array_1ga6a789c8a5cb56c6dd62506179808f763})\n * \n * @param src input single-channel array.\n * \n * @param dst output array of the same size and type as src.\n * \n * @param flags operation flags, a combination of SortFlags\n */\nexport declare function sort(src: InputArray, dst: OutputArray, flags: int): void\n\n/**\n * The function [cv::sortIdx](#d2/de8/group__core__array_1gadf35157cbf97f3cb85a545380e383506}) sorts\n * each matrix row or each matrix column in the ascending or descending order. So you should pass two\n * operation flags to get desired behaviour. Instead of reordering the elements themselves, it stores\n * the indices of sorted elements in the output array. For example: \n * \n * ```cpp\n * Mat A = Mat::eye(3,3,CV_32F), B;\n * sortIdx(A, B, SORT_EVERY_ROW + SORT_ASCENDING);\n * // B will probably contain\n * // (because of equal elements in A some permutations are possible):\n * // [[1, 2, 0], [0, 2, 1], [0, 1, 2]]\n * ```\n * \n * [sort](#d2/de8/group__core__array_1ga45dd56da289494ce874be2324856898f}),\n * [randShuffle](#d2/de8/group__core__array_1ga6a789c8a5cb56c6dd62506179808f763})\n * \n * @param src input single-channel array.\n * \n * @param dst output integer array of the same size as src.\n * \n * @param flags operation flags that could be a combination of cv::SortFlags\n */\nexport declare function sortIdx(src: InputArray, dst: OutputArray, flags: int): void\n\n/**\n * The function [cv::split](#d2/de8/group__core__array_1ga0547c7fed86152d7e9d0096029c8518a}) splits a\n * multi-channel array into separate single-channel arrays: `\\\\[\\\\texttt{mv} [c](I) = \\\\texttt{src}\n * (I)_c\\\\]` If you need to extract a single channel or do some other sophisticated channel\n * permutation, use mixChannels .\n * \n * The following example demonstrates how to split a 3-channel matrix into 3 single channel matrices. \n * \n * ```cpp\n *     char d[] = {1,2,3,4,5,6,7,8,9,10,11,12};\n *     Mat m(2, 2, CV_8UC3, d);\n *     Mat channels[3];\n *     split(m, channels);\n * \n *     /*\n *     channels[0] =\n *     [  1,   4;\n *        7,  10]\n *     channels[1] =\n *     [  2,   5;\n *        8,  11]\n *     channels[2] =\n *     [  3,   6;\n *        9,  12]\n * \\/\n * ```\n * \n * [merge](#d2/de8/group__core__array_1ga7d7b4d6c6ee504b30a20b1680029c7b4}),\n * [mixChannels](#d2/de8/group__core__array_1ga51d768c270a1cdd3497255017c4504be}),\n * [cvtColor](#d8/d01/group__imgproc__color__conversions_1ga397ae87e1288a81d2363b61574eb8cab})\n * \n * @param src input multi-channel array.\n * \n * @param mvbegin output array; the number of arrays must match src.channels(); the arrays themselves\n * are reallocated, if needed.\n */\nexport declare function split(src: any, mvbegin: any): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n * \n * @param m input multi-channel array.\n * \n * @param mv output vector of arrays; the arrays themselves are reallocated, if needed.\n */\nexport declare function split(m: InputArray, mv: OutputArrayOfArrays): void\n\n/**\n * The function [cv::sqrt](#d7/dcc/group__core__utils__softfloat_1ga682082a1892db64a2856403ec17ba297})\n * calculates a square root of each input array element. In case of multi-channel arrays, each channel\n * is processed independently. The accuracy is approximately the same as of the built-in std::sqrt .\n * \n * @param src input floating-point array.\n * \n * @param dst output array of the same size and type as src.\n */\nexport declare function sqrt(src: InputArray, dst: OutputArray): void\n\n/**\n * The function subtract calculates:\n * \n * Difference between two arrays, when both input arrays have the same size and the same number of\n * channels: `\\\\[\\\\texttt{dst}(I) = \\\\texttt{saturate} ( \\\\texttt{src1}(I) - \\\\texttt{src2}(I)) \\\\quad\n * \\\\texttt{if mask}(I) \\\\ne0\\\\]`\n * Difference between an array and a scalar, when src2 is constructed from Scalar or has the same\n * number of elements as `src1.channels()`: `\\\\[\\\\texttt{dst}(I) = \\\\texttt{saturate} (\n * \\\\texttt{src1}(I) - \\\\texttt{src2} ) \\\\quad \\\\texttt{if mask}(I) \\\\ne0\\\\]`\n * Difference between a scalar and an array, when src1 is constructed from Scalar or has the same\n * number of elements as `src2.channels()`: `\\\\[\\\\texttt{dst}(I) = \\\\texttt{saturate} ( \\\\texttt{src1}\n * - \\\\texttt{src2}(I) ) \\\\quad \\\\texttt{if mask}(I) \\\\ne0\\\\]`\n * The reverse difference between a scalar and an array in the case of `SubRS`: `\\\\[\\\\texttt{dst}(I) =\n * \\\\texttt{saturate} ( \\\\texttt{src2} - \\\\texttt{src1}(I) ) \\\\quad \\\\texttt{if mask}(I) \\\\ne0\\\\]`\n * where I is a multi-dimensional index of array elements. In case of multi-channel arrays, each\n * channel is processed independently.\n * \n * The first function in the list above can be replaced with matrix expressions: \n * \n * ```cpp\n * dst = src1 - src2;\n * dst -= src1; // equivalent to subtract(dst, src1, dst);\n * ```\n * \n *  The input arrays and the output array can all have the same or different depths. For example, you\n * can subtract to 8-bit unsigned arrays and store the difference in a 16-bit signed array. Depth of\n * the output array is determined by dtype parameter. In the second and third cases above, as well as\n * in the first case, when src1.depth() == src2.depth(), dtype can be set to the default -1. In this\n * case the output array will have the same depth as the input array, be it src1, src2 or both. \n * \n * Saturation is not applied when the output array has the depth CV_32S. You may even get result of an\n * incorrect sign in the case of overflow. \n * \n * [add](#d2/de8/group__core__array_1ga10ac1bfb180e2cfda1701d06c24fdbd6}),\n * [addWeighted](#d2/de8/group__core__array_1gafafb2513349db3bcff51f54ee5592a19}),\n * [scaleAdd](#d2/de8/group__core__array_1ga9e0845db4135f55dcf20227402f00d98}),\n * [Mat::convertTo](#d3/d63/classcv_1_1Mat_1adf88c60c5b4980e05bb556080916978b})\n * \n * @param src1 first input array or a scalar.\n * \n * @param src2 second input array or a scalar.\n * \n * @param dst output array of the same size and the same number of channels as the input array.\n * \n * @param mask optional operation mask; this is an 8-bit single channel array that specifies elements\n * of the output array to be changed.\n * \n * @param dtype optional depth of the output array\n */\nexport declare function subtract(src1: InputArray, src2: InputArray, dst: OutputArray, mask?: InputArray, dtype?: int): void\n\n/**\n * The function [cv::sum](#d2/de8/group__core__array_1ga716e10a2dd9e228e4d3c95818f106722}) calculates\n * and returns the sum of array elements, independently for each channel. \n * \n * [countNonZero](#d2/de8/group__core__array_1gaa4b89393263bb4d604e0fe5986723914}),\n * [mean](#d2/de8/group__core__array_1ga191389f8a0e58180bb13a727782cd461}),\n * [meanStdDev](#d2/de8/group__core__array_1ga846c858f4004d59493d7c6a4354b301d}),\n * [norm](#dc/d84/group__core__basic_1ga4e556cb8ad35a643a1ea66e035711bb9}),\n * [minMaxLoc](#d2/de8/group__core__array_1gab473bf2eb6d14ff97e89b355dac20707}),\n * [reduce](#d2/de8/group__core__array_1ga4b78072a303f29d9031d56e5638da78e})\n * \n * @param src input array that must have from 1 to 4 channels.\n */\nexport declare function sum(src: InputArray): Scalar\n\n/**\n * wrap [SVD::backSubst](#df/df7/classcv_1_1SVD_1a7c28935c9999977dbe34285d13d43190})\n */\nexport declare function SVBackSubst(w: InputArray, u: InputArray, vt: InputArray, rhs: InputArray, dst: OutputArray): void\n\n/**\n * wrap [SVD::compute](#df/df7/classcv_1_1SVD_1a76f0b2044df458160292045a3d3714c6})\n */\nexport declare function SVDecomp(src: InputArray, w: OutputArray, u: OutputArray, vt: OutputArray, flags?: int): void\n\n/**\n * The function [cv::theRNG](#d2/de8/group__core__array_1ga75843061d150ad6564b5447e38e57722}) returns\n * the default random number generator. For each thread, there is a separate random number generator,\n * so you can use the function safely in multi-thread environments. If you just need to get a single\n * random number using this generator or initialize an array, you can use randu or randn instead. But\n * if you are going to generate many random numbers inside a loop, it is much faster to use this\n * function to retrieve the generator and then use RNG::operator _Tp() . \n * \n * [RNG](#d1/dd6/classcv_1_1RNG}),\n * [randu](#d2/de8/group__core__array_1ga1ba1026dca0807b27057ba6a49d258c0}),\n * [randn](#d2/de8/group__core__array_1gaeff1f61e972d133a04ce3a5f81cf6808})\n */\nexport declare function theRNG(): any\n\n/**\n * The function [cv::trace](#dc/d84/group__core__basic_1ga36ad18631177b097a38198c4e83c6e2b}) returns\n * the sum of the diagonal elements of the matrix mtx . `\\\\[\\\\mathrm{tr} ( \\\\texttt{mtx} ) = \\\\sum _i\n * \\\\texttt{mtx} (i,i)\\\\]`\n * \n * @param mtx input matrix.\n */\nexport declare function trace(mtx: InputArray): Scalar\n\n/**\n * The function [cv::transform](#d2/de8/group__core__array_1ga393164aa54bb9169ce0a8cc44e08ff22})\n * performs the matrix transformation of every element of the array src and stores the results in dst :\n * `\\\\[\\\\texttt{dst} (I) = \\\\texttt{m} \\\\cdot \\\\texttt{src} (I)\\\\]` (when m.cols=src.channels() ), or\n * `\\\\[\\\\texttt{dst} (I) = \\\\texttt{m} \\\\cdot [ \\\\texttt{src} (I); 1]\\\\]` (when m.cols=src.channels()+1\n * )\n * \n * Every element of the N -channel array src is interpreted as N -element vector that is transformed\n * using the M x N or M x (N+1) matrix m to M-element vector - the corresponding element of the output\n * array dst .\n * \n * The function may be used for geometrical transformation of N -dimensional points, arbitrary linear\n * color space transformation (such as various kinds of RGB to YUV transforms), shuffling the image\n * channels, and so forth. \n * \n * [perspectiveTransform](#d2/de8/group__core__array_1gad327659ac03e5fd6894b90025e6900a7}),\n * [getAffineTransform](#da/d54/group__imgproc__transform_1ga8f6d378f9f8eebb5cb55cd3ae295a999}),\n * [estimateAffine2D](#d9/d0c/group__calib3d_1ga27865b1d26bac9ce91efaee83e94d4dd}),\n * [warpAffine](#da/d54/group__imgproc__transform_1ga0203d9ee5fcd28d40dbc4a1ea4451983}),\n * [warpPerspective](#da/d54/group__imgproc__transform_1gaf73673a7e8e18ec6963e3774e6a94b87})\n * \n * @param src input array that must have as many channels (1 to 4) as m.cols or m.cols-1.\n * \n * @param dst output array of the same size and depth as src; it has as many channels as m.rows.\n * \n * @param m transformation 2x2 or 2x3 floating-point matrix.\n */\nexport declare function transform(src: InputArray, dst: OutputArray, m: InputArray): void\n\n/**\n * The function [cv::transpose](#d2/de8/group__core__array_1ga46630ed6c0ea6254a35f447289bd7404})\n * transposes the matrix src : `\\\\[\\\\texttt{dst} (i,j) = \\\\texttt{src} (j,i)\\\\]` \n * \n * No complex conjugation is done in case of a complex matrix. It should be done separately if needed.\n * \n * @param src input array.\n * \n * @param dst output array of the same type as src.\n */\nexport declare function transpose(src: InputArray, dst: OutputArray): void\n\n/**\n * The function vertically concatenates two or more [cv::Mat](#d3/d63/classcv_1_1Mat}) matrices (with\n * the same number of cols). \n * \n * ```cpp\n * cv::Mat matArray[] = { cv::Mat(1, 4, CV_8UC1, cv::Scalar(1)),\n *                        cv::Mat(1, 4, CV_8UC1, cv::Scalar(2)),\n *                        cv::Mat(1, 4, CV_8UC1, cv::Scalar(3)),};\n * \n * cv::Mat out;\n * cv::vconcat( matArray, 3, out );\n * //out:\n * //[1,   1,   1,   1;\n * // 2,   2,   2,   2;\n * // 3,   3,   3,   3]\n * ```\n * \n * [cv::hconcat(const Mat*, size_t,\n * OutputArray)](#d2/de8/group__core__array_1gaf9771c991763233866bf76b5b5d1776f}), \n * \n * [cv::hconcat(InputArrayOfArrays,\n * OutputArray)](#d2/de8/group__core__array_1ga4676b1376cdc4e528dab6bd9edc51c1a}) and \n * \n * [cv::hconcat(InputArray, InputArray,\n * OutputArray)](#d2/de8/group__core__array_1gaab5ceee39e0580f879df645a872c6bf7})\n * \n * @param src input array or vector of matrices. all of the matrices must have the same number of cols\n * and the same depth.\n * \n * @param nsrc number of matrices in src.\n * \n * @param dst output array. It has the same number of cols and depth as the src, and the sum of rows of\n * the src.\n */\nexport declare function vconcat(src: any, nsrc: size_t, dst: OutputArray): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts. \n * \n * ```cpp\n * cv::Mat_<float> A = (cv::Mat_<float>(3, 2) << 1, 7,\n *                                               2, 8,\n *                                               3, 9);\n * cv::Mat_<float> B = (cv::Mat_<float>(3, 2) << 4, 10,\n *                                               5, 11,\n *                                               6, 12);\n * \n * cv::Mat C;\n * cv::vconcat(A, B, C);\n * //C:\n * //[1, 7;\n * // 2, 8;\n * // 3, 9;\n * // 4, 10;\n * // 5, 11;\n * // 6, 12]\n * ```\n * \n * @param src1 first input array to be considered for vertical concatenation.\n * \n * @param src2 second input array to be considered for vertical concatenation.\n * \n * @param dst output array. It has the same number of cols and depth as the src1 and src2, and the sum\n * of rows of the src1 and src2.\n */\nexport declare function vconcat(src1: InputArray, src2: InputArray, dst: OutputArray): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts. \n * \n * ```cpp\n * std::vector<cv::Mat> matrices = { cv::Mat(1, 4, CV_8UC1, cv::Scalar(1)),\n *                                   cv::Mat(1, 4, CV_8UC1, cv::Scalar(2)),\n *                                   cv::Mat(1, 4, CV_8UC1, cv::Scalar(3)),};\n * \n * cv::Mat out;\n * cv::vconcat( matrices, out );\n * //out:\n * //[1,   1,   1,   1;\n * // 2,   2,   2,   2;\n * // 3,   3,   3,   3]\n * ```\n * \n * @param src input array or vector of matrices. all of the matrices must have the same number of cols\n * and the same depth\n * \n * @param dst output array. It has the same number of cols and depth as the src, and the sum of rows of\n * the src. same depth.\n */\nexport declare function vconcat(src: InputArrayOfArrays, dst: OutputArray): void\n\nexport declare const BORDER_CONSTANT: BorderTypes // initializer: = 0\n\nexport declare const BORDER_REPLICATE: BorderTypes // initializer: = 1\n\nexport declare const BORDER_REFLECT: BorderTypes // initializer: = 2\n\nexport declare const BORDER_WRAP: BorderTypes // initializer: = 3\n\nexport declare const BORDER_REFLECT_101: BorderTypes // initializer: = 4\n\nexport declare const BORDER_TRANSPARENT: BorderTypes // initializer: = 5\n\nexport declare const BORDER_REFLECT101: BorderTypes // initializer: = BORDER_REFLECT_101\n\nexport declare const BORDER_DEFAULT: BorderTypes // initializer: = BORDER_REFLECT_101\n\nexport declare const BORDER_ISOLATED: BorderTypes // initializer: = 16\n\nexport declare const CMP_EQ: CmpTypes // initializer: = 0\n\nexport declare const CMP_GT: CmpTypes // initializer: = 1\n\nexport declare const CMP_GE: CmpTypes // initializer: = 2\n\nexport declare const CMP_LT: CmpTypes // initializer: = 3\n\nexport declare const CMP_LE: CmpTypes // initializer: = 4\n\nexport declare const CMP_NE: CmpTypes // initializer: = 5\n\n/**\n * Gaussian elimination with the optimal pivot element chosen.\n * \n */\nexport declare const DECOMP_LU: DecompTypes // initializer: = 0\n\n/**\n * singular value decomposition ([SVD](#df/df7/classcv_1_1SVD})) method; the system can be over-defined\n * and/or the matrix src1 can be singular\n * \n */\nexport declare const DECOMP_SVD: DecompTypes // initializer: = 1\n\n/**\n * eigenvalue decomposition; the matrix src1 must be symmetrical\n * \n */\nexport declare const DECOMP_EIG: DecompTypes // initializer: = 2\n\n/**\n * Cholesky `$LL^T$` factorization; the matrix src1 must be symmetrical and positively defined\n * \n */\nexport declare const DECOMP_CHOLESKY: DecompTypes // initializer: = 3\n\n/**\n * QR factorization; the system can be over-defined and/or the matrix src1 can be singular\n * \n */\nexport declare const DECOMP_QR: DecompTypes // initializer: = 4\n\n/**\n * while all the previous flags are mutually exclusive, this flag can be used together with any of the\n * previous; it means that the normal equations\n * `$\\\\texttt{src1}^T\\\\cdot\\\\texttt{src1}\\\\cdot\\\\texttt{dst}=\\\\texttt{src1}^T\\\\texttt{src2}$` are\n * solved instead of the original system `$\\\\texttt{src1}\\\\cdot\\\\texttt{dst}=\\\\texttt{src2}$`\n * \n */\nexport declare const DECOMP_NORMAL: DecompTypes // initializer: = 16\n\n/**\n * performs an inverse 1D or 2D transform instead of the default forward transform.\n * \n */\nexport declare const DFT_INVERSE: DftFlags // initializer: = 1\n\n/**\n * scales the result: divide it by the number of array elements. Normally, it is combined with\n * DFT_INVERSE.\n * \n */\nexport declare const DFT_SCALE: DftFlags // initializer: = 2\n\n/**\n * performs a forward or inverse transform of every individual row of the input matrix; this flag\n * enables you to transform multiple vectors simultaneously and can be used to decrease the overhead\n * (which is sometimes several times larger than the processing itself) to perform 3D and\n * higher-dimensional transformations and so forth.\n * \n */\nexport declare const DFT_ROWS: DftFlags // initializer: = 4\n\n/**\n * performs a forward transformation of 1D or 2D real array; the result, though being a complex array,\n * has complex-conjugate symmetry (*CCS*, see the function description below for details), and such an\n * array can be packed into a real array of the same size as input, which is the fastest option and\n * which is what the function does by default; however, you may wish to get a full complex array (for\n * simpler spectrum analysis, and so on) - pass the flag to enable the function to produce a full-size\n * complex output array.\n * \n */\nexport declare const DFT_COMPLEX_OUTPUT: DftFlags // initializer: = 16\n\n/**\n * performs an inverse transformation of a 1D or 2D complex array; the result is normally a complex\n * array of the same size, however, if the input array has conjugate-complex symmetry (for example, it\n * is a result of forward transformation with DFT_COMPLEX_OUTPUT flag), the output is a real array;\n * while the function itself does not check whether the input is symmetrical or not, you can pass the\n * flag and then the function will assume the symmetry and produce the real output array (note that\n * when the input is packed into a real array and inverse transformation is executed, the function\n * treats the input as a packed complex-conjugate symmetrical array, and the output will also be a real\n * array).\n * \n */\nexport declare const DFT_REAL_OUTPUT: DftFlags // initializer: = 32\n\n/**\n * specifies that input is complex input. If this flag is set, the input must have 2 channels. On the\n * other hand, for backwards compatibility reason, if input has 2 channels, input is already considered\n * complex.\n * \n */\nexport declare const DFT_COMPLEX_INPUT: DftFlags // initializer: = 64\n\n/**\n * performs an inverse 1D or 2D transform instead of the default forward transform.\n * \n */\nexport declare const DCT_INVERSE: DftFlags // initializer: = DFT_INVERSE\n\n/**\n * performs a forward or inverse transform of every individual row of the input matrix. This flag\n * enables you to transform multiple vectors simultaneously and can be used to decrease the overhead\n * (which is sometimes several times larger than the processing itself) to perform 3D and\n * higher-dimensional transforms and so forth.\n * \n */\nexport declare const DCT_ROWS: DftFlags // initializer: = DFT_ROWS\n\nexport declare const GEMM_1_T: GemmFlags // initializer: = 1\n\nexport declare const GEMM_2_T: GemmFlags // initializer: = 2\n\nexport declare const GEMM_3_T: GemmFlags // initializer: = 4\n\n/**\n * `\\\\[ norm = \\\\forkthree {\\\\|\\\\texttt{src1}\\\\|_{L_{\\\\infty}} = \\\\max _I | \\\\texttt{src1} (I)|}{if\n * \\\\(\\\\texttt{normType} = \\\\texttt{NORM_INF}\\\\) } {\\\\|\\\\texttt{src1}-\\\\texttt{src2}\\\\|_{L_{\\\\infty}} =\n * \\\\max _I | \\\\texttt{src1} (I) - \\\\texttt{src2} (I)|}{if \\\\(\\\\texttt{normType} =\n * \\\\texttt{NORM_INF}\\\\) } {\\\\frac{\\\\|\\\\texttt{src1}-\\\\texttt{src2}\\\\|_{L_{\\\\infty}}\n * }{\\\\|\\\\texttt{src2}\\\\|_{L_{\\\\infty}} }}{if \\\\(\\\\texttt{normType} = \\\\texttt{NORM_RELATIVE |\n * NORM_INF}\\\\) } \\\\]`\n * \n */\nexport declare const NORM_INF: NormTypes // initializer: = 1\n\n/**\n * `\\\\[ norm = \\\\forkthree {\\\\| \\\\texttt{src1} \\\\| _{L_1} = \\\\sum _I | \\\\texttt{src1} (I)|}{if\n * \\\\(\\\\texttt{normType} = \\\\texttt{NORM_L1}\\\\)} { \\\\| \\\\texttt{src1} - \\\\texttt{src2} \\\\| _{L_1} =\n * \\\\sum _I | \\\\texttt{src1} (I) - \\\\texttt{src2} (I)|}{if \\\\(\\\\texttt{normType} = \\\\texttt{NORM_L1}\\\\)\n * } { \\\\frac{\\\\|\\\\texttt{src1}-\\\\texttt{src2}\\\\|_{L_1} }{\\\\|\\\\texttt{src2}\\\\|_{L_1}} }{if\n * \\\\(\\\\texttt{normType} = \\\\texttt{NORM_RELATIVE | NORM_L1}\\\\) } \\\\]`\n * \n */\nexport declare const NORM_L1: NormTypes // initializer: = 2\n\n/**\n * `\\\\[ norm = \\\\forkthree { \\\\| \\\\texttt{src1} \\\\| _{L_2} = \\\\sqrt{\\\\sum_I \\\\texttt{src1}(I)^2} }{if\n * \\\\(\\\\texttt{normType} = \\\\texttt{NORM_L2}\\\\) } { \\\\| \\\\texttt{src1} - \\\\texttt{src2} \\\\| _{L_2} =\n * \\\\sqrt{\\\\sum_I (\\\\texttt{src1}(I) - \\\\texttt{src2}(I))^2} }{if \\\\(\\\\texttt{normType} =\n * \\\\texttt{NORM_L2}\\\\) } { \\\\frac{\\\\|\\\\texttt{src1}-\\\\texttt{src2}\\\\|_{L_2}\n * }{\\\\|\\\\texttt{src2}\\\\|_{L_2}} }{if \\\\(\\\\texttt{normType} = \\\\texttt{NORM_RELATIVE | NORM_L2}\\\\) }\n * \\\\]`\n * \n */\nexport declare const NORM_L2: NormTypes // initializer: = 4\n\n/**\n * `\\\\[ norm = \\\\forkthree { \\\\| \\\\texttt{src1} \\\\| _{L_2} ^{2} = \\\\sum_I \\\\texttt{src1}(I)^2} {if\n * \\\\(\\\\texttt{normType} = \\\\texttt{NORM_L2SQR}\\\\)} { \\\\| \\\\texttt{src1} - \\\\texttt{src2} \\\\| _{L_2}\n * ^{2} = \\\\sum_I (\\\\texttt{src1}(I) - \\\\texttt{src2}(I))^2 }{if \\\\(\\\\texttt{normType} =\n * \\\\texttt{NORM_L2SQR}\\\\) } { \\\\left(\\\\frac{\\\\|\\\\texttt{src1}-\\\\texttt{src2}\\\\|_{L_2}\n * }{\\\\|\\\\texttt{src2}\\\\|_{L_2}}\\\\right)^2 }{if \\\\(\\\\texttt{normType} = \\\\texttt{NORM_RELATIVE |\n * NORM_L2SQR}\\\\) } \\\\]`\n * \n */\nexport declare const NORM_L2SQR: NormTypes // initializer: = 5\n\n/**\n * In the case of one input array, calculates the [Hamming](#d3/d59/structcv_1_1Hamming}) distance of\n * the array from zero, In the case of two input arrays, calculates the\n * [Hamming](#d3/d59/structcv_1_1Hamming}) distance between the arrays.\n * \n */\nexport declare const NORM_HAMMING: NormTypes // initializer: = 6\n\n/**\n * Similar to NORM_HAMMING, but in the calculation, each two bits of the input sequence will be added\n * and treated as a single bit to be used in the same calculation as NORM_HAMMING.\n * \n */\nexport declare const NORM_HAMMING2: NormTypes // initializer: = 7\n\nexport declare const NORM_TYPE_MASK: NormTypes // initializer: = 7\n\nexport declare const NORM_RELATIVE: NormTypes // initializer: = 8\n\nexport declare const NORM_MINMAX: NormTypes // initializer: = 32\n\nexport declare const ROTATE_90_CLOCKWISE: RotateFlags // initializer: = 0\n\nexport declare const ROTATE_180: RotateFlags // initializer: = 1\n\nexport declare const ROTATE_90_COUNTERCLOCKWISE: RotateFlags // initializer: = 2\n\n/**\n * Various border types, image boundaries are denoted with `|` \n * \n * [borderInterpolate](#d2/de8/group__core__array_1ga247f571aa6244827d3d798f13892da58}),\n * [copyMakeBorder](#d2/de8/group__core__array_1ga2ac1049c2c3dd25c2b41bffe17658a36})\n * \n */\nexport type BorderTypes = any\n\n/**\n * Various border types, image boundaries are denoted with `|` \n * \n * [borderInterpolate](#d2/de8/group__core__array_1ga247f571aa6244827d3d798f13892da58}),\n * [copyMakeBorder](#d2/de8/group__core__array_1ga2ac1049c2c3dd25c2b41bffe17658a36})\n * \n */\nexport type CmpTypes = any\n\n/**\n * Various border types, image boundaries are denoted with `|` \n * \n * [borderInterpolate](#d2/de8/group__core__array_1ga247f571aa6244827d3d798f13892da58}),\n * [copyMakeBorder](#d2/de8/group__core__array_1ga2ac1049c2c3dd25c2b41bffe17658a36})\n * \n */\nexport type DecompTypes = any\n\n/**\n * Various border types, image boundaries are denoted with `|` \n * \n * [borderInterpolate](#d2/de8/group__core__array_1ga247f571aa6244827d3d798f13892da58}),\n * [copyMakeBorder](#d2/de8/group__core__array_1ga2ac1049c2c3dd25c2b41bffe17658a36})\n * \n */\nexport type DftFlags = any\n\n/**\n * Various border types, image boundaries are denoted with `|` \n * \n * [borderInterpolate](#d2/de8/group__core__array_1ga247f571aa6244827d3d798f13892da58}),\n * [copyMakeBorder](#d2/de8/group__core__array_1ga2ac1049c2c3dd25c2b41bffe17658a36})\n * \n */\nexport type GemmFlags = any\n\n/**\n * Various border types, image boundaries are denoted with `|` \n * \n * [borderInterpolate](#d2/de8/group__core__array_1ga247f571aa6244827d3d798f13892da58}),\n * [copyMakeBorder](#d2/de8/group__core__array_1ga2ac1049c2c3dd25c2b41bffe17658a36})\n * \n */\nexport type NormTypes = any\n\n/**\n * Various border types, image boundaries are denoted with `|` \n * \n * [borderInterpolate](#d2/de8/group__core__array_1ga247f571aa6244827d3d798f13892da58}),\n * [copyMakeBorder](#d2/de8/group__core__array_1ga2ac1049c2c3dd25c2b41bffe17658a36})\n * \n */\nexport type RotateFlags = any\n\n"},"node_modules_mirada_dist_src_types_opencv_core_utils_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_core_utils_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/core_utils.d.ts","content":"\nimport { AsyncArray, bool, double, ErrorCallback, float, float16_t, InputArray, InputArrayOfArrays, InputOutputArray, InputOutputArrayOfArrays, int, int64, schar, short, size_t, uchar, uint64, unsigned, ushort, _Tp } from './_types'\n/*\n * # Utility and system functions and macros\n * \n */\n/**\n * The function returns the aligned pointer of the same type as the input pointer:\n * `\\\\[\\\\texttt{(_Tp*)(((size_t)ptr + n-1) & -n)}\\\\]`\n * \n * @param ptr Aligned pointer.\n * \n * @param n Alignment size that must be a power of two.\n */\nexport declare function alignPtr(arg92: any, ptr: any, n?: int): any\n\n/**\n * The function returns the minimum number that is greater than or equal to sz and is divisible by n :\n * `\\\\[\\\\texttt{(sz + n-1) & -n}\\\\]`\n * \n * @param sz Buffer size to align.\n * \n * @param n Alignment size that must be a power of two.\n */\nexport declare function alignSize(sz: size_t, n: int): size_t\n\n/**\n * The function returns true if the host hardware supports the specified feature. When user calls\n * setUseOptimized(false), the subsequent calls to\n * [checkHardwareSupport()](#db/de0/group__core__utils_1ga83400136ccc28490087722ef5b3a27d9}) will\n * return false until setUseOptimized(true) is called. This way user can dynamically switch on and off\n * the optimized code in OpenCV.\n * \n * @param feature The feature of interest, one of cv::CpuFeatures\n */\nexport declare function checkHardwareSupport(feature: int): bool\n\n/**\n * proxy for hal::Cholesky\n */\nexport declare function Cholesky(A: any, astep: size_t, m: int, b: any, bstep: size_t, n: int): bool\n\n/**\n * proxy for hal::Cholesky\n */\nexport declare function Cholesky(A: any, astep: size_t, m: int, b: any, bstep: size_t, n: int): bool\n\n/**\n * The function cubeRoot computes `$\\\\sqrt[3]{\\\\texttt{val}}$`. Negative arguments are handled\n * correctly. NaN and Inf are not handled. The accuracy approaches the maximum possible accuracy for\n * single-precision data.\n * \n * @param val A function argument.\n */\nexport declare function cubeRoot(val: float): float\n\nexport declare function cv_abs(arg93: any, x: _Tp): any\n\nexport declare function cv_abs(x: uchar): uchar\n\nexport declare function cv_abs(x: schar): schar\n\nexport declare function cv_abs(x: ushort): ushort\n\nexport declare function cv_abs(x: short): int\n\nexport declare function CV_XADD(addr: any, delta: int): any\n\n/**\n * The function computes an integer i such that: `\\\\[i \\\\le \\\\texttt{value} < i+1\\\\]`\n * \n * @param value floating-point number. If the value is outside of INT_MIN ... INT_MAX range, the result\n * is not defined.\n */\nexport declare function cvCeil(value: double): int\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function cvCeil(value: float): int\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function cvCeil(value: int): int\n\n/**\n * The function computes an integer i such that: `\\\\[i \\\\le \\\\texttt{value} < i+1\\\\]`\n * \n * @param value floating-point number. If the value is outside of INT_MIN ... INT_MAX range, the result\n * is not defined.\n */\nexport declare function cvFloor(value: double): int\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function cvFloor(value: float): int\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function cvFloor(value: int): int\n\n/**\n * The function returns 1 if the argument is a plus or minus infinity (as defined by IEEE754 standard)\n * and 0 otherwise.\n * \n * @param value The input floating-point value\n */\nexport declare function cvIsInf(value: double): int\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function cvIsInf(value: float): int\n\n/**\n * The function returns 1 if the argument is Not A Number (as defined by IEEE754 standard), 0\n * otherwise.\n * \n * @param value The input floating-point value\n */\nexport declare function cvIsNaN(value: double): int\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function cvIsNaN(value: float): int\n\n/**\n * @param value floating-point number. If the value is outside of INT_MIN ... INT_MAX range, the result\n * is not defined.\n */\nexport declare function cvRound(value: double): int\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function cvRound(value: float): int\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function cvRound(value: int): int\n\n/**\n * Use this function instead of `ceil((float)a / b)` expressions.\n * \n * [alignSize](#db/de0/group__core__utils_1gaf2ff8f837e7a44f288b050765492f800})\n */\nexport declare function divUp(a: int, b: any): int\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function divUp(a: size_t, b: any): size_t\n\nexport declare function dumpInputArray(argument: InputArray): String\n\nexport declare function dumpInputArrayOfArrays(argument: InputArrayOfArrays): String\n\nexport declare function dumpInputOutputArray(argument: InputOutputArray): String\n\nexport declare function dumpInputOutputArrayOfArrays(argument: InputOutputArrayOfArrays): String\n\n/**\n * By default the function prints information about the error to stderr, then it either stops if\n * [cv::setBreakOnError()](#db/de0/group__core__utils_1gae4904ef072dede53cf161d9a6869083f}) had been\n * called before or raises the exception. It is possible to alternate error processing by using\n * [redirectError()](#db/de0/group__core__utils_1ga8cd28eccccb695570cdaf86a572d2c0c}).\n * \n * @param exc the exception raisen.\n */\nexport declare function error(exc: any): void\n\n/**\n * By default the function prints information about the error to stderr, then it either stops if\n * [setBreakOnError()](#db/de0/group__core__utils_1gae4904ef072dede53cf161d9a6869083f}) had been called\n * before or raises the exception. It is possible to alternate error processing by using\n * [redirectError()](#db/de0/group__core__utils_1ga8cd28eccccb695570cdaf86a572d2c0c}). \n * \n * [CV_Error](#db/de0/group__core__utils_1ga5b48c333c777666e076bd7052799f891}),\n * [CV_Error_](#db/de0/group__core__utils_1ga1c0cd6e5bd9a5f915c6cab9c0632f969}),\n * [CV_Assert](#db/de0/group__core__utils_1gaf62bcd90f70e275191ab95136d85906b}),\n * [CV_DbgAssert](#db/de0/group__core__utils_1gafbcb487cba05bd288dbe18c433de4f6f})\n * \n * @param _code - error code (Error::Code)\n * \n * @param _err - error description\n * \n * @param _func - function name. Available only when the compiler supports getting it\n * \n * @param _file - source file name where the error has occurred\n * \n * @param _line - line number in the source file where the error has occurred\n */\nexport declare function error(_code: int, _err: any, _func: any, _file: any, _line: int): void\n\n/**\n * The function fastAtan2 calculates the full-range angle of an input 2D vector. The angle is measured\n * in degrees and varies from 0 to 360 degrees. The accuracy is about 0.3 degrees.\n * \n * @param y y-coordinate of the vector.\n * \n * @param x x-coordinate of the vector.\n */\nexport declare function fastAtan2(y: float, x: float): float\n\n/**\n * The function deallocates the buffer allocated with fastMalloc . If NULL pointer is passed, the\n * function does nothing. C version of the function clears the pointer *pptr* to avoid problems with\n * double memory deallocation.\n * \n * @param ptr Pointer to the allocated buffer.\n */\nexport declare function fastFree(ptr: any): void\n\n/**\n * The function allocates the buffer of the specified size and returns it. When the buffer size is 16\n * bytes or more, the returned buffer is aligned to 16 bytes.\n * \n * @param bufSize Allocated buffer size.\n */\nexport declare function fastMalloc(bufSize: size_t): any\n\nexport declare function forEach_impl(arg94: any, arg95: any, operation: any): any\n\n/**\n * Returned value is raw cmake output including version control system revision, compiler version,\n * compiler flags, enabled modules and third party libraries, etc. Output format depends on target\n * architecture.\n */\nexport declare function getBuildInformation(): any\n\n/**\n * Returned value is a string containing space separated list of CPU features with following markers:\n * \n * no markers - baseline features\n * prefix `*` - features enabled in dispatcher\n * suffix `?` - features enabled but not available in HW\n * \n * Example: `SSE SSE2 SSE3 *SSE4.1 *SSE4.2 *FP16 *AVX *AVX2 *AVX512-SKX?`\n */\nexport declare function getCPUFeaturesLine(): any\n\n/**\n * The function returns the current number of CPU ticks on some architectures (such as x86, x64,\n * PowerPC). On other platforms the function is equivalent to getTickCount. It can also be used for\n * very accurate time measurements, as well as for [RNG](#d1/dd6/classcv_1_1RNG}) initialization. Note\n * that in case of multi-CPU systems a thread, from which getCPUTickCount is called, can be suspended\n * and resumed at another CPU with its own counter. So, theoretically (and practically) the subsequent\n * calls to the function do not necessary return the monotonously increasing values. Also, since a\n * modern CPU varies the CPU frequency depending on the load, the number of CPU clocks spent in some\n * code cannot be directly converted to time units. Therefore, getTickCount is generally a preferable\n * solution for measuring execution time.\n */\nexport declare function getCPUTickCount(): int64\n\nexport declare function getElemSize(type: int): size_t\n\n/**\n * Returns empty string if feature is not defined\n */\nexport declare function getHardwareFeatureName(feature: int): String\n\nexport declare function getNumberOfCPUs(): int\n\n/**\n * Always returns 1 if OpenCV is built without threading support.\n * \n * The exact meaning of return value depends on the threading framework used by OpenCV library:\n * \n * `TBB` - The number of threads, that OpenCV will try to use for parallel regions. If there is any\n * tbb::thread_scheduler_init in user code conflicting with OpenCV, then function returns default\n * number of threads used by TBB library.\n * `OpenMP` - An upper bound on the number of threads that could be used to form a new team.\n * `Concurrency` - The number of threads, that OpenCV will try to use for parallel regions.\n * `GCD` - Unsupported; returns the GCD thread pool limit (512) for compatibility.\n * `C=` - The number of threads, that OpenCV will try to use for parallel regions, if before called\n * setNumThreads with threads > 0, otherwise returns the number of logical CPUs, available for the\n * process. \n * \n * [setNumThreads](#db/de0/group__core__utils_1gae78625c3c2aa9e0b83ed31b73c6549c0}),\n * [getThreadNum](#db/de0/group__core__utils_1gaf9cc0cb10097686a9da60f6c587e5774})\n */\nexport declare function getNumThreads(): int\n\n/**\n * The exact meaning of the return value depends on the threading framework used by OpenCV library:\n * \n * `TBB` - Unsupported with current 4.1 TBB release. Maybe will be supported in future.\n * `OpenMP` - The thread number, within the current team, of the calling thread.\n * `Concurrency` - An ID for the virtual processor that the current context is executing on (0 for\n * master thread and unique number for others, but not necessary 1,2,3,...).\n * `GCD` - System calling thread's ID. Never returns 0 inside parallel region.\n * `C=` - The index of the current parallel task. \n * \n * [setNumThreads](#db/de0/group__core__utils_1gae78625c3c2aa9e0b83ed31b73c6549c0}),\n * [getNumThreads](#db/de0/group__core__utils_1ga2db334ec41d98da3129ef4a2342fc4d4})\n */\nexport declare function getThreadNum(): int\n\n/**\n * The function returns the number of ticks after the certain event (for example, when the machine was\n * turned on). It can be used to initialize [RNG](#d1/dd6/classcv_1_1RNG}) or to measure a function\n * execution time by reading the tick count before and after the function call. \n * \n * [getTickFrequency](#db/de0/group__core__utils_1ga705441a9ef01f47acdc55d87fbe5090c}),\n * [TickMeter](#d9/d6f/classcv_1_1TickMeter})\n */\nexport declare function getTickCount(): int64\n\n/**\n * The function returns the number of ticks per second. That is, the following code computes the\n * execution time in seconds: \n * \n * ```cpp\n * double t = (double)getTickCount();\n * // do something ...\n * t = ((double)getTickCount() - t)/getTickFrequency();\n * ```\n * \n * [getTickCount](#db/de0/group__core__utils_1gae73f58000611a1af25dd36d496bf4487}),\n * [TickMeter](#d9/d6f/classcv_1_1TickMeter})\n */\nexport declare function getTickFrequency(): double\n\nexport declare function getVersionMajor(): int\n\nexport declare function getVersionMinor(): int\n\nexport declare function getVersionRevision(): int\n\n/**\n * For example \"3.4.1-dev\".\n * \n * getMajorVersion, getMinorVersion, getRevisionVersion\n */\nexport declare function getVersionString(): String\n\nexport declare function glob(pattern: String, result: any, recursive?: bool): void\n\n/**\n * proxy for hal::LU\n */\nexport declare function LU(A: any, astep: size_t, m: int, b: any, bstep: size_t, n: int): int\n\n/**\n * proxy for hal::LU\n */\nexport declare function LU(A: any, astep: size_t, m: int, b: any, bstep: size_t, n: int): int\n\nexport declare function normInf(arg96: any, arg97: any, a: any, n: int): any\n\nexport declare function normInf(arg98: any, arg99: any, a: any, b: any, n: int): any\n\nexport declare function normL1(arg100: any, arg101: any, a: any, n: int): any\n\nexport declare function normL1(arg102: any, arg103: any, a: any, b: any, n: int): any\n\nexport declare function normL1(a: any, b: any, n: int): float\n\nexport declare function normL1(a: uchar, b: uchar, n: int): uchar\n\nexport declare function normL2Sqr(arg104: any, arg105: any, a: any, n: int): any\n\nexport declare function normL2Sqr(arg106: any, arg107: any, a: any, b: any, n: int): any\n\nexport declare function normL2Sqr(a: any, b: any, n: int): float\n\nexport declare function parallel_for_(range: any, body: any, nstripes?: double): void\n\nexport declare function parallel_for_(range: any, functor: any, nstripes?: double): void\n\n/**\n * The function sets the new error handler, called from\n * [cv::error()](#db/de0/group__core__utils_1gacbd081fdb20423a63cf731569ba70b2b}).\n * \n * the previous error handler\n * \n * @param errCallback the new error handler. If NULL, the default error handler is used.\n * \n * @param userdata the optional user data pointer, passed to the callback.\n * \n * @param prevUserdata the optional output parameter where the previous user data pointer is stored\n */\nexport declare function redirectError(errCallback: ErrorCallback, userdata?: any, prevUserdata?: any): ErrorCallback\n\n/**\n * Use this function instead of `ceil((float)a / b) * b` expressions.\n * \n * [divUp](#db/de0/group__core__utils_1ga52d39d0a7310cba0cf526bc9dbda5404})\n */\nexport declare function roundUp(a: int, b: any): int\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function roundUp(a: size_t, b: any): size_t\n\n/**\n * The function saturate_cast resembles the standard C++ cast operations, such as static_cast<T>() and\n * others. It perform an efficient and accurate conversion from one primitive type to another (see the\n * introduction chapter). saturate in the name means that when the input value v is out of the range of\n * the target type, the result is not formed just by taking low bits of the input, but instead the\n * value is clipped. For example: \n * \n * ```cpp\n * uchar a = saturate_cast<uchar>(-100); // a = 0 (UCHAR_MIN)\n * short b = saturate_cast<short>(33333.33333); // b = 32767 (SHRT_MAX)\n * ```\n * \n *  Such clipping is done when the target type is unsigned char , signed char , unsigned short or\n * signed short . For 32-bit integers, no clipping is done.\n * \n * When the parameter is a floating-point value and the target type is an integer (8-, 16- or 32-bit),\n * the floating-point value is first rounded to the nearest integer and then clipped if needed (when\n * the target type is 8- or 16-bit).\n * \n * This operation is used in the simplest or most complex image processing functions in OpenCV.\n * \n * [add](#d2/de8/group__core__array_1ga10ac1bfb180e2cfda1701d06c24fdbd6}),\n * [subtract](#d2/de8/group__core__array_1gaa0f00d98b4b5edeaeb7b8333b2de353b}),\n * [multiply](#d2/de8/group__core__array_1ga979d898a58d7f61c53003e162e7ad89f}),\n * [divide](#d2/de8/group__core__array_1ga6db555d30115642fedae0cda05604874}),\n * [Mat::convertTo](#d3/d63/classcv_1_1Mat_1adf88c60c5b4980e05bb556080916978b})\n * \n * @param v Function parameter.\n */\nexport declare function saturate_cast(arg108: any, v: uchar): uchar\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function saturate_cast(arg109: any, v: schar): schar\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function saturate_cast(arg110: any, v: ushort): ushort\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function saturate_cast(arg111: any, v: short): any\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function saturate_cast(arg112: any, v: unsigned): any\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function saturate_cast(arg113: any, v: int): any\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function saturate_cast(arg114: any, v: float): any\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function saturate_cast(arg115: any, v: double): any\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function saturate_cast(arg116: any, v: int64): int64\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function saturate_cast(arg117: any, v: uint64): uint64\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function saturate_cast(arg118: any, v: float16_t): any\n\n/**\n * When the break-on-error mode is set, the default error handler issues a hardware exception, which\n * can make debugging more convenient.\n * \n * the previous state\n */\nexport declare function setBreakOnError(flag: bool): bool\n\n/**\n * If threads == 0, OpenCV will disable threading optimizations and run all it's functions\n * sequentially. Passing threads < 0 will reset threads number to system default. This function must be\n * called outside of parallel region.\n * \n * OpenCV will try to run its functions with specified threads number, but some behaviour differs from\n * framework:\n * \n * `TBB` - User-defined parallel constructions will run with the same threads number, if another is not\n * specified. If later on user creates his own scheduler, OpenCV will use it.\n * `OpenMP` - No special defined behaviour.\n * `Concurrency` - If threads == 1, OpenCV will disable threading optimizations and run its functions\n * sequentially.\n * `GCD` - Supports only values <= 0.\n * `C=` - No special defined behaviour. \n * \n * [getNumThreads](#db/de0/group__core__utils_1ga2db334ec41d98da3129ef4a2342fc4d4}),\n * [getThreadNum](#db/de0/group__core__utils_1gaf9cc0cb10097686a9da60f6c587e5774})\n * \n * @param nthreads Number of threads used by OpenCV.\n */\nexport declare function setNumThreads(nthreads: int): void\n\n/**\n * The function can be used to dynamically turn on and off optimized dispatched code (code that uses\n * SSE4.2, AVX/AVX2, and other instructions on the platforms that support it). It sets a global flag\n * that is further checked by OpenCV functions. Since the flag is not checked in the inner OpenCV\n * loops, it is only safe to call the function on the very top level in your application where you can\n * be sure that no other OpenCV function is currently executed.\n * \n * By default, the optimized code is enabled unless you disable it in CMake. The current status can be\n * retrieved using useOptimized.\n * \n * @param onoff The boolean flag specifying whether the optimized code should be used (onoff=true) or\n * not (onoff=false).\n */\nexport declare function setUseOptimized(onoff: bool): void\n\nexport declare function tempfile(suffix?: any): String\n\nexport declare function testAsyncArray(argument: InputArray): AsyncArray\n\nexport declare function testAsyncException(): AsyncArray\n\n/**\n * The function returns true if the optimized code is enabled. Otherwise, it returns false.\n */\nexport declare function useOptimized(): bool\n\nexport declare const CPU_MMX: CpuFeatures // initializer: = 1\n\nexport declare const CPU_SSE: CpuFeatures // initializer: = 2\n\nexport declare const CPU_SSE2: CpuFeatures // initializer: = 3\n\nexport declare const CPU_SSE3: CpuFeatures // initializer: = 4\n\nexport declare const CPU_SSSE3: CpuFeatures // initializer: = 5\n\nexport declare const CPU_SSE4_1: CpuFeatures // initializer: = 6\n\nexport declare const CPU_SSE4_2: CpuFeatures // initializer: = 7\n\nexport declare const CPU_POPCNT: CpuFeatures // initializer: = 8\n\nexport declare const CPU_FP16: CpuFeatures // initializer: = 9\n\nexport declare const CPU_AVX: CpuFeatures // initializer: = 10\n\nexport declare const CPU_AVX2: CpuFeatures // initializer: = 11\n\nexport declare const CPU_FMA3: CpuFeatures // initializer: = 12\n\nexport declare const CPU_AVX_512F: CpuFeatures // initializer: = 13\n\nexport declare const CPU_AVX_512BW: CpuFeatures // initializer: = 14\n\nexport declare const CPU_AVX_512CD: CpuFeatures // initializer: = 15\n\nexport declare const CPU_AVX_512DQ: CpuFeatures // initializer: = 16\n\nexport declare const CPU_AVX_512ER: CpuFeatures // initializer: = 17\n\nexport declare const CPU_AVX_512IFMA512: CpuFeatures // initializer: = 18\n\nexport declare const CPU_AVX_512IFMA: CpuFeatures // initializer: = 18\n\nexport declare const CPU_AVX_512PF: CpuFeatures // initializer: = 19\n\nexport declare const CPU_AVX_512VBMI: CpuFeatures // initializer: = 20\n\nexport declare const CPU_AVX_512VL: CpuFeatures // initializer: = 21\n\nexport declare const CPU_AVX_512VBMI2: CpuFeatures // initializer: = 22\n\nexport declare const CPU_AVX_512VNNI: CpuFeatures // initializer: = 23\n\nexport declare const CPU_AVX_512BITALG: CpuFeatures // initializer: = 24\n\nexport declare const CPU_AVX_512VPOPCNTDQ: CpuFeatures // initializer: = 25\n\nexport declare const CPU_AVX_5124VNNIW: CpuFeatures // initializer: = 26\n\nexport declare const CPU_AVX_5124FMAPS: CpuFeatures // initializer: = 27\n\nexport declare const CPU_NEON: CpuFeatures // initializer: = 100\n\nexport declare const CPU_VSX: CpuFeatures // initializer: = 200\n\nexport declare const CPU_VSX3: CpuFeatures // initializer: = 201\n\nexport declare const CPU_AVX512_SKX: CpuFeatures // initializer: = 256\n\nexport declare const CPU_AVX512_COMMON: CpuFeatures // initializer: = 257\n\nexport declare const CPU_AVX512_KNL: CpuFeatures // initializer: = 258\n\nexport declare const CPU_AVX512_KNM: CpuFeatures // initializer: = 259\n\nexport declare const CPU_AVX512_CNL: CpuFeatures // initializer: = 260\n\nexport declare const CPU_AVX512_CEL: CpuFeatures // initializer: = 261\n\nexport declare const CPU_AVX512_ICL: CpuFeatures // initializer: = 262\n\nexport declare const CPU_MAX_FEATURE: CpuFeatures // initializer: = 512\n\nexport declare const SORT_EVERY_ROW: SortFlags // initializer: = 0\n\n/**\n * each matrix column is sorted independently; this flag and the previous one are mutually exclusive.\n * \n */\nexport declare const SORT_EVERY_COLUMN: SortFlags // initializer: = 1\n\n/**\n * each matrix row is sorted in the ascending order.\n * \n */\nexport declare const SORT_ASCENDING: SortFlags // initializer: = 0\n\n/**\n * each matrix row is sorted in the descending order; this flag and the previous one are also mutually\n * exclusive.\n * \n */\nexport declare const SORT_DESCENDING: SortFlags // initializer: = 16\n\nexport type CpuFeatures = any\n\nexport type SortFlags = any\n\n"},"node_modules_mirada_dist_src_types_opencv_DescriptorMatcher_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_DescriptorMatcher_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/DescriptorMatcher.d.ts","content":"\nimport { bool, DMatch, FileNode, FileStorage, float, InputArray, InputArrayOfArrays, int, Mat, Ptr } from './_types'\n\n/**\n * It has two groups of match methods: for matching descriptors of an image with another image or with\n * an image set.\n * \n * Source:\n * [opencv2/features2d.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/features2d.hpp#L860).\n * \n */\nexport declare class DescriptorMatcher {\n\n  /**\n   *   If the collection is not empty, the new descriptors are added to existing train descriptors.\n   *   \n   *   @param descriptors Descriptors to add. Each descriptors[i] is a set of descriptors from the same\n   * train image.\n   */\n  public add(descriptors: InputArrayOfArrays): InputArrayOfArrays\n\n  public clear(): void\n\n  /**\n   *   @param emptyTrainData If emptyTrainData is false, the method creates a deep copy of the object,\n   * that is, copies both parameters and train data. If emptyTrainData is true, the method creates an\n   * object copy with the current parameters but with empty train data.\n   */\n  public clone(emptyTrainData?: bool): Ptr\n\n  public empty(): bool\n\n  public getTrainDescriptors(): Mat\n\n  public isMaskSupported(): bool\n\n  /**\n   *   These extended variants of\n   * [DescriptorMatcher::match](#db/d39/classcv_1_1DescriptorMatcher_1a0f046f47b68ec7074391e1e85c750cba})\n   * methods find several best matches for each query descriptor. The matches are returned in the\n   * distance increasing order. See\n   * [DescriptorMatcher::match](#db/d39/classcv_1_1DescriptorMatcher_1a0f046f47b68ec7074391e1e85c750cba})\n   * for the details about query and train descriptors.\n   *   \n   *   @param queryDescriptors Query set of descriptors.\n   *   \n   *   @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors\n   * collection stored in the class object.\n   *   \n   *   @param matches Matches. Each matches[i] is k or less matches for the same query descriptor.\n   *   \n   *   @param k Count of best matches found per each query descriptor or less if a query descriptor has\n   * less than k possible matches in total.\n   *   \n   *   @param mask Mask specifying permissible matches between an input query and train matrices of\n   * descriptors.\n   *   \n   *   @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is\n   * false, the matches vector has the same size as queryDescriptors rows. If compactResult is true, the\n   * matches vector does not contain matches for fully masked-out query descriptors.\n   */\n  public knnMatch(queryDescriptors: InputArray, trainDescriptors: InputArray, matches: DMatch, k: int, mask?: InputArray, compactResult?: bool): InputArray\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param queryDescriptors Query set of descriptors.\n   *   \n   *   @param matches Matches. Each matches[i] is k or less matches for the same query descriptor.\n   *   \n   *   @param k Count of best matches found per each query descriptor or less if a query descriptor has\n   * less than k possible matches in total.\n   *   \n   *   @param masks Set of masks. Each masks[i] specifies permissible matches between the input query\n   * descriptors and stored train descriptors from the i-th image trainDescCollection[i].\n   *   \n   *   @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is\n   * false, the matches vector has the same size as queryDescriptors rows. If compactResult is true, the\n   * matches vector does not contain matches for fully masked-out query descriptors.\n   */\n  public knnMatch(queryDescriptors: InputArray, matches: DMatch, k: int, masks?: InputArrayOfArrays, compactResult?: bool): InputArray\n\n  /**\n   *   In the first variant of this method, the train descriptors are passed as an input argument. In the\n   * second variant of the method, train descriptors collection that was set by\n   * [DescriptorMatcher::add](#db/d39/classcv_1_1DescriptorMatcher_1a623a2b07755cf7fb1c79554af73cdbb0})\n   * is used. Optional mask (or masks) can be passed to specify which query and training descriptors can\n   * be matched. Namely, queryDescriptors[i] can be matched with trainDescriptors[j] only if\n   * mask.at<uchar>(i,j) is non-zero.\n   *   \n   *   @param queryDescriptors Query set of descriptors.\n   *   \n   *   @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors\n   * collection stored in the class object.\n   *   \n   *   @param matches Matches. If a query descriptor is masked out in mask , no match is added for this\n   * descriptor. So, matches size may be smaller than the query descriptors count.\n   *   \n   *   @param mask Mask specifying permissible matches between an input query and train matrices of\n   * descriptors.\n   */\n  public match(queryDescriptors: InputArray, trainDescriptors: InputArray, matches: DMatch, mask?: InputArray): InputArray\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param queryDescriptors Query set of descriptors.\n   *   \n   *   @param matches Matches. If a query descriptor is masked out in mask , no match is added for this\n   * descriptor. So, matches size may be smaller than the query descriptors count.\n   *   \n   *   @param masks Set of masks. Each masks[i] specifies permissible matches between the input query\n   * descriptors and stored train descriptors from the i-th image trainDescCollection[i].\n   */\n  public match(queryDescriptors: InputArray, matches: DMatch, masks?: InputArrayOfArrays): InputArray\n\n  /**\n   *   For each query descriptor, the methods find such training descriptors that the distance between\n   * the query descriptor and the training descriptor is equal or smaller than maxDistance. Found matches\n   * are returned in the distance increasing order.\n   *   \n   *   @param queryDescriptors Query set of descriptors.\n   *   \n   *   @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors\n   * collection stored in the class object.\n   *   \n   *   @param matches Found matches.\n   *   \n   *   @param maxDistance Threshold for the distance between matched descriptors. Distance means here\n   * metric distance (e.g. Hamming distance), not the distance between coordinates (which is measured in\n   * Pixels)!\n   *   \n   *   @param mask Mask specifying permissible matches between an input query and train matrices of\n   * descriptors.\n   *   \n   *   @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is\n   * false, the matches vector has the same size as queryDescriptors rows. If compactResult is true, the\n   * matches vector does not contain matches for fully masked-out query descriptors.\n   */\n  public radiusMatch(queryDescriptors: InputArray, trainDescriptors: InputArray, matches: DMatch, maxDistance: float, mask?: InputArray, compactResult?: bool): InputArray\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param queryDescriptors Query set of descriptors.\n   *   \n   *   @param matches Found matches.\n   *   \n   *   @param maxDistance Threshold for the distance between matched descriptors. Distance means here\n   * metric distance (e.g. Hamming distance), not the distance between coordinates (which is measured in\n   * Pixels)!\n   *   \n   *   @param masks Set of masks. Each masks[i] specifies permissible matches between the input query\n   * descriptors and stored train descriptors from the i-th image trainDescCollection[i].\n   *   \n   *   @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is\n   * false, the matches vector has the same size as queryDescriptors rows. If compactResult is true, the\n   * matches vector does not contain matches for fully masked-out query descriptors.\n   */\n  public radiusMatch(queryDescriptors: InputArray, matches: DMatch, maxDistance: float, masks?: InputArrayOfArrays, compactResult?: bool): InputArray\n\n  public read(fileName: String): String\n\n  public read(fn: FileNode): FileNode\n\n  /**\n   *   Trains a descriptor matcher (for example, the flann index). In all methods to match, the method\n   * [train()](#db/d39/classcv_1_1DescriptorMatcher_1a80e9fd98de5908f5348c17696eeb1a32}) is run every\n   * time before matching. Some descriptor matchers (for example, BruteForceMatcher) have an empty\n   * implementation of this method. Other matchers really train their inner structures (for example,\n   * [FlannBasedMatcher](#dc/de2/classcv_1_1FlannBasedMatcher}) trains\n   * [flann::Index](#d1/db2/classcv_1_1flann_1_1Index}) ).\n   */\n  public train(): void\n\n  public write(fileName: String): String\n\n  public write(fs: FileStorage): FileStorage\n\n  public write(fs: Ptr, name?: String): Ptr\n\n  /**\n   *   @param descriptorMatcherType Descriptor matcher type. Now the following matcher types are\n   * supported:\n   *   BruteForce (it uses L2 )BruteForce-L1BruteForce-HammingBruteForce-Hamming(2)FlannBased\n   */\n  public static create(descriptorMatcherType: String): Ptr\n\n  public static create(matcherType: any): Ptr\n}\n\nexport declare const FLANNBASED: MatcherType // initializer: = 1\n\nexport declare const BRUTEFORCE: MatcherType // initializer: = 2\n\nexport declare const BRUTEFORCE_L1: MatcherType // initializer: = 3\n\nexport declare const BRUTEFORCE_HAMMING: MatcherType // initializer: = 4\n\nexport declare const BRUTEFORCE_HAMMINGLUT: MatcherType // initializer: = 5\n\nexport declare const BRUTEFORCE_SL2: MatcherType // initializer: = 6\n\nexport type MatcherType = any\n\n"},"node_modules_mirada_dist_src_types_opencv_DynamicBitset_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_DynamicBitset_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/DynamicBitset.d.ts","content":"\nimport { bool, size_t } from './_types'\n\n/**\n * Class re-implementing the boost version of it This helps not depending on boost, it also does not do\n * the bound checks and has a way to reset a block for speed\n * \n * Source:\n * [opencv2/flann/dynamic_bitset.h](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/flann/dynamic_bitset.h#L150).\n * \n */\nexport declare class DynamicBitset {\n\n  /**\n   *   default constructor\n   */\n  public constructor()\n\n  /**\n   *   only constructor we use in our code\n   *   \n   *   @param sz the size of the bitset (in bits)\n   */\n  public constructor(sz: size_t)\n\n  /**\n   *   Sets all the bits to 0\n   */\n  public clear(): void\n\n  /**\n   *   true if the bitset is empty\n   */\n  public empty(): bool\n\n  /**\n   *   set all the bits to 0\n   */\n  public reset(): void\n\n  public reset(index: size_t): void\n\n  public reset_block(index: size_t): void\n\n  /**\n   *   resize the bitset so that it contains at least sz bits\n   */\n  public resize(sz: size_t): void\n\n  /**\n   *   set a bit to true\n   *   \n   *   @param index the index of the bit to set to 1\n   */\n  public set(index: size_t): void\n\n  /**\n   *   gives the number of contained bits\n   */\n  public size(): size_t\n\n  /**\n   *   check if a bit is set \n   *   \n   *   true if the bit is set\n   *   \n   *   @param index the index of the bit to check\n   */\n  public test(index: size_t): bool\n}\n\n"},"node_modules_mirada_dist_src_types_opencv_dnn_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_dnn_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/dnn.d.ts","content":"\nimport { bool, double, InputArray, InputArrayOfArrays, int, Mat, Net, OutputArray, OutputArrayOfArrays, Size, size_t, uchar } from './_types'\n/*\n * # Deep Neural Network module\n * This module contains:\n * \n * \n * \n *  * API for new layers creation, layers are building bricks of neural networks;\n *  * set of built-in most-useful Layers;\n *  * API to construct and modify comprehensive neural networks from layers;\n *  * functionality for loading serialized networks models from different frameworks.\n * \n * \n * Functionality of this module is designed only for forward pass computations (i.e. network testing). A network training is in principle not supported.\n */\n/**\n * if `crop` is true, input image is resized so one side after resize is equal to corresponding\n * dimension in `size` and another one is equal or larger. Then, crop from the center is performed. If\n * `crop` is false, direct resize without cropping and preserving aspect ratio is performed. \n * \n * 4-dimensional [Mat](#d3/d63/classcv_1_1Mat}) with NCHW dimensions order.\n * \n * @param image input image (with 1-, 3- or 4-channels).\n * \n * @param scalefactor multiplier for image values.\n * \n * @param size spatial size for output image\n * \n * @param mean scalar with mean values which are subtracted from channels. Values are intended to be in\n * (mean-R, mean-G, mean-B) order if image has BGR ordering and swapRB is true.\n * \n * @param swapRB flag which indicates that swap first and last channels in 3-channel image is\n * necessary.\n * \n * @param crop flag which indicates whether image will be cropped after resize or not\n * \n * @param ddepth Depth of output blob. Choose CV_32F or CV_8U.\n */\nexport declare function blobFromImage(image: InputArray, scalefactor?: double, size?: any, mean?: any, swapRB?: bool, crop?: bool, ddepth?: int): Mat\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function blobFromImage(image: InputArray, blob: OutputArray, scalefactor?: double, size?: any, mean?: any, swapRB?: bool, crop?: bool, ddepth?: int): void\n\n/**\n * if `crop` is true, input image is resized so one side after resize is equal to corresponding\n * dimension in `size` and another one is equal or larger. Then, crop from the center is performed. If\n * `crop` is false, direct resize without cropping and preserving aspect ratio is performed. \n * \n * 4-dimensional [Mat](#d3/d63/classcv_1_1Mat}) with NCHW dimensions order.\n * \n * @param images input images (all with 1-, 3- or 4-channels).\n * \n * @param scalefactor multiplier for images values.\n * \n * @param size spatial size for output image\n * \n * @param mean scalar with mean values which are subtracted from channels. Values are intended to be in\n * (mean-R, mean-G, mean-B) order if image has BGR ordering and swapRB is true.\n * \n * @param swapRB flag which indicates that swap first and last channels in 3-channel image is\n * necessary.\n * \n * @param crop flag which indicates whether image will be cropped after resize or not\n * \n * @param ddepth Depth of output blob. Choose CV_32F or CV_8U.\n */\nexport declare function blobFromImages(images: InputArrayOfArrays, scalefactor?: double, size?: Size, mean?: any, swapRB?: bool, crop?: bool, ddepth?: int): Mat\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function blobFromImages(images: InputArrayOfArrays, blob: OutputArray, scalefactor?: double, size?: Size, mean?: any, swapRB?: bool, crop?: bool, ddepth?: int): void\n\nexport declare function getAvailableBackends(): any\n\nexport declare function getAvailableTargets(be: Backend): any\n\n/**\n * @param blob_ 4 dimensional array (images, channels, height, width) in floating point precision\n * (CV_32F) from which you would like to extract the images.\n * \n * @param images_ array of 2D Mat containing the images extracted from the blob in floating point\n * precision (CV_32F). They are non normalized neither mean added. The number of returned images equals\n * the first dimension of the blob (batch size). Every image has a number of channels equals to the\n * second dimension of the blob (depth).\n */\nexport declare function imagesFromBlob(blob_: any, images_: OutputArrayOfArrays): any\n\n/**\n * @param bboxes a set of bounding boxes to apply NMS.\n * \n * @param scores a set of corresponding confidences.\n * \n * @param score_threshold a threshold used to filter boxes by score.\n * \n * @param nms_threshold a threshold used in non maximum suppression.\n * \n * @param indices the kept indices of bboxes after NMS.\n * \n * @param eta a coefficient in adaptive threshold formula: $nms\\_threshold_{i+1}=eta\\cdot\n * nms\\_threshold_i$.\n * \n * @param top_k if >0, keep at most top_k picked indices.\n */\nexport declare function NMSBoxes(bboxes: any, scores: any, score_threshold: any, nms_threshold: any, indices: any, eta?: any, top_k?: any): void\n\nexport declare function NMSBoxes(bboxes: any, scores: any, score_threshold: any, nms_threshold: any, indices: any, eta?: any, top_k?: any): void\n\nexport declare function NMSBoxes(bboxes: any, scores: any, score_threshold: any, nms_threshold: any, indices: any, eta?: any, top_k?: any): void\n\n/**\n * [Net](#db/d30/classcv_1_1dnn_1_1Net}) object.\n * This function automatically detects an origin framework of trained model and calls an appropriate\n * function such [readNetFromCaffe](#d6/d0f/group__dnn_1ga29d0ea5e52b1d1a6c2681e3f7d68473a}),\n * [readNetFromTensorflow](#d6/d0f/group__dnn_1gad820b280978d06773234ba6841e77e8d}),\n * [readNetFromTorch](#d6/d0f/group__dnn_1ga65a1da76cb7d6852bdf7abbd96f19084}) or\n * [readNetFromDarknet](#d6/d0f/group__dnn_1gafde362956af949cce087f3f25c6aff0d}). An order of `model`\n * and `config` arguments does not matter.\n * \n * @param model Binary file contains trained weights. The following file extensions are expected for\n * models from different frameworks:\n * .caffemodel (Caffe, http://caffe.berkeleyvision.org/)*.pb (TensorFlow,\n * https://www.tensorflow.org/)*.t7 | *.net (Torch, http://torch.ch/)*.weights (Darknet,\n * https://pjreddie.com/darknet/)*.bin (DLDT, https://software.intel.com/openvino-toolkit)*.onnx (ONNX,\n * https://onnx.ai/)\n * \n * @param config Text file contains network configuration. It could be a file with the following\n * extensions:\n * .prototxt (Caffe, http://caffe.berkeleyvision.org/)*.pbtxt (TensorFlow,\n * https://www.tensorflow.org/)*.cfg (Darknet, https://pjreddie.com/darknet/)*.xml (DLDT,\n * https://software.intel.com/openvino-toolkit)\n * \n * @param framework Explicit framework name tag to determine a format.\n */\nexport declare function readNet(model: any, config?: any, framework?: any): Net\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts. \n * \n * [Net](#db/d30/classcv_1_1dnn_1_1Net}) object.\n * \n * @param framework Name of origin framework.\n * \n * @param bufferModel A buffer with a content of binary file with weights\n * \n * @param bufferConfig A buffer with a content of text file contains network configuration.\n */\nexport declare function readNet(framework: any, bufferModel: uchar, bufferConfig?: uchar): uchar\n\n/**\n * [Net](#db/d30/classcv_1_1dnn_1_1Net}) object.\n * \n * @param prototxt path to the .prototxt file with text description of the network architecture.\n * \n * @param caffeModel path to the .caffemodel file with learned network.\n */\nexport declare function readNetFromCaffe(prototxt: any, caffeModel?: any): Net\n\n/**\n * [Net](#db/d30/classcv_1_1dnn_1_1Net}) object.\n * \n * @param bufferProto buffer containing the content of the .prototxt file\n * \n * @param bufferModel buffer containing the content of the .caffemodel file\n */\nexport declare function readNetFromCaffe(bufferProto: uchar, bufferModel?: uchar): uchar\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts. \n * \n * [Net](#db/d30/classcv_1_1dnn_1_1Net}) object.\n * \n * @param bufferProto buffer containing the content of the .prototxt file\n * \n * @param lenProto length of bufferProto\n * \n * @param bufferModel buffer containing the content of the .caffemodel file\n * \n * @param lenModel length of bufferModel\n */\nexport declare function readNetFromCaffe(bufferProto: any, lenProto: size_t, bufferModel?: any, lenModel?: size_t): Net\n\n/**\n * Network object that ready to do forward, throw an exception in failure cases. \n * \n * [Net](#db/d30/classcv_1_1dnn_1_1Net}) object.\n * \n * @param cfgFile path to the .cfg file with text description of the network architecture.\n * \n * @param darknetModel path to the .weights file with learned network.\n */\nexport declare function readNetFromDarknet(cfgFile: any, darknetModel?: any): Net\n\n/**\n * [Net](#db/d30/classcv_1_1dnn_1_1Net}) object.\n * \n * @param bufferCfg A buffer contains a content of .cfg file with text description of the network\n * architecture.\n * \n * @param bufferModel A buffer contains a content of .weights file with learned network.\n */\nexport declare function readNetFromDarknet(bufferCfg: uchar, bufferModel?: uchar): uchar\n\n/**\n * [Net](#db/d30/classcv_1_1dnn_1_1Net}) object.\n * \n * @param bufferCfg A buffer contains a content of .cfg file with text description of the network\n * architecture.\n * \n * @param lenCfg Number of bytes to read from bufferCfg\n * \n * @param bufferModel A buffer contains a content of .weights file with learned network.\n * \n * @param lenModel Number of bytes to read from bufferModel\n */\nexport declare function readNetFromDarknet(bufferCfg: any, lenCfg: size_t, bufferModel?: any, lenModel?: size_t): Net\n\n/**\n * [Net](#db/d30/classcv_1_1dnn_1_1Net}) object. Networks imported from Intel's\n * [Model](#d3/df0/classcv_1_1dnn_1_1Model}) Optimizer are launched in Intel's Inference Engine\n * backend.\n * \n * @param xml XML configuration file with network's topology.\n * \n * @param bin Binary file with trained weights.\n */\nexport declare function readNetFromModelOptimizer(xml: any, bin: any): Net\n\n/**\n * Network object that ready to do forward, throw an exception in failure cases.\n * \n * @param onnxFile path to the .onnx file with text description of the network architecture.\n */\nexport declare function readNetFromONNX(onnxFile: any): Net\n\n/**\n * Network object that ready to do forward, throw an exception in failure cases.\n * \n * @param buffer memory address of the first byte of the buffer.\n * \n * @param sizeBuffer size of the buffer.\n */\nexport declare function readNetFromONNX(buffer: any, sizeBuffer: size_t): Net\n\n/**\n * Network object that ready to do forward, throw an exception in failure cases.\n * \n * @param buffer in-memory buffer that stores the ONNX model bytes.\n */\nexport declare function readNetFromONNX(buffer: uchar): uchar\n\n/**\n * [Net](#db/d30/classcv_1_1dnn_1_1Net}) object.\n * \n * @param model path to the .pb file with binary protobuf description of the network architecture\n * \n * @param config path to the .pbtxt file that contains text graph definition in protobuf format.\n * Resulting Net object is built by text graph using weights from a binary one that let us make it more\n * flexible.\n */\nexport declare function readNetFromTensorflow(model: any, config?: any): Net\n\n/**\n * [Net](#db/d30/classcv_1_1dnn_1_1Net}) object.\n * \n * @param bufferModel buffer containing the content of the pb file\n * \n * @param bufferConfig buffer containing the content of the pbtxt file\n */\nexport declare function readNetFromTensorflow(bufferModel: uchar, bufferConfig?: uchar): uchar\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n * \n * @param bufferModel buffer containing the content of the pb file\n * \n * @param lenModel length of bufferModel\n * \n * @param bufferConfig buffer containing the content of the pbtxt file\n * \n * @param lenConfig length of bufferConfig\n */\nexport declare function readNetFromTensorflow(bufferModel: any, lenModel: size_t, bufferConfig?: any, lenConfig?: size_t): Net\n\n/**\n * [Net](#db/d30/classcv_1_1dnn_1_1Net}) object.\n * \n * Ascii mode of Torch serializer is more preferable, because binary mode extensively use `long` type\n * of C language, which has various bit-length on different systems.\n * The loading file must contain serialized  object with importing network. Try to eliminate a custom\n * objects from serialazing data to avoid importing errors.\n * \n * List of supported layers (i.e. object instances derived from Torch nn.Module class):\n * \n * nn.Sequential\n * nn.Parallel\n * nn.Concat\n * nn.Linear\n * nn.SpatialConvolution\n * nn.SpatialMaxPooling, nn.SpatialAveragePooling\n * nn.ReLU, nn.TanH, nn.Sigmoid\n * nn.Reshape\n * nn.SoftMax, nn.LogSoftMax\n * \n * Also some equivalents of these classes from cunn, cudnn, and fbcunn may be successfully imported.\n * \n * @param model path to the file, dumped from Torch by using torch.save() function.\n * \n * @param isBinary specifies whether the network was serialized in ascii mode or binary.\n * \n * @param evaluate specifies testing phase of network. If true, it's similar to evaluate() method in\n * Torch.\n */\nexport declare function readNetFromTorch(model: any, isBinary?: bool, evaluate?: bool): Net\n\n/**\n * [Mat](#d3/d63/classcv_1_1Mat}).\n * \n * @param path to the .pb file with input tensor.\n */\nexport declare function readTensorFromONNX(path: any): Mat\n\n/**\n * This function has the same limitations as\n * [readNetFromTorch()](#d6/d0f/group__dnn_1ga65a1da76cb7d6852bdf7abbd96f19084}).\n */\nexport declare function readTorchBlob(filename: any, isBinary?: bool): Mat\n\n/**\n * Shrinked model has no origin float32 weights so it can't be used in origin Caffe framework anymore.\n * However the structure of data is taken from NVidia's Caffe fork: . So the resulting model may be\n * used there.\n * \n * @param src Path to origin model from Caffe framework contains single precision floating point\n * weights (usually has .caffemodel extension).\n * \n * @param dst Path to destination model with updated weights.\n * \n * @param layersTypes Set of layers types which parameters will be converted. By default, converts only\n * Convolutional and Fully-Connected layers' weights.\n */\nexport declare function shrinkCaffeModel(src: any, dst: any, layersTypes?: any): void\n\n/**\n * To reduce output file size, trained weights are not included.\n * \n * @param model A path to binary network.\n * \n * @param output A path to output text file to be created.\n */\nexport declare function writeTextGraph(model: any, output: any): void\n\n/**\n * DNN_BACKEND_DEFAULT equals to DNN_BACKEND_INFERENCE_ENGINE if OpenCV is built with Intel's Inference\n * Engine library or DNN_BACKEND_OPENCV otherwise.\n * \n */\nexport declare const DNN_BACKEND_DEFAULT: Backend // initializer: \n\nexport declare const DNN_BACKEND_HALIDE: Backend // initializer: \n\nexport declare const DNN_BACKEND_INFERENCE_ENGINE: Backend // initializer: \n\nexport declare const DNN_BACKEND_OPENCV: Backend // initializer: \n\nexport declare const DNN_BACKEND_VKCOM: Backend // initializer: \n\nexport declare const DNN_TARGET_CPU: Target // initializer: \n\nexport declare const DNN_TARGET_OPENCL: Target // initializer: \n\nexport declare const DNN_TARGET_OPENCL_FP16: Target // initializer: \n\nexport declare const DNN_TARGET_MYRIAD: Target // initializer: \n\nexport declare const DNN_TARGET_VULKAN: Target // initializer: \n\nexport declare const DNN_TARGET_FPGA: Target // initializer: \n\n/**\n * [Net::setPreferableBackend](#db/d30/classcv_1_1dnn_1_1Net_1a7f767df11386d39374db49cd8df8f59e})\n * \n */\nexport type Backend = any\n\n/**\n * [Net::setPreferableBackend](#db/d30/classcv_1_1dnn_1_1Net_1a7f767df11386d39374db49cd8df8f59e})\n * \n */\nexport type Target = any\n\n"},"node_modules_mirada_dist_src_types_opencv_Exception_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_Exception_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/Exception.d.ts","content":"\nimport { int } from './_types'\n\n/**\n * This class encapsulates all or almost all necessary information about the error happened in the\n * program. The exception is usually constructed and thrown implicitly via CV_Error and CV_Error_\n * macros. \n * \n * [error](#db/de0/group__core__utils_1gacbd081fdb20423a63cf731569ba70b2b})\n * \n * Source:\n * [opencv2/core.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/core.hpp#L135).\n * \n */\nexport declare class Exception {\n\n  /**\n   *   CVStatus\n   *   \n   */\n  public code: int\n\n  public err: String\n\n  public file: String\n\n  public func: String\n\n  public line: int\n\n  public msg: String\n\n  /**\n   *   Default constructor\n   */\n  public constructor()\n\n  /**\n   *   Full constructor. Normally the constructor is not called explicitly. Instead, the macros\n   * [CV_Error()](#db/de0/group__core__utils_1ga5b48c333c777666e076bd7052799f891}),\n   * [CV_Error_()](#db/de0/group__core__utils_1ga1c0cd6e5bd9a5f915c6cab9c0632f969}) and\n   * [CV_Assert()](#db/de0/group__core__utils_1gaf62bcd90f70e275191ab95136d85906b}) are used.\n   */\n  public constructor(_code: int, _err: String, _func: String, _file: String, _line: int)\n\n  public formatMessage(): void\n\n  /**\n   *   the error description and the context as a text string.\n   */\n  public what(): any\n}\n\n"},"node_modules_mirada_dist_src_types_opencv_features2d_draw_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_features2d_draw_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/features2d_draw.d.ts","content":"\nimport { InputArray, InputOutputArray } from './_types'\n/*\n * # Drawing Function of Keypoints and Matches\n * \n */\n/**\n * For Python API, flags are modified as cv.DRAW_MATCHES_FLAGS_DEFAULT,\n * cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS, cv.DRAW_MATCHES_FLAGS_DRAW_OVER_OUTIMG,\n * cv.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS\n * \n * @param image Source image.\n * \n * @param keypoints Keypoints from the source image.\n * \n * @param outImage Output image. Its content depends on the flags value defining what is drawn in the\n * output image. See possible flags bit values below.\n * \n * @param color Color of keypoints.\n * \n * @param flags Flags setting drawing features. Possible flags bit values are defined by\n * DrawMatchesFlags. See details above in drawMatches .\n */\nexport declare function drawKeypoints(image: InputArray, keypoints: any, outImage: InputOutputArray, color?: any, flags?: DrawMatchesFlags): void\n\n/**\n * This function draws matches of keypoints from two images in the output image. Match is a line\n * connecting two keypoints (circles). See\n * [cv::DrawMatchesFlags](#d4/d5d/group__features2d__draw_1ga2c2ede79cd5141534ae70a3fd9f324c8}).\n * \n * @param img1 First source image.\n * \n * @param keypoints1 Keypoints from the first source image.\n * \n * @param img2 Second source image.\n * \n * @param keypoints2 Keypoints from the second source image.\n * \n * @param matches1to2 Matches from the first image to the second one, which means that keypoints1[i]\n * has a corresponding point in keypoints2[matches[i]] .\n * \n * @param outImg Output image. Its content depends on the flags value defining what is drawn in the\n * output image. See possible flags bit values below.\n * \n * @param matchColor Color of matches (lines and connected keypoints). If matchColor==Scalar::all(-1) ,\n * the color is generated randomly.\n * \n * @param singlePointColor Color of single keypoints (circles), which means that keypoints do not have\n * the matches. If singlePointColor==Scalar::all(-1) , the color is generated randomly.\n * \n * @param matchesMask Mask determining which matches are drawn. If the mask is empty, all matches are\n * drawn.\n * \n * @param flags Flags setting drawing features. Possible flags bit values are defined by\n * DrawMatchesFlags.\n */\nexport declare function drawMatches(img1: InputArray, keypoints1: any, img2: InputArray, keypoints2: any, matches1to2: any, outImg: InputOutputArray, matchColor?: any, singlePointColor?: any, matchesMask?: any, flags?: DrawMatchesFlags): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function drawMatches(img1: InputArray, keypoints1: any, img2: InputArray, keypoints2: any, matches1to2: any, outImg: InputOutputArray, matchColor?: any, singlePointColor?: any, matchesMask?: any, flags?: DrawMatchesFlags): void\n\n/**\n * Output image matrix will be created\n * ([Mat::create](#d3/d63/classcv_1_1Mat_1a55ced2c8d844d683ea9a725c60037ad0})), i.e. existing memory of\n * output image may be reused. Two source image, matches and single keypoints will be drawn. For each\n * keypoint only the center point will be drawn (without the circle around keypoint with keypoint size\n * and orientation).\n * \n */\nexport declare const DEFAULT: DrawMatchesFlags // initializer: = 0\n\n/**\n * Output image matrix will not be created\n * ([Mat::create](#d3/d63/classcv_1_1Mat_1a55ced2c8d844d683ea9a725c60037ad0})). Matches will be drawn\n * on existing content of output image.\n * \n */\nexport declare const DRAW_OVER_OUTIMG: DrawMatchesFlags // initializer: = 1\n\nexport declare const NOT_DRAW_SINGLE_POINTS: DrawMatchesFlags // initializer: = 2\n\n/**\n * For each keypoint the circle around keypoint with keypoint size and orientation will be drawn.\n * \n */\nexport declare const DRAW_RICH_KEYPOINTS: DrawMatchesFlags // initializer: = 4\n\nexport type DrawMatchesFlags = any\n\n"},"node_modules_mirada_dist_src_types_opencv_FlannBasedMatcher_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_FlannBasedMatcher_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/FlannBasedMatcher.d.ts","content":"\nimport { bool, FileNode, FileStorage, InputArrayOfArrays, Ptr } from './_types'\n\n/**\n * This matcher trains [cv::flann::Index](#d1/db2/classcv_1_1flann_1_1Index}) on a train descriptor\n * collection and calls its nearest search methods to find the best matches. So, this matcher may be\n * faster when matching a large train collection than the brute force matcher.\n * [FlannBasedMatcher](#dc/de2/classcv_1_1FlannBasedMatcher}) does not support masking permissible\n * matches of descriptor sets because [flann::Index](#d1/db2/classcv_1_1flann_1_1Index}) does not\n * support this. :\n * \n * Source:\n * [opencv2/features2d.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/features2d.hpp#L1187).\n * \n */\nexport declare class FlannBasedMatcher {\n\n  public constructor(indexParams?: Ptr, searchParams?: Ptr)\n\n  /**\n   *   If the collection is not empty, the new descriptors are added to existing train descriptors.\n   *   \n   *   @param descriptors Descriptors to add. Each descriptors[i] is a set of descriptors from the same\n   * train image.\n   */\n  public add(descriptors: InputArrayOfArrays): InputArrayOfArrays\n\n  public clear(): void\n\n  /**\n   *   @param emptyTrainData If emptyTrainData is false, the method creates a deep copy of the object,\n   * that is, copies both parameters and train data. If emptyTrainData is true, the method creates an\n   * object copy with the current parameters but with empty train data.\n   */\n  public clone(emptyTrainData?: bool): Ptr\n\n  public isMaskSupported(): bool\n\n  public read(fn: FileNode): FileNode\n\n  /**\n   *   Trains a descriptor matcher (for example, the flann index). In all methods to match, the method\n   * [train()](#dc/de2/classcv_1_1FlannBasedMatcher_1a4fc9f506ff3185caa168b937c4e71080}) is run every\n   * time before matching. Some descriptor matchers (for example, BruteForceMatcher) have an empty\n   * implementation of this method. Other matchers really train their inner structures (for example,\n   * [FlannBasedMatcher](#dc/de2/classcv_1_1FlannBasedMatcher}) trains\n   * [flann::Index](#d1/db2/classcv_1_1flann_1_1Index}) ).\n   */\n  public train(): void\n\n  public write(fs: FileStorage): FileStorage\n\n  public static create(): Ptr\n}\n\n"},"node_modules_mirada_dist_src_types_opencv_HOGDescriptor_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_HOGDescriptor_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/HOGDescriptor.d.ts","content":"\nimport { bool, DetectionROI, double, FileNode, FileStorage, float, InputArray, InputOutputArray, int, Point, Rect, Size, size_t, UMat } from './_types'\n\n/**\n * the HOG descriptor algorithm introduced by Navneet Dalal and Bill Triggs Dalal2005 .\n * \n * useful links:\n * \n * Source:\n * [opencv2/objdetect.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/objdetect.hpp#L377).\n * \n */\nexport declare class HOGDescriptor {\n\n  public blockSize: Size\n\n  public blockStride: Size\n\n  public cellSize: Size\n\n  public derivAperture: int\n\n  public free_coef: float\n\n  public gammaCorrection: bool\n\n  public histogramNormType: any\n\n  public L2HysThreshold: double\n\n  public nbins: int\n\n  public nlevels: int\n\n  public oclSvmDetector: UMat\n\n  public signedGradient: bool\n\n  public svmDetector: any\n\n  public winSigma: double\n\n  public winSize: Size\n\n  /**\n   *   aqual to [HOGDescriptor](#d5/d33/structcv_1_1HOGDescriptor})(Size(64,128), Size(16,16), Size(8,8),\n   * Size(8,8), 9 )\n   */\n  public constructor()\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param _winSize sets winSize with given value.\n   *   \n   *   @param _blockSize sets blockSize with given value.\n   *   \n   *   @param _blockStride sets blockStride with given value.\n   *   \n   *   @param _cellSize sets cellSize with given value.\n   *   \n   *   @param _nbins sets nbins with given value.\n   *   \n   *   @param _derivAperture sets derivAperture with given value.\n   *   \n   *   @param _winSigma sets winSigma with given value.\n   *   \n   *   @param _histogramNormType sets histogramNormType with given value.\n   *   \n   *   @param _L2HysThreshold sets L2HysThreshold with given value.\n   *   \n   *   @param _gammaCorrection sets gammaCorrection with given value.\n   *   \n   *   @param _nlevels sets nlevels with given value.\n   *   \n   *   @param _signedGradient sets signedGradient with given value.\n   */\n  public constructor(_winSize: Size, _blockSize: Size, _blockStride: Size, _cellSize: Size, _nbins: int, _derivAperture?: int, _winSigma?: double, _histogramNormType?: any, _L2HysThreshold?: double, _gammaCorrection?: bool, _nlevels?: int, _signedGradient?: bool)\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param filename The file name containing HOGDescriptor properties and coefficients for the linear\n   * SVM classifier.\n   */\n  public constructor(filename: String)\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param d the HOGDescriptor which cloned to create a new one.\n   */\n  public constructor(d: HOGDescriptor)\n\n  public checkDetectorSize(): bool\n\n  /**\n   *   @param img Matrix of the type CV_8U containing an image where HOG features will be calculated.\n   *   \n   *   @param descriptors Matrix of the type CV_32F\n   *   \n   *   @param winStride Window stride. It must be a multiple of block stride.\n   *   \n   *   @param padding Padding\n   *   \n   *   @param locations Vector of Point\n   */\n  public compute(img: InputArray, descriptors: any, winStride?: Size, padding?: Size, locations?: Point): InputArray\n\n  /**\n   *   @param img Matrix contains the image to be computed\n   *   \n   *   @param grad Matrix of type CV_32FC2 contains computed gradients\n   *   \n   *   @param angleOfs Matrix of type CV_8UC2 contains quantized gradient orientations\n   *   \n   *   @param paddingTL Padding from top-left\n   *   \n   *   @param paddingBR Padding from bottom-right\n   */\n  public computeGradient(img: InputArray, grad: InputOutputArray, angleOfs: InputOutputArray, paddingTL?: Size, paddingBR?: Size): InputArray\n\n  /**\n   *   @param c cloned HOGDescriptor\n   */\n  public copyTo(c: HOGDescriptor): HOGDescriptor\n\n  /**\n   *   @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.\n   *   \n   *   @param foundLocations Vector of point where each point contains left-top corner point of detected\n   * object boundaries.\n   *   \n   *   @param weights Vector that will contain confidence values for each detected object.\n   *   \n   *   @param hitThreshold Threshold for the distance between features and SVM classifying plane. Usually\n   * it is 0 and should be specified in the detector coefficients (as the last free coefficient). But if\n   * the free coefficient is omitted (which is allowed), you can specify it manually here.\n   *   \n   *   @param winStride Window stride. It must be a multiple of block stride.\n   *   \n   *   @param padding Padding\n   *   \n   *   @param searchLocations Vector of Point includes set of requested locations to be evaluated.\n   */\n  public detect(img: InputArray, foundLocations: Point, weights: any, hitThreshold?: double, winStride?: Size, padding?: Size, searchLocations?: Point): InputArray\n\n  /**\n   *   @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.\n   *   \n   *   @param foundLocations Vector of point where each point contains left-top corner point of detected\n   * object boundaries.\n   *   \n   *   @param hitThreshold Threshold for the distance between features and SVM classifying plane. Usually\n   * it is 0 and should be specified in the detector coefficients (as the last free coefficient). But if\n   * the free coefficient is omitted (which is allowed), you can specify it manually here.\n   *   \n   *   @param winStride Window stride. It must be a multiple of block stride.\n   *   \n   *   @param padding Padding\n   *   \n   *   @param searchLocations Vector of Point includes locations to search.\n   */\n  public detect(img: InputArray, foundLocations: Point, hitThreshold?: double, winStride?: Size, padding?: Size, searchLocations?: Point): InputArray\n\n  /**\n   *   @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.\n   *   \n   *   @param foundLocations Vector of rectangles where each rectangle contains the detected object.\n   *   \n   *   @param foundWeights Vector that will contain confidence values for each detected object.\n   *   \n   *   @param hitThreshold Threshold for the distance between features and SVM classifying plane. Usually\n   * it is 0 and should be specified in the detector coefficients (as the last free coefficient). But if\n   * the free coefficient is omitted (which is allowed), you can specify it manually here.\n   *   \n   *   @param winStride Window stride. It must be a multiple of block stride.\n   *   \n   *   @param padding Padding\n   *   \n   *   @param scale Coefficient of the detection window increase.\n   *   \n   *   @param finalThreshold Final threshold\n   *   \n   *   @param useMeanshiftGrouping indicates grouping algorithm\n   */\n  public detectMultiScale(img: InputArray, foundLocations: Rect, foundWeights: any, hitThreshold?: double, winStride?: Size, padding?: Size, scale?: double, finalThreshold?: double, useMeanshiftGrouping?: bool): InputArray\n\n  /**\n   *   @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.\n   *   \n   *   @param foundLocations Vector of rectangles where each rectangle contains the detected object.\n   *   \n   *   @param hitThreshold Threshold for the distance between features and SVM classifying plane. Usually\n   * it is 0 and should be specified in the detector coefficients (as the last free coefficient). But if\n   * the free coefficient is omitted (which is allowed), you can specify it manually here.\n   *   \n   *   @param winStride Window stride. It must be a multiple of block stride.\n   *   \n   *   @param padding Padding\n   *   \n   *   @param scale Coefficient of the detection window increase.\n   *   \n   *   @param finalThreshold Final threshold\n   *   \n   *   @param useMeanshiftGrouping indicates grouping algorithm\n   */\n  public detectMultiScale(img: InputArray, foundLocations: Rect, hitThreshold?: double, winStride?: Size, padding?: Size, scale?: double, finalThreshold?: double, useMeanshiftGrouping?: bool): InputArray\n\n  /**\n   *   @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.\n   *   \n   *   @param foundLocations Vector of rectangles where each rectangle contains the detected object.\n   *   \n   *   @param locations Vector of DetectionROI\n   *   \n   *   @param hitThreshold Threshold for the distance between features and SVM classifying plane. Usually\n   * it is 0 and should be specified in the detector coefficients (as the last free coefficient). But if\n   * the free coefficient is omitted (which is allowed), you can specify it manually here.\n   *   \n   *   @param groupThreshold Minimum possible number of rectangles minus 1. The threshold is used in a\n   * group of rectangles to retain it.\n   */\n  public detectMultiScaleROI(img: InputArray, foundLocations: any, locations: DetectionROI, hitThreshold?: double, groupThreshold?: int): InputArray\n\n  /**\n   *   @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.\n   *   \n   *   @param locations Vector of Point\n   *   \n   *   @param foundLocations Vector of Point where each Point is detected object's top-left point.\n   *   \n   *   @param confidences confidences\n   *   \n   *   @param hitThreshold Threshold for the distance between features and SVM classifying plane. Usually\n   * it is 0 and should be specified in the detector coefficients (as the last free coefficient). But if\n   * the free coefficient is omitted (which is allowed), you can specify it manually here\n   *   \n   *   @param winStride winStride\n   *   \n   *   @param padding padding\n   */\n  public detectROI(img: InputArray, locations: any, foundLocations: any, confidences: any, hitThreshold?: double, winStride?: any, padding?: any): InputArray\n\n  public getDescriptorSize(): size_t\n\n  public getWinSigma(): double\n\n  /**\n   *   @param rectList Input/output vector of rectangles. Output vector includes retained and grouped\n   * rectangles. (The Python list is not modified in place.)\n   *   \n   *   @param weights Input/output vector of weights of rectangles. Output vector includes weights of\n   * retained and grouped rectangles. (The Python list is not modified in place.)\n   *   \n   *   @param groupThreshold Minimum possible number of rectangles minus 1. The threshold is used in a\n   * group of rectangles to retain it.\n   *   \n   *   @param eps Relative difference between sides of the rectangles to merge them into a group.\n   */\n  public groupRectangles(rectList: any, weights: any, groupThreshold: int, eps: double): any\n\n  /**\n   *   @param filename Path of the file to read.\n   *   \n   *   @param objname The optional name of the node to read (if empty, the first top-level node will be\n   * used).\n   */\n  public load(filename: String, objname?: String): String\n\n  /**\n   *   @param fn File node\n   */\n  public read(fn: FileNode): FileNode\n\n  /**\n   *   @param filename File name\n   *   \n   *   @param objname Object name\n   */\n  public save(filename: String, objname?: String): String\n\n  /**\n   *   @param svmdetector coefficients for the linear SVM classifier.\n   */\n  public setSVMDetector(svmdetector: InputArray): InputArray\n\n  /**\n   *   @param fs File storage\n   *   \n   *   @param objname Object name\n   */\n  public write(fs: FileStorage, objname: String): FileStorage\n\n  public static getDaimlerPeopleDetector(): any\n\n  public static getDefaultPeopleDetector(): any\n}\n\nexport declare const DEFAULT_NLEVELS: any // initializer: = 64\n\nexport declare const DESCR_FORMAT_COL_BY_COL: DescriptorStorageFormat // initializer: \n\nexport declare const DESCR_FORMAT_ROW_BY_ROW: DescriptorStorageFormat // initializer: \n\nexport declare const L2Hys: HistogramNormType // initializer: = 0\n\nexport type DescriptorStorageFormat = any\n\nexport type HistogramNormType = any\n\n"},"node_modules_mirada_dist_src_types_opencv_imgproc_color_conversions_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_imgproc_color_conversions_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/imgproc_color_conversions.d.ts","content":"\nimport { InputArray, int, OutputArray } from './_types'\n/*\n * # Color Space Conversions\n * \n */\n/**\n * The function converts an input image from one color space to another. In case of a transformation\n * to-from RGB color space, the order of the channels should be specified explicitly (RGB or BGR). Note\n * that the default color format in OpenCV is often referred to as RGB but it is actually BGR (the\n * bytes are reversed). So the first byte in a standard (24-bit) color image will be an 8-bit Blue\n * component, the second byte will be Green, and the third byte will be Red. The fourth, fifth, and\n * sixth bytes would then be the second pixel (Blue, then Green, then Red), and so on.\n * \n * The conventional ranges for R, G, and B channel values are:\n * \n * 0 to 255 for CV_8U images\n * 0 to 65535 for CV_16U images\n * 0 to 1 for CV_32F images\n * \n * In case of linear transformations, the range does not matter. But in case of a non-linear\n * transformation, an input RGB image should be normalized to the proper value range to get the correct\n * results, for example, for RGB `$\\\\rightarrow$` L*u*v* transformation. For example, if you have a\n * 32-bit floating-point image directly converted from an 8-bit image without any scaling, then it will\n * have the 0..255 value range instead of 0..1 assumed by the function. So, before calling\n * [cvtColor](#d8/d01/group__imgproc__color__conversions_1ga397ae87e1288a81d2363b61574eb8cab}) , you\n * need first to scale the image down: \n * \n * ```cpp\n * img *= 1./255;\n * cvtColor(img, img, COLOR_BGR2Luv);\n * ```\n * \n *  If you use\n * [cvtColor](#d8/d01/group__imgproc__color__conversions_1ga397ae87e1288a81d2363b61574eb8cab}) with\n * 8-bit images, the conversion will have some information lost. For many applications, this will not\n * be noticeable but it is recommended to use 32-bit images in applications that need the full range of\n * colors or that convert an image before an operation and then convert back.\n * \n * If conversion adds the alpha channel, its value will set to the maximum of corresponding channel\n * range: 255 for CV_8U, 65535 for CV_16U, 1 for CV_32F.\n * \n * [Color conversions](#de/d25/imgproc_color_conversions})\n * \n * @param src input image: 8-bit unsigned, 16-bit unsigned ( CV_16UC... ), or single-precision\n * floating-point.\n * \n * @param dst output image of the same size and depth as src.\n * \n * @param code color space conversion code (see ColorConversionCodes).\n * \n * @param dstCn number of channels in the destination image; if the parameter is 0, the number of the\n * channels is derived automatically from src and code.\n */\nexport declare function cvtColor(src: InputArray, dst: OutputArray, code: int, dstCn?: int): void\n\n/**\n * This function only supports YUV420 to RGB conversion as of now.\n * \n * @param src1 8-bit image (CV_8U) of the Y plane.\n * \n * @param src2 image containing interleaved U/V plane.\n * \n * @param dst output image.\n * \n * @param code Specifies the type of conversion. It can take any of the following values:\n * COLOR_YUV2BGR_NV12COLOR_YUV2RGB_NV12COLOR_YUV2BGRA_NV12COLOR_YUV2RGBA_NV12COLOR_YUV2BGR_NV21COLOR_YUV2RGB_NV21COLOR_YUV2BGRA_NV21COLOR_YUV2RGBA_NV21\n */\nexport declare function cvtColorTwoPlane(src1: InputArray, src2: InputArray, dst: OutputArray, code: int): void\n\n/**\n * The function can do the following transformations:\n * \n * Demosaicing using bilinear\n * interpolation[COLOR_BayerBG2BGR](#d8/d01/group__imgproc__color__conversions_1gga4e0972be5de079fed4e3a10e24ef5ef0a8945844ab075687f4d4196abe1ce0db4})\n * ,\n * [COLOR_BayerGB2BGR](#d8/d01/group__imgproc__color__conversions_1gga4e0972be5de079fed4e3a10e24ef5ef0acef801137d9696dcb622122a7ef266c6})\n * ,\n * [COLOR_BayerRG2BGR](#d8/d01/group__imgproc__color__conversions_1gga4e0972be5de079fed4e3a10e24ef5ef0a275d4d0ff99fdf45b2b6b421a14ec831})\n * ,\n * [COLOR_BayerGR2BGR](#d8/d01/group__imgproc__color__conversions_1gga4e0972be5de079fed4e3a10e24ef5ef0afe3d71ad80f5f067d3d76b376cf8d951})[COLOR_BayerBG2GRAY](#d8/d01/group__imgproc__color__conversions_1gga4e0972be5de079fed4e3a10e24ef5ef0a08febd33b0214417dd33a7fb014bf99a})\n * ,\n * [COLOR_BayerGB2GRAY](#d8/d01/group__imgproc__color__conversions_1gga4e0972be5de079fed4e3a10e24ef5ef0a63667769d13ad6dff2b5a296f4c966d2})\n * ,\n * [COLOR_BayerRG2GRAY](#d8/d01/group__imgproc__color__conversions_1gga4e0972be5de079fed4e3a10e24ef5ef0ad4fbbce0080be39beb5397716bac3ccc})\n * ,\n * [COLOR_BayerGR2GRAY](#d8/d01/group__imgproc__color__conversions_1gga4e0972be5de079fed4e3a10e24ef5ef0aaab526ce2ad4ce74603c711b3c22a38a})\n * Demosaicing using Variable Number of\n * Gradients.[COLOR_BayerBG2BGR_VNG](#d8/d01/group__imgproc__color__conversions_1gga4e0972be5de079fed4e3a10e24ef5ef0a03fa881afa10795e9f4344a50b80db7f})\n * ,\n * [COLOR_BayerGB2BGR_VNG](#d8/d01/group__imgproc__color__conversions_1gga4e0972be5de079fed4e3a10e24ef5ef0ae47bd67001d93fbee5638f61ce256b68})\n * ,\n * [COLOR_BayerRG2BGR_VNG](#d8/d01/group__imgproc__color__conversions_1gga4e0972be5de079fed4e3a10e24ef5ef0ad298bb184eda5bf3a58fbc4e509c0e43})\n * ,\n * [COLOR_BayerGR2BGR_VNG](#d8/d01/group__imgproc__color__conversions_1gga4e0972be5de079fed4e3a10e24ef5ef0ad787e2911c5b21eaf4d7ffe6f85ad5a8})\n * Edge-Aware\n * Demosaicing.[COLOR_BayerBG2BGR_EA](#d8/d01/group__imgproc__color__conversions_1gga4e0972be5de079fed4e3a10e24ef5ef0af945cf163b1b5d01b69feabfe10d62bc})\n * ,\n * [COLOR_BayerGB2BGR_EA](#d8/d01/group__imgproc__color__conversions_1gga4e0972be5de079fed4e3a10e24ef5ef0a3ac0015fd225d6e02485c822fb26b4b6})\n * ,\n * [COLOR_BayerRG2BGR_EA](#d8/d01/group__imgproc__color__conversions_1gga4e0972be5de079fed4e3a10e24ef5ef0a9a7ff84cd856119c6c5b8ecb81ba9284})\n * ,\n * [COLOR_BayerGR2BGR_EA](#d8/d01/group__imgproc__color__conversions_1gga4e0972be5de079fed4e3a10e24ef5ef0af51a3d5aceb2256a59c3a4c8e499d7e3})\n * Demosaicing with alpha\n * channel[COLOR_BayerBG2BGRA](#d8/d01/group__imgproc__color__conversions_1gga4e0972be5de079fed4e3a10e24ef5ef0a64d2dcd6fd8f41e865801fda7a2b75e4})\n * ,\n * [COLOR_BayerGB2BGRA](#d8/d01/group__imgproc__color__conversions_1gga4e0972be5de079fed4e3a10e24ef5ef0ace94e0ec556c55476cd451fbcd411bb8})\n * ,\n * [COLOR_BayerRG2BGRA](#d8/d01/group__imgproc__color__conversions_1gga4e0972be5de079fed4e3a10e24ef5ef0af3d528d5f0f7c24ac08dd5e5a8f19ddc})\n * ,\n * [COLOR_BayerGR2BGRA](#d8/d01/group__imgproc__color__conversions_1gga4e0972be5de079fed4e3a10e24ef5ef0a850bc919c36bb360f1270ffb9f839532})\n * \n * [cvtColor](#d8/d01/group__imgproc__color__conversions_1ga397ae87e1288a81d2363b61574eb8cab})\n * \n * @param src input image: 8-bit unsigned or 16-bit unsigned.\n * \n * @param dst output image of the same size and depth as src.\n * \n * @param code Color space conversion code (see the description below).\n * \n * @param dstCn number of channels in the destination image; if the parameter is 0, the number of the\n * channels is derived automatically from src and code.\n */\nexport declare function demosaicing(src: InputArray, dst: OutputArray, code: int, dstCn?: int): void\n\nexport declare const COLOR_BGR2BGRA: ColorConversionCodes // initializer: = 0\n\nexport declare const COLOR_RGB2RGBA: ColorConversionCodes // initializer: = COLOR_BGR2BGRA\n\nexport declare const COLOR_BGRA2BGR: ColorConversionCodes // initializer: = 1\n\nexport declare const COLOR_RGBA2RGB: ColorConversionCodes // initializer: = COLOR_BGRA2BGR\n\nexport declare const COLOR_BGR2RGBA: ColorConversionCodes // initializer: = 2\n\nexport declare const COLOR_RGB2BGRA: ColorConversionCodes // initializer: = COLOR_BGR2RGBA\n\nexport declare const COLOR_RGBA2BGR: ColorConversionCodes // initializer: = 3\n\nexport declare const COLOR_BGRA2RGB: ColorConversionCodes // initializer: = COLOR_RGBA2BGR\n\nexport declare const COLOR_BGR2RGB: ColorConversionCodes // initializer: = 4\n\nexport declare const COLOR_RGB2BGR: ColorConversionCodes // initializer: = COLOR_BGR2RGB\n\nexport declare const COLOR_BGRA2RGBA: ColorConversionCodes // initializer: = 5\n\nexport declare const COLOR_RGBA2BGRA: ColorConversionCodes // initializer: = COLOR_BGRA2RGBA\n\nexport declare const COLOR_BGR2GRAY: ColorConversionCodes // initializer: = 6\n\nexport declare const COLOR_RGB2GRAY: ColorConversionCodes // initializer: = 7\n\nexport declare const COLOR_GRAY2BGR: ColorConversionCodes // initializer: = 8\n\nexport declare const COLOR_GRAY2RGB: ColorConversionCodes // initializer: = COLOR_GRAY2BGR\n\nexport declare const COLOR_GRAY2BGRA: ColorConversionCodes // initializer: = 9\n\nexport declare const COLOR_GRAY2RGBA: ColorConversionCodes // initializer: = COLOR_GRAY2BGRA\n\nexport declare const COLOR_BGRA2GRAY: ColorConversionCodes // initializer: = 10\n\nexport declare const COLOR_RGBA2GRAY: ColorConversionCodes // initializer: = 11\n\nexport declare const COLOR_BGR2BGR565: ColorConversionCodes // initializer: = 12\n\nexport declare const COLOR_RGB2BGR565: ColorConversionCodes // initializer: = 13\n\nexport declare const COLOR_BGR5652BGR: ColorConversionCodes // initializer: = 14\n\nexport declare const COLOR_BGR5652RGB: ColorConversionCodes // initializer: = 15\n\nexport declare const COLOR_BGRA2BGR565: ColorConversionCodes // initializer: = 16\n\nexport declare const COLOR_RGBA2BGR565: ColorConversionCodes // initializer: = 17\n\nexport declare const COLOR_BGR5652BGRA: ColorConversionCodes // initializer: = 18\n\nexport declare const COLOR_BGR5652RGBA: ColorConversionCodes // initializer: = 19\n\nexport declare const COLOR_GRAY2BGR565: ColorConversionCodes // initializer: = 20\n\nexport declare const COLOR_BGR5652GRAY: ColorConversionCodes // initializer: = 21\n\nexport declare const COLOR_BGR2BGR555: ColorConversionCodes // initializer: = 22\n\nexport declare const COLOR_RGB2BGR555: ColorConversionCodes // initializer: = 23\n\nexport declare const COLOR_BGR5552BGR: ColorConversionCodes // initializer: = 24\n\nexport declare const COLOR_BGR5552RGB: ColorConversionCodes // initializer: = 25\n\nexport declare const COLOR_BGRA2BGR555: ColorConversionCodes // initializer: = 26\n\nexport declare const COLOR_RGBA2BGR555: ColorConversionCodes // initializer: = 27\n\nexport declare const COLOR_BGR5552BGRA: ColorConversionCodes // initializer: = 28\n\nexport declare const COLOR_BGR5552RGBA: ColorConversionCodes // initializer: = 29\n\nexport declare const COLOR_GRAY2BGR555: ColorConversionCodes // initializer: = 30\n\nexport declare const COLOR_BGR5552GRAY: ColorConversionCodes // initializer: = 31\n\nexport declare const COLOR_BGR2XYZ: ColorConversionCodes // initializer: = 32\n\nexport declare const COLOR_RGB2XYZ: ColorConversionCodes // initializer: = 33\n\nexport declare const COLOR_XYZ2BGR: ColorConversionCodes // initializer: = 34\n\nexport declare const COLOR_XYZ2RGB: ColorConversionCodes // initializer: = 35\n\nexport declare const COLOR_BGR2YCrCb: ColorConversionCodes // initializer: = 36\n\nexport declare const COLOR_RGB2YCrCb: ColorConversionCodes // initializer: = 37\n\nexport declare const COLOR_YCrCb2BGR: ColorConversionCodes // initializer: = 38\n\nexport declare const COLOR_YCrCb2RGB: ColorConversionCodes // initializer: = 39\n\nexport declare const COLOR_BGR2HSV: ColorConversionCodes // initializer: = 40\n\nexport declare const COLOR_RGB2HSV: ColorConversionCodes // initializer: = 41\n\nexport declare const COLOR_BGR2Lab: ColorConversionCodes // initializer: = 44\n\nexport declare const COLOR_RGB2Lab: ColorConversionCodes // initializer: = 45\n\nexport declare const COLOR_BGR2Luv: ColorConversionCodes // initializer: = 50\n\nexport declare const COLOR_RGB2Luv: ColorConversionCodes // initializer: = 51\n\nexport declare const COLOR_BGR2HLS: ColorConversionCodes // initializer: = 52\n\nexport declare const COLOR_RGB2HLS: ColorConversionCodes // initializer: = 53\n\nexport declare const COLOR_HSV2BGR: ColorConversionCodes // initializer: = 54\n\nexport declare const COLOR_HSV2RGB: ColorConversionCodes // initializer: = 55\n\nexport declare const COLOR_Lab2BGR: ColorConversionCodes // initializer: = 56\n\nexport declare const COLOR_Lab2RGB: ColorConversionCodes // initializer: = 57\n\nexport declare const COLOR_Luv2BGR: ColorConversionCodes // initializer: = 58\n\nexport declare const COLOR_Luv2RGB: ColorConversionCodes // initializer: = 59\n\nexport declare const COLOR_HLS2BGR: ColorConversionCodes // initializer: = 60\n\nexport declare const COLOR_HLS2RGB: ColorConversionCodes // initializer: = 61\n\nexport declare const COLOR_BGR2HSV_FULL: ColorConversionCodes // initializer: = 66\n\nexport declare const COLOR_RGB2HSV_FULL: ColorConversionCodes // initializer: = 67\n\nexport declare const COLOR_BGR2HLS_FULL: ColorConversionCodes // initializer: = 68\n\nexport declare const COLOR_RGB2HLS_FULL: ColorConversionCodes // initializer: = 69\n\nexport declare const COLOR_HSV2BGR_FULL: ColorConversionCodes // initializer: = 70\n\nexport declare const COLOR_HSV2RGB_FULL: ColorConversionCodes // initializer: = 71\n\nexport declare const COLOR_HLS2BGR_FULL: ColorConversionCodes // initializer: = 72\n\nexport declare const COLOR_HLS2RGB_FULL: ColorConversionCodes // initializer: = 73\n\nexport declare const COLOR_LBGR2Lab: ColorConversionCodes // initializer: = 74\n\nexport declare const COLOR_LRGB2Lab: ColorConversionCodes // initializer: = 75\n\nexport declare const COLOR_LBGR2Luv: ColorConversionCodes // initializer: = 76\n\nexport declare const COLOR_LRGB2Luv: ColorConversionCodes // initializer: = 77\n\nexport declare const COLOR_Lab2LBGR: ColorConversionCodes // initializer: = 78\n\nexport declare const COLOR_Lab2LRGB: ColorConversionCodes // initializer: = 79\n\nexport declare const COLOR_Luv2LBGR: ColorConversionCodes // initializer: = 80\n\nexport declare const COLOR_Luv2LRGB: ColorConversionCodes // initializer: = 81\n\nexport declare const COLOR_BGR2YUV: ColorConversionCodes // initializer: = 82\n\nexport declare const COLOR_RGB2YUV: ColorConversionCodes // initializer: = 83\n\nexport declare const COLOR_YUV2BGR: ColorConversionCodes // initializer: = 84\n\nexport declare const COLOR_YUV2RGB: ColorConversionCodes // initializer: = 85\n\nexport declare const COLOR_YUV2RGB_NV12: ColorConversionCodes // initializer: = 90\n\nexport declare const COLOR_YUV2BGR_NV12: ColorConversionCodes // initializer: = 91\n\nexport declare const COLOR_YUV2RGB_NV21: ColorConversionCodes // initializer: = 92\n\nexport declare const COLOR_YUV2BGR_NV21: ColorConversionCodes // initializer: = 93\n\nexport declare const COLOR_YUV420sp2RGB: ColorConversionCodes // initializer: = COLOR_YUV2RGB_NV21\n\nexport declare const COLOR_YUV420sp2BGR: ColorConversionCodes // initializer: = COLOR_YUV2BGR_NV21\n\nexport declare const COLOR_YUV2RGBA_NV12: ColorConversionCodes // initializer: = 94\n\nexport declare const COLOR_YUV2BGRA_NV12: ColorConversionCodes // initializer: = 95\n\nexport declare const COLOR_YUV2RGBA_NV21: ColorConversionCodes // initializer: = 96\n\nexport declare const COLOR_YUV2BGRA_NV21: ColorConversionCodes // initializer: = 97\n\nexport declare const COLOR_YUV420sp2RGBA: ColorConversionCodes // initializer: = COLOR_YUV2RGBA_NV21\n\nexport declare const COLOR_YUV420sp2BGRA: ColorConversionCodes // initializer: = COLOR_YUV2BGRA_NV21\n\nexport declare const COLOR_YUV2RGB_YV12: ColorConversionCodes // initializer: = 98\n\nexport declare const COLOR_YUV2BGR_YV12: ColorConversionCodes // initializer: = 99\n\nexport declare const COLOR_YUV2RGB_IYUV: ColorConversionCodes // initializer: = 100\n\nexport declare const COLOR_YUV2BGR_IYUV: ColorConversionCodes // initializer: = 101\n\nexport declare const COLOR_YUV2RGB_I420: ColorConversionCodes // initializer: = COLOR_YUV2RGB_IYUV\n\nexport declare const COLOR_YUV2BGR_I420: ColorConversionCodes // initializer: = COLOR_YUV2BGR_IYUV\n\nexport declare const COLOR_YUV420p2RGB: ColorConversionCodes // initializer: = COLOR_YUV2RGB_YV12\n\nexport declare const COLOR_YUV420p2BGR: ColorConversionCodes // initializer: = COLOR_YUV2BGR_YV12\n\nexport declare const COLOR_YUV2RGBA_YV12: ColorConversionCodes // initializer: = 102\n\nexport declare const COLOR_YUV2BGRA_YV12: ColorConversionCodes // initializer: = 103\n\nexport declare const COLOR_YUV2RGBA_IYUV: ColorConversionCodes // initializer: = 104\n\nexport declare const COLOR_YUV2BGRA_IYUV: ColorConversionCodes // initializer: = 105\n\nexport declare const COLOR_YUV2RGBA_I420: ColorConversionCodes // initializer: = COLOR_YUV2RGBA_IYUV\n\nexport declare const COLOR_YUV2BGRA_I420: ColorConversionCodes // initializer: = COLOR_YUV2BGRA_IYUV\n\nexport declare const COLOR_YUV420p2RGBA: ColorConversionCodes // initializer: = COLOR_YUV2RGBA_YV12\n\nexport declare const COLOR_YUV420p2BGRA: ColorConversionCodes // initializer: = COLOR_YUV2BGRA_YV12\n\nexport declare const COLOR_YUV2GRAY_420: ColorConversionCodes // initializer: = 106\n\nexport declare const COLOR_YUV2GRAY_NV21: ColorConversionCodes // initializer: = COLOR_YUV2GRAY_420\n\nexport declare const COLOR_YUV2GRAY_NV12: ColorConversionCodes // initializer: = COLOR_YUV2GRAY_420\n\nexport declare const COLOR_YUV2GRAY_YV12: ColorConversionCodes // initializer: = COLOR_YUV2GRAY_420\n\nexport declare const COLOR_YUV2GRAY_IYUV: ColorConversionCodes // initializer: = COLOR_YUV2GRAY_420\n\nexport declare const COLOR_YUV2GRAY_I420: ColorConversionCodes // initializer: = COLOR_YUV2GRAY_420\n\nexport declare const COLOR_YUV420sp2GRAY: ColorConversionCodes // initializer: = COLOR_YUV2GRAY_420\n\nexport declare const COLOR_YUV420p2GRAY: ColorConversionCodes // initializer: = COLOR_YUV2GRAY_420\n\nexport declare const COLOR_YUV2RGB_UYVY: ColorConversionCodes // initializer: = 107\n\nexport declare const COLOR_YUV2BGR_UYVY: ColorConversionCodes // initializer: = 108\n\nexport declare const COLOR_YUV2RGB_Y422: ColorConversionCodes // initializer: = COLOR_YUV2RGB_UYVY\n\nexport declare const COLOR_YUV2BGR_Y422: ColorConversionCodes // initializer: = COLOR_YUV2BGR_UYVY\n\nexport declare const COLOR_YUV2RGB_UYNV: ColorConversionCodes // initializer: = COLOR_YUV2RGB_UYVY\n\nexport declare const COLOR_YUV2BGR_UYNV: ColorConversionCodes // initializer: = COLOR_YUV2BGR_UYVY\n\nexport declare const COLOR_YUV2RGBA_UYVY: ColorConversionCodes // initializer: = 111\n\nexport declare const COLOR_YUV2BGRA_UYVY: ColorConversionCodes // initializer: = 112\n\nexport declare const COLOR_YUV2RGBA_Y422: ColorConversionCodes // initializer: = COLOR_YUV2RGBA_UYVY\n\nexport declare const COLOR_YUV2BGRA_Y422: ColorConversionCodes // initializer: = COLOR_YUV2BGRA_UYVY\n\nexport declare const COLOR_YUV2RGBA_UYNV: ColorConversionCodes // initializer: = COLOR_YUV2RGBA_UYVY\n\nexport declare const COLOR_YUV2BGRA_UYNV: ColorConversionCodes // initializer: = COLOR_YUV2BGRA_UYVY\n\nexport declare const COLOR_YUV2RGB_YUY2: ColorConversionCodes // initializer: = 115\n\nexport declare const COLOR_YUV2BGR_YUY2: ColorConversionCodes // initializer: = 116\n\nexport declare const COLOR_YUV2RGB_YVYU: ColorConversionCodes // initializer: = 117\n\nexport declare const COLOR_YUV2BGR_YVYU: ColorConversionCodes // initializer: = 118\n\nexport declare const COLOR_YUV2RGB_YUYV: ColorConversionCodes // initializer: = COLOR_YUV2RGB_YUY2\n\nexport declare const COLOR_YUV2BGR_YUYV: ColorConversionCodes // initializer: = COLOR_YUV2BGR_YUY2\n\nexport declare const COLOR_YUV2RGB_YUNV: ColorConversionCodes // initializer: = COLOR_YUV2RGB_YUY2\n\nexport declare const COLOR_YUV2BGR_YUNV: ColorConversionCodes // initializer: = COLOR_YUV2BGR_YUY2\n\nexport declare const COLOR_YUV2RGBA_YUY2: ColorConversionCodes // initializer: = 119\n\nexport declare const COLOR_YUV2BGRA_YUY2: ColorConversionCodes // initializer: = 120\n\nexport declare const COLOR_YUV2RGBA_YVYU: ColorConversionCodes // initializer: = 121\n\nexport declare const COLOR_YUV2BGRA_YVYU: ColorConversionCodes // initializer: = 122\n\nexport declare const COLOR_YUV2RGBA_YUYV: ColorConversionCodes // initializer: = COLOR_YUV2RGBA_YUY2\n\nexport declare const COLOR_YUV2BGRA_YUYV: ColorConversionCodes // initializer: = COLOR_YUV2BGRA_YUY2\n\nexport declare const COLOR_YUV2RGBA_YUNV: ColorConversionCodes // initializer: = COLOR_YUV2RGBA_YUY2\n\nexport declare const COLOR_YUV2BGRA_YUNV: ColorConversionCodes // initializer: = COLOR_YUV2BGRA_YUY2\n\nexport declare const COLOR_YUV2GRAY_UYVY: ColorConversionCodes // initializer: = 123\n\nexport declare const COLOR_YUV2GRAY_YUY2: ColorConversionCodes // initializer: = 124\n\nexport declare const COLOR_YUV2GRAY_Y422: ColorConversionCodes // initializer: = COLOR_YUV2GRAY_UYVY\n\nexport declare const COLOR_YUV2GRAY_UYNV: ColorConversionCodes // initializer: = COLOR_YUV2GRAY_UYVY\n\nexport declare const COLOR_YUV2GRAY_YVYU: ColorConversionCodes // initializer: = COLOR_YUV2GRAY_YUY2\n\nexport declare const COLOR_YUV2GRAY_YUYV: ColorConversionCodes // initializer: = COLOR_YUV2GRAY_YUY2\n\nexport declare const COLOR_YUV2GRAY_YUNV: ColorConversionCodes // initializer: = COLOR_YUV2GRAY_YUY2\n\nexport declare const COLOR_RGBA2mRGBA: ColorConversionCodes // initializer: = 125\n\nexport declare const COLOR_mRGBA2RGBA: ColorConversionCodes // initializer: = 126\n\nexport declare const COLOR_RGB2YUV_I420: ColorConversionCodes // initializer: = 127\n\nexport declare const COLOR_BGR2YUV_I420: ColorConversionCodes // initializer: = 128\n\nexport declare const COLOR_RGB2YUV_IYUV: ColorConversionCodes // initializer: = COLOR_RGB2YUV_I420\n\nexport declare const COLOR_BGR2YUV_IYUV: ColorConversionCodes // initializer: = COLOR_BGR2YUV_I420\n\nexport declare const COLOR_RGBA2YUV_I420: ColorConversionCodes // initializer: = 129\n\nexport declare const COLOR_BGRA2YUV_I420: ColorConversionCodes // initializer: = 130\n\nexport declare const COLOR_RGBA2YUV_IYUV: ColorConversionCodes // initializer: = COLOR_RGBA2YUV_I420\n\nexport declare const COLOR_BGRA2YUV_IYUV: ColorConversionCodes // initializer: = COLOR_BGRA2YUV_I420\n\nexport declare const COLOR_RGB2YUV_YV12: ColorConversionCodes // initializer: = 131\n\nexport declare const COLOR_BGR2YUV_YV12: ColorConversionCodes // initializer: = 132\n\nexport declare const COLOR_RGBA2YUV_YV12: ColorConversionCodes // initializer: = 133\n\nexport declare const COLOR_BGRA2YUV_YV12: ColorConversionCodes // initializer: = 134\n\nexport declare const COLOR_BayerBG2BGR: ColorConversionCodes // initializer: = 46\n\nexport declare const COLOR_BayerGB2BGR: ColorConversionCodes // initializer: = 47\n\nexport declare const COLOR_BayerRG2BGR: ColorConversionCodes // initializer: = 48\n\nexport declare const COLOR_BayerGR2BGR: ColorConversionCodes // initializer: = 49\n\nexport declare const COLOR_BayerBG2RGB: ColorConversionCodes // initializer: = COLOR_BayerRG2BGR\n\nexport declare const COLOR_BayerGB2RGB: ColorConversionCodes // initializer: = COLOR_BayerGR2BGR\n\nexport declare const COLOR_BayerRG2RGB: ColorConversionCodes // initializer: = COLOR_BayerBG2BGR\n\nexport declare const COLOR_BayerGR2RGB: ColorConversionCodes // initializer: = COLOR_BayerGB2BGR\n\nexport declare const COLOR_BayerBG2GRAY: ColorConversionCodes // initializer: = 86\n\nexport declare const COLOR_BayerGB2GRAY: ColorConversionCodes // initializer: = 87\n\nexport declare const COLOR_BayerRG2GRAY: ColorConversionCodes // initializer: = 88\n\nexport declare const COLOR_BayerGR2GRAY: ColorConversionCodes // initializer: = 89\n\nexport declare const COLOR_BayerBG2BGR_VNG: ColorConversionCodes // initializer: = 62\n\nexport declare const COLOR_BayerGB2BGR_VNG: ColorConversionCodes // initializer: = 63\n\nexport declare const COLOR_BayerRG2BGR_VNG: ColorConversionCodes // initializer: = 64\n\nexport declare const COLOR_BayerGR2BGR_VNG: ColorConversionCodes // initializer: = 65\n\nexport declare const COLOR_BayerBG2RGB_VNG: ColorConversionCodes // initializer: = COLOR_BayerRG2BGR_VNG\n\nexport declare const COLOR_BayerGB2RGB_VNG: ColorConversionCodes // initializer: = COLOR_BayerGR2BGR_VNG\n\nexport declare const COLOR_BayerRG2RGB_VNG: ColorConversionCodes // initializer: = COLOR_BayerBG2BGR_VNG\n\nexport declare const COLOR_BayerGR2RGB_VNG: ColorConversionCodes // initializer: = COLOR_BayerGB2BGR_VNG\n\nexport declare const COLOR_BayerBG2BGR_EA: ColorConversionCodes // initializer: = 135\n\nexport declare const COLOR_BayerGB2BGR_EA: ColorConversionCodes // initializer: = 136\n\nexport declare const COLOR_BayerRG2BGR_EA: ColorConversionCodes // initializer: = 137\n\nexport declare const COLOR_BayerGR2BGR_EA: ColorConversionCodes // initializer: = 138\n\nexport declare const COLOR_BayerBG2RGB_EA: ColorConversionCodes // initializer: = COLOR_BayerRG2BGR_EA\n\nexport declare const COLOR_BayerGB2RGB_EA: ColorConversionCodes // initializer: = COLOR_BayerGR2BGR_EA\n\nexport declare const COLOR_BayerRG2RGB_EA: ColorConversionCodes // initializer: = COLOR_BayerBG2BGR_EA\n\nexport declare const COLOR_BayerGR2RGB_EA: ColorConversionCodes // initializer: = COLOR_BayerGB2BGR_EA\n\nexport declare const COLOR_BayerBG2BGRA: ColorConversionCodes // initializer: = 139\n\nexport declare const COLOR_BayerGB2BGRA: ColorConversionCodes // initializer: = 140\n\nexport declare const COLOR_BayerRG2BGRA: ColorConversionCodes // initializer: = 141\n\nexport declare const COLOR_BayerGR2BGRA: ColorConversionCodes // initializer: = 142\n\nexport declare const COLOR_BayerBG2RGBA: ColorConversionCodes // initializer: = COLOR_BayerRG2BGRA\n\nexport declare const COLOR_BayerGB2RGBA: ColorConversionCodes // initializer: = COLOR_BayerGR2BGRA\n\nexport declare const COLOR_BayerRG2RGBA: ColorConversionCodes // initializer: = COLOR_BayerBG2BGRA\n\nexport declare const COLOR_BayerGR2RGBA: ColorConversionCodes // initializer: = COLOR_BayerGB2BGRA\n\nexport declare const COLOR_COLORCVT_MAX: ColorConversionCodes // initializer: = 143\n\n/**\n * the color conversion codes \n * \n * [Color conversions](#de/d25/imgproc_color_conversions})\n * \n */\nexport type ColorConversionCodes = any\n\n"},"node_modules_mirada_dist_src_types_opencv_imgproc_draw_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_imgproc_draw_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/imgproc_draw.d.ts","content":"\nimport { bool, double, InputArray, InputArrayOfArrays, InputOutputArray, int, Point, Point2d, Rect, Scalar, Size, Size2d, Size2l } from './_types'\n/*\n * # Drawing Functions\n * Drawing functions work with matrices/images of arbitrary depth. The boundaries of the shapes can be rendered with antialiasing (implemented only for 8-bit images for now). All the functions include the parameter color that uses an RGB value (that may be constructed with the Scalar constructor ) for color images and brightness for grayscale images. For color images, the channel ordering is normally *Blue, Green, Red*. This is what imshow, imread, and imwrite expect. So, if you form a color using the Scalar constructor, it should look like:\n * \n * `\\[\\texttt{Scalar} (blue \\_ component, green \\_ component, red \\_ component[, alpha \\_ component])\\]`\n * \n * If you are using your own image rendering and I/O functions, you can use any channel ordering. The drawing functions process each channel independently and do not depend on the channel order or even on the used color space. The whole image can be converted from BGR to RGB or to a different color space using cvtColor .\n * \n * If a drawn figure is partially or completely outside the image, the drawing functions clip it. Also, many drawing functions can handle pixel coordinates specified with sub-pixel accuracy. This means that the coordinates can be passed as fixed-point numbers encoded as integers. The number of fractional bits is specified by the shift parameter and the real point coordinates are calculated as `$\\texttt{Point}(x,y)\\rightarrow\\texttt{Point2f}(x*2^{-shift},y*2^{-shift})$` . This feature is especially effective when rendering antialiased shapes.\n * \n * \n * \n * The functions do not support alpha-transparency when the target image is 4-channel. In this case, the color[3] is simply copied to the repainted pixels. Thus, if you want to paint semi-transparent shapes, you can paint them in a separate buffer and then blend it with the main image.\n */\n/**\n * The function [cv::arrowedLine](#d6/d6e/group__imgproc__draw_1ga0a165a3ca093fd488ac709fdf10c05b2})\n * draws an arrow between pt1 and pt2 points in the image. See also\n * [line](#d6/d6e/group__imgproc__draw_1ga7078a9fae8c7e7d13d24dac2520ae4a2}).\n * \n * @param img Image.\n * \n * @param pt1 The point the arrow starts from.\n * \n * @param pt2 The point the arrow points to.\n * \n * @param color Line color.\n * \n * @param thickness Line thickness.\n * \n * @param line_type Type of the line. See LineTypes\n * \n * @param shift Number of fractional bits in the point coordinates.\n * \n * @param tipLength The length of the arrow tip in relation to the arrow length\n */\nexport declare function arrowedLine(img: InputOutputArray, pt1: Point, pt2: Point, color: any, thickness?: int, line_type?: int, shift?: int, tipLength?: double): void\n\n/**\n * The function [cv::circle](#d6/d6e/group__imgproc__draw_1gaf10604b069374903dbd0f0488cb43670}) draws a\n * simple or filled circle with a given center and radius.\n * \n * @param img Image where the circle is drawn.\n * \n * @param center Center of the circle.\n * \n * @param radius Radius of the circle.\n * \n * @param color Circle color.\n * \n * @param thickness Thickness of the circle outline, if positive. Negative values, like FILLED, mean\n * that a filled circle is to be drawn.\n * \n * @param lineType Type of the circle boundary. See LineTypes\n * \n * @param shift Number of fractional bits in the coordinates of the center and in the radius value.\n */\nexport declare function circle(img: InputOutputArray, center: Point, radius: int, color: any, thickness?: int, lineType?: int, shift?: int): void\n\n/**\n * The function [cv::clipLine](#d6/d6e/group__imgproc__draw_1gaf483cb46ad6b049bc35ec67052ef1c2c})\n * calculates a part of the line segment that is entirely within the specified rectangle. it returns\n * false if the line segment is completely outside the rectangle. Otherwise, it returns true .\n * \n * @param imgSize Image size. The image rectangle is Rect(0, 0, imgSize.width, imgSize.height) .\n * \n * @param pt1 First line point.\n * \n * @param pt2 Second line point.\n */\nexport declare function clipLine(imgSize: Size, pt1: any, pt2: any): bool\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n * \n * @param imgSize Image size. The image rectangle is Rect(0, 0, imgSize.width, imgSize.height) .\n * \n * @param pt1 First line point.\n * \n * @param pt2 Second line point.\n */\nexport declare function clipLine(imgSize: Size2l, pt1: any, pt2: any): bool\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n * \n * @param imgRect Image rectangle.\n * \n * @param pt1 First line point.\n * \n * @param pt2 Second line point.\n */\nexport declare function clipLine(imgRect: Rect, pt1: any, pt2: any): bool\n\n/**\n * The function draws contour outlines in the image if `$\\\\texttt{thickness} \\\\ge 0$` or fills the area\n * bounded by the contours if `$\\\\texttt{thickness}<0$` . The example below shows how to retrieve\n * connected components from the binary image and label them: : \n * \n * ```cpp\n * #include \"opencv2/imgproc.hpp\"\n * #include \"opencv2/highgui.hpp\"\n * \n * using namespace cv;\n * using namespace std;\n * \n * int main( int argc, char** argv )\n * {\n *     Mat src;\n *     // the first command-line parameter must be a filename of the binary\n *     // (black-n-white) image\n *     if( argc != 2 || !(src=imread(argv[1], 0)).data)\n *         return -1;\n * \n *     Mat dst = Mat::zeros(src.rows, src.cols, CV_8UC3);\n * \n *     src = src > 1;\n *     namedWindow( \"Source\", 1 );\n *     imshow( \"Source\", src );\n * \n *     vector<vector<Point> > contours;\n *     vector<Vec4i> hierarchy;\n * \n *     findContours( src, contours, hierarchy,\n *         RETR_CCOMP, CHAIN_APPROX_SIMPLE );\n * \n *     // iterate through all the top-level contours,\n *     // draw each connected component with its own random color\n *     int idx = 0;\n *     for( ; idx >= 0; idx = hierarchy[idx][0] )\n *     {\n *         Scalar color( rand()&255, rand()&255, rand()&255 );\n *         drawContours( dst, contours, idx, color, FILLED, 8, hierarchy );\n *     }\n * \n *     namedWindow( \"Components\", 1 );\n *     imshow( \"Components\", dst );\n *     waitKey(0);\n * }\n * ```\n * \n * When\n * thickness=[FILLED](#d6/d6e/group__imgproc__draw_1ggaf076ef45de481ac96e0ab3dc2c29a777a89c5f6beef080e6df347167f85e07b9e}),\n * the function is designed to handle connected components with holes correctly even when no hierarchy\n * date is provided. This is done by analyzing all the outlines together using even-odd rule. This may\n * give incorrect results if you have a joint collection of separately retrieved contours. In order to\n * solve this problem, you need to call\n * [drawContours](#d6/d6e/group__imgproc__draw_1ga746c0625f1781f1ffc9056259103edbc}) separately for\n * each sub-group of contours, or iterate over the collection using contourIdx parameter.\n * \n * @param image Destination image.\n * \n * @param contours All the input contours. Each contour is stored as a point vector.\n * \n * @param contourIdx Parameter indicating a contour to draw. If it is negative, all the contours are\n * drawn.\n * \n * @param color Color of the contours.\n * \n * @param thickness Thickness of lines the contours are drawn with. If it is negative (for example,\n * thickness=FILLED ), the contour interiors are drawn.\n * \n * @param lineType Line connectivity. See LineTypes\n * \n * @param hierarchy Optional information about hierarchy. It is only needed if you want to draw only\n * some of the contours (see maxLevel ).\n * \n * @param maxLevel Maximal level for drawn contours. If it is 0, only the specified contour is drawn.\n * If it is 1, the function draws the contour(s) and all the nested contours. If it is 2, the function\n * draws the contours, all the nested contours, all the nested-to-nested contours, and so on. This\n * parameter is only taken into account when there is hierarchy available.\n * \n * @param offset Optional contour shift parameter. Shift all the drawn contours by the specified\n * $\\texttt{offset}=(dx,dy)$ .\n */\nexport declare function drawContours(image: InputOutputArray, contours: InputArrayOfArrays, contourIdx: int, color: any, thickness?: int, lineType?: int, hierarchy?: InputArray, maxLevel?: int, offset?: Point): void\n\n/**\n * The function [cv::drawMarker](#d6/d6e/group__imgproc__draw_1ga644c4a170d4799a56b29f864ce984b7e})\n * draws a marker on a given position in the image. For the moment several marker types are supported,\n * see [MarkerTypes](#d6/d6e/group__imgproc__draw_1ga0ad87faebef1039ec957737ecc633b7b}) for more\n * information.\n * \n * @param img Image.\n * \n * @param position The point where the crosshair is positioned.\n * \n * @param color Line color.\n * \n * @param markerType The specific type of marker you want to use, see MarkerTypes\n * \n * @param markerSize The length of the marker axis [default = 20 pixels]\n * \n * @param thickness Line thickness.\n * \n * @param line_type Type of the line, See LineTypes\n */\nexport declare function drawMarker(img: InputOutputArray, position: Point, color: any, markerType?: int, markerSize?: int, thickness?: int, line_type?: int): void\n\n/**\n * The function [cv::ellipse](#d6/d6e/group__imgproc__draw_1ga28b2267d35786f5f890ca167236cbc69}) with\n * more parameters draws an ellipse outline, a filled ellipse, an elliptic arc, or a filled ellipse\n * sector. The drawing code uses general parametric form. A piecewise-linear curve is used to\n * approximate the elliptic arc boundary. If you need more control of the ellipse rendering, you can\n * retrieve the curve using\n * [ellipse2Poly](#d6/d6e/group__imgproc__draw_1ga727a72a3f6a625a2ae035f957c61051f}) and then render it\n * with [polylines](#d6/d6e/group__imgproc__draw_1ga1ea127ffbbb7e0bfc4fd6fd2eb64263c}) or fill it with\n * [fillPoly](#d6/d6e/group__imgproc__draw_1ga8c69b68fab5f25e2223b6496aa60dad5}). If you use the first\n * variant of the function and want to draw the whole ellipse, not an arc, pass `startAngle=0` and\n * `endAngle=360`. If `startAngle` is greater than `endAngle`, they are swapped. The figure below\n * explains the meaning of the parameters to draw the blue arc.\n * \n * @param img Image.\n * \n * @param center Center of the ellipse.\n * \n * @param axes Half of the size of the ellipse main axes.\n * \n * @param angle Ellipse rotation angle in degrees.\n * \n * @param startAngle Starting angle of the elliptic arc in degrees.\n * \n * @param endAngle Ending angle of the elliptic arc in degrees.\n * \n * @param color Ellipse color.\n * \n * @param thickness Thickness of the ellipse arc outline, if positive. Otherwise, this indicates that a\n * filled ellipse sector is to be drawn.\n * \n * @param lineType Type of the ellipse boundary. See LineTypes\n * \n * @param shift Number of fractional bits in the coordinates of the center and values of axes.\n */\nexport declare function ellipse(img: InputOutputArray, center: Point, axes: Size, angle: double, startAngle: double, endAngle: double, color: any, thickness?: int, lineType?: int, shift?: int): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n * \n * @param img Image.\n * \n * @param box Alternative ellipse representation via RotatedRect. This means that the function draws an\n * ellipse inscribed in the rotated rectangle.\n * \n * @param color Ellipse color.\n * \n * @param thickness Thickness of the ellipse arc outline, if positive. Otherwise, this indicates that a\n * filled ellipse sector is to be drawn.\n * \n * @param lineType Type of the ellipse boundary. See LineTypes\n */\nexport declare function ellipse(img: InputOutputArray, box: any, color: any, thickness?: int, lineType?: int): void\n\n/**\n * The function ellipse2Poly computes the vertices of a polyline that approximates the specified\n * elliptic arc. It is used by\n * [ellipse](#d6/d6e/group__imgproc__draw_1ga28b2267d35786f5f890ca167236cbc69}). If `arcStart` is\n * greater than `arcEnd`, they are swapped.\n * \n * @param center Center of the arc.\n * \n * @param axes Half of the size of the ellipse main axes. See ellipse for details.\n * \n * @param angle Rotation angle of the ellipse in degrees. See ellipse for details.\n * \n * @param arcStart Starting angle of the elliptic arc in degrees.\n * \n * @param arcEnd Ending angle of the elliptic arc in degrees.\n * \n * @param delta Angle between the subsequent polyline vertices. It defines the approximation accuracy.\n * \n * @param pts Output vector of polyline vertices.\n */\nexport declare function ellipse2Poly(center: Point, axes: Size, angle: int, arcStart: int, arcEnd: int, delta: int, pts: any): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n * \n * @param center Center of the arc.\n * \n * @param axes Half of the size of the ellipse main axes. See ellipse for details.\n * \n * @param angle Rotation angle of the ellipse in degrees. See ellipse for details.\n * \n * @param arcStart Starting angle of the elliptic arc in degrees.\n * \n * @param arcEnd Ending angle of the elliptic arc in degrees.\n * \n * @param delta Angle between the subsequent polyline vertices. It defines the approximation accuracy.\n * \n * @param pts Output vector of polyline vertices.\n */\nexport declare function ellipse2Poly(center: Point2d, axes: Size2d, angle: int, arcStart: int, arcEnd: int, delta: int, pts: any): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function fillConvexPoly(img: InputOutputArray, pts: any, npts: int, color: any, lineType?: int, shift?: int): void\n\n/**\n * The function [cv::fillConvexPoly](#d6/d6e/group__imgproc__draw_1ga3069baf93b51565e386c8e591f8418e6})\n * draws a filled convex polygon. This function is much faster than the function\n * [fillPoly](#d6/d6e/group__imgproc__draw_1ga8c69b68fab5f25e2223b6496aa60dad5}) . It can fill not only\n * convex polygons but any monotonic polygon without self-intersections, that is, a polygon whose\n * contour intersects every horizontal line (scan line) twice at the most (though, its top-most and/or\n * the bottom edge could be horizontal).\n * \n * @param img Image.\n * \n * @param points Polygon vertices.\n * \n * @param color Polygon color.\n * \n * @param lineType Type of the polygon boundaries. See LineTypes\n * \n * @param shift Number of fractional bits in the vertex coordinates.\n */\nexport declare function fillConvexPoly(img: InputOutputArray, points: InputArray, color: any, lineType?: int, shift?: int): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function fillPoly(img: InputOutputArray, pts: any, npts: any, ncontours: int, color: any, lineType?: int, shift?: int, offset?: Point): void\n\n/**\n * The function [cv::fillPoly](#d6/d6e/group__imgproc__draw_1ga8c69b68fab5f25e2223b6496aa60dad5}) fills\n * an area bounded by several polygonal contours. The function can fill complex areas, for example,\n * areas with holes, contours with self-intersections (some of their parts), and so forth.\n * \n * @param img Image.\n * \n * @param pts Array of polygons where each polygon is represented as an array of points.\n * \n * @param color Polygon color.\n * \n * @param lineType Type of the polygon boundaries. See LineTypes\n * \n * @param shift Number of fractional bits in the vertex coordinates.\n * \n * @param offset Optional offset of all points of the contours.\n */\nexport declare function fillPoly(img: InputOutputArray, pts: InputArrayOfArrays, color: any, lineType?: int, shift?: int, offset?: Point): void\n\n/**\n * The fontSize to use for\n * [cv::putText](#d6/d6e/group__imgproc__draw_1ga5126f47f883d730f633d74f07456c576})\n * \n * [cv::putText](#d6/d6e/group__imgproc__draw_1ga5126f47f883d730f633d74f07456c576})\n * \n * @param fontFace Font to use, see cv::HersheyFonts.\n * \n * @param pixelHeight Pixel height to compute the fontScale for\n * \n * @param thickness Thickness of lines used to render the text.See putText for details.\n */\nexport declare function getFontScaleFromHeight(fontFace: any, pixelHeight: any, thickness?: any): double\n\n/**\n * The function [cv::getTextSize](#d6/d6e/group__imgproc__draw_1ga3d2abfcb995fd2db908c8288199dba82})\n * calculates and returns the size of a box that contains the specified text. That is, the following\n * code renders some text, the tight box surrounding it, and the baseline: : \n * \n * ```cpp\n * String text = \"Funny text inside the box\";\n * int fontFace = FONT_HERSHEY_SCRIPT_SIMPLEX;\n * double fontScale = 2;\n * int thickness = 3;\n * \n * Mat img(600, 800, CV_8UC3, Scalar::all(0));\n * \n * int baseline=0;\n * Size textSize = getTextSize(text, fontFace,\n *                             fontScale, thickness, &baseline);\n * baseline += thickness;\n * \n * // center the text\n * Point textOrg((img.cols - textSize.width)/2,\n *               (img.rows + textSize.height)/2);\n * \n * // draw the box\n * rectangle(img, textOrg + Point(0, baseline),\n *           textOrg + Point(textSize.width, -textSize.height),\n *           Scalar(0,0,255));\n * // ... and the baseline first\n * line(img, textOrg + Point(0, thickness),\n *      textOrg + Point(textSize.width, thickness),\n *      Scalar(0, 0, 255));\n * \n * // then put the text itself\n * putText(img, text, textOrg, fontFace, fontScale,\n *         Scalar::all(255), thickness, 8);\n * ```\n * \n * The size of a box that contains the specified text.\n * \n * [putText](#d6/d6e/group__imgproc__draw_1ga5126f47f883d730f633d74f07456c576})\n * \n * @param text Input text string.\n * \n * @param fontFace Font to use, see HersheyFonts.\n * \n * @param fontScale Font scale factor that is multiplied by the font-specific base size.\n * \n * @param thickness Thickness of lines used to render the text. See putText for details.\n * \n * @param baseLine y-coordinate of the baseline relative to the bottom-most text point.\n */\nexport declare function getTextSize(text: any, fontFace: int, fontScale: double, thickness: int, baseLine: any): Size\n\n/**\n * The function line draws the line segment between pt1 and pt2 points in the image. The line is\n * clipped by the image boundaries. For non-antialiased lines with integer coordinates, the 8-connected\n * or 4-connected Bresenham algorithm is used. Thick lines are drawn with rounding endings. Antialiased\n * lines are drawn using Gaussian filtering.\n * \n * @param img Image.\n * \n * @param pt1 First point of the line segment.\n * \n * @param pt2 Second point of the line segment.\n * \n * @param color Line color.\n * \n * @param thickness Line thickness.\n * \n * @param lineType Type of the line. See LineTypes.\n * \n * @param shift Number of fractional bits in the point coordinates.\n */\nexport declare function line(img: InputOutputArray, pt1: Point, pt2: Point, color: any, thickness?: int, lineType?: int, shift?: int): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function polylines(img: InputOutputArray, pts: any, npts: any, ncontours: int, isClosed: bool, color: any, thickness?: int, lineType?: int, shift?: int): void\n\n/**\n * The function [cv::polylines](#d6/d6e/group__imgproc__draw_1ga1ea127ffbbb7e0bfc4fd6fd2eb64263c})\n * draws one or more polygonal curves.\n * \n * @param img Image.\n * \n * @param pts Array of polygonal curves.\n * \n * @param isClosed Flag indicating whether the drawn polylines are closed or not. If they are closed,\n * the function draws a line from the last vertex of each curve to its first vertex.\n * \n * @param color Polyline color.\n * \n * @param thickness Thickness of the polyline edges.\n * \n * @param lineType Type of the line segments. See LineTypes\n * \n * @param shift Number of fractional bits in the vertex coordinates.\n */\nexport declare function polylines(img: InputOutputArray, pts: InputArrayOfArrays, isClosed: bool, color: any, thickness?: int, lineType?: int, shift?: int): void\n\n/**\n * The function [cv::putText](#d6/d6e/group__imgproc__draw_1ga5126f47f883d730f633d74f07456c576})\n * renders the specified text string in the image. Symbols that cannot be rendered using the specified\n * font are replaced by question marks. See\n * [getTextSize](#d6/d6e/group__imgproc__draw_1ga3d2abfcb995fd2db908c8288199dba82}) for a text\n * rendering code example.\n * \n * @param img Image.\n * \n * @param text Text string to be drawn.\n * \n * @param org Bottom-left corner of the text string in the image.\n * \n * @param fontFace Font type, see HersheyFonts.\n * \n * @param fontScale Font scale factor that is multiplied by the font-specific base size.\n * \n * @param color Text color.\n * \n * @param thickness Thickness of the lines used to draw a text.\n * \n * @param lineType Line type. See LineTypes\n * \n * @param bottomLeftOrigin When true, the image data origin is at the bottom-left corner. Otherwise, it\n * is at the top-left corner.\n */\nexport declare function putText(img: InputOutputArray, text: any, org: Point, fontFace: int, fontScale: double, color: Scalar, thickness?: int, lineType?: int, bottomLeftOrigin?: bool): void\n\n/**\n * The function [cv::rectangle](#d6/d6e/group__imgproc__draw_1ga07d2f74cadcf8e305e810ce8eed13bc9})\n * draws a rectangle outline or a filled rectangle whose two opposite corners are pt1 and pt2.\n * \n * @param img Image.\n * \n * @param pt1 Vertex of the rectangle.\n * \n * @param pt2 Vertex of the rectangle opposite to pt1 .\n * \n * @param color Rectangle color or brightness (grayscale image).\n * \n * @param thickness Thickness of lines that make up the rectangle. Negative values, like FILLED, mean\n * that the function has to draw a filled rectangle.\n * \n * @param lineType Type of the line. See LineTypes\n * \n * @param shift Number of fractional bits in the point coordinates.\n */\nexport declare function rectangle(img: InputOutputArray, pt1: Point, pt2: Point, color: any, thickness?: int, lineType?: int, shift?: int): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n * \n * use `rec` parameter as alternative specification of the drawn rectangle: `r.tl() and\n * r.br()-Point(1,1)` are opposite corners\n */\nexport declare function rectangle(img: InputOutputArray, rec: Rect, color: any, thickness?: int, lineType?: int, shift?: int): void\n\nexport declare const FONT_HERSHEY_SIMPLEX: HersheyFonts // initializer: = 0\n\nexport declare const FONT_HERSHEY_PLAIN: HersheyFonts // initializer: = 1\n\nexport declare const FONT_HERSHEY_DUPLEX: HersheyFonts // initializer: = 2\n\nexport declare const FONT_HERSHEY_COMPLEX: HersheyFonts // initializer: = 3\n\nexport declare const FONT_HERSHEY_TRIPLEX: HersheyFonts // initializer: = 4\n\nexport declare const FONT_HERSHEY_COMPLEX_SMALL: HersheyFonts // initializer: = 5\n\nexport declare const FONT_HERSHEY_SCRIPT_SIMPLEX: HersheyFonts // initializer: = 6\n\nexport declare const FONT_HERSHEY_SCRIPT_COMPLEX: HersheyFonts // initializer: = 7\n\nexport declare const FONT_ITALIC: HersheyFonts // initializer: = 16\n\nexport declare const FILLED: LineTypes // initializer: = -1\n\nexport declare const LINE_4: LineTypes // initializer: = 4\n\nexport declare const LINE_8: LineTypes // initializer: = 8\n\nexport declare const LINE_AA: LineTypes // initializer: = 16\n\nexport declare const MARKER_CROSS: MarkerTypes // initializer: = 0\n\nexport declare const MARKER_TILTED_CROSS: MarkerTypes // initializer: = 1\n\nexport declare const MARKER_STAR: MarkerTypes // initializer: = 2\n\nexport declare const MARKER_DIAMOND: MarkerTypes // initializer: = 3\n\nexport declare const MARKER_SQUARE: MarkerTypes // initializer: = 4\n\nexport declare const MARKER_TRIANGLE_UP: MarkerTypes // initializer: = 5\n\nexport declare const MARKER_TRIANGLE_DOWN: MarkerTypes // initializer: = 6\n\n/**\n * Only a subset of Hershey fonts  are supported\n * \n */\nexport type HersheyFonts = any\n\n/**\n * Only a subset of Hershey fonts  are supported\n * \n */\nexport type LineTypes = any\n\n/**\n * Only a subset of Hershey fonts  are supported\n * \n */\nexport type MarkerTypes = any\n\n"},"node_modules_mirada_dist_src_types_opencv_imgproc_hist_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_imgproc_hist_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/imgproc_hist.d.ts","content":"\nimport { bool, double, float, InputArray, InputArrayOfArrays, int, OutputArray, Size } from './_types'\n/*\n * # Histograms\n * \n */\n/**\n * The function\n * [cv::calcBackProject](#d6/dc7/group__imgproc__hist_1ga3a0af640716b456c3d14af8aee12e3ca}) calculates\n * the back project of the histogram. That is, similarly to\n * [calcHist](#d6/dc7/group__imgproc__hist_1ga4b2b5fd75503ff9e6844cc4dcdaed35d}) , at each location (x,\n * y) the function collects the values from the selected channels in the input images and finds the\n * corresponding histogram bin. But instead of incrementing it, the function reads the bin value,\n * scales it by scale , and stores in backProject(x,y) . In terms of statistics, the function computes\n * probability of each element value in respect with the empirical probability distribution represented\n * by the histogram. See how, for example, you can find and track a bright-colored object in a scene:\n * \n * Before tracking, show the object to the camera so that it covers almost the whole frame. Calculate a\n * hue histogram. The histogram may have strong maximums, corresponding to the dominant colors in the\n * object.\n * When tracking, calculate a back projection of a hue plane of each input video frame using that\n * pre-computed histogram. Threshold the back projection to suppress weak colors. It may also make\n * sense to suppress pixels with non-sufficient color saturation and too dark or too bright pixels.\n * Find connected components in the resulting picture and choose, for example, the largest component.\n * \n * This is an approximate algorithm of the CamShift color object tracker.\n * \n * [calcHist](#d6/dc7/group__imgproc__hist_1ga4b2b5fd75503ff9e6844cc4dcdaed35d}),\n * [compareHist](#d6/dc7/group__imgproc__hist_1gaf4190090efa5c47cb367cf97a9a519bd})\n * \n * @param images Source arrays. They all should have the same depth, CV_8U, CV_16U or CV_32F , and the\n * same size. Each of them can have an arbitrary number of channels.\n * \n * @param nimages Number of source images.\n * \n * @param channels The list of channels used to compute the back projection. The number of channels\n * must match the histogram dimensionality. The first array channels are numerated from 0 to\n * images[0].channels()-1 , the second array channels are counted from images[0].channels() to\n * images[0].channels() + images[1].channels()-1, and so on.\n * \n * @param hist Input histogram that can be dense or sparse.\n * \n * @param backProject Destination back projection array that is a single-channel array of the same size\n * and depth as images[0] .\n * \n * @param ranges Array of arrays of the histogram bin boundaries in each dimension. See calcHist .\n * \n * @param scale Optional scale factor for the output back projection.\n * \n * @param uniform Flag indicating whether the histogram is uniform or not (see above).\n */\nexport declare function calcBackProject(images: any, nimages: int, channels: any, hist: InputArray, backProject: OutputArray, ranges: any, scale?: double, uniform?: bool): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function calcBackProject(images: any, nimages: int, channels: any, hist: any, backProject: OutputArray, ranges: any, scale?: double, uniform?: bool): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function calcBackProject(images: InputArrayOfArrays, channels: any, hist: InputArray, dst: OutputArray, ranges: any, scale: double): void\n\n/**\n * The function [cv::calcHist](#d6/dc7/group__imgproc__hist_1ga4b2b5fd75503ff9e6844cc4dcdaed35d})\n * calculates the histogram of one or more arrays. The elements of a tuple used to increment a\n * histogram bin are taken from the corresponding input arrays at the same location. The sample below\n * shows how to compute a 2D Hue-Saturation histogram for a color image. : \n * \n * ```cpp\n * #include <opencv2/imgproc.hpp>\n * #include <opencv2/highgui.hpp>\n * \n * using namespace cv;\n * \n * int main( int argc, char** argv )\n * {\n *     Mat src, hsv;\n *     if( argc != 2 || !(src=imread(argv[1], 1)).data )\n *         return -1;\n * \n *     cvtColor(src, hsv, COLOR_BGR2HSV);\n * \n *     // Quantize the hue to 30 levels\n *     // and the saturation to 32 levels\n *     int hbins = 30, sbins = 32;\n *     int histSize[] = {hbins, sbins};\n *     // hue varies from 0 to 179, see cvtColor\n *     float hranges[] = { 0, 180 };\n *     // saturation varies from 0 (black-gray-white) to\n *     // 255 (pure spectrum color)\n *     float sranges[] = { 0, 256 };\n *     const float* ranges[] = { hranges, sranges };\n *     MatND hist;\n *     // we compute the histogram from the 0-th and 1-st channels\n *     int channels[] = {0, 1};\n * \n *     calcHist( &hsv, 1, channels, Mat(), // do not use mask\n *              hist, 2, histSize, ranges,\n *              true, // the histogram is uniform\n *              false );\n *     double maxVal=0;\n *     minMaxLoc(hist, 0, &maxVal, 0, 0);\n * \n *     int scale = 10;\n *     Mat histImg = Mat::zeros(sbins*scale, hbins*10, CV_8UC3);\n * \n *     for( int h = 0; h < hbins; h++ )\n *         for( int s = 0; s < sbins; s++ )\n *         {\n *             float binVal = hist.at<float>(h, s);\n *             int intensity = cvRound(binVal*255/maxVal);\n *             rectangle( histImg, Point(h*scale, s*scale),\n *                         Point( (h+1)*scale - 1, (s+1)*scale - 1),\n *                         Scalar::all(intensity),\n *                         -1 );\n *         }\n * \n *     namedWindow( \"Source\", 1 );\n *     imshow( \"Source\", src );\n * \n *     namedWindow( \"H-S Histogram\", 1 );\n *     imshow( \"H-S Histogram\", histImg );\n *     waitKey();\n * }\n * ```\n * \n * @param images Source arrays. They all should have the same depth, CV_8U, CV_16U or CV_32F , and the\n * same size. Each of them can have an arbitrary number of channels.\n * \n * @param nimages Number of source images.\n * \n * @param channels List of the dims channels used to compute the histogram. The first array channels\n * are numerated from 0 to images[0].channels()-1 , the second array channels are counted from\n * images[0].channels() to images[0].channels() + images[1].channels()-1, and so on.\n * \n * @param mask Optional mask. If the matrix is not empty, it must be an 8-bit array of the same size as\n * images[i] . The non-zero mask elements mark the array elements counted in the histogram.\n * \n * @param hist Output histogram, which is a dense or sparse dims -dimensional array.\n * \n * @param dims Histogram dimensionality that must be positive and not greater than CV_MAX_DIMS (equal\n * to 32 in the current OpenCV version).\n * \n * @param histSize Array of histogram sizes in each dimension.\n * \n * @param ranges Array of the dims arrays of the histogram bin boundaries in each dimension. When the\n * histogram is uniform ( uniform =true), then for each dimension i it is enough to specify the lower\n * (inclusive) boundary $L_0$ of the 0-th histogram bin and the upper (exclusive) boundary\n * $U_{\\texttt{histSize}[i]-1}$ for the last histogram bin histSize[i]-1 . That is, in case of a\n * uniform histogram each of ranges[i] is an array of 2 elements. When the histogram is not uniform (\n * uniform=false ), then each of ranges[i] contains histSize[i]+1 elements: $L_0, U_0=L_1, U_1=L_2,\n * ..., U_{\\texttt{histSize[i]}-2}=L_{\\texttt{histSize[i]}-1}, U_{\\texttt{histSize[i]}-1}$ . The array\n * elements, that are not between $L_0$ and $U_{\\texttt{histSize[i]}-1}$ , are not counted in the\n * histogram.\n * \n * @param uniform Flag indicating whether the histogram is uniform or not (see above).\n * \n * @param accumulate Accumulation flag. If it is set, the histogram is not cleared in the beginning\n * when it is allocated. This feature enables you to compute a single histogram from several sets of\n * arrays, or to update the histogram in time.\n */\nexport declare function calcHist(images: any, nimages: int, channels: any, mask: InputArray, hist: OutputArray, dims: int, histSize: any, ranges: any, uniform?: bool, accumulate?: bool): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n * \n * this variant uses SparseMat for output\n */\nexport declare function calcHist(images: any, nimages: int, channels: any, mask: InputArray, hist: any, dims: int, histSize: any, ranges: any, uniform?: bool, accumulate?: bool): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function calcHist(images: InputArrayOfArrays, channels: any, mask: InputArray, hist: OutputArray, histSize: any, ranges: any, accumulate?: bool): void\n\n/**\n * The function [cv::compareHist](#d6/dc7/group__imgproc__hist_1gaf4190090efa5c47cb367cf97a9a519bd})\n * compares two dense or two sparse histograms using the specified method.\n * \n * The function returns `$d(H_1, H_2)$` .\n * \n * While the function works well with 1-, 2-, 3-dimensional dense histograms, it may not be suitable\n * for high-dimensional sparse histograms. In such histograms, because of aliasing and sampling\n * problems, the coordinates of non-zero histogram bins can slightly shift. To compare such histograms\n * or more general sparse configurations of weighted points, consider using the\n * [EMD](#d6/dc7/group__imgproc__hist_1ga902b8e60cc7075c8947345489221e0e0}) function.\n * \n * @param H1 First compared histogram.\n * \n * @param H2 Second compared histogram of the same size as H1 .\n * \n * @param method Comparison method, see HistCompMethods\n */\nexport declare function compareHist(H1: InputArray, H2: InputArray, method: int): double\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function compareHist(H1: any, H2: any, method: int): double\n\n/**\n * @param clipLimit Threshold for contrast limiting.\n * \n * @param tileGridSize Size of grid for histogram equalization. Input image will be divided into\n * equally sized rectangular tiles. tileGridSize defines the number of tiles in row and column.\n */\nexport declare function createCLAHE(clipLimit?: double, tileGridSize?: Size): any\n\n/**\n * The function computes the earth mover distance and/or a lower boundary of the distance between the\n * two weighted point configurations. One of the applications described in RubnerSept98, Rubner2000 is\n * multi-dimensional histogram comparison for image retrieval. EMD is a transportation problem that is\n * solved using some modification of a simplex algorithm, thus the complexity is exponential in the\n * worst case, though, on average it is much faster. In the case of a real metric the lower boundary\n * can be calculated even faster (using linear-time algorithm) and it can be used to determine roughly\n * whether the two signatures are far enough so that they cannot relate to the same object.\n * \n * @param signature1 First signature, a $\\texttt{size1}\\times \\texttt{dims}+1$ floating-point matrix.\n * Each row stores the point weight followed by the point coordinates. The matrix is allowed to have a\n * single column (weights only) if the user-defined cost matrix is used. The weights must be\n * non-negative and have at least one non-zero value.\n * \n * @param signature2 Second signature of the same format as signature1 , though the number of rows may\n * be different. The total weights may be different. In this case an extra \"dummy\" point is added to\n * either signature1 or signature2. The weights must be non-negative and have at least one non-zero\n * value.\n * \n * @param distType Used metric. See DistanceTypes.\n * \n * @param cost User-defined $\\texttt{size1}\\times \\texttt{size2}$ cost matrix. Also, if a cost matrix\n * is used, lower boundary lowerBound cannot be calculated because it needs a metric function.\n * \n * @param lowerBound Optional input/output parameter: lower boundary of a distance between the two\n * signatures that is a distance between mass centers. The lower boundary may not be calculated if the\n * user-defined cost matrix is used, the total weights of point configurations are not equal, or if the\n * signatures consist of weights only (the signature matrices have a single column). You must**\n * initialize *lowerBound . If the calculated distance between mass centers is greater or equal to\n * *lowerBound (it means that the signatures are far enough), the function does not calculate EMD. In\n * any case *lowerBound is set to the calculated distance between mass centers on return. Thus, if you\n * want to calculate both distance between mass centers and EMD, *lowerBound should be set to 0.\n * \n * @param flow Resultant $\\texttt{size1} \\times \\texttt{size2}$ flow matrix: $\\texttt{flow}_{i,j}$ is a\n * flow from $i$ -th point of signature1 to $j$ -th point of signature2 .\n */\nexport declare function EMD(signature1: InputArray, signature2: InputArray, distType: int, cost?: InputArray, lowerBound?: any, flow?: OutputArray): float\n\n/**\n * The function equalizes the histogram of the input image using the following algorithm:\n * \n * Calculate the histogram `$H$` for src .\n * Normalize the histogram so that the sum of histogram bins is 255.\n * Compute the integral of the histogram: `\\\\[H'_i = \\\\sum _{0 \\\\le j < i} H(j)\\\\]`\n * Transform the image using `$H'$` as a look-up table: `$\\\\texttt{dst}(x,y) = H'(\\\\texttt{src}(x,y))$`\n * \n * The algorithm normalizes the brightness and increases the contrast of the image.\n * \n * @param src Source 8-bit single channel image.\n * \n * @param dst Destination image of the same size and type as src .\n */\nexport declare function equalizeHist(src: InputArray, dst: OutputArray): void\n\nexport declare function wrapperEMD(signature1: InputArray, signature2: InputArray, distType: int, cost?: InputArray, lowerBound?: any, flow?: OutputArray): float\n\n/**\n * Correlation `\\\\[d(H_1,H_2) = \\\\frac{\\\\sum_I (H_1(I) - \\\\bar{H_1}) (H_2(I) -\n * \\\\bar{H_2})}{\\\\sqrt{\\\\sum_I(H_1(I) - \\\\bar{H_1})^2 \\\\sum_I(H_2(I) - \\\\bar{H_2})^2}}\\\\]` where\n * `\\\\[\\\\bar{H_k} = \\\\frac{1}{N} \\\\sum _J H_k(J)\\\\]` and `$N$` is a total number of histogram bins.\n * \n */\nexport declare const HISTCMP_CORREL: HistCompMethods // initializer: = 0\n\n/**\n * Chi-Square `\\\\[d(H_1,H_2) = \\\\sum _I \\\\frac{\\\\left(H_1(I)-H_2(I)\\\\right)^2}{H_1(I)}\\\\]`\n * \n */\nexport declare const HISTCMP_CHISQR: HistCompMethods // initializer: = 1\n\n/**\n * Intersection `\\\\[d(H_1,H_2) = \\\\sum _I \\\\min (H_1(I), H_2(I))\\\\]`\n * \n */\nexport declare const HISTCMP_INTERSECT: HistCompMethods // initializer: = 2\n\n/**\n * Bhattacharyya distance (In fact, OpenCV computes Hellinger distance, which is related to\n * Bhattacharyya coefficient.) `\\\\[d(H_1,H_2) = \\\\sqrt{1 - \\\\frac{1}{\\\\sqrt{\\\\bar{H_1} \\\\bar{H_2} N^2}}\n * \\\\sum_I \\\\sqrt{H_1(I) \\\\cdot H_2(I)}}\\\\]`\n * \n */\nexport declare const HISTCMP_BHATTACHARYYA: HistCompMethods // initializer: = 3\n\nexport declare const HISTCMP_HELLINGER: HistCompMethods // initializer: = HISTCMP_BHATTACHARYYA\n\n/**\n * Alternative Chi-Square `\\\\[d(H_1,H_2) = 2 * \\\\sum _I\n * \\\\frac{\\\\left(H_1(I)-H_2(I)\\\\right)^2}{H_1(I)+H_2(I)}\\\\]` This alternative formula is regularly used\n * for texture comparison. See e.g. Puzicha1997\n * \n */\nexport declare const HISTCMP_CHISQR_ALT: HistCompMethods // initializer: = 4\n\n/**\n * Kullback-Leibler divergence `\\\\[d(H_1,H_2) = \\\\sum _I H_1(I) \\\\log\n * \\\\left(\\\\frac{H_1(I)}{H_2(I)}\\\\right)\\\\]`\n * \n */\nexport declare const HISTCMP_KL_DIV: HistCompMethods // initializer: = 5\n\n/**\n * Histogram comparison methods\n * \n */\nexport type HistCompMethods = any\n\n"},"node_modules_mirada_dist_src_types_opencv_imgproc_filter_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_imgproc_filter_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/imgproc_filter.d.ts","content":"\nimport { bool, double, InputArray, int, Mat, OutputArray, OutputArrayOfArrays, Point, Scalar, Size, TermCriteria } from './_types'\n/*\n * # Image Filtering\n * Functions and classes described in this section are used to perform various linear or non-linear filtering operations on 2D images (represented as [Mat](#d3/d63/classcv_1_1Mat})'s). It means that for each pixel location `$(x,y)$` in the source image (normally, rectangular), its neighborhood is considered and used to compute the response. In case of a linear filter, it is a weighted sum of pixel values. In case of morphological operations, it is the minimum or maximum values, and so on. The computed response is stored in the destination image at the same location `$(x,y)$`. It means that the output image will be of the same size as the input image. Normally, the functions support multi-channel arrays, in which case every channel is processed independently. Therefore, the output image will also have the same number of channels as the input one.\n * \n * Another common feature of the functions and classes described in this section is that, unlike simple arithmetic functions, they need to extrapolate values of some non-existing pixels. For example, if you want to smooth an image using a Gaussian `$3 \\times 3$` filter, then, when processing the left-most pixels in each row, you need pixels to the left of them, that is, outside of the image. You can let these pixels be the same as the left-most image pixels (\"replicated\n * border\" extrapolation method), or assume that all the non-existing pixels are zeros (\"constant\n * border\" extrapolation method), and so on. OpenCV enables you to specify the extrapolation method. For details, see [BorderTypes](#d2/de8/group__core__array_1ga209f2f4869e304c82d07739337eae7c5})\n * \n * <a name=\"d4/d86/group__imgproc__filter_1filter_depths\"></a>\n * \n * ## Depth combinations\n * \n * \n * \n * \n * \n * when ddepth=-1, the output image will have the same depth as the source.\n */\n/**\n * The function applies bilateral filtering to the input image, as described in  bilateralFilter can\n * reduce unwanted noise very well while keeping edges fairly sharp. However, it is very slow compared\n * to most filters.\n * \n * Sigma values*: For simplicity, you can set the 2 sigma values to be the same. If they are small (<\n * 10), the filter will not have much effect, whereas if they are large (> 150), they will have a very\n * strong effect, making the image look \"cartoonish\".\n * \n * Filter size*: Large filters (d > 5) are very slow, so it is recommended to use d=5 for real-time\n * applications, and perhaps d=9 for offline applications that need heavy noise filtering.\n * \n * This filter does not work inplace.\n * \n * @param src Source 8-bit or floating-point, 1-channel or 3-channel image.\n * \n * @param dst Destination image of the same size and type as src .\n * \n * @param d Diameter of each pixel neighborhood that is used during filtering. If it is non-positive,\n * it is computed from sigmaSpace.\n * \n * @param sigmaColor Filter sigma in the color space. A larger value of the parameter means that\n * farther colors within the pixel neighborhood (see sigmaSpace) will be mixed together, resulting in\n * larger areas of semi-equal color.\n * \n * @param sigmaSpace Filter sigma in the coordinate space. A larger value of the parameter means that\n * farther pixels will influence each other as long as their colors are close enough (see sigmaColor ).\n * When d>0, it specifies the neighborhood size regardless of sigmaSpace. Otherwise, d is proportional\n * to sigmaSpace.\n * \n * @param borderType border mode used to extrapolate pixels outside of the image, see BorderTypes\n */\nexport declare function bilateralFilter(src: InputArray, dst: OutputArray, d: int, sigmaColor: double, sigmaSpace: double, borderType?: int): void\n\n/**\n * The function smooths an image using the kernel:\n * \n * `\\\\[\\\\texttt{K} = \\\\frac{1}{\\\\texttt{ksize.width*ksize.height}} \\\\begin{bmatrix} 1 & 1 & 1 & \\\\cdots\n * & 1 & 1 \\\\\\\\ 1 & 1 & 1 & \\\\cdots & 1 & 1 \\\\\\\\ \\\\hdotsfor{6} \\\\\\\\ 1 & 1 & 1 & \\\\cdots & 1 & 1 \\\\\\\\\n * \\\\end{bmatrix}\\\\]`\n * \n * The call `blur(src, dst, ksize, anchor, borderType)` is equivalent to `boxFilter(src, dst,\n * src.type(), anchor, true, borderType)`.\n * \n * [boxFilter](#d4/d86/group__imgproc__filter_1gad533230ebf2d42509547d514f7d3fbc3}),\n * [bilateralFilter](#d4/d86/group__imgproc__filter_1ga9d7064d478c95d60003cf839430737ed}),\n * [GaussianBlur](#d4/d86/group__imgproc__filter_1gaabe8c836e97159a9193fb0b11ac52cf1}),\n * [medianBlur](#d4/d86/group__imgproc__filter_1ga564869aa33e58769b4469101aac458f9})\n * \n * @param src input image; it can have any number of channels, which are processed independently, but\n * the depth should be CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.\n * \n * @param dst output image of the same size and type as src.\n * \n * @param ksize blurring kernel size.\n * \n * @param anchor anchor point; default value Point(-1,-1) means that the anchor is at the kernel\n * center.\n * \n * @param borderType border mode used to extrapolate pixels outside of the image, see BorderTypes\n */\nexport declare function blur(src: InputArray, dst: OutputArray, ksize: Size, anchor?: Point, borderType?: int): void\n\n/**\n * The function smooths an image using the kernel:\n * \n * `\\\\[\\\\texttt{K} = \\\\alpha \\\\begin{bmatrix} 1 & 1 & 1 & \\\\cdots & 1 & 1 \\\\\\\\ 1 & 1 & 1 & \\\\cdots & 1\n * & 1 \\\\\\\\ \\\\hdotsfor{6} \\\\\\\\ 1 & 1 & 1 & \\\\cdots & 1 & 1 \\\\end{bmatrix}\\\\]`\n * \n * where\n * \n * `\\\\[\\\\alpha = \\\\fork{\\\\frac{1}{\\\\texttt{ksize.width*ksize.height}}}{when\n * \\\\texttt{normalize=true}}{1}{otherwise}\\\\]`\n * \n * Unnormalized box filter is useful for computing various integral characteristics over each pixel\n * neighborhood, such as covariance matrices of image derivatives (used in dense optical flow\n * algorithms, and so on). If you need to compute pixel sums over variable-size windows, use\n * [integral](#d7/d1b/group__imgproc__misc_1gadeaf38d7701d7ad371278d663c50c77d}).\n * \n * [blur](#d4/d86/group__imgproc__filter_1ga8c45db9afe636703801b0b2e440fce37}),\n * [bilateralFilter](#d4/d86/group__imgproc__filter_1ga9d7064d478c95d60003cf839430737ed}),\n * [GaussianBlur](#d4/d86/group__imgproc__filter_1gaabe8c836e97159a9193fb0b11ac52cf1}),\n * [medianBlur](#d4/d86/group__imgproc__filter_1ga564869aa33e58769b4469101aac458f9}),\n * [integral](#d7/d1b/group__imgproc__misc_1gadeaf38d7701d7ad371278d663c50c77d})\n * \n * @param src input image.\n * \n * @param dst output image of the same size and type as src.\n * \n * @param ddepth the output image depth (-1 to use src.depth()).\n * \n * @param ksize blurring kernel size.\n * \n * @param anchor anchor point; default value Point(-1,-1) means that the anchor is at the kernel\n * center.\n * \n * @param normalize flag, specifying whether the kernel is normalized by its area or not.\n * \n * @param borderType border mode used to extrapolate pixels outside of the image, see BorderTypes\n */\nexport declare function boxFilter(src: InputArray, dst: OutputArray, ddepth: int, ksize: Size, anchor?: Point, normalize?: bool, borderType?: int): void\n\n/**\n * The function constructs a vector of images and builds the Gaussian pyramid by recursively applying\n * pyrDown to the previously built pyramid layers, starting from `dst[0]==src`.\n * \n * @param src Source image. Check pyrDown for the list of supported types.\n * \n * @param dst Destination vector of maxlevel+1 images of the same type as src. dst[0] will be the same\n * as src. dst[1] is the next pyramid layer, a smoothed and down-sized src, and so on.\n * \n * @param maxlevel 0-based index of the last (the smallest) pyramid layer. It must be non-negative.\n * \n * @param borderType Pixel extrapolation method, see BorderTypes (BORDER_CONSTANT isn't supported)\n */\nexport declare function buildPyramid(src: InputArray, dst: OutputArrayOfArrays, maxlevel: int, borderType?: int): void\n\n/**\n * The function dilates the source image using the specified structuring element that determines the\n * shape of a pixel neighborhood over which the maximum is taken: `\\\\[\\\\texttt{dst} (x,y) = \\\\max\n * _{(x',y'): \\\\, \\\\texttt{element} (x',y') \\\\ne0 } \\\\texttt{src} (x+x',y+y')\\\\]`\n * \n * The function supports the in-place mode. Dilation can be applied several ( iterations ) times. In\n * case of multi-channel images, each channel is processed independently.\n * \n * [erode](#d4/d86/group__imgproc__filter_1gaeb1e0c1033e3f6b891a25d0511362aeb}),\n * [morphologyEx](#d4/d86/group__imgproc__filter_1ga67493776e3ad1a3df63883829375201f}),\n * [getStructuringElement](#d4/d86/group__imgproc__filter_1gac342a1bb6eabf6f55c803b09268e36dc})\n * \n * @param src input image; the number of channels can be arbitrary, but the depth should be one of\n * CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.\n * \n * @param dst output image of the same size and type as src.\n * \n * @param kernel structuring element used for dilation; if elemenat=Mat(), a 3 x 3 rectangular\n * structuring element is used. Kernel can be created using getStructuringElement\n * \n * @param anchor position of the anchor within the element; default value (-1, -1) means that the\n * anchor is at the element center.\n * \n * @param iterations number of times dilation is applied.\n * \n * @param borderType pixel extrapolation method, see BorderTypes\n * \n * @param borderValue border value in case of a constant border\n */\nexport declare function dilate(src: InputArray, dst: OutputArray, kernel: InputArray, anchor?: Point, iterations?: int, borderType?: int, borderValue?: any): void\n\n/**\n * The function erodes the source image using the specified structuring element that determines the\n * shape of a pixel neighborhood over which the minimum is taken:\n * \n * `\\\\[\\\\texttt{dst} (x,y) = \\\\min _{(x',y'): \\\\, \\\\texttt{element} (x',y') \\\\ne0 } \\\\texttt{src}\n * (x+x',y+y')\\\\]`\n * \n * The function supports the in-place mode. Erosion can be applied several ( iterations ) times. In\n * case of multi-channel images, each channel is processed independently.\n * \n * [dilate](#d4/d86/group__imgproc__filter_1ga4ff0f3318642c4f469d0e11f242f3b6c}),\n * [morphologyEx](#d4/d86/group__imgproc__filter_1ga67493776e3ad1a3df63883829375201f}),\n * [getStructuringElement](#d4/d86/group__imgproc__filter_1gac342a1bb6eabf6f55c803b09268e36dc})\n * \n * @param src input image; the number of channels can be arbitrary, but the depth should be one of\n * CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.\n * \n * @param dst output image of the same size and type as src.\n * \n * @param kernel structuring element used for erosion; if element=Mat(), a 3 x 3 rectangular\n * structuring element is used. Kernel can be created using getStructuringElement.\n * \n * @param anchor position of the anchor within the element; default value (-1, -1) means that the\n * anchor is at the element center.\n * \n * @param iterations number of times erosion is applied.\n * \n * @param borderType pixel extrapolation method, see BorderTypes\n * \n * @param borderValue border value in case of a constant border\n */\nexport declare function erode(src: InputArray, dst: OutputArray, kernel: InputArray, anchor?: Point, iterations?: int, borderType?: int, borderValue?: any): void\n\n/**\n * The function applies an arbitrary linear filter to an image. In-place operation is supported. When\n * the aperture is partially outside the image, the function interpolates outlier pixel values\n * according to the specified border mode.\n * \n * The function does actually compute correlation, not the convolution:\n * \n * `\\\\[\\\\texttt{dst} (x,y) = \\\\sum _{ \\\\stackrel{0\\\\leq x' < \\\\texttt{kernel.cols},}{0\\\\leq y' <\n * \\\\texttt{kernel.rows}} } \\\\texttt{kernel} (x',y')* \\\\texttt{src} (x+x'- \\\\texttt{anchor.x} ,y+y'-\n * \\\\texttt{anchor.y} )\\\\]`\n * \n * That is, the kernel is not mirrored around the anchor point. If you need a real convolution, flip\n * the kernel using [flip](#d2/de8/group__core__array_1gaca7be533e3dac7feb70fc60635adf441}) and set the\n * new anchor to `(kernel.cols - anchor.x - 1, kernel.rows - anchor.y - 1)`.\n * \n * The function uses the DFT-based algorithm in case of sufficiently large kernels (~`11 x 11` or\n * larger) and the direct algorithm for small kernels.\n * \n * [sepFilter2D](#d4/d86/group__imgproc__filter_1ga910e29ff7d7b105057d1625a4bf6318d}),\n * [dft](#d2/de8/group__core__array_1gadd6cf9baf2b8b704a11b5f04aaf4f39d}),\n * [matchTemplate](#df/dfb/group__imgproc__object_1ga586ebfb0a7fb604b35a23d85391329be})\n * \n * @param src input image.\n * \n * @param dst output image of the same size and the same number of channels as src.\n * \n * @param ddepth desired depth of the destination image, see combinations\n * \n * @param kernel convolution kernel (or rather a correlation kernel), a single-channel floating point\n * matrix; if you want to apply different kernels to different channels, split the image into separate\n * color planes using split and process them individually.\n * \n * @param anchor anchor of the kernel that indicates the relative position of a filtered point within\n * the kernel; the anchor should lie within the kernel; default value (-1,-1) means that the anchor is\n * at the kernel center.\n * \n * @param delta optional value added to the filtered pixels before storing them in dst.\n * \n * @param borderType pixel extrapolation method, see BorderTypes\n */\nexport declare function filter2D(src: InputArray, dst: OutputArray, ddepth: int, kernel: InputArray, anchor?: Point, delta?: double, borderType?: int): void\n\n/**\n * The function convolves the source image with the specified Gaussian kernel. In-place filtering is\n * supported.\n * \n * [sepFilter2D](#d4/d86/group__imgproc__filter_1ga910e29ff7d7b105057d1625a4bf6318d}),\n * [filter2D](#d4/d86/group__imgproc__filter_1ga27c049795ce870216ddfb366086b5a04}),\n * [blur](#d4/d86/group__imgproc__filter_1ga8c45db9afe636703801b0b2e440fce37}),\n * [boxFilter](#d4/d86/group__imgproc__filter_1gad533230ebf2d42509547d514f7d3fbc3}),\n * [bilateralFilter](#d4/d86/group__imgproc__filter_1ga9d7064d478c95d60003cf839430737ed}),\n * [medianBlur](#d4/d86/group__imgproc__filter_1ga564869aa33e58769b4469101aac458f9})\n * \n * @param src input image; the image can have any number of channels, which are processed\n * independently, but the depth should be CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.\n * \n * @param dst output image of the same size and type as src.\n * \n * @param ksize Gaussian kernel size. ksize.width and ksize.height can differ but they both must be\n * positive and odd. Or, they can be zero's and then they are computed from sigma.\n * \n * @param sigmaX Gaussian kernel standard deviation in X direction.\n * \n * @param sigmaY Gaussian kernel standard deviation in Y direction; if sigmaY is zero, it is set to be\n * equal to sigmaX, if both sigmas are zeros, they are computed from ksize.width and ksize.height,\n * respectively (see getGaussianKernel for details); to fully control the result regardless of possible\n * future modifications of all this semantics, it is recommended to specify all of ksize, sigmaX, and\n * sigmaY.\n * \n * @param borderType pixel extrapolation method, see BorderTypes\n */\nexport declare function GaussianBlur(src: InputArray, dst: OutputArray, ksize: Size, sigmaX: double, sigmaY?: double, borderType?: int): void\n\n/**\n * The function computes and returns the filter coefficients for spatial image derivatives. When\n * `ksize=FILTER_SCHARR`, the Scharr `$3 \\\\times 3$` kernels are generated (see\n * [Scharr](#d4/d86/group__imgproc__filter_1gaa13106761eedf14798f37aa2d60404c9})). Otherwise, Sobel\n * kernels are generated (see\n * [Sobel](#d4/d86/group__imgproc__filter_1gacea54f142e81b6758cb6f375ce782c8d})). The filters are\n * normally passed to\n * [sepFilter2D](#d4/d86/group__imgproc__filter_1ga910e29ff7d7b105057d1625a4bf6318d}) or to\n * \n * @param kx Output matrix of row filter coefficients. It has the type ktype .\n * \n * @param ky Output matrix of column filter coefficients. It has the type ktype .\n * \n * @param dx Derivative order in respect of x.\n * \n * @param dy Derivative order in respect of y.\n * \n * @param ksize Aperture size. It can be FILTER_SCHARR, 1, 3, 5, or 7.\n * \n * @param normalize Flag indicating whether to normalize (scale down) the filter coefficients or not.\n * Theoretically, the coefficients should have the denominator $=2^{ksize*2-dx-dy-2}$. If you are going\n * to filter floating-point images, you are likely to use the normalized kernels. But if you compute\n * derivatives of an 8-bit image, store the results in a 16-bit image, and wish to preserve all the\n * fractional bits, you may want to set normalize=false .\n * \n * @param ktype Type of filter coefficients. It can be CV_32f or CV_64F .\n */\nexport declare function getDerivKernels(kx: OutputArray, ky: OutputArray, dx: int, dy: int, ksize: int, normalize?: bool, ktype?: int): void\n\n/**\n * For more details about gabor filter equations and parameters, see: .\n * \n * @param ksize Size of the filter returned.\n * \n * @param sigma Standard deviation of the gaussian envelope.\n * \n * @param theta Orientation of the normal to the parallel stripes of a Gabor function.\n * \n * @param lambd Wavelength of the sinusoidal factor.\n * \n * @param gamma Spatial aspect ratio.\n * \n * @param psi Phase offset.\n * \n * @param ktype Type of filter coefficients. It can be CV_32F or CV_64F .\n */\nexport declare function getGaborKernel(ksize: Size, sigma: double, theta: double, lambd: double, gamma: double, psi?: double, ktype?: int): Mat\n\n/**\n * The function computes and returns the `$\\\\texttt{ksize} \\\\times 1$` matrix of Gaussian filter\n * coefficients:\n * \n * `\\\\[G_i= \\\\alpha *e^{-(i-( \\\\texttt{ksize} -1)/2)^2/(2* \\\\texttt{sigma}^2)},\\\\]`\n * \n * where `$i=0..\\\\texttt{ksize}-1$` and `$\\\\alpha$` is the scale factor chosen so that `$\\\\sum_i\n * G_i=1$`.\n * \n * Two of such generated kernels can be passed to sepFilter2D. Those functions automatically recognize\n * smoothing kernels (a symmetrical kernel with sum of weights equal to 1) and handle them accordingly.\n * You may also use the higher-level GaussianBlur. \n * \n * [sepFilter2D](#d4/d86/group__imgproc__filter_1ga910e29ff7d7b105057d1625a4bf6318d}),\n * [getDerivKernels](#d4/d86/group__imgproc__filter_1ga6d6c23f7bd3f5836c31cfae994fc4aea}),\n * [getStructuringElement](#d4/d86/group__imgproc__filter_1gac342a1bb6eabf6f55c803b09268e36dc}),\n * [GaussianBlur](#d4/d86/group__imgproc__filter_1gaabe8c836e97159a9193fb0b11ac52cf1})\n * \n * @param ksize Aperture size. It should be odd ( $\\texttt{ksize} \\mod 2 = 1$ ) and positive.\n * \n * @param sigma Gaussian standard deviation. If it is non-positive, it is computed from ksize as sigma\n * = 0.3*((ksize-1)*0.5 - 1) + 0.8.\n * \n * @param ktype Type of filter coefficients. It can be CV_32F or CV_64F .\n */\nexport declare function getGaussianKernel(ksize: int, sigma: double, ktype?: int): Mat\n\n/**\n * The function constructs and returns the structuring element that can be further passed to\n * [erode](#d4/d86/group__imgproc__filter_1gaeb1e0c1033e3f6b891a25d0511362aeb}),\n * [dilate](#d4/d86/group__imgproc__filter_1ga4ff0f3318642c4f469d0e11f242f3b6c}) or\n * [morphologyEx](#d4/d86/group__imgproc__filter_1ga67493776e3ad1a3df63883829375201f}). But you can\n * also construct an arbitrary binary mask yourself and use it as the structuring element.\n * \n * @param shape Element shape that could be one of MorphShapes\n * \n * @param ksize Size of the structuring element.\n * \n * @param anchor Anchor position within the element. The default value $(-1, -1)$ means that the anchor\n * is at the center. Note that only the shape of a cross-shaped element depends on the anchor position.\n * In other cases the anchor just regulates how much the result of the morphological operation is\n * shifted.\n */\nexport declare function getStructuringElement(shape: int, ksize: Size, anchor?: Point): Mat\n\n/**\n * The function calculates the Laplacian of the source image by adding up the second x and y\n * derivatives calculated using the Sobel operator:\n * \n * `\\\\[\\\\texttt{dst} = \\\\Delta \\\\texttt{src} = \\\\frac{\\\\partial^2 \\\\texttt{src}}{\\\\partial x^2} +\n * \\\\frac{\\\\partial^2 \\\\texttt{src}}{\\\\partial y^2}\\\\]`\n * \n * This is done when `ksize > 1`. When `ksize == 1`, the Laplacian is computed by filtering the image\n * with the following `$3 \\\\times 3$` aperture:\n * \n * `\\\\[\\\\vecthreethree {0}{1}{0}{1}{-4}{1}{0}{1}{0}\\\\]`\n * \n * [Sobel](#d4/d86/group__imgproc__filter_1gacea54f142e81b6758cb6f375ce782c8d}),\n * [Scharr](#d4/d86/group__imgproc__filter_1gaa13106761eedf14798f37aa2d60404c9})\n * \n * @param src Source image.\n * \n * @param dst Destination image of the same size and the same number of channels as src .\n * \n * @param ddepth Desired depth of the destination image.\n * \n * @param ksize Aperture size used to compute the second-derivative filters. See getDerivKernels for\n * details. The size must be positive and odd.\n * \n * @param scale Optional scale factor for the computed Laplacian values. By default, no scaling is\n * applied. See getDerivKernels for details.\n * \n * @param delta Optional delta value that is added to the results prior to storing them in dst .\n * \n * @param borderType Pixel extrapolation method, see BorderTypes\n */\nexport declare function Laplacian(src: InputArray, dst: OutputArray, ddepth: int, ksize?: int, scale?: double, delta?: double, borderType?: int): void\n\n/**\n * The function smoothes an image using the median filter with the `$\\\\texttt{ksize} \\\\times\n * \\\\texttt{ksize}$` aperture. Each channel of a multi-channel image is processed independently.\n * In-place operation is supported.\n * \n * The median filter uses\n * [BORDER_REPLICATE](#d2/de8/group__core__array_1gga209f2f4869e304c82d07739337eae7c5aa1de4cff95e3377d6d0cbe7569bd4e9f})\n * internally to cope with border pixels, see\n * [BorderTypes](#d2/de8/group__core__array_1ga209f2f4869e304c82d07739337eae7c5})\n * \n * [bilateralFilter](#d4/d86/group__imgproc__filter_1ga9d7064d478c95d60003cf839430737ed}),\n * [blur](#d4/d86/group__imgproc__filter_1ga8c45db9afe636703801b0b2e440fce37}),\n * [boxFilter](#d4/d86/group__imgproc__filter_1gad533230ebf2d42509547d514f7d3fbc3}),\n * [GaussianBlur](#d4/d86/group__imgproc__filter_1gaabe8c836e97159a9193fb0b11ac52cf1})\n * \n * @param src input 1-, 3-, or 4-channel image; when ksize is 3 or 5, the image depth should be CV_8U,\n * CV_16U, or CV_32F, for larger aperture sizes, it can only be CV_8U.\n * \n * @param dst destination array of the same size and type as src.\n * \n * @param ksize aperture linear size; it must be odd and greater than 1, for example: 3, 5, 7 ...\n */\nexport declare function medianBlur(src: InputArray, dst: OutputArray, ksize: int): void\n\nexport declare function morphologyDefaultBorderValue(): Scalar\n\n/**\n * The function [cv::morphologyEx](#d4/d86/group__imgproc__filter_1ga67493776e3ad1a3df63883829375201f})\n * can perform advanced morphological transformations using an erosion and dilation as basic\n * operations.\n * \n * Any of the operations can be done in-place. In case of multi-channel images, each channel is\n * processed independently.\n * \n * [dilate](#d4/d86/group__imgproc__filter_1ga4ff0f3318642c4f469d0e11f242f3b6c}),\n * [erode](#d4/d86/group__imgproc__filter_1gaeb1e0c1033e3f6b891a25d0511362aeb}),\n * [getStructuringElement](#d4/d86/group__imgproc__filter_1gac342a1bb6eabf6f55c803b09268e36dc}) \n * \n * The number of iterations is the number of times erosion or dilatation operation will be applied. For\n * instance, an opening operation\n * ([MORPH_OPEN](#d4/d86/group__imgproc__filter_1gga7be549266bad7b2e6a04db49827f9f32a08d3cc3a2ace00cec488966d31fa29ea}))\n * with two iterations is equivalent to apply successively: erode -> erode -> dilate -> dilate (and not\n * erode -> dilate -> erode -> dilate).\n * \n * @param src Source image. The number of channels can be arbitrary. The depth should be one of CV_8U,\n * CV_16U, CV_16S, CV_32F or CV_64F.\n * \n * @param dst Destination image of the same size and type as source image.\n * \n * @param op Type of a morphological operation, see MorphTypes\n * \n * @param kernel Structuring element. It can be created using getStructuringElement.\n * \n * @param anchor Anchor position with the kernel. Negative values mean that the anchor is at the kernel\n * center.\n * \n * @param iterations Number of times erosion and dilation are applied.\n * \n * @param borderType Pixel extrapolation method, see BorderTypes\n * \n * @param borderValue Border value in case of a constant border. The default value has a special\n * meaning.\n */\nexport declare function morphologyEx(src: InputArray, dst: OutputArray, op: int, kernel: InputArray, anchor?: Point, iterations?: int, borderType?: int, borderValue?: any): void\n\n/**\n * By default, size of the output image is computed as `Size((src.cols+1)/2, (src.rows+1)/2)`, but in\n * any case, the following conditions should be satisfied:\n * \n * `\\\\[\\\\begin{array}{l} | \\\\texttt{dstsize.width} *2-src.cols| \\\\leq 2 \\\\\\\\ | \\\\texttt{dstsize.height}\n * *2-src.rows| \\\\leq 2 \\\\end{array}\\\\]`\n * \n * The function performs the downsampling step of the Gaussian pyramid construction. First, it\n * convolves the source image with the kernel:\n * \n * `\\\\[\\\\frac{1}{256} \\\\begin{bmatrix} 1 & 4 & 6 & 4 & 1 \\\\\\\\ 4 & 16 & 24 & 16 & 4 \\\\\\\\ 6 & 24 & 36 &\n * 24 & 6 \\\\\\\\ 4 & 16 & 24 & 16 & 4 \\\\\\\\ 1 & 4 & 6 & 4 & 1 \\\\end{bmatrix}\\\\]`\n * \n * Then, it downsamples the image by rejecting even rows and columns.\n * \n * @param src input image.\n * \n * @param dst output image; it has the specified size and the same type as src.\n * \n * @param dstsize size of the output image.\n * \n * @param borderType Pixel extrapolation method, see BorderTypes (BORDER_CONSTANT isn't supported)\n */\nexport declare function pyrDown(src: InputArray, dst: OutputArray, dstsize?: any, borderType?: int): void\n\n/**\n * The function implements the filtering stage of meanshift segmentation, that is, the output of the\n * function is the filtered \"posterized\" image with color gradients and fine-grain texture flattened.\n * At every pixel (X,Y) of the input image (or down-sized input image, see below) the function executes\n * meanshift iterations, that is, the pixel (X,Y) neighborhood in the joint space-color hyperspace is\n * considered:\n * \n * `\\\\[(x,y): X- \\\\texttt{sp} \\\\le x \\\\le X+ \\\\texttt{sp} , Y- \\\\texttt{sp} \\\\le y \\\\le Y+ \\\\texttt{sp}\n * , ||(R,G,B)-(r,g,b)|| \\\\le \\\\texttt{sr}\\\\]`\n * \n * where (R,G,B) and (r,g,b) are the vectors of color components at (X,Y) and (x,y), respectively\n * (though, the algorithm does not depend on the color space used, so any 3-component color space can\n * be used instead). Over the neighborhood the average spatial value (X',Y') and average color vector\n * (R',G',B') are found and they act as the neighborhood center on the next iteration:\n * \n * `\\\\[(X,Y)~(X',Y'), (R,G,B)~(R',G',B').\\\\]`\n * \n * After the iterations over, the color components of the initial pixel (that is, the pixel from where\n * the iterations started) are set to the final value (average color at the last iteration):\n * \n * `\\\\[I(X,Y) <- (R*,G*,B*)\\\\]`\n * \n * When maxLevel > 0, the gaussian pyramid of maxLevel+1 levels is built, and the above procedure is\n * run on the smallest layer first. After that, the results are propagated to the larger layer and the\n * iterations are run again only on those pixels where the layer colors differ by more than sr from the\n * lower-resolution layer of the pyramid. That makes boundaries of color regions sharper. Note that the\n * results will be actually different from the ones obtained by running the meanshift procedure on the\n * whole original image (i.e. when maxLevel==0).\n * \n * @param src The source 8-bit, 3-channel image.\n * \n * @param dst The destination image of the same format and the same size as the source.\n * \n * @param sp The spatial window radius.\n * \n * @param sr The color window radius.\n * \n * @param maxLevel Maximum level of the pyramid for the segmentation.\n * \n * @param termcrit Termination criteria: when to stop meanshift iterations.\n */\nexport declare function pyrMeanShiftFiltering(src: InputArray, dst: OutputArray, sp: double, sr: double, maxLevel?: int, termcrit?: TermCriteria): void\n\n/**\n * By default, size of the output image is computed as `Size(src.cols\\\\*2, (src.rows\\\\*2)`, but in any\n * case, the following conditions should be satisfied:\n * \n * `\\\\[\\\\begin{array}{l} | \\\\texttt{dstsize.width} -src.cols*2| \\\\leq ( \\\\texttt{dstsize.width} \\\\mod\n * 2) \\\\\\\\ | \\\\texttt{dstsize.height} -src.rows*2| \\\\leq ( \\\\texttt{dstsize.height} \\\\mod 2)\n * \\\\end{array}\\\\]`\n * \n * The function performs the upsampling step of the Gaussian pyramid construction, though it can\n * actually be used to construct the Laplacian pyramid. First, it upsamples the source image by\n * injecting even zero rows and columns and then convolves the result with the same kernel as in\n * pyrDown multiplied by 4.\n * \n * @param src input image.\n * \n * @param dst output image. It has the specified size and the same type as src .\n * \n * @param dstsize size of the output image.\n * \n * @param borderType Pixel extrapolation method, see BorderTypes (only BORDER_DEFAULT is supported)\n */\nexport declare function pyrUp(src: InputArray, dst: OutputArray, dstsize?: any, borderType?: int): void\n\n/**\n * The function computes the first x- or y- spatial image derivative using the Scharr operator. The\n * call\n * \n * `\\\\[\\\\texttt{Scharr(src, dst, ddepth, dx, dy, scale, delta, borderType)}\\\\]`\n * \n * is equivalent to\n * \n * `\\\\[\\\\texttt{Sobel(src, dst, ddepth, dx, dy, FILTER_SCHARR, scale, delta, borderType)} .\\\\]`\n * \n * [cartToPolar](#d2/de8/group__core__array_1gac5f92f48ec32cacf5275969c33ee837d})\n * \n * @param src input image.\n * \n * @param dst output image of the same size and the same number of channels as src.\n * \n * @param ddepth output image depth, see combinations\n * \n * @param dx order of the derivative x.\n * \n * @param dy order of the derivative y.\n * \n * @param scale optional scale factor for the computed derivative values; by default, no scaling is\n * applied (see getDerivKernels for details).\n * \n * @param delta optional delta value that is added to the results prior to storing them in dst.\n * \n * @param borderType pixel extrapolation method, see BorderTypes\n */\nexport declare function Scharr(src: InputArray, dst: OutputArray, ddepth: int, dx: int, dy: int, scale?: double, delta?: double, borderType?: int): void\n\n/**\n * The function applies a separable linear filter to the image. That is, first, every row of src is\n * filtered with the 1D kernel kernelX. Then, every column of the result is filtered with the 1D kernel\n * kernelY. The final result shifted by delta is stored in dst .\n * \n * [filter2D](#d4/d86/group__imgproc__filter_1ga27c049795ce870216ddfb366086b5a04}),\n * [Sobel](#d4/d86/group__imgproc__filter_1gacea54f142e81b6758cb6f375ce782c8d}),\n * [GaussianBlur](#d4/d86/group__imgproc__filter_1gaabe8c836e97159a9193fb0b11ac52cf1}),\n * [boxFilter](#d4/d86/group__imgproc__filter_1gad533230ebf2d42509547d514f7d3fbc3}),\n * [blur](#d4/d86/group__imgproc__filter_1ga8c45db9afe636703801b0b2e440fce37})\n * \n * @param src Source image.\n * \n * @param dst Destination image of the same size and the same number of channels as src .\n * \n * @param ddepth Destination image depth, see combinations\n * \n * @param kernelX Coefficients for filtering each row.\n * \n * @param kernelY Coefficients for filtering each column.\n * \n * @param anchor Anchor position within the kernel. The default value $(-1,-1)$ means that the anchor\n * is at the kernel center.\n * \n * @param delta Value added to the filtered results before storing them.\n * \n * @param borderType Pixel extrapolation method, see BorderTypes\n */\nexport declare function sepFilter2D(src: InputArray, dst: OutputArray, ddepth: int, kernelX: InputArray, kernelY: InputArray, anchor?: Point, delta?: double, borderType?: int): void\n\n/**\n * In all cases except one, the `$\\\\texttt{ksize} \\\\times \\\\texttt{ksize}$` separable kernel is used to\n * calculate the derivative. When `$\\\\texttt{ksize = 1}$`, the `$3 \\\\times 1$` or `$1 \\\\times 3$`\n * kernel is used (that is, no Gaussian smoothing is done). `ksize = 1` can only be used for the first\n * or the second x- or y- derivatives.\n * \n * There is also the special value `ksize =\n * [FILTER_SCHARR](#d4/d86/group__imgproc__filter_1ggad8e695e87dee497e227716576c244598a460c815d2bb921fb7f53f4220e19c1d5})\n * (-1)` that corresponds to the `$3\\\\times3$` Scharr filter that may give more accurate results than\n * the `$3\\\\times3$` Sobel. The Scharr aperture is\n * \n * `\\\\[\\\\vecthreethree{-3}{0}{3}{-10}{0}{10}{-3}{0}{3}\\\\]`\n * \n * for the x-derivative, or transposed for the y-derivative.\n * \n * The function calculates an image derivative by convolving the image with the appropriate kernel:\n * \n * `\\\\[\\\\texttt{dst} = \\\\frac{\\\\partial^{xorder+yorder} \\\\texttt{src}}{\\\\partial x^{xorder} \\\\partial\n * y^{yorder}}\\\\]`\n * \n * The Sobel operators combine Gaussian smoothing and differentiation, so the result is more or less\n * resistant to the noise. Most often, the function is called with ( xorder = 1, yorder = 0, ksize = 3)\n * or ( xorder = 0, yorder = 1, ksize = 3) to calculate the first x- or y- image derivative. The first\n * case corresponds to a kernel of:\n * \n * `\\\\[\\\\vecthreethree{-1}{0}{1}{-2}{0}{2}{-1}{0}{1}\\\\]`\n * \n * The second case corresponds to a kernel of:\n * \n * `\\\\[\\\\vecthreethree{-1}{-2}{-1}{0}{0}{0}{1}{2}{1}\\\\]`\n * \n * [Scharr](#d4/d86/group__imgproc__filter_1gaa13106761eedf14798f37aa2d60404c9}),\n * [Laplacian](#d4/d86/group__imgproc__filter_1gad78703e4c8fe703d479c1860d76429e6}),\n * [sepFilter2D](#d4/d86/group__imgproc__filter_1ga910e29ff7d7b105057d1625a4bf6318d}),\n * [filter2D](#d4/d86/group__imgproc__filter_1ga27c049795ce870216ddfb366086b5a04}),\n * [GaussianBlur](#d4/d86/group__imgproc__filter_1gaabe8c836e97159a9193fb0b11ac52cf1}),\n * [cartToPolar](#d2/de8/group__core__array_1gac5f92f48ec32cacf5275969c33ee837d})\n * \n * @param src input image.\n * \n * @param dst output image of the same size and the same number of channels as src .\n * \n * @param ddepth output image depth, see combinations; in the case of 8-bit input images it will result\n * in truncated derivatives.\n * \n * @param dx order of the derivative x.\n * \n * @param dy order of the derivative y.\n * \n * @param ksize size of the extended Sobel kernel; it must be 1, 3, 5, or 7.\n * \n * @param scale optional scale factor for the computed derivative values; by default, no scaling is\n * applied (see getDerivKernels for details).\n * \n * @param delta optional delta value that is added to the results prior to storing them in dst.\n * \n * @param borderType pixel extrapolation method, see BorderTypes\n */\nexport declare function Sobel(src: InputArray, dst: OutputArray, ddepth: int, dx: int, dy: int, ksize?: int, scale?: double, delta?: double, borderType?: int): void\n\n/**\n * Equivalent to calling:\n * \n * ```cpp\n * Sobel( src, dx, CV_16SC1, 1, 0, 3 );\n * Sobel( src, dy, CV_16SC1, 0, 1, 3 );\n * ```\n * \n * [Sobel](#d4/d86/group__imgproc__filter_1gacea54f142e81b6758cb6f375ce782c8d})\n * \n * @param src input image.\n * \n * @param dx output image with first-order derivative in x.\n * \n * @param dy output image with first-order derivative in y.\n * \n * @param ksize size of Sobel kernel. It must be 3.\n * \n * @param borderType pixel extrapolation method, see BorderTypes\n */\nexport declare function spatialGradient(src: InputArray, dx: OutputArray, dy: OutputArray, ksize?: int, borderType?: int): void\n\n/**\n * For every pixel `$ (x, y) $` in the source image, the function calculates the sum of squares of\n * those neighboring pixel values which overlap the filter placed over the pixel `$ (x, y) $`.\n * \n * The unnormalized square box filter can be useful in computing local image statistics such as the the\n * local variance and standard deviation around the neighborhood of a pixel.\n * \n * [boxFilter](#d4/d86/group__imgproc__filter_1gad533230ebf2d42509547d514f7d3fbc3})\n * \n * @param src input image\n * \n * @param dst output image of the same size and type as _src\n * \n * @param ddepth the output image depth (-1 to use src.depth())\n * \n * @param ksize kernel size\n * \n * @param anchor kernel anchor point. The default value of Point(-1, -1) denotes that the anchor is at\n * the kernel center.\n * \n * @param normalize flag, specifying whether the kernel is to be normalized by it's area or not.\n * \n * @param borderType border mode used to extrapolate pixels outside of the image, see BorderTypes\n */\nexport declare function sqrBoxFilter(src: InputArray, dst: OutputArray, ddepth: int, ksize: Size, anchor?: Point, normalize?: bool, borderType?: int): void\n\nexport declare const MORPH_RECT: MorphShapes // initializer: = 0\n\n/**\n * a cross-shaped structuring element: `\\\\[E_{ij} = \\\\fork{1}{if i=\\\\texttt{anchor.y} or\n * j=\\\\texttt{anchor.x}}{0}{otherwise}\\\\]`\n * \n */\nexport declare const MORPH_CROSS: MorphShapes // initializer: = 1\n\n/**\n * an elliptic structuring element, that is, a filled ellipse inscribed into the rectangle Rect(0, 0,\n * esize.width, 0.esize.height)\n * \n */\nexport declare const MORPH_ELLIPSE: MorphShapes // initializer: = 2\n\nexport declare const MORPH_ERODE: MorphTypes // initializer: = 0\n\nexport declare const MORPH_DILATE: MorphTypes // initializer: = 1\n\n/**\n * an opening operation `\\\\[\\\\texttt{dst} = \\\\mathrm{open} ( \\\\texttt{src} , \\\\texttt{element} )=\n * \\\\mathrm{dilate} ( \\\\mathrm{erode} ( \\\\texttt{src} , \\\\texttt{element} ))\\\\]`\n * \n */\nexport declare const MORPH_OPEN: MorphTypes // initializer: = 2\n\n/**\n * a closing operation `\\\\[\\\\texttt{dst} = \\\\mathrm{close} ( \\\\texttt{src} , \\\\texttt{element} )=\n * \\\\mathrm{erode} ( \\\\mathrm{dilate} ( \\\\texttt{src} , \\\\texttt{element} ))\\\\]`\n * \n */\nexport declare const MORPH_CLOSE: MorphTypes // initializer: = 3\n\n/**\n * a morphological gradient `\\\\[\\\\texttt{dst} = \\\\mathrm{morph\\\\_grad} ( \\\\texttt{src} ,\n * \\\\texttt{element} )= \\\\mathrm{dilate} ( \\\\texttt{src} , \\\\texttt{element} )- \\\\mathrm{erode} (\n * \\\\texttt{src} , \\\\texttt{element} )\\\\]`\n * \n */\nexport declare const MORPH_GRADIENT: MorphTypes // initializer: = 4\n\n/**\n * \"top hat\" `\\\\[\\\\texttt{dst} = \\\\mathrm{tophat} ( \\\\texttt{src} , \\\\texttt{element} )= \\\\texttt{src}\n * - \\\\mathrm{open} ( \\\\texttt{src} , \\\\texttt{element} )\\\\]`\n * \n */\nexport declare const MORPH_TOPHAT: MorphTypes // initializer: = 5\n\n/**\n * \"black hat\" `\\\\[\\\\texttt{dst} = \\\\mathrm{blackhat} ( \\\\texttt{src} , \\\\texttt{element} )=\n * \\\\mathrm{close} ( \\\\texttt{src} , \\\\texttt{element} )- \\\\texttt{src}\\\\]`\n * \n */\nexport declare const MORPH_BLACKHAT: MorphTypes // initializer: = 6\n\n/**\n * \"hit or miss\" .- Only supported for CV_8UC1 binary images. A tutorial can be found in the\n * documentation\n * \n */\nexport declare const MORPH_HITMISS: MorphTypes // initializer: = 7\n\nexport declare const FILTER_SCHARR: SpecialFilter // initializer: = -1\n\nexport type MorphShapes = any\n\nexport type MorphTypes = any\n\nexport type SpecialFilter = any\n\n"},"node_modules_mirada_dist_src_types_opencv_imgproc_object_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_imgproc_object_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/imgproc_object.d.ts","content":"\nimport { InputArray, int, OutputArray } from './_types'\n/*\n * # Object Detection\n * \n */\n/**\n * The function slides through image , compares the overlapped patches of size `$w \\\\times h$` against\n * templ using the specified method and stores the comparison results in result . Here are the formulae\n * for the available comparison methods ( `$I$` denotes image, `$T$` template, `$R$` result ). The\n * summation is done over template and/or the image patch: `$x' = 0...w-1, y' = 0...h-1$`\n * \n * After the function finishes the comparison, the best matches can be found as global minimums (when\n * [TM_SQDIFF](#df/dfb/group__imgproc__object_1gga3a7850640f1fe1f58fe91a2d7583695dab65c042ed62c9e9e095a1e7e41fe2773})\n * was used) or maximums (when\n * [TM_CCORR](#df/dfb/group__imgproc__object_1gga3a7850640f1fe1f58fe91a2d7583695da5be00b45a4d99b5e42625b4400bfde65})\n * or\n * [TM_CCOEFF](#df/dfb/group__imgproc__object_1gga3a7850640f1fe1f58fe91a2d7583695dac5babb7dfda59544e3e31ea928f8cb16})\n * was used) using the [minMaxLoc](#d2/de8/group__core__array_1gab473bf2eb6d14ff97e89b355dac20707})\n * function. In case of a color image, template summation in the numerator and each sum in the\n * denominator is done over all of the channels and separate mean values are used for each channel.\n * That is, the function can take a color template and a color image. The result will still be a\n * single-channel image, which is easier to analyze.\n * \n * @param image Image where the search is running. It must be 8-bit or 32-bit floating-point.\n * \n * @param templ Searched template. It must be not greater than the source image and have the same data\n * type.\n * \n * @param result Map of comparison results. It must be single-channel 32-bit floating-point. If image\n * is $W \\times H$ and templ is $w \\times h$ , then result is $(W-w+1) \\times (H-h+1)$ .\n * \n * @param method Parameter specifying the comparison method, see TemplateMatchModes\n * \n * @param mask Mask of searched template. It must have the same datatype and size with templ. It is not\n * set by default. Currently, only the TM_SQDIFF and TM_CCORR_NORMED methods are supported.\n */\nexport declare function matchTemplate(image: InputArray, templ: InputArray, result: OutputArray, method: int, mask?: InputArray): void\n\nexport declare const TM_SQDIFF: TemplateMatchModes // initializer: = 0\n\nexport declare const TM_SQDIFF_NORMED: TemplateMatchModes // initializer: = 1\n\nexport declare const TM_CCORR: TemplateMatchModes // initializer: = 2\n\nexport declare const TM_CCORR_NORMED: TemplateMatchModes // initializer: = 3\n\n/**\n * `\\\\[R(x,y)= \\\\sum _{x',y'} (T'(x',y') \\\\cdot I'(x+x',y+y'))\\\\]` where `\\\\[\\\\begin{array}{l}\n * T'(x',y')=T(x',y') - 1/(w \\\\cdot h) \\\\cdot \\\\sum _{x'',y''} T(x'',y'') \\\\\\\\\n * I'(x+x',y+y')=I(x+x',y+y') - 1/(w \\\\cdot h) \\\\cdot \\\\sum _{x'',y''} I(x+x'',y+y'') \\\\end{array}\\\\]`\n * \n */\nexport declare const TM_CCOEFF: TemplateMatchModes // initializer: = 4\n\nexport declare const TM_CCOEFF_NORMED: TemplateMatchModes // initializer: = 5\n\nexport type TemplateMatchModes = any\n\n"},"node_modules_mirada_dist_src_types_opencv_imgproc_shape_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_imgproc_shape_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/imgproc_shape.d.ts","content":"\nimport { bool, double, float, InputArray, int, Moments, OutputArray, OutputArrayOfArrays, Point, Point2f, Rect, RotatedRect } from './_types'\n/*\n * # Structural Analysis and Shape Descriptors\n * \n */\n/**\n * The function [cv::approxPolyDP](#d3/dc0/group__imgproc__shape_1ga0012a5fdaea70b8a9970165d98722b4c})\n * approximates a curve or a polygon with another curve/polygon with less vertices so that the distance\n * between them is less or equal to the specified precision. It uses the Douglas-Peucker algorithm\n * \n * @param curve Input vector of a 2D point stored in std::vector or Mat\n * \n * @param approxCurve Result of the approximation. The type should match the type of the input curve.\n * \n * @param epsilon Parameter specifying the approximation accuracy. This is the maximum distance between\n * the original curve and its approximation.\n * \n * @param closed If true, the approximated curve is closed (its first and last vertices are connected).\n * Otherwise, it is not closed.\n */\nexport declare function approxPolyDP(curve: InputArray, approxCurve: OutputArray, epsilon: double, closed: bool): void\n\n/**\n * The function computes a curve length or a closed contour perimeter.\n * \n * @param curve Input vector of 2D points, stored in std::vector or Mat.\n * \n * @param closed Flag indicating whether the curve is closed or not.\n */\nexport declare function arcLength(curve: InputArray, closed: bool): double\n\n/**\n * The function calculates and returns the minimal up-right bounding rectangle for the specified point\n * set or non-zero pixels of gray-scale image.\n * \n * @param array Input gray-scale image or 2D point set, stored in std::vector or Mat.\n */\nexport declare function boundingRect(array: InputArray): Rect\n\n/**\n * The function finds the four vertices of a rotated rectangle. This function is useful to draw the\n * rectangle. In C++, instead of using this function, you can directly use\n * [RotatedRect::points](#db/dd6/classcv_1_1RotatedRect_1a69d648b086f26dbce0029facae9bfb2d}) method.\n * Please visit the [tutorial on Creating Bounding rotated boxes and ellipses for\n * contours](#de/d62/tutorial_bounding_rotated_ellipses}) for more information.\n * \n * @param box The input rotated rectangle. It may be the output of\n * \n * @param points The output array of four vertices of rectangles.\n */\nexport declare function boxPoints(box: RotatedRect, points: OutputArray): void\n\n/**\n * image with 4 or 8 way connectivity - returns N, the total number of labels [0, N-1] where 0\n * represents the background label. ltype specifies the output label image type, an important\n * consideration based on the total number of labels or alternatively the total number of pixels in the\n * source image. ccltype specifies the connected components labeling algorithm to use, currently Grana\n * (BBDT) and Wu's (SAUF) algorithms are supported, see the\n * [ConnectedComponentsAlgorithmsTypes](#d3/dc0/group__imgproc__shape_1ga5ed7784614678adccb699c70fb841075})\n * for details. Note that SAUF algorithm forces a row major ordering of labels while BBDT does not.\n * This function uses parallel version of both Grana and Wu's algorithms if at least one allowed\n * parallel framework is enabled and if the rows of the image are at least twice the number returned by\n * [getNumberOfCPUs](#db/de0/group__core__utils_1gadf09fc982bf4f17bc84bd1abce5d0863}).\n * \n * @param image the 8-bit single-channel image to be labeled\n * \n * @param labels destination labeled image\n * \n * @param connectivity 8 or 4 for 8-way or 4-way connectivity respectively\n * \n * @param ltype output image label type. Currently CV_32S and CV_16U are supported.\n * \n * @param ccltype connected components algorithm type (see the ConnectedComponentsAlgorithmsTypes).\n */\nexport declare function connectedComponents(image: InputArray, labels: OutputArray, connectivity: int, ltype: int, ccltype: int): int\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n * \n * @param image the 8-bit single-channel image to be labeled\n * \n * @param labels destination labeled image\n * \n * @param connectivity 8 or 4 for 8-way or 4-way connectivity respectively\n * \n * @param ltype output image label type. Currently CV_32S and CV_16U are supported.\n */\nexport declare function connectedComponents(image: InputArray, labels: OutputArray, connectivity?: int, ltype?: int): int\n\n/**\n * image with 4 or 8 way connectivity - returns N, the total number of labels [0, N-1] where 0\n * represents the background label. ltype specifies the output label image type, an important\n * consideration based on the total number of labels or alternatively the total number of pixels in the\n * source image. ccltype specifies the connected components labeling algorithm to use, currently\n * Grana's (BBDT) and Wu's (SAUF) algorithms are supported, see the\n * [ConnectedComponentsAlgorithmsTypes](#d3/dc0/group__imgproc__shape_1ga5ed7784614678adccb699c70fb841075})\n * for details. Note that SAUF algorithm forces a row major ordering of labels while BBDT does not.\n * This function uses parallel version of both Grana and Wu's algorithms (statistics included) if at\n * least one allowed parallel framework is enabled and if the rows of the image are at least twice the\n * number returned by\n * [getNumberOfCPUs](#db/de0/group__core__utils_1gadf09fc982bf4f17bc84bd1abce5d0863}).\n * \n * @param image the 8-bit single-channel image to be labeled\n * \n * @param labels destination labeled image\n * \n * @param stats statistics output for each label, including the background label, see below for\n * available statistics. Statistics are accessed via stats(label, COLUMN) where COLUMN is one of\n * ConnectedComponentsTypes. The data type is CV_32S.\n * \n * @param centroids centroid output for each label, including the background label. Centroids are\n * accessed via centroids(label, 0) for x and centroids(label, 1) for y. The data type CV_64F.\n * \n * @param connectivity 8 or 4 for 8-way or 4-way connectivity respectively\n * \n * @param ltype output image label type. Currently CV_32S and CV_16U are supported.\n * \n * @param ccltype connected components algorithm type (see ConnectedComponentsAlgorithmsTypes).\n */\nexport declare function connectedComponentsWithStats(image: InputArray, labels: OutputArray, stats: OutputArray, centroids: OutputArray, connectivity: int, ltype: int, ccltype: int): int\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n * \n * @param image the 8-bit single-channel image to be labeled\n * \n * @param labels destination labeled image\n * \n * @param stats statistics output for each label, including the background label, see below for\n * available statistics. Statistics are accessed via stats(label, COLUMN) where COLUMN is one of\n * ConnectedComponentsTypes. The data type is CV_32S.\n * \n * @param centroids centroid output for each label, including the background label. Centroids are\n * accessed via centroids(label, 0) for x and centroids(label, 1) for y. The data type CV_64F.\n * \n * @param connectivity 8 or 4 for 8-way or 4-way connectivity respectively\n * \n * @param ltype output image label type. Currently CV_32S and CV_16U are supported.\n */\nexport declare function connectedComponentsWithStats(image: InputArray, labels: OutputArray, stats: OutputArray, centroids: OutputArray, connectivity?: int, ltype?: int): int\n\n/**\n * The function computes a contour area. Similarly to moments , the area is computed using the Green\n * formula. Thus, the returned area and the number of non-zero pixels, if you draw the contour using\n * [drawContours](#d6/d6e/group__imgproc__draw_1ga746c0625f1781f1ffc9056259103edbc}) or\n * [fillPoly](#d6/d6e/group__imgproc__draw_1ga8c69b68fab5f25e2223b6496aa60dad5}) , can be different.\n * Also, the function will most certainly give a wrong results for contours with self-intersections.\n * \n * Example: \n * \n * ```cpp\n * vector<Point> contour;\n * contour.push_back(Point2f(0, 0));\n * contour.push_back(Point2f(10, 0));\n * contour.push_back(Point2f(10, 10));\n * contour.push_back(Point2f(5, 4));\n * \n * double area0 = contourArea(contour);\n * vector<Point> approx;\n * approxPolyDP(contour, approx, 5, true);\n * double area1 = contourArea(approx);\n * \n * cout << \"area0 =\" << area0 << endl <<\n *         \"area1 =\" << area1 << endl <<\n *         \"approx poly vertices\" << approx.size() << endl;\n * ```\n * \n * @param contour Input vector of 2D points (contour vertices), stored in std::vector or Mat.\n * \n * @param oriented Oriented area flag. If it is true, the function returns a signed area value,\n * depending on the contour orientation (clockwise or counter-clockwise). Using this feature you can\n * determine orientation of a contour by taking the sign of an area. By default, the parameter is\n * false, which means that the absolute value is returned.\n */\nexport declare function contourArea(contour: InputArray, oriented?: bool): double\n\n/**\n * The function [cv::convexHull](#d3/dc0/group__imgproc__shape_1ga014b28e56cb8854c0de4a211cb2be656})\n * finds the convex hull of a 2D point set using the Sklansky's algorithm Sklansky82 that has *O(N\n * logN)* complexity in the current implementation.\n * \n * `points` and `hull` should be different arrays, inplace processing isn't supported.\n * Check [the corresponding tutorial](#d7/d1d/tutorial_hull}) for more details.\n * \n * useful links:\n * \n * @param points Input 2D point set, stored in std::vector or Mat.\n * \n * @param hull Output convex hull. It is either an integer vector of indices or vector of points. In\n * the first case, the hull elements are 0-based indices of the convex hull points in the original\n * array (since the set of convex hull points is a subset of the original point set). In the second\n * case, hull elements are the convex hull points themselves.\n * \n * @param clockwise Orientation flag. If it is true, the output convex hull is oriented clockwise.\n * Otherwise, it is oriented counter-clockwise. The assumed coordinate system has its X axis pointing\n * to the right, and its Y axis pointing upwards.\n * \n * @param returnPoints Operation flag. In case of a matrix, when the flag is true, the function returns\n * convex hull points. Otherwise, it returns indices of the convex hull points. When the output array\n * is std::vector, the flag is ignored, and the output depends on the type of the vector:\n * std::vector<int> implies returnPoints=false, std::vector<Point> implies returnPoints=true.\n */\nexport declare function convexHull(points: InputArray, hull: OutputArray, clockwise?: bool, returnPoints?: bool): void\n\n/**\n * The figure below displays convexity defects of a hand contour:\n * \n * @param contour Input contour.\n * \n * @param convexhull Convex hull obtained using convexHull that should contain indices of the contour\n * points that make the hull.\n * \n * @param convexityDefects The output vector of convexity defects. In C++ and the new Python/Java\n * interface each convexity defect is represented as 4-element integer vector (a.k.a. Vec4i):\n * (start_index, end_index, farthest_pt_index, fixpt_depth), where indices are 0-based indices in the\n * original contour of the convexity defect beginning, end and the farthest point, and fixpt_depth is\n * fixed-point approximation (with 8 fractional bits) of the distance between the farthest contour\n * point and the hull. That is, to get the floating-point value of the depth will be fixpt_depth/256.0.\n */\nexport declare function convexityDefects(contour: InputArray, convexhull: InputArray, convexityDefects: OutputArray): void\n\nexport declare function createGeneralizedHoughBallard(): any\n\nexport declare function createGeneralizedHoughGuil(): any\n\n/**\n * The function retrieves contours from the binary image using the algorithm Suzuki85 . The contours\n * are a useful tool for shape analysis and object detection and recognition. See squares.cpp in the\n * OpenCV sample directory. \n * \n * Since opencv 3.2 source image is not modified by this function.\n * \n * @param image Source, an 8-bit single-channel image. Non-zero pixels are treated as 1's. Zero pixels\n * remain 0's, so the image is treated as binary . You can use compare, inRange, threshold ,\n * adaptiveThreshold, Canny, and others to create a binary image out of a grayscale or color one. If\n * mode equals to RETR_CCOMP or RETR_FLOODFILL, the input can also be a 32-bit integer image of labels\n * (CV_32SC1).\n * \n * @param contours Detected contours. Each contour is stored as a vector of points (e.g.\n * std::vector<std::vector<cv::Point> >).\n * \n * @param hierarchy Optional output vector (e.g. std::vector<cv::Vec4i>), containing information about\n * the image topology. It has as many elements as the number of contours. For each i-th contour\n * contours[i], the elements hierarchy[i][0] , hierarchy[i][1] , hierarchy[i][2] , and hierarchy[i][3]\n * are set to 0-based indices in contours of the next and previous contours at the same hierarchical\n * level, the first child contour and the parent contour, respectively. If for the contour i there are\n * no next, previous, parent, or nested contours, the corresponding elements of hierarchy[i] will be\n * negative.\n * \n * @param mode Contour retrieval mode, see RetrievalModes\n * \n * @param method Contour approximation method, see ContourApproximationModes\n * \n * @param offset Optional offset by which every contour point is shifted. This is useful if the\n * contours are extracted from the image ROI and then they should be analyzed in the whole image\n * context.\n */\nexport declare function findContours(image: InputArray, contours: OutputArrayOfArrays, hierarchy: OutputArray, mode: int, method: int, offset?: Point): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function findContours(image: InputArray, contours: OutputArrayOfArrays, mode: int, method: int, offset?: Point): void\n\n/**\n * The function calculates the ellipse that fits (in a least-squares sense) a set of 2D points best of\n * all. It returns the rotated rectangle in which the ellipse is inscribed. The first algorithm\n * described by Fitzgibbon95 is used. Developer should keep in mind that it is possible that the\n * returned ellipse/rotatedRect data contains negative indices, due to the data points being close to\n * the border of the containing [Mat](#d3/d63/classcv_1_1Mat}) element.\n * \n * @param points Input 2D point set, stored in std::vector<> or Mat\n */\nexport declare function fitEllipse(points: InputArray): RotatedRect\n\n/**\n * The function calculates the ellipse that fits a set of 2D points. It returns the rotated rectangle\n * in which the ellipse is inscribed. The Approximate Mean Square (AMS) proposed by Taubin1991 is used.\n * \n * For an ellipse, this basis set is `$ \\\\chi= \\\\left(x^2, x y, y^2, x, y, 1\\\\right) $`, which is a set\n * of six free coefficients `$\n * A^T=\\\\left\\\\{A_{\\\\text{xx}},A_{\\\\text{xy}},A_{\\\\text{yy}},A_x,A_y,A_0\\\\right\\\\} $`. However, to\n * specify an ellipse, all that is needed is five numbers; the major and minor axes lengths `$ (a,b)\n * $`, the position `$ (x_0,y_0) $`, and the orientation `$ \\\\theta $`. This is because the basis set\n * includes lines, quadratics, parabolic and hyperbolic functions as well as elliptical functions as\n * possible fits. If the fit is found to be a parabolic or hyperbolic function then the standard\n * [fitEllipse](#d3/dc0/group__imgproc__shape_1gaf259efaad93098103d6c27b9e4900ffa}) method is used. The\n * AMS method restricts the fit to parabolic, hyperbolic and elliptical curves by imposing the\n * condition that `$ A^T ( D_x^T D_x + D_y^T D_y) A = 1 $` where the matrices `$ Dx $` and `$ Dy $` are\n * the partial derivatives of the design matrix `$ D $` with respect to x and y. The matrices are\n * formed row by row applying the following to each of the points in the set: `\\\\begin{align*}\n * D(i,:)&=\\\\left\\\\{x_i^2, x_i y_i, y_i^2, x_i, y_i, 1\\\\right\\\\} & D_x(i,:)&=\\\\left\\\\{2\n * x_i,y_i,0,1,0,0\\\\right\\\\} & D_y(i,:)&=\\\\left\\\\{0,x_i,2 y_i,0,1,0\\\\right\\\\} \\\\end{align*}` The AMS\n * method minimizes the cost function `\\\\begin{equation*} \\\\epsilon ^2=\\\\frac{ A^T D^T D A }{ A^T\n * (D_x^T D_x + D_y^T D_y) A^T } \\\\end{equation*}`\n * \n * The minimum cost is found by solving the generalized eigenvalue problem.\n * \n * `\\\\begin{equation*} D^T D A = \\\\lambda \\\\left( D_x^T D_x + D_y^T D_y\\\\right) A \\\\end{equation*}`\n * \n * @param points Input 2D point set, stored in std::vector<> or Mat\n */\nexport declare function fitEllipseAMS(points: InputArray): RotatedRect\n\n/**\n * The function calculates the ellipse that fits a set of 2D points. It returns the rotated rectangle\n * in which the ellipse is inscribed. The Direct least square (Direct) method by Fitzgibbon1999 is\n * used.\n * \n * For an ellipse, this basis set is `$ \\\\chi= \\\\left(x^2, x y, y^2, x, y, 1\\\\right) $`, which is a set\n * of six free coefficients `$\n * A^T=\\\\left\\\\{A_{\\\\text{xx}},A_{\\\\text{xy}},A_{\\\\text{yy}},A_x,A_y,A_0\\\\right\\\\} $`. However, to\n * specify an ellipse, all that is needed is five numbers; the major and minor axes lengths `$ (a,b)\n * $`, the position `$ (x_0,y_0) $`, and the orientation `$ \\\\theta $`. This is because the basis set\n * includes lines, quadratics, parabolic and hyperbolic functions as well as elliptical functions as\n * possible fits. The Direct method confines the fit to ellipses by ensuring that `$ 4 A_{xx} A_{yy}-\n * A_{xy}^2 > 0 $`. The condition imposed is that `$ 4 A_{xx} A_{yy}- A_{xy}^2=1 $` which satisfies the\n * inequality and as the coefficients can be arbitrarily scaled is not overly restrictive.\n * \n * `\\\\begin{equation*} \\\\epsilon ^2= A^T D^T D A \\\\quad \\\\text{with} \\\\quad A^T C A =1 \\\\quad\n * \\\\text{and} \\\\quad C=\\\\left(\\\\begin{matrix} 0 & 0 & 2 & 0 & 0 & 0 \\\\\\\\ 0 & -1 & 0 & 0 & 0 & 0 \\\\\\\\ 2\n * & 0 & 0 & 0 & 0 & 0 \\\\\\\\ 0 & 0 & 0 & 0 & 0 & 0 \\\\\\\\ 0 & 0 & 0 & 0 & 0 & 0 \\\\\\\\ 0 & 0 & 0 & 0 & 0 & 0\n * \\\\end{matrix} \\\\right) \\\\end{equation*}`\n * \n * The minimum cost is found by solving the generalized eigenvalue problem.\n * \n * `\\\\begin{equation*} D^T D A = \\\\lambda \\\\left( C\\\\right) A \\\\end{equation*}`\n * \n * The system produces only one positive eigenvalue `$ \\\\lambda$` which is chosen as the solution with\n * its eigenvector `$\\\\mathbf{u}$`. These are used to find the coefficients\n * \n * `\\\\begin{equation*} A = \\\\sqrt{\\\\frac{1}{\\\\mathbf{u}^T C \\\\mathbf{u}}} \\\\mathbf{u} \\\\end{equation*}`\n * The scaling factor guarantees that `$A^T C A =1$`.\n * \n * @param points Input 2D point set, stored in std::vector<> or Mat\n */\nexport declare function fitEllipseDirect(points: InputArray): RotatedRect\n\n/**\n * The function fitLine fits a line to a 2D or 3D point set by minimizing `$\\\\sum_i \\\\rho(r_i)$` where\n * `$r_i$` is a distance between the `$i^{th}$` point, the line and `$\\\\rho(r)$` is a distance\n * function, one of the following:\n * \n * DIST_L2 `\\\\[\\\\rho (r) = r^2/2 \\\\quad \\\\text{(the simplest and the fastest least-squares method)}\\\\]`\n * DIST_L1 `\\\\[\\\\rho (r) = r\\\\]`\n * DIST_L12 `\\\\[\\\\rho (r) = 2 \\\\cdot ( \\\\sqrt{1 + \\\\frac{r^2}{2}} - 1)\\\\]`\n * DIST_FAIR `\\\\[\\\\rho \\\\left (r \\\\right ) = C^2 \\\\cdot \\\\left ( \\\\frac{r}{C} - \\\\log{\\\\left(1 +\n * \\\\frac{r}{C}\\\\right)} \\\\right ) \\\\quad \\\\text{where} \\\\quad C=1.3998\\\\]`\n * DIST_WELSCH `\\\\[\\\\rho \\\\left (r \\\\right ) = \\\\frac{C^2}{2} \\\\cdot \\\\left ( 1 -\n * \\\\exp{\\\\left(-\\\\left(\\\\frac{r}{C}\\\\right)^2\\\\right)} \\\\right ) \\\\quad \\\\text{where} \\\\quad\n * C=2.9846\\\\]`\n * DIST_HUBER `\\\\[\\\\rho (r) = \\\\fork{r^2/2}{if \\\\(r < C\\\\)}{C \\\\cdot (r-C/2)}{otherwise} \\\\quad\n * \\\\text{where} \\\\quad C=1.345\\\\]`\n * \n * The algorithm is based on the M-estimator (  ) technique that iteratively fits the line using the\n * weighted least-squares algorithm. After each iteration the weights `$w_i$` are adjusted to be\n * inversely proportional to `$\\\\rho(r_i)$` .\n * \n * @param points Input vector of 2D or 3D points, stored in std::vector<> or Mat.\n * \n * @param line Output line parameters. In case of 2D fitting, it should be a vector of 4 elements (like\n * Vec4f) - (vx, vy, x0, y0), where (vx, vy) is a normalized vector collinear to the line and (x0, y0)\n * is a point on the line. In case of 3D fitting, it should be a vector of 6 elements (like Vec6f) -\n * (vx, vy, vz, x0, y0, z0), where (vx, vy, vz) is a normalized vector collinear to the line and (x0,\n * y0, z0) is a point on the line.\n * \n * @param distType Distance used by the M-estimator, see DistanceTypes\n * \n * @param param Numerical parameter ( C ) for some types of distances. If it is 0, an optimal value is\n * chosen.\n * \n * @param reps Sufficient accuracy for the radius (distance between the coordinate origin and the\n * line).\n * \n * @param aeps Sufficient accuracy for the angle. 0.01 would be a good default value for reps and aeps.\n */\nexport declare function fitLine(points: InputArray, line: OutputArray, distType: int, param: double, reps: double, aeps: double): void\n\n/**\n * The function calculates seven Hu invariants (introduced in Hu62; see also ) defined as:\n * \n * `\\\\[\\\\begin{array}{l} hu[0]= \\\\eta _{20}+ \\\\eta _{02} \\\\\\\\ hu[1]=( \\\\eta _{20}- \\\\eta _{02})^{2}+4\n * \\\\eta _{11}^{2} \\\\\\\\ hu[2]=( \\\\eta _{30}-3 \\\\eta _{12})^{2}+ (3 \\\\eta _{21}- \\\\eta _{03})^{2} \\\\\\\\\n * hu[3]=( \\\\eta _{30}+ \\\\eta _{12})^{2}+ ( \\\\eta _{21}+ \\\\eta _{03})^{2} \\\\\\\\ hu[4]=( \\\\eta _{30}-3\n * \\\\eta _{12})( \\\\eta _{30}+ \\\\eta _{12})[( \\\\eta _{30}+ \\\\eta _{12})^{2}-3( \\\\eta _{21}+ \\\\eta\n * _{03})^{2}]+(3 \\\\eta _{21}- \\\\eta _{03})( \\\\eta _{21}+ \\\\eta _{03})[3( \\\\eta _{30}+ \\\\eta\n * _{12})^{2}-( \\\\eta _{21}+ \\\\eta _{03})^{2}] \\\\\\\\ hu[5]=( \\\\eta _{20}- \\\\eta _{02})[( \\\\eta _{30}+\n * \\\\eta _{12})^{2}- ( \\\\eta _{21}+ \\\\eta _{03})^{2}]+4 \\\\eta _{11}( \\\\eta _{30}+ \\\\eta _{12})( \\\\eta\n * _{21}+ \\\\eta _{03}) \\\\\\\\ hu[6]=(3 \\\\eta _{21}- \\\\eta _{03})( \\\\eta _{21}+ \\\\eta _{03})[3( \\\\eta\n * _{30}+ \\\\eta _{12})^{2}-( \\\\eta _{21}+ \\\\eta _{03})^{2}]-( \\\\eta _{30}-3 \\\\eta _{12})( \\\\eta _{21}+\n * \\\\eta _{03})[3( \\\\eta _{30}+ \\\\eta _{12})^{2}-( \\\\eta _{21}+ \\\\eta _{03})^{2}] \\\\\\\\ \\\\end{array}\\\\]`\n * \n * where `$\\\\eta_{ji}$` stands for `$\\\\texttt{Moments::nu}_{ji}$` .\n * \n * These values are proved to be invariants to the image scale, rotation, and reflection except the\n * seventh one, whose sign is changed by reflection. This invariance is proved with the assumption of\n * infinite image resolution. In case of raster images, the computed Hu invariants for the original and\n * transformed images are a bit different.\n * \n * [matchShapes](#d3/dc0/group__imgproc__shape_1gaadc90cb16e2362c9bd6e7363e6e4c317})\n * \n * @param moments Input moments computed with moments .\n * \n * @param hu Output Hu invariants.\n */\nexport declare function HuMoments(moments: any, hu: double): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function HuMoments(m: any, hu: OutputArray): void\n\nexport declare function intersectConvexConvex(_p1: InputArray, _p2: InputArray, _p12: OutputArray, handleNested?: bool): float\n\n/**\n * The function tests whether the input contour is convex or not. The contour must be simple, that is,\n * without self-intersections. Otherwise, the function output is undefined.\n * \n * @param contour Input vector of 2D points, stored in std::vector<> or Mat\n */\nexport declare function isContourConvex(contour: InputArray): bool\n\n/**\n * The function compares two shapes. All three implemented methods use the Hu invariants (see\n * [HuMoments](#d3/dc0/group__imgproc__shape_1gab001db45c1f1af6cbdbe64df04c4e944}))\n * \n * @param contour1 First contour or grayscale image.\n * \n * @param contour2 Second contour or grayscale image.\n * \n * @param method Comparison method, see ShapeMatchModes\n * \n * @param parameter Method-specific parameter (not supported now).\n */\nexport declare function matchShapes(contour1: InputArray, contour2: InputArray, method: int, parameter: double): double\n\n/**\n * The function calculates and returns the minimum-area bounding rectangle (possibly rotated) for a\n * specified point set. Developer should keep in mind that the returned\n * [RotatedRect](#db/dd6/classcv_1_1RotatedRect}) can contain negative indices when data is close to\n * the containing [Mat](#d3/d63/classcv_1_1Mat}) element boundary.\n * \n * @param points Input vector of 2D points, stored in std::vector<> or Mat\n */\nexport declare function minAreaRect(points: InputArray): RotatedRect\n\n/**\n * The function finds the minimal enclosing circle of a 2D point set using an iterative algorithm.\n * \n * @param points Input vector of 2D points, stored in std::vector<> or Mat\n * \n * @param center Output center of the circle.\n * \n * @param radius Output radius of the circle.\n */\nexport declare function minEnclosingCircle(points: InputArray, center: any, radius: any): void\n\n/**\n * The function finds a triangle of minimum area enclosing the given set of 2D points and returns its\n * area. The output for a given 2D point set is shown in the image below. 2D points are depicted in\n * red* and the enclosing triangle in *yellow*.\n * \n *  The implementation of the algorithm is based on O'Rourke's ORourke86 and Klee and Laskowski's\n * KleeLaskowski85 papers. O'Rourke provides a `$\\\\theta(n)$` algorithm for finding the minimal\n * enclosing triangle of a 2D convex polygon with n vertices. Since the\n * [minEnclosingTriangle](#d3/dc0/group__imgproc__shape_1ga1513e72f6bbdfc370563664f71e0542f}) function\n * takes a 2D point set as input an additional preprocessing step of computing the convex hull of the\n * 2D point set is required. The complexity of the\n * [convexHull](#d3/dc0/group__imgproc__shape_1ga014b28e56cb8854c0de4a211cb2be656}) function is `$O(n\n * log(n))$` which is higher than `$\\\\theta(n)$`. Thus the overall complexity of the function is `$O(n\n * log(n))$`.\n * \n * @param points Input vector of 2D points with depth CV_32S or CV_32F, stored in std::vector<> or Mat\n * \n * @param triangle Output vector of three 2D points defining the vertices of the triangle. The depth of\n * the OutputArray must be CV_32F.\n */\nexport declare function minEnclosingTriangle(points: InputArray, triangle: OutputArray): double\n\n/**\n * The function computes moments, up to the 3rd order, of a vector shape or a rasterized shape. The\n * results are returned in the structure [cv::Moments](#d8/d23/classcv_1_1Moments}).\n * \n * moments.\n * \n * Only applicable to contour moments calculations from Python bindings: Note that the numpy type for\n * the input array should be either np.int32 or np.float32.\n * \n * [contourArea](#d3/dc0/group__imgproc__shape_1ga2c759ed9f497d4a618048a2f56dc97f1}),\n * [arcLength](#d3/dc0/group__imgproc__shape_1ga8d26483c636be6b35c3ec6335798a47c})\n * \n * @param array Raster image (single-channel, 8-bit or floating-point 2D array) or an array ( $1 \\times\n * N$ or $N \\times 1$ ) of 2D points (Point or Point2f ).\n * \n * @param binaryImage If it is true, all non-zero image pixels are treated as 1's. The parameter is\n * used for images only.\n */\nexport declare function moments(array: InputArray, binaryImage?: bool): Moments\n\n/**\n * The function determines whether the point is inside a contour, outside, or lies on an edge (or\n * coincides with a vertex). It returns positive (inside), negative (outside), or zero (on an edge)\n * value, correspondingly. When measureDist=false , the return value is +1, -1, and 0, respectively.\n * Otherwise, the return value is a signed distance between the point and the nearest contour edge.\n * \n * See below a sample output of the function where each image pixel is tested against the contour:\n * \n * @param contour Input contour.\n * \n * @param pt Point tested against the contour.\n * \n * @param measureDist If true, the function estimates the signed distance from the point to the nearest\n * contour edge. Otherwise, the function only checks if the point is inside a contour or not.\n */\nexport declare function pointPolygonTest(contour: InputArray, pt: Point2f, measureDist: bool): double\n\n/**\n * If there is then the vertices of the intersecting region are returned as well.\n * \n * Below are some examples of intersection configurations. The hatched pattern indicates the\n * intersecting region and the red vertices are returned by the function.\n * \n * One of\n * [RectanglesIntersectTypes](#d3/dc0/group__imgproc__shape_1gaaf0eb9e10bd5adcbd446cd31fea2db68})\n * \n * @param rect1 First rectangle\n * \n * @param rect2 Second rectangle\n * \n * @param intersectingRegion The output array of the vertices of the intersecting region. It returns at\n * most 8 vertices. Stored as std::vector<cv::Point2f> or cv::Mat as Mx1 of type CV_32FC2.\n */\nexport declare function rotatedRectangleIntersection(rect1: any, rect2: any, intersectingRegion: OutputArray): int\n\nexport declare const CCL_WU: ConnectedComponentsAlgorithmsTypes // initializer: = 0\n\nexport declare const CCL_DEFAULT: ConnectedComponentsAlgorithmsTypes // initializer: = -1\n\nexport declare const CCL_GRANA: ConnectedComponentsAlgorithmsTypes // initializer: = 1\n\n/**\n * The leftmost (x) coordinate which is the inclusive start of the bounding box in the horizontal\n * direction.\n * \n */\nexport declare const CC_STAT_LEFT: ConnectedComponentsTypes // initializer: = 0\n\n/**\n * The topmost (y) coordinate which is the inclusive start of the bounding box in the vertical\n * direction.\n * \n */\nexport declare const CC_STAT_TOP: ConnectedComponentsTypes // initializer: = 1\n\nexport declare const CC_STAT_WIDTH: ConnectedComponentsTypes // initializer: = 2\n\nexport declare const CC_STAT_HEIGHT: ConnectedComponentsTypes // initializer: = 3\n\nexport declare const CC_STAT_AREA: ConnectedComponentsTypes // initializer: = 4\n\nexport declare const CC_STAT_MAX: ConnectedComponentsTypes // initializer: = 5\n\n/**\n * stores absolutely all the contour points. That is, any 2 subsequent points (x1,y1) and (x2,y2) of\n * the contour will be either horizontal, vertical or diagonal neighbors, that is,\n * max(abs(x1-x2),abs(y2-y1))==1.\n * \n */\nexport declare const CHAIN_APPROX_NONE: ContourApproximationModes // initializer: = 1\n\n/**\n * compresses horizontal, vertical, and diagonal segments and leaves only their end points. For\n * example, an up-right rectangular contour is encoded with 4 points.\n * \n */\nexport declare const CHAIN_APPROX_SIMPLE: ContourApproximationModes // initializer: = 2\n\n/**\n * applies one of the flavors of the Teh-Chin chain approximation algorithm TehChin89\n * \n */\nexport declare const CHAIN_APPROX_TC89_L1: ContourApproximationModes // initializer: = 3\n\n/**\n * applies one of the flavors of the Teh-Chin chain approximation algorithm TehChin89\n * \n */\nexport declare const CHAIN_APPROX_TC89_KCOS: ContourApproximationModes // initializer: = 4\n\nexport declare const INTERSECT_NONE: RectanglesIntersectTypes // initializer: = 0\n\nexport declare const INTERSECT_PARTIAL: RectanglesIntersectTypes // initializer: = 1\n\nexport declare const INTERSECT_FULL: RectanglesIntersectTypes // initializer: = 2\n\n/**\n * retrieves only the extreme outer contours. It sets `hierarchy[i][2]=hierarchy[i][3]=-1` for all the\n * contours.\n * \n */\nexport declare const RETR_EXTERNAL: RetrievalModes // initializer: = 0\n\n/**\n * retrieves all of the contours without establishing any hierarchical relationships.\n * \n */\nexport declare const RETR_LIST: RetrievalModes // initializer: = 1\n\n/**\n * retrieves all of the contours and organizes them into a two-level hierarchy. At the top level, there\n * are external boundaries of the components. At the second level, there are boundaries of the holes.\n * If there is another contour inside a hole of a connected component, it is still put at the top\n * level.\n * \n */\nexport declare const RETR_CCOMP: RetrievalModes // initializer: = 2\n\n/**\n * retrieves all of the contours and reconstructs a full hierarchy of nested contours.\n * \n */\nexport declare const RETR_TREE: RetrievalModes // initializer: = 3\n\nexport declare const RETR_FLOODFILL: RetrievalModes // initializer: = 4\n\nexport declare const CONTOURS_MATCH_I1: ShapeMatchModes // initializer: =1\n\nexport declare const CONTOURS_MATCH_I2: ShapeMatchModes // initializer: =2\n\nexport declare const CONTOURS_MATCH_I3: ShapeMatchModes // initializer: =3\n\nexport type ConnectedComponentsAlgorithmsTypes = any\n\nexport type ConnectedComponentsTypes = any\n\nexport type ContourApproximationModes = any\n\nexport type RectanglesIntersectTypes = any\n\nexport type RetrievalModes = any\n\nexport type ShapeMatchModes = any\n\n"},"node_modules_mirada_dist_src_types_opencv_imgproc_misc_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_imgproc_misc_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/imgproc_misc.d.ts","content":"\nimport { double, InputArray, InputOutputArray, int, OutputArray, Point, Rect, Scalar } from './_types'\n/*\n * # Miscellaneous Image Transformations\n * \n */\n/**\n * The function transforms a grayscale image to a binary image according to the formulae:\n * \n * **THRESH_BINARY** `\\\\[dst(x,y) = \\\\fork{\\\\texttt{maxValue}}{if \\\\(src(x,y) >\n * T(x,y)\\\\)}{0}{otherwise}\\\\]`\n * **THRESH_BINARY_INV** `\\\\[dst(x,y) = \\\\fork{0}{if \\\\(src(x,y) >\n * T(x,y)\\\\)}{\\\\texttt{maxValue}}{otherwise}\\\\]` where `$T(x,y)$` is a threshold calculated\n * individually for each pixel (see adaptiveMethod parameter).\n * \n * The function can process the image in-place.\n * \n * [threshold](#d7/d1b/group__imgproc__misc_1gae8a4a146d1ca78c626a53577199e9c57}),\n * [blur](#d4/d86/group__imgproc__filter_1ga8c45db9afe636703801b0b2e440fce37}),\n * [GaussianBlur](#d4/d86/group__imgproc__filter_1gaabe8c836e97159a9193fb0b11ac52cf1})\n * \n * @param src Source 8-bit single-channel image.\n * \n * @param dst Destination image of the same size and the same type as src.\n * \n * @param maxValue Non-zero value assigned to the pixels for which the condition is satisfied\n * \n * @param adaptiveMethod Adaptive thresholding algorithm to use, see AdaptiveThresholdTypes. The\n * BORDER_REPLICATE | BORDER_ISOLATED is used to process boundaries.\n * \n * @param thresholdType Thresholding type that must be either THRESH_BINARY or THRESH_BINARY_INV, see\n * ThresholdTypes.\n * \n * @param blockSize Size of a pixel neighborhood that is used to calculate a threshold value for the\n * pixel: 3, 5, 7, and so on.\n * \n * @param C Constant subtracted from the mean or weighted mean (see the details below). Normally, it is\n * positive but may be zero or negative as well.\n */\nexport declare function adaptiveThreshold(src: InputArray, dst: OutputArray, maxValue: double, adaptiveMethod: int, thresholdType: int, blockSize: int, C: double): void\n\n/**\n * Performs linear blending of two images: `\\\\[ \\\\texttt{dst}(i,j) =\n * \\\\texttt{weights1}(i,j)*\\\\texttt{src1}(i,j) + \\\\texttt{weights2}(i,j)*\\\\texttt{src2}(i,j) \\\\]`\n * \n * @param src1 It has a type of CV_8UC(n) or CV_32FC(n), where n is a positive integer.\n * \n * @param src2 It has the same type and size as src1.\n * \n * @param weights1 It has a type of CV_32FC1 and the same size with src1.\n * \n * @param weights2 It has a type of CV_32FC1 and the same size with src1.\n * \n * @param dst It is created if it does not have the same size and type with src1.\n */\nexport declare function blendLinear(src1: InputArray, src2: InputArray, weights1: InputArray, weights2: InputArray, dst: OutputArray): void\n\n/**\n * The function\n * [cv::distanceTransform](#d7/d1b/group__imgproc__misc_1ga8a0b7fdfcb7a13dde018988ba3a43042})\n * calculates the approximate or precise distance from every binary image pixel to the nearest zero\n * pixel. For zero image pixels, the distance will obviously be zero.\n * \n * When maskSize ==\n * [DIST_MASK_PRECISE](#d7/d1b/group__imgproc__misc_1ggaaa68392323ccf7fad87570e41259b497a71e13e06c1d12eabd2acd2669f94f9ca})\n * and distanceType ==\n * [DIST_L2](#d7/d1b/group__imgproc__misc_1ggaa2bfbebbc5c320526897996aafa1d8ebaff0d1f5be0fc152a56a9b9716d158b96})\n * , the function runs the algorithm described in Felzenszwalb04 . This algorithm is parallelized with\n * the TBB library.\n * \n * In other cases, the algorithm Borgefors86 is used. This means that for a pixel the function finds\n * the shortest path to the nearest zero pixel consisting of basic shifts: horizontal, vertical,\n * diagonal, or knight's move (the latest is available for a `$5\\\\times 5$` mask). The overall distance\n * is calculated as a sum of these basic distances. Since the distance function should be symmetric,\n * all of the horizontal and vertical shifts must have the same cost (denoted as a ), all the diagonal\n * shifts must have the same cost (denoted as `b`), and all knight's moves must have the same cost\n * (denoted as `c`). For the\n * [DIST_C](#d7/d1b/group__imgproc__misc_1ggaa2bfbebbc5c320526897996aafa1d8eba507b16eb5ef95ea784ca1b3cb7b0d7ee})\n * and\n * [DIST_L1](#d7/d1b/group__imgproc__misc_1ggaa2bfbebbc5c320526897996aafa1d8ebae5b2dfaf2ba5024d7ce47885001fad86})\n * types, the distance is calculated precisely, whereas for\n * [DIST_L2](#d7/d1b/group__imgproc__misc_1ggaa2bfbebbc5c320526897996aafa1d8ebaff0d1f5be0fc152a56a9b9716d158b96})\n * (Euclidean distance) the distance can be calculated only with a relative error (a `$5\\\\times 5$`\n * mask gives more accurate results). For `a`,`b`, and `c`, OpenCV uses the values suggested in the\n * original paper:\n * \n * DIST_L1: `a = 1, b = 2`\n * DIST_L2:\n * \n * `3 x 3`: `a=0.955, b=1.3693`\n * `5 x 5`: `a=1, b=1.4, c=2.1969`\n * \n * DIST_C: `a = 1, b = 1`\n * \n * Typically, for a fast, coarse distance estimation\n * [DIST_L2](#d7/d1b/group__imgproc__misc_1ggaa2bfbebbc5c320526897996aafa1d8ebaff0d1f5be0fc152a56a9b9716d158b96}),\n * a `$3\\\\times 3$` mask is used. For a more accurate distance estimation\n * [DIST_L2](#d7/d1b/group__imgproc__misc_1ggaa2bfbebbc5c320526897996aafa1d8ebaff0d1f5be0fc152a56a9b9716d158b96}),\n * a `$5\\\\times 5$` mask or the precise algorithm is used. Note that both the precise and the\n * approximate algorithms are linear on the number of pixels.\n * \n * This variant of the function does not only compute the minimum distance for each pixel `$(x, y)$`\n * but also identifies the nearest connected component consisting of zero pixels\n * (labelType==[DIST_LABEL_CCOMP](#d7/d1b/group__imgproc__misc_1gga3fe343d63844c40318ee627bd1c1c42fa631de3e838ee72d6a9d991b8fbce4c1d}))\n * or the nearest zero pixel\n * (labelType==[DIST_LABEL_PIXEL](#d7/d1b/group__imgproc__misc_1gga3fe343d63844c40318ee627bd1c1c42fa5d291835de98b72caa12a9947c2cd92a})).\n * Index of the component/pixel is stored in `labels(x, y)`. When\n * labelType==[DIST_LABEL_CCOMP](#d7/d1b/group__imgproc__misc_1gga3fe343d63844c40318ee627bd1c1c42fa631de3e838ee72d6a9d991b8fbce4c1d}),\n * the function automatically finds connected components of zero pixels in the input image and marks\n * them with distinct labels. When\n * labelType==[DIST_LABEL_CCOMP](#d7/d1b/group__imgproc__misc_1gga3fe343d63844c40318ee627bd1c1c42fa631de3e838ee72d6a9d991b8fbce4c1d}),\n * the function scans through the input image and marks all the zero pixels with distinct labels.\n * \n * In this mode, the complexity is still linear. That is, the function provides a very fast way to\n * compute the Voronoi diagram for a binary image. Currently, the second variant can use only the\n * approximate distance transform algorithm, i.e.\n * maskSize=[DIST_MASK_PRECISE](#d7/d1b/group__imgproc__misc_1ggaaa68392323ccf7fad87570e41259b497a71e13e06c1d12eabd2acd2669f94f9ca})\n * is not supported yet.\n * \n * @param src 8-bit, single-channel (binary) source image.\n * \n * @param dst Output image with calculated distances. It is a 8-bit or 32-bit floating-point,\n * single-channel image of the same size as src.\n * \n * @param labels Output 2D array of labels (the discrete Voronoi diagram). It has the type CV_32SC1 and\n * the same size as src.\n * \n * @param distanceType Type of distance, see DistanceTypes\n * \n * @param maskSize Size of the distance transform mask, see DistanceTransformMasks. DIST_MASK_PRECISE\n * is not supported by this variant. In case of the DIST_L1 or DIST_C distance type, the parameter is\n * forced to 3 because a $3\\times 3$ mask gives the same result as $5\\times 5$ or any larger aperture.\n * \n * @param labelType Type of the label array to build, see DistanceTransformLabelTypes.\n */\nexport declare function distanceTransform(src: InputArray, dst: OutputArray, labels: OutputArray, distanceType: int, maskSize: int, labelType?: int): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n * \n * @param src 8-bit, single-channel (binary) source image.\n * \n * @param dst Output image with calculated distances. It is a 8-bit or 32-bit floating-point,\n * single-channel image of the same size as src .\n * \n * @param distanceType Type of distance, see DistanceTypes\n * \n * @param maskSize Size of the distance transform mask, see DistanceTransformMasks. In case of the\n * DIST_L1 or DIST_C distance type, the parameter is forced to 3 because a $3\\times 3$ mask gives the\n * same result as $5\\times 5$ or any larger aperture.\n * \n * @param dstType Type of output image. It can be CV_8U or CV_32F. Type CV_8U can be used only for the\n * first variant of the function and distanceType == DIST_L1.\n */\nexport declare function distanceTransform(src: InputArray, dst: OutputArray, distanceType: int, maskSize: int, dstType?: int): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n * \n * variant without `mask` parameter\n */\nexport declare function floodFill(image: InputOutputArray, seedPoint: Point, newVal: Scalar, rect?: any, loDiff?: Scalar, upDiff?: Scalar, flags?: int): int\n\n/**\n * The function [cv::floodFill](#d7/d1b/group__imgproc__misc_1gaf1f55a048f8a45bc3383586e80b1f0d0})\n * fills a connected component starting from the seed point with the specified color. The connectivity\n * is determined by the color/brightness closeness of the neighbor pixels. The pixel at `$(x,y)$` is\n * considered to belong to the repainted domain if:\n * \n * in case of a grayscale image and floating range `\\\\[\\\\texttt{src} (x',y')- \\\\texttt{loDiff} \\\\leq\n * \\\\texttt{src} (x,y) \\\\leq \\\\texttt{src} (x',y')+ \\\\texttt{upDiff}\\\\]`\n * in case of a grayscale image and fixed range `\\\\[\\\\texttt{src} ( \\\\texttt{seedPoint} .x,\n * \\\\texttt{seedPoint} .y)- \\\\texttt{loDiff} \\\\leq \\\\texttt{src} (x,y) \\\\leq \\\\texttt{src} (\n * \\\\texttt{seedPoint} .x, \\\\texttt{seedPoint} .y)+ \\\\texttt{upDiff}\\\\]`\n * in case of a color image and floating range `\\\\[\\\\texttt{src} (x',y')_r- \\\\texttt{loDiff} _r \\\\leq\n * \\\\texttt{src} (x,y)_r \\\\leq \\\\texttt{src} (x',y')_r+ \\\\texttt{upDiff} _r,\\\\]` `\\\\[\\\\texttt{src}\n * (x',y')_g- \\\\texttt{loDiff} _g \\\\leq \\\\texttt{src} (x,y)_g \\\\leq \\\\texttt{src} (x',y')_g+\n * \\\\texttt{upDiff} _g\\\\]` and `\\\\[\\\\texttt{src} (x',y')_b- \\\\texttt{loDiff} _b \\\\leq \\\\texttt{src}\n * (x,y)_b \\\\leq \\\\texttt{src} (x',y')_b+ \\\\texttt{upDiff} _b\\\\]`\n * in case of a color image and fixed range `\\\\[\\\\texttt{src} ( \\\\texttt{seedPoint} .x,\n * \\\\texttt{seedPoint} .y)_r- \\\\texttt{loDiff} _r \\\\leq \\\\texttt{src} (x,y)_r \\\\leq \\\\texttt{src} (\n * \\\\texttt{seedPoint} .x, \\\\texttt{seedPoint} .y)_r+ \\\\texttt{upDiff} _r,\\\\]` `\\\\[\\\\texttt{src} (\n * \\\\texttt{seedPoint} .x, \\\\texttt{seedPoint} .y)_g- \\\\texttt{loDiff} _g \\\\leq \\\\texttt{src} (x,y)_g\n * \\\\leq \\\\texttt{src} ( \\\\texttt{seedPoint} .x, \\\\texttt{seedPoint} .y)_g+ \\\\texttt{upDiff} _g\\\\]` and\n * `\\\\[\\\\texttt{src} ( \\\\texttt{seedPoint} .x, \\\\texttt{seedPoint} .y)_b- \\\\texttt{loDiff} _b \\\\leq\n * \\\\texttt{src} (x,y)_b \\\\leq \\\\texttt{src} ( \\\\texttt{seedPoint} .x, \\\\texttt{seedPoint} .y)_b+\n * \\\\texttt{upDiff} _b\\\\]`\n * \n * where `$src(x',y')$` is the value of one of pixel neighbors that is already known to belong to the\n * component. That is, to be added to the connected component, a color/brightness of the pixel should\n * be close enough to:\n * \n * Color/brightness of one of its neighbors that already belong to the connected component in case of a\n * floating range.\n * Color/brightness of the seed point in case of a fixed range.\n * \n * Use these functions to either mark a connected component with the specified color in-place, or build\n * a mask and then extract the contour, or copy the region to another image, and so on.\n * \n * Since the mask is larger than the filled image, a pixel `$(x, y)$` in image corresponds to the pixel\n * `$(x+1, y+1)$` in the mask .\n * \n * [findContours](#d3/dc0/group__imgproc__shape_1gadf1ad6a0b82947fa1fe3c3d497f260e0})\n * \n * @param image Input/output 1- or 3-channel, 8-bit, or floating-point image. It is modified by the\n * function unless the FLOODFILL_MASK_ONLY flag is set in the second variant of the function. See the\n * details below.\n * \n * @param mask Operation mask that should be a single-channel 8-bit image, 2 pixels wider and 2 pixels\n * taller than image. Since this is both an input and output parameter, you must take responsibility of\n * initializing it. Flood-filling cannot go across non-zero pixels in the input mask. For example, an\n * edge detector output can be used as a mask to stop filling at edges. On output, pixels in the mask\n * corresponding to filled pixels in the image are set to 1 or to the a value specified in flags as\n * described below. Additionally, the function fills the border of the mask with ones to simplify\n * internal processing. It is therefore possible to use the same mask in multiple calls to the function\n * to make sure the filled areas do not overlap.\n * \n * @param seedPoint Starting point.\n * \n * @param newVal New value of the repainted domain pixels.\n * \n * @param rect Optional output parameter set by the function to the minimum bounding rectangle of the\n * repainted domain.\n * \n * @param loDiff Maximal lower brightness/color difference between the currently observed pixel and one\n * of its neighbors belonging to the component, or a seed pixel being added to the component.\n * \n * @param upDiff Maximal upper brightness/color difference between the currently observed pixel and one\n * of its neighbors belonging to the component, or a seed pixel being added to the component.\n * \n * @param flags Operation flags. The first 8 bits contain a connectivity value. The default value of 4\n * means that only the four nearest neighbor pixels (those that share an edge) are considered. A\n * connectivity value of 8 means that the eight nearest neighbor pixels (those that share a corner)\n * will be considered. The next 8 bits (8-16) contain a value between 1 and 255 with which to fill the\n * mask (the default value is 1). For example, 4 | ( 255 << 8 ) will consider 4 nearest neighbours and\n * fill the mask with a value of 255. The following additional options occupy higher bits and therefore\n * may be further combined with the connectivity and mask fill values using bit-wise or (|), see\n * FloodFillFlags.\n */\nexport declare function floodFill(image: InputOutputArray, mask: InputOutputArray, seedPoint: Point, newVal: Scalar, rect?: any, loDiff?: Scalar, upDiff?: Scalar, flags?: int): int\n\n/**\n * The function implements the .\n * \n * @param img Input 8-bit 3-channel image.\n * \n * @param mask Input/output 8-bit single-channel mask. The mask is initialized by the function when\n * mode is set to GC_INIT_WITH_RECT. Its elements may have one of the GrabCutClasses.\n * \n * @param rect ROI containing a segmented object. The pixels outside of the ROI are marked as \"obvious\n * background\". The parameter is only used when mode==GC_INIT_WITH_RECT .\n * \n * @param bgdModel Temporary array for the background model. Do not modify it while you are processing\n * the same image.\n * \n * @param fgdModel Temporary arrays for the foreground model. Do not modify it while you are processing\n * the same image.\n * \n * @param iterCount Number of iterations the algorithm should make before returning the result. Note\n * that the result can be refined with further calls with mode==GC_INIT_WITH_MASK or mode==GC_EVAL .\n * \n * @param mode Operation mode that could be one of the GrabCutModes\n */\nexport declare function grabCut(img: InputArray, mask: InputOutputArray, rect: Rect, bgdModel: InputOutputArray, fgdModel: InputOutputArray, iterCount: int, mode?: int): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function integral(src: InputArray, sum: OutputArray, sdepth?: int): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function integral(src: InputArray, sum: OutputArray, sqsum: OutputArray, sdepth?: int, sqdepth?: int): void\n\n/**\n * The function calculates one or more integral images for the source image as follows:\n * \n * `\\\\[\\\\texttt{sum} (X,Y) = \\\\sum _{x<X,y<Y} \\\\texttt{image} (x,y)\\\\]`\n * \n * `\\\\[\\\\texttt{sqsum} (X,Y) = \\\\sum _{x<X,y<Y} \\\\texttt{image} (x,y)^2\\\\]`\n * \n * `\\\\[\\\\texttt{tilted} (X,Y) = \\\\sum _{y<Y,abs(x-X+1) \\\\leq Y-y-1} \\\\texttt{image} (x,y)\\\\]`\n * \n * Using these integral images, you can calculate sum, mean, and standard deviation over a specific\n * up-right or rotated rectangular region of the image in a constant time, for example:\n * \n * `\\\\[\\\\sum _{x_1 \\\\leq x < x_2, \\\\, y_1 \\\\leq y < y_2} \\\\texttt{image} (x,y) = \\\\texttt{sum}\n * (x_2,y_2)- \\\\texttt{sum} (x_1,y_2)- \\\\texttt{sum} (x_2,y_1)+ \\\\texttt{sum} (x_1,y_1)\\\\]`\n * \n * It makes possible to do a fast blurring or fast block correlation with a variable window size, for\n * example. In case of multi-channel images, sums for each channel are accumulated independently.\n * \n * As a practical example, the next figure shows the calculation of the integral of a straight\n * rectangle Rect(3,3,3,2) and of a tilted rectangle Rect(5,1,2,3) . The selected pixels in the\n * original image are shown, as well as the relative pixels in the integral images sum and tilted .\n * \n * @param src input image as $W \\times H$, 8-bit or floating-point (32f or 64f).\n * \n * @param sum integral image as $(W+1)\\times (H+1)$ , 32-bit integer or floating-point (32f or 64f).\n * \n * @param sqsum integral image for squared pixel values; it is $(W+1)\\times (H+1)$, double-precision\n * floating-point (64f) array.\n * \n * @param tilted integral for the image rotated by 45 degrees; it is $(W+1)\\times (H+1)$ array with the\n * same data type as sum.\n * \n * @param sdepth desired depth of the integral and the tilted integral images, CV_32S, CV_32F, or\n * CV_64F.\n * \n * @param sqdepth desired depth of the integral image of squared pixel values, CV_32F or CV_64F.\n */\nexport declare function integral(src: InputArray, sum: OutputArray, sqsum: OutputArray, tilted: OutputArray, sdepth?: int, sqdepth?: int): void\n\n/**\n * The function applies fixed-level thresholding to a multiple-channel array. The function is typically\n * used to get a bi-level (binary) image out of a grayscale image (\n * [compare](#d2/de8/group__core__array_1ga303cfb72acf8cbb36d884650c09a3a97}) could be also used for\n * this purpose) or for removing a noise, that is, filtering out pixels with too small or too large\n * values. There are several types of thresholding supported by the function. They are determined by\n * type parameter.\n * \n * Also, the special values\n * [THRESH_OTSU](#d7/d1b/group__imgproc__misc_1ggaa9e58d2860d4afa658ef70a9b1115576a95251923e8e22f368ffa86ba8bce87ff})\n * or\n * [THRESH_TRIANGLE](#d7/d1b/group__imgproc__misc_1ggaa9e58d2860d4afa658ef70a9b1115576a22ffcf680811aed95be6c7f5cd809621})\n * may be combined with one of the above values. In these cases, the function determines the optimal\n * threshold value using the Otsu's or Triangle algorithm and uses it instead of the specified thresh.\n * \n * Currently, the Otsu's and Triangle methods are implemented only for 8-bit single-channel images.\n * \n * the computed threshold value if Otsu's or Triangle methods used.\n * \n * [adaptiveThreshold](#d7/d1b/group__imgproc__misc_1ga72b913f352e4a1b1b397736707afcde3}),\n * [findContours](#d3/dc0/group__imgproc__shape_1gadf1ad6a0b82947fa1fe3c3d497f260e0}),\n * [compare](#d2/de8/group__core__array_1ga303cfb72acf8cbb36d884650c09a3a97}),\n * [min](#d7/dcc/group__core__utils__softfloat_1gac48df53b8fd34b87e7b121fa8fd4c379}),\n * [max](#d7/dcc/group__core__utils__softfloat_1ga78f988f6cfa6223610298cbd4f86ec66})\n * \n * @param src input array (multiple-channel, 8-bit or 32-bit floating point).\n * \n * @param dst output array of the same size and type and the same number of channels as src.\n * \n * @param thresh threshold value.\n * \n * @param maxval maximum value to use with the THRESH_BINARY and THRESH_BINARY_INV thresholding types.\n * \n * @param type thresholding type (see ThresholdTypes).\n */\nexport declare function threshold(src: InputArray, dst: OutputArray, thresh: double, maxval: double, type: int): double\n\n/**\n * The function implements one of the variants of watershed, non-parametric marker-based segmentation\n * algorithm, described in Meyer92 .\n * \n * Before passing the image to the function, you have to roughly outline the desired regions in the\n * image markers with positive (>0) indices. So, every region is represented as one or more connected\n * components with the pixel values 1, 2, 3, and so on. Such markers can be retrieved from a binary\n * mask using [findContours](#d3/dc0/group__imgproc__shape_1gadf1ad6a0b82947fa1fe3c3d497f260e0}) and\n * [drawContours](#d6/d6e/group__imgproc__draw_1ga746c0625f1781f1ffc9056259103edbc}) (see the\n * watershed.cpp demo). The markers are \"seeds\" of the future image regions. All the other pixels in\n * markers , whose relation to the outlined regions is not known and should be defined by the\n * algorithm, should be set to 0's. In the function output, each pixel in markers is set to a value of\n * the \"seed\" components or to -1 at boundaries between the regions.\n * \n * Any two neighbor connected components are not necessarily separated by a watershed boundary (-1's\n * pixels); for example, they can touch each other in the initial marker image passed to the function.\n * \n * [findContours](#d3/dc0/group__imgproc__shape_1gadf1ad6a0b82947fa1fe3c3d497f260e0})\n * \n * @param image Input 8-bit 3-channel image.\n * \n * @param markers Input/output 32-bit single-channel image (map) of markers. It should have the same\n * size as image .\n */\nexport declare function watershed(image: InputArray, markers: InputOutputArray): void\n\n/**\n * the threshold value `$T(x,y)$` is a mean of the `$\\\\texttt{blockSize} \\\\times \\\\texttt{blockSize}$`\n * neighborhood of `$(x, y)$` minus C\n * \n */\nexport declare const ADAPTIVE_THRESH_MEAN_C: AdaptiveThresholdTypes // initializer: = 0\n\n/**\n * the threshold value `$T(x, y)$` is a weighted sum (cross-correlation with a Gaussian window) of the\n * `$\\\\texttt{blockSize} \\\\times \\\\texttt{blockSize}$` neighborhood of `$(x, y)$` minus C . The default\n * sigma (standard deviation) is used for the specified blockSize . See\n * [getGaussianKernel](#d4/d86/group__imgproc__filter_1gac05a120c1ae92a6060dd0db190a61afa})\n * \n */\nexport declare const ADAPTIVE_THRESH_GAUSSIAN_C: AdaptiveThresholdTypes // initializer: = 1\n\n/**\n * each connected component of zeros in src (as well as all the non-zero pixels closest to the\n * connected component) will be assigned the same label\n * \n */\nexport declare const DIST_LABEL_CCOMP: DistanceTransformLabelTypes // initializer: = 0\n\n/**\n * each zero pixel (and all the non-zero pixels closest to it) gets its own label.\n * \n */\nexport declare const DIST_LABEL_PIXEL: DistanceTransformLabelTypes // initializer: = 1\n\nexport declare const DIST_MASK_3: DistanceTransformMasks // initializer: = 3\n\nexport declare const DIST_MASK_5: DistanceTransformMasks // initializer: = 5\n\nexport declare const DIST_MASK_PRECISE: DistanceTransformMasks // initializer: = 0\n\nexport declare const DIST_USER: DistanceTypes // initializer: = -1\n\nexport declare const DIST_L1: DistanceTypes // initializer: = 1\n\nexport declare const DIST_L2: DistanceTypes // initializer: = 2\n\nexport declare const DIST_C: DistanceTypes // initializer: = 3\n\nexport declare const DIST_L12: DistanceTypes // initializer: = 4\n\nexport declare const DIST_FAIR: DistanceTypes // initializer: = 5\n\nexport declare const DIST_WELSCH: DistanceTypes // initializer: = 6\n\nexport declare const DIST_HUBER: DistanceTypes // initializer: = 7\n\n/**\n * If set, the difference between the current pixel and seed pixel is considered. Otherwise, the\n * difference between neighbor pixels is considered (that is, the range is floating).\n * \n */\nexport declare const FLOODFILL_FIXED_RANGE: FloodFillFlags // initializer: = 1 << 16\n\n/**\n * If set, the function does not change the image ( newVal is ignored), and only fills the mask with\n * the value specified in bits 8-16 of flags as described above. This option only make sense in\n * function variants that have the mask parameter.\n * \n */\nexport declare const FLOODFILL_MASK_ONLY: FloodFillFlags // initializer: = 1 << 17\n\nexport declare const GC_BGD: GrabCutClasses // initializer: = 0\n\nexport declare const GC_FGD: GrabCutClasses // initializer: = 1\n\nexport declare const GC_PR_BGD: GrabCutClasses // initializer: = 2\n\nexport declare const GC_PR_FGD: GrabCutClasses // initializer: = 3\n\n/**\n * The function initializes the state and the mask using the provided rectangle. After that it runs\n * iterCount iterations of the algorithm.\n * \n */\nexport declare const GC_INIT_WITH_RECT: GrabCutModes // initializer: = 0\n\n/**\n * The function initializes the state using the provided mask. Note that GC_INIT_WITH_RECT and\n * GC_INIT_WITH_MASK can be combined. Then, all the pixels outside of the ROI are automatically\n * initialized with GC_BGD .\n * \n */\nexport declare const GC_INIT_WITH_MASK: GrabCutModes // initializer: = 1\n\n/**\n * The value means that the algorithm should just resume.\n * \n */\nexport declare const GC_EVAL: GrabCutModes // initializer: = 2\n\n/**\n * The value means that the algorithm should just run the grabCut algorithm (a single iteration) with\n * the fixed model\n * \n */\nexport declare const GC_EVAL_FREEZE_MODEL: GrabCutModes // initializer: = 3\n\nexport declare const THRESH_BINARY: ThresholdTypes // initializer: = 0\n\nexport declare const THRESH_BINARY_INV: ThresholdTypes // initializer: = 1\n\nexport declare const THRESH_TRUNC: ThresholdTypes // initializer: = 2\n\nexport declare const THRESH_TOZERO: ThresholdTypes // initializer: = 3\n\nexport declare const THRESH_TOZERO_INV: ThresholdTypes // initializer: = 4\n\nexport declare const THRESH_MASK: ThresholdTypes // initializer: = 7\n\nexport declare const THRESH_OTSU: ThresholdTypes // initializer: = 8\n\nexport declare const THRESH_TRIANGLE: ThresholdTypes // initializer: = 16\n\n/**\n * adaptive threshold algorithm \n * \n * [adaptiveThreshold](#d7/d1b/group__imgproc__misc_1ga72b913f352e4a1b1b397736707afcde3})\n * \n */\nexport type AdaptiveThresholdTypes = any\n\n/**\n * adaptive threshold algorithm \n * \n * [adaptiveThreshold](#d7/d1b/group__imgproc__misc_1ga72b913f352e4a1b1b397736707afcde3})\n * \n */\nexport type DistanceTransformLabelTypes = any\n\n/**\n * adaptive threshold algorithm \n * \n * [adaptiveThreshold](#d7/d1b/group__imgproc__misc_1ga72b913f352e4a1b1b397736707afcde3})\n * \n */\nexport type DistanceTransformMasks = any\n\n/**\n * adaptive threshold algorithm \n * \n * [adaptiveThreshold](#d7/d1b/group__imgproc__misc_1ga72b913f352e4a1b1b397736707afcde3})\n * \n */\nexport type DistanceTypes = any\n\n/**\n * adaptive threshold algorithm \n * \n * [adaptiveThreshold](#d7/d1b/group__imgproc__misc_1ga72b913f352e4a1b1b397736707afcde3})\n * \n */\nexport type FloodFillFlags = any\n\n/**\n * adaptive threshold algorithm \n * \n * [adaptiveThreshold](#d7/d1b/group__imgproc__misc_1ga72b913f352e4a1b1b397736707afcde3})\n * \n */\nexport type GrabCutClasses = any\n\n/**\n * adaptive threshold algorithm \n * \n * [adaptiveThreshold](#d7/d1b/group__imgproc__misc_1ga72b913f352e4a1b1b397736707afcde3})\n * \n */\nexport type GrabCutModes = any\n\n/**\n * adaptive threshold algorithm \n * \n * [adaptiveThreshold](#d7/d1b/group__imgproc__misc_1ga72b913f352e4a1b1b397736707afcde3})\n * \n */\nexport type ThresholdTypes = any\n\n"},"node_modules_mirada_dist_src_types_opencv_index_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_index_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/index.d.ts","content":"import * as _CV from './_types'\nexport type CV = typeof _CV\nexport * from './_hacks'\nexport * from './_types'\n"},"node_modules_mirada_dist_src_types_opencv_imgproc_feature_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_imgproc_feature_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/imgproc_feature.d.ts","content":"\nimport { bool, double, InputArray, InputOutputArray, int, OutputArray, Size, TermCriteria } from './_types'\n/*\n * # Feature Detection\n * \n */\n/**\n * The function finds edges in the input image and marks them in the output map edges using the Canny\n * algorithm. The smallest value between threshold1 and threshold2 is used for edge linking. The\n * largest value is used to find initial segments of strong edges. See\n * \n * @param image 8-bit input image.\n * \n * @param edges output edge map; single channels 8-bit image, which has the same size as image .\n * \n * @param threshold1 first threshold for the hysteresis procedure.\n * \n * @param threshold2 second threshold for the hysteresis procedure.\n * \n * @param apertureSize aperture size for the Sobel operator.\n * \n * @param L2gradient a flag, indicating whether a more accurate $L_2$ norm $=\\sqrt{(dI/dx)^2 +\n * (dI/dy)^2}$ should be used to calculate the image gradient magnitude ( L2gradient=true ), or whether\n * the default $L_1$ norm $=|dI/dx|+|dI/dy|$ is enough ( L2gradient=false ).\n */\nexport declare function Canny(image: InputArray, edges: OutputArray, threshold1: double, threshold2: double, apertureSize?: int, L2gradient?: bool): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n * \n * Finds edges in an image using the Canny algorithm with custom image gradient.\n * \n * @param dx 16-bit x derivative of input image (CV_16SC1 or CV_16SC3).\n * \n * @param dy 16-bit y derivative of input image (same type as dx).\n * \n * @param edges output edge map; single channels 8-bit image, which has the same size as image .\n * \n * @param threshold1 first threshold for the hysteresis procedure.\n * \n * @param threshold2 second threshold for the hysteresis procedure.\n * \n * @param L2gradient a flag, indicating whether a more accurate $L_2$ norm $=\\sqrt{(dI/dx)^2 +\n * (dI/dy)^2}$ should be used to calculate the image gradient magnitude ( L2gradient=true ), or whether\n * the default $L_1$ norm $=|dI/dx|+|dI/dy|$ is enough ( L2gradient=false ).\n */\nexport declare function Canny(dx: InputArray, dy: InputArray, edges: OutputArray, threshold1: double, threshold2: double, L2gradient?: bool): void\n\n/**\n * For every pixel `$p$` , the function cornerEigenValsAndVecs considers a blockSize `$\\\\times$`\n * blockSize neighborhood `$S(p)$` . It calculates the covariation matrix of derivatives over the\n * neighborhood as:\n * \n * `\\\\[M = \\\\begin{bmatrix} \\\\sum _{S(p)}(dI/dx)^2 & \\\\sum _{S(p)}dI/dx dI/dy \\\\\\\\ \\\\sum _{S(p)}dI/dx\n * dI/dy & \\\\sum _{S(p)}(dI/dy)^2 \\\\end{bmatrix}\\\\]`\n * \n * where the derivatives are computed using the Sobel operator.\n * \n * After that, it finds eigenvectors and eigenvalues of `$M$` and stores them in the destination image\n * as `$(\\\\lambda_1, \\\\lambda_2, x_1, y_1, x_2, y_2)$` where\n * \n * `$\\\\lambda_1, \\\\lambda_2$` are the non-sorted eigenvalues of `$M$`\n * `$x_1, y_1$` are the eigenvectors corresponding to `$\\\\lambda_1$`\n * `$x_2, y_2$` are the eigenvectors corresponding to `$\\\\lambda_2$`\n * \n * The output of the function can be used for robust edge or corner detection.\n * \n * [cornerMinEigenVal](#dd/d1a/group__imgproc__feature_1ga3dbce297c1feb859ee36707e1003e0a8}),\n * [cornerHarris](#dd/d1a/group__imgproc__feature_1gac1fc3598018010880e370e2f709b4345}),\n * [preCornerDetect](#dd/d1a/group__imgproc__feature_1gaa819f39b5c994871774081803ae22586})\n * \n * @param src Input single-channel 8-bit or floating-point image.\n * \n * @param dst Image to store the results. It has the same size as src and the type CV_32FC(6) .\n * \n * @param blockSize Neighborhood size (see details below).\n * \n * @param ksize Aperture parameter for the Sobel operator.\n * \n * @param borderType Pixel extrapolation method. See BorderTypes.\n */\nexport declare function cornerEigenValsAndVecs(src: InputArray, dst: OutputArray, blockSize: int, ksize: int, borderType?: int): void\n\n/**\n * The function runs the Harris corner detector on the image. Similarly to cornerMinEigenVal and\n * cornerEigenValsAndVecs , for each pixel `$(x, y)$` it calculates a `$2\\\\times2$` gradient covariance\n * matrix `$M^{(x,y)}$` over a `$\\\\texttt{blockSize} \\\\times \\\\texttt{blockSize}$` neighborhood. Then,\n * it computes the following characteristic:\n * \n * `\\\\[\\\\texttt{dst} (x,y) = \\\\mathrm{det} M^{(x,y)} - k \\\\cdot \\\\left ( \\\\mathrm{tr} M^{(x,y)} \\\\right\n * )^2\\\\]`\n * \n * Corners in the image can be found as the local maxima of this response map.\n * \n * @param src Input single-channel 8-bit or floating-point image.\n * \n * @param dst Image to store the Harris detector responses. It has the type CV_32FC1 and the same size\n * as src .\n * \n * @param blockSize Neighborhood size (see the details on cornerEigenValsAndVecs ).\n * \n * @param ksize Aperture parameter for the Sobel operator.\n * \n * @param k Harris detector free parameter. See the formula above.\n * \n * @param borderType Pixel extrapolation method. See BorderTypes.\n */\nexport declare function cornerHarris(src: InputArray, dst: OutputArray, blockSize: int, ksize: int, k: double, borderType?: int): void\n\n/**\n * The function is similar to cornerEigenValsAndVecs but it calculates and stores only the minimal\n * eigenvalue of the covariance matrix of derivatives, that is, `$\\\\min(\\\\lambda_1, \\\\lambda_2)$` in\n * terms of the formulae in the cornerEigenValsAndVecs description.\n * \n * @param src Input single-channel 8-bit or floating-point image.\n * \n * @param dst Image to store the minimal eigenvalues. It has the type CV_32FC1 and the same size as src\n * .\n * \n * @param blockSize Neighborhood size (see the details on cornerEigenValsAndVecs ).\n * \n * @param ksize Aperture parameter for the Sobel operator.\n * \n * @param borderType Pixel extrapolation method. See BorderTypes.\n */\nexport declare function cornerMinEigenVal(src: InputArray, dst: OutputArray, blockSize: int, ksize?: int, borderType?: int): void\n\n/**\n * The function iterates to find the sub-pixel accurate location of corners or radial saddle points, as\n * shown on the figure below.\n * \n *  Sub-pixel accurate corner locator is based on the observation that every vector from the center\n * `$q$` to a point `$p$` located within a neighborhood of `$q$` is orthogonal to the image gradient at\n * `$p$` subject to image and measurement noise. Consider the expression:\n * \n * `\\\\[\\\\epsilon _i = {DI_{p_i}}^T \\\\cdot (q - p_i)\\\\]`\n * \n * where `${DI_{p_i}}$` is an image gradient at one of the points `$p_i$` in a neighborhood of `$q$` .\n * The value of `$q$` is to be found so that `$\\\\epsilon_i$` is minimized. A system of equations may be\n * set up with `$\\\\epsilon_i$` set to zero:\n * \n * `\\\\[\\\\sum _i(DI_{p_i} \\\\cdot {DI_{p_i}}^T) \\\\cdot q - \\\\sum _i(DI_{p_i} \\\\cdot {DI_{p_i}}^T \\\\cdot\n * p_i)\\\\]`\n * \n * where the gradients are summed within a neighborhood (\"search window\") of `$q$` . Calling the first\n * gradient term `$G$` and the second gradient term `$b$` gives:\n * \n * `\\\\[q = G^{-1} \\\\cdot b\\\\]`\n * \n * The algorithm sets the center of the neighborhood window at this new center `$q$` and then iterates\n * until the center stays within a set threshold.\n * \n * @param image Input single-channel, 8-bit or float image.\n * \n * @param corners Initial coordinates of the input corners and refined coordinates provided for output.\n * \n * @param winSize Half of the side length of the search window. For example, if winSize=Size(5,5) ,\n * then a $(5*2+1) \\times (5*2+1) = 11 \\times 11$ search window is used.\n * \n * @param zeroZone Half of the size of the dead region in the middle of the search zone over which the\n * summation in the formula below is not done. It is used sometimes to avoid possible singularities of\n * the autocorrelation matrix. The value of (-1,-1) indicates that there is no such a size.\n * \n * @param criteria Criteria for termination of the iterative process of corner refinement. That is, the\n * process of corner position refinement stops either after criteria.maxCount iterations or when the\n * corner position moves by less than criteria.epsilon on some iteration.\n */\nexport declare function cornerSubPix(image: InputArray, corners: InputOutputArray, winSize: Size, zeroZone: Size, criteria: TermCriteria): void\n\n/**\n * The [LineSegmentDetector](#db/d73/classcv_1_1LineSegmentDetector}) algorithm is defined using the\n * standard values. Only advanced users may want to edit those, as to tailor it for their own\n * application.\n * \n * Implementation has been removed due original code license conflict\n * \n * @param _refine The way found lines will be refined, see LineSegmentDetectorModes\n * \n * @param _scale The scale of the image that will be used to find the lines. Range (0..1].\n * \n * @param _sigma_scale Sigma for Gaussian filter. It is computed as sigma = _sigma_scale/_scale.\n * \n * @param _quant Bound to the quantization error on the gradient norm.\n * \n * @param _ang_th Gradient angle tolerance in degrees.\n * \n * @param _log_eps Detection threshold: -log10(NFA) > log_eps. Used only when advance refinement is\n * chosen.\n * \n * @param _density_th Minimal density of aligned region points in the enclosing rectangle.\n * \n * @param _n_bins Number of bins in pseudo-ordering of gradient modulus.\n */\nexport declare function createLineSegmentDetector(_refine?: int, _scale?: double, _sigma_scale?: double, _quant?: double, _ang_th?: double, _log_eps?: double, _density_th?: double, _n_bins?: int): any\n\n/**\n * The function finds the most prominent corners in the image or in the specified image region, as\n * described in Shi94\n * \n * Function calculates the corner quality measure at every source image pixel using the\n * [cornerMinEigenVal](#dd/d1a/group__imgproc__feature_1ga3dbce297c1feb859ee36707e1003e0a8}) or\n * [cornerHarris](#dd/d1a/group__imgproc__feature_1gac1fc3598018010880e370e2f709b4345}) .\n * Function performs a non-maximum suppression (the local maximums in *3 x 3* neighborhood are\n * retained).\n * The corners with the minimal eigenvalue less than `$\\\\texttt{qualityLevel} \\\\cdot \\\\max_{x,y}\n * qualityMeasureMap(x,y)$` are rejected.\n * The remaining corners are sorted by the quality measure in the descending order.\n * Function throws away each corner for which there is a stronger corner at a distance less than\n * maxDistance.\n * \n * The function can be used to initialize a point-based tracker of an object.\n * \n * If the function is called with different values A and B of the parameter qualityLevel , and A > B,\n * the vector of returned corners with qualityLevel=A will be the prefix of the output vector with\n * qualityLevel=B .\n * \n * [cornerMinEigenVal](#dd/d1a/group__imgproc__feature_1ga3dbce297c1feb859ee36707e1003e0a8}),\n * [cornerHarris](#dd/d1a/group__imgproc__feature_1gac1fc3598018010880e370e2f709b4345}),\n * [calcOpticalFlowPyrLK](#dc/d6b/group__video__track_1ga473e4b886d0bcc6b65831eb88ed93323}),\n * [estimateRigidTransform](#dc/d6b/group__video__track_1ga762cbe5efd52cf078950196f3c616d48}),\n * \n * @param image Input 8-bit or floating-point 32-bit, single-channel image.\n * \n * @param corners Output vector of detected corners.\n * \n * @param maxCorners Maximum number of corners to return. If there are more corners than are found, the\n * strongest of them is returned. maxCorners <= 0 implies that no limit on the maximum is set and all\n * detected corners are returned.\n * \n * @param qualityLevel Parameter characterizing the minimal accepted quality of image corners. The\n * parameter value is multiplied by the best corner quality measure, which is the minimal eigenvalue\n * (see cornerMinEigenVal ) or the Harris function response (see cornerHarris ). The corners with the\n * quality measure less than the product are rejected. For example, if the best corner has the quality\n * measure = 1500, and the qualityLevel=0.01 , then all the corners with the quality measure less than\n * 15 are rejected.\n * \n * @param minDistance Minimum possible Euclidean distance between the returned corners.\n * \n * @param mask Optional region of interest. If the image is not empty (it needs to have the type\n * CV_8UC1 and the same size as image ), it specifies the region in which the corners are detected.\n * \n * @param blockSize Size of an average block for computing a derivative covariation matrix over each\n * pixel neighborhood. See cornerEigenValsAndVecs .\n * \n * @param useHarrisDetector Parameter indicating whether to use a Harris detector (see cornerHarris) or\n * cornerMinEigenVal.\n * \n * @param k Free parameter of the Harris detector.\n */\nexport declare function goodFeaturesToTrack(image: InputArray, corners: OutputArray, maxCorners: int, qualityLevel: double, minDistance: double, mask?: InputArray, blockSize?: int, useHarrisDetector?: bool, k?: double): void\n\nexport declare function goodFeaturesToTrack(image: InputArray, corners: OutputArray, maxCorners: int, qualityLevel: double, minDistance: double, mask: InputArray, blockSize: int, gradientSize: int, useHarrisDetector?: bool, k?: double): void\n\n/**\n * The function finds circles in a grayscale image using a modification of the Hough transform.\n * \n * Example: : \n * \n * ```cpp\n * #include <opencv2/imgproc.hpp>\n * #include <opencv2/highgui.hpp>\n * #include <math.h>\n * \n * using namespace cv;\n * using namespace std;\n * \n * int main(int argc, char** argv)\n * {\n *     Mat img, gray;\n *     if( argc != 2 || !(img=imread(argv[1], 1)).data)\n *         return -1;\n *     cvtColor(img, gray, COLOR_BGR2GRAY);\n *     // smooth it, otherwise a lot of false circles may be detected\n *     GaussianBlur( gray, gray, Size(9, 9), 2, 2 );\n *     vector<Vec3f> circles;\n *     HoughCircles(gray, circles, HOUGH_GRADIENT,\n *                  2, gray.rows/4, 200, 100 );\n *     for( size_t i = 0; i < circles.size(); i++ )\n *     {\n *          Point center(cvRound(circles[i][0]), cvRound(circles[i][1]));\n *          int radius = cvRound(circles[i][2]);\n *          // draw the circle center\n *          circle( img, center, 3, Scalar(0,255,0), -1, 8, 0 );\n *          // draw the circle outline\n *          circle( img, center, radius, Scalar(0,0,255), 3, 8, 0 );\n *     }\n *     namedWindow( \"circles\", 1 );\n *     imshow( \"circles\", img );\n * \n *     waitKey(0);\n *     return 0;\n * }\n * ```\n * \n * Usually the function detects the centers of circles well. However, it may fail to find correct\n * radii. You can assist to the function by specifying the radius range ( minRadius and maxRadius ) if\n * you know it. Or, you may set maxRadius to a negative number to return centers only without radius\n * search, and find the correct radius using an additional procedure.\n * \n * [fitEllipse](#d3/dc0/group__imgproc__shape_1gaf259efaad93098103d6c27b9e4900ffa}),\n * [minEnclosingCircle](#d3/dc0/group__imgproc__shape_1ga8ce13c24081bbc7151e9326f412190f1})\n * \n * @param image 8-bit, single-channel, grayscale input image.\n * \n * @param circles Output vector of found circles. Each vector is encoded as 3 or 4 element\n * floating-point vector $(x, y, radius)$ or $(x, y, radius, votes)$ .\n * \n * @param method Detection method, see HoughModes. Currently, the only implemented method is\n * HOUGH_GRADIENT\n * \n * @param dp Inverse ratio of the accumulator resolution to the image resolution. For example, if dp=1\n * , the accumulator has the same resolution as the input image. If dp=2 , the accumulator has half as\n * big width and height.\n * \n * @param minDist Minimum distance between the centers of the detected circles. If the parameter is too\n * small, multiple neighbor circles may be falsely detected in addition to a true one. If it is too\n * large, some circles may be missed.\n * \n * @param param1 First method-specific parameter. In case of HOUGH_GRADIENT , it is the higher\n * threshold of the two passed to the Canny edge detector (the lower one is twice smaller).\n * \n * @param param2 Second method-specific parameter. In case of HOUGH_GRADIENT , it is the accumulator\n * threshold for the circle centers at the detection stage. The smaller it is, the more false circles\n * may be detected. Circles, corresponding to the larger accumulator values, will be returned first.\n * \n * @param minRadius Minimum circle radius.\n * \n * @param maxRadius Maximum circle radius. If <= 0, uses the maximum image dimension. If < 0, returns\n * centers without finding the radius.\n */\nexport declare function HoughCircles(image: InputArray, circles: OutputArray, method: int, dp: double, minDist: double, param1?: double, param2?: double, minRadius?: int, maxRadius?: int): void\n\n/**\n * The function implements the standard or standard multi-scale Hough transform algorithm for line\n * detection. See  for a good explanation of Hough transform.\n * \n * @param image 8-bit, single-channel binary source image. The image may be modified by the function.\n * \n * @param lines Output vector of lines. Each line is represented by a 2 or 3 element vector $(\\rho,\n * \\theta)$ or $(\\rho, \\theta, \\textrm{votes})$ . $\\rho$ is the distance from the coordinate origin\n * $(0,0)$ (top-left corner of the image). $\\theta$ is the line rotation angle in radians ( $0 \\sim\n * \\textrm{vertical line}, \\pi/2 \\sim \\textrm{horizontal line}$ ). $\\textrm{votes}$ is the value of\n * accumulator.\n * \n * @param rho Distance resolution of the accumulator in pixels.\n * \n * @param theta Angle resolution of the accumulator in radians.\n * \n * @param threshold Accumulator threshold parameter. Only those lines are returned that get enough\n * votes ( $>\\texttt{threshold}$ ).\n * \n * @param srn For the multi-scale Hough transform, it is a divisor for the distance resolution rho .\n * The coarse accumulator distance resolution is rho and the accurate accumulator resolution is rho/srn\n * . If both srn=0 and stn=0 , the classical Hough transform is used. Otherwise, both these parameters\n * should be positive.\n * \n * @param stn For the multi-scale Hough transform, it is a divisor for the distance resolution theta.\n * \n * @param min_theta For standard and multi-scale Hough transform, minimum angle to check for lines.\n * Must fall between 0 and max_theta.\n * \n * @param max_theta For standard and multi-scale Hough transform, maximum angle to check for lines.\n * Must fall between min_theta and CV_PI.\n */\nexport declare function HoughLines(image: InputArray, lines: OutputArray, rho: double, theta: double, threshold: int, srn?: double, stn?: double, min_theta?: double, max_theta?: double): void\n\n/**\n * The function implements the probabilistic Hough transform algorithm for line detection, described in\n * Matas00\n * \n * See the line detection example below: \n * \n * ```cpp\n * #include <opencv2/imgproc.hpp>\n * #include <opencv2/highgui.hpp>\n * \n * using namespace cv;\n * using namespace std;\n * \n * int main(int argc, char** argv)\n * {\n *     Mat src, dst, color_dst;\n *     if( argc != 2 || !(src=imread(argv[1], 0)).data)\n *         return -1;\n * \n *     Canny( src, dst, 50, 200, 3 );\n *     cvtColor( dst, color_dst, COLOR_GRAY2BGR );\n * \n *     vector<Vec4i> lines;\n *     HoughLinesP( dst, lines, 1, CV_PI/180, 80, 30, 10 );\n *     for( size_t i = 0; i < lines.size(); i++ )\n *     {\n *         line( color_dst, Point(lines[i][0], lines[i][1]),\n *         Point( lines[i][2], lines[i][3]), Scalar(0,0,255), 3, 8 );\n *     }\n *     namedWindow( \"Source\", 1 );\n *     imshow( \"Source\", src );\n * \n *     namedWindow( \"Detected Lines\", 1 );\n *     imshow( \"Detected Lines\", color_dst );\n * \n *     waitKey(0);\n *     return 0;\n * }\n * ```\n * \n *  This is a sample picture the function parameters have been tuned for:\n * \n *  And this is the output of the above program in case of the probabilistic Hough transform:\n * \n * [LineSegmentDetector](#db/d73/classcv_1_1LineSegmentDetector})\n * \n * @param image 8-bit, single-channel binary source image. The image may be modified by the function.\n * \n * @param lines Output vector of lines. Each line is represented by a 4-element vector $(x_1, y_1, x_2,\n * y_2)$ , where $(x_1,y_1)$ and $(x_2, y_2)$ are the ending points of each detected line segment.\n * \n * @param rho Distance resolution of the accumulator in pixels.\n * \n * @param theta Angle resolution of the accumulator in radians.\n * \n * @param threshold Accumulator threshold parameter. Only those lines are returned that get enough\n * votes ( $>\\texttt{threshold}$ ).\n * \n * @param minLineLength Minimum line length. Line segments shorter than that are rejected.\n * \n * @param maxLineGap Maximum allowed gap between points on the same line to link them.\n */\nexport declare function HoughLinesP(image: InputArray, lines: OutputArray, rho: double, theta: double, threshold: int, minLineLength?: double, maxLineGap?: double): void\n\n/**\n * The function finds lines in a set of points using a modification of the Hough transform. \n * \n * ```cpp\n * #include <opencv2/core.hpp>\n * #include <opencv2/imgproc.hpp>\n * \n * using namespace cv;\n * using namespace std;\n * \n * int main()\n * {\n *     Mat lines;\n *     vector<Vec3d> line3d;\n *     vector<Point2f> point;\n *     const static float Points[20][2] = {\n *     { 0.0f,   369.0f }, { 10.0f,  364.0f }, { 20.0f,  358.0f }, { 30.0f,  352.0f },\n *     { 40.0f,  346.0f }, { 50.0f,  341.0f }, { 60.0f,  335.0f }, { 70.0f,  329.0f },\n *     { 80.0f,  323.0f }, { 90.0f,  318.0f }, { 100.0f, 312.0f }, { 110.0f, 306.0f },\n *     { 120.0f, 300.0f }, { 130.0f, 295.0f }, { 140.0f, 289.0f }, { 150.0f, 284.0f },\n *     { 160.0f, 277.0f }, { 170.0f, 271.0f }, { 180.0f, 266.0f }, { 190.0f, 260.0f }\n *     };\n * \n *     for (int i = 0; i < 20; i++)\n *     {\n *         point.push_back(Point2f(Points[i][0],Points[i][1]));\n *     }\n * \n *     double rhoMin = 0.0f, rhoMax = 360.0f, rhoStep = 1;\n *     double thetaMin = 0.0f, thetaMax = CV_PI / 2.0f, thetaStep = CV_PI / 180.0f;\n * \n *     HoughLinesPointSet(point, lines, 20, 1,\n *                        rhoMin, rhoMax, rhoStep,\n *                        thetaMin, thetaMax, thetaStep);\n * \n *     lines.copyTo(line3d);\n *     printf(\"votes:%d, rho:%.7f, theta:%.7f\\\\n\",(int)line3d.at(0).val[0], line3d.at(0).val[1],\n * line3d.at(0).val[2]);\n * }\n * ```\n * \n * @param _point Input vector of points. Each vector must be encoded as a Point vector $(x,y)$. Type\n * must be CV_32FC2 or CV_32SC2.\n * \n * @param _lines Output vector of found lines. Each vector is encoded as a vector<Vec3d> $(votes, rho,\n * theta)$. The larger the value of 'votes', the higher the reliability of the Hough line.\n * \n * @param lines_max Max count of hough lines.\n * \n * @param threshold Accumulator threshold parameter. Only those lines are returned that get enough\n * votes ( $>\\texttt{threshold}$ )\n * \n * @param min_rho Minimum Distance value of the accumulator in pixels.\n * \n * @param max_rho Maximum Distance value of the accumulator in pixels.\n * \n * @param rho_step Distance resolution of the accumulator in pixels.\n * \n * @param min_theta Minimum angle value of the accumulator in radians.\n * \n * @param max_theta Maximum angle value of the accumulator in radians.\n * \n * @param theta_step Angle resolution of the accumulator in radians.\n */\nexport declare function HoughLinesPointSet(_point: InputArray, _lines: OutputArray, lines_max: int, threshold: int, min_rho: double, max_rho: double, rho_step: double, min_theta: double, max_theta: double, theta_step: double): void\n\n/**\n * The function calculates the complex spatial derivative-based function of the source image\n * \n * `\\\\[\\\\texttt{dst} = (D_x \\\\texttt{src} )^2 \\\\cdot D_{yy} \\\\texttt{src} + (D_y \\\\texttt{src} )^2\n * \\\\cdot D_{xx} \\\\texttt{src} - 2 D_x \\\\texttt{src} \\\\cdot D_y \\\\texttt{src} \\\\cdot D_{xy}\n * \\\\texttt{src}\\\\]`\n * \n * where `$D_x$`, `$D_y$` are the first image derivatives, `$D_{xx}$`, `$D_{yy}$` are the second image\n * derivatives, and `$D_{xy}$` is the mixed derivative.\n * \n * The corners can be found as local maximums of the functions, as shown below: \n * \n * ```cpp\n * Mat corners, dilated_corners;\n * preCornerDetect(image, corners, 3);\n * // dilation with 3x3 rectangular structuring element\n * dilate(corners, dilated_corners, Mat(), 1);\n * Mat corner_mask = corners == dilated_corners;\n * ```\n * \n * @param src Source single-channel 8-bit of floating-point image.\n * \n * @param dst Output image that has the type CV_32F and the same size as src .\n * \n * @param ksize Aperture size of the Sobel .\n * \n * @param borderType Pixel extrapolation method. See BorderTypes.\n */\nexport declare function preCornerDetect(src: InputArray, dst: OutputArray, ksize: int, borderType?: int): void\n\n/**\n * classical or standard Hough transform. Every line is represented by two floating-point numbers\n * `$(\\\\rho, \\\\theta)$` , where `$\\\\rho$` is a distance between (0,0) point and the line, and\n * `$\\\\theta$` is the angle between x-axis and the normal to the line. Thus, the matrix must be (the\n * created sequence will be) of CV_32FC2 type\n * \n */\nexport declare const HOUGH_STANDARD: HoughModes // initializer: = 0\n\n/**\n * probabilistic Hough transform (more efficient in case if the picture contains a few long linear\n * segments). It returns line segments rather than the whole line. Each segment is represented by\n * starting and ending points, and the matrix must be (the created sequence will be) of the CV_32SC4\n * type.\n * \n */\nexport declare const HOUGH_PROBABILISTIC: HoughModes // initializer: = 1\n\n/**\n * multi-scale variant of the classical Hough transform. The lines are encoded the same way as\n * HOUGH_STANDARD.\n * \n */\nexport declare const HOUGH_MULTI_SCALE: HoughModes // initializer: = 2\n\nexport declare const HOUGH_GRADIENT: HoughModes // initializer: = 3\n\nexport declare const LSD_REFINE_NONE: LineSegmentDetectorModes // initializer: = 0\n\nexport declare const LSD_REFINE_STD: LineSegmentDetectorModes // initializer: = 1\n\n/**\n * Advanced refinement. Number of false alarms is calculated, lines are refined through increase of\n * precision, decrement in size, etc.\n * \n */\nexport declare const LSD_REFINE_ADV: LineSegmentDetectorModes // initializer: = 2\n\nexport type HoughModes = any\n\nexport type LineSegmentDetectorModes = any\n\n"},"node_modules_mirada_dist_src_types_opencv_Logger_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_Logger_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/Logger.d.ts","content":"\nimport { int } from './_types'\n\nexport declare class Logger {\n\n  public static error(fmt: any, arg121: any): int\n\n  public static fatal(fmt: any, arg122: any): int\n\n  public static info(fmt: any, arg123: any): int\n\n  /**\n   *   Print log message\n   *   \n   *   @param level Log level\n   *   \n   *   @param fmt Message format\n   */\n  public static log(level: int, fmt: any, arg124: any): int\n\n  /**\n   *   Sets the logging destination\n   *   \n   *   @param name Filename or NULL for console\n   */\n  public static setDestination(name: any): void\n\n  /**\n   *   Sets the logging level. All messages with lower priority will be ignored.\n   *   \n   *   @param level Logging level\n   */\n  public static setLevel(level: int): void\n\n  public static warn(fmt: any, arg125: any): int\n}\n\n"},"node_modules_mirada_dist_src_types_opencv_LshTable_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_LshTable_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/LshTable.d.ts","content":"\nimport { Bucket, BucketKey, LshStats, Matrix, size_t } from './_types'\n\n/**\n * Lsh hash table. As its key is a sub-feature, and as usually the size of it is pretty small, we keep\n * it as a continuous memory array. The value is an index in the corpus of features (we keep it as an\n * unsigned int for pure memory reasons, it could be a size_t)\n * \n * Source:\n * [opencv2/flann/lsh_table.h](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/flann/lsh_table.h#L261).\n * \n */\nexport declare class LshTable {\n\n  /**\n   *   Default constructor\n   */\n  public constructor()\n\n  /**\n   *   Default constructor Create the mask and allocate the memory\n   *   \n   *   @param feature_size is the size of the feature (considered as a ElementType[])\n   *   \n   *   @param key_size is the number of bits that are turned on in the feature\n   */\n  public constructor(feature_size: any, key_size: any)\n\n  public constructor(feature_size: any, subsignature_size: any)\n\n  /**\n   *   Add a feature to the table\n   *   \n   *   @param value the value to store for that feature\n   *   \n   *   @param feature the feature itself\n   */\n  public add(value: any, feature: any): void\n\n  /**\n   *   Add a set of features to the table\n   *   \n   *   @param dataset the values to store\n   */\n  public add(dataset: Matrix): Matrix\n\n  /**\n   *   Get a bucket given the key\n   */\n  public getBucketFromKey(key: BucketKey): Bucket\n\n  /**\n   *   Compute the sub-signature of a feature\n   */\n  public getKey(arg50: any): size_t\n\n  /**\n   *   Return the Subsignature of a feature\n   *   \n   *   @param feature the feature to analyze\n   */\n  public getKey(feature: any): size_t\n\n  /**\n   *   Get statistics about the table\n   */\n  public getStats(): LshStats\n\n  public getStats(): LshStats\n}\n\nexport declare const kArray: SpeedLevel // initializer: \n\nexport declare const kBitsetHash: SpeedLevel // initializer: \n\nexport declare const kHash: SpeedLevel // initializer: \n\n/**\n * defines the speed fo the implementation kArray uses a vector for storing data kBitsetHash uses a\n * hash map but checks for the validity of a key with a bitset kHash uses a hash map only\n * \n */\nexport type SpeedLevel = any\n\n"},"node_modules_mirada_dist_src_types_opencv_imgproc_transform_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_imgproc_transform_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/imgproc_transform.d.ts","content":"\nimport { bool, double, InputArray, int, Mat, OutputArray, Point2f, Size } from './_types'\n/*\n * # Geometric Image Transformations\n * The functions in this section perform various geometrical transformations of 2D images. They do not change the image content but deform the pixel grid and map this deformed grid to the destination image. In fact, to avoid sampling artifacts, the mapping is done in the reverse order, from destination to the source. That is, for each pixel `$(x, y)$` of the destination image, the functions compute coordinates of the corresponding \"donor\" pixel in the source image and copy the pixel value:\n * \n * `\\[\\texttt{dst} (x,y)= \\texttt{src} (f_x(x,y), f_y(x,y))\\]`\n * \n * In case when you specify the forward mapping `$\\left<g_x, g_y\\right>: \\texttt{src} \\rightarrow \\texttt{dst}$`, the OpenCV functions first compute the corresponding inverse mapping `$\\left<f_x, f_y\\right>: \\texttt{dst} \\rightarrow \\texttt{src}$` and then use the above formula.\n * \n * The actual implementations of the geometrical transformations, from the most generic remap and to the simplest and the fastest resize, need to solve two main problems with the above formula:\n * \n * \n * \n * \n * \n *  * Extrapolation of non-existing pixels. Similarly to the filtering functions described in the previous section, for some `$(x,y)$`, either one of `$f_x(x,y)$`, or `$f_y(x,y)$`, or both of them may fall outside of the image. In this case, an extrapolation method needs to be used. OpenCV provides the same selection of extrapolation methods as in the filtering functions. In addition, it provides the method [BORDER_TRANSPARENT](#d2/de8/group__core__array_1gga209f2f4869e304c82d07739337eae7c5a886a5eb6b466854d63f9e742d5c8eefe}). This means that the corresponding pixels in the destination image will not be modified at all.\n *  * Interpolation of pixel values. Usually `$f_x(x,y)$` and `$f_y(x,y)$` are floating-point numbers. This means that `$\\left<f_x, f_y\\right>$` can be either an affine or perspective transformation, or radial lens distortion correction, and so on. So, a pixel value at fractional coordinates needs to be retrieved. In the simplest case, the coordinates can be just rounded to the nearest integer coordinates and the corresponding pixel can be used. This is called a nearest-neighbor interpolation. However, a better result can be achieved by using more sophisticated  , where a polynomial function is fit into some neighborhood of the computed pixel `$(f_x(x,y), f_y(x,y))$`, and then the value of the polynomial at `$(f_x(x,y), f_y(x,y))$` is taken as the interpolated pixel value. In OpenCV, you can choose between several interpolation methods. See resize for details.\n * \n * \n * \n * \n * The geometrical transformations do not work with `CV_8S` or `CV_32S` images.\n */\n/**\n * The function converts a pair of maps for remap from one representation to another. The following\n * options ( (map1.type(), map2.type()) `$\\\\rightarrow$` (dstmap1.type(), dstmap2.type()) ) are\n * supported:\n * \n * `$\\\\texttt{(CV_32FC1, CV_32FC1)} \\\\rightarrow \\\\texttt{(CV_16SC2, CV_16UC1)}$`. This is the most\n * frequently used conversion operation, in which the original floating-point maps (see remap ) are\n * converted to a more compact and much faster fixed-point representation. The first output array\n * contains the rounded coordinates and the second array (created only when nninterpolation=false )\n * contains indices in the interpolation tables.\n * `$\\\\texttt{(CV_32FC2)} \\\\rightarrow \\\\texttt{(CV_16SC2, CV_16UC1)}$`. The same as above but the\n * original maps are stored in one 2-channel matrix.\n * Reverse conversion. Obviously, the reconstructed floating-point maps will not be exactly the same as\n * the originals.\n * \n * [remap](#da/d54/group__imgproc__transform_1gab75ef31ce5cdfb5c44b6da5f3b908ea4}),\n * [undistort](#d9/d0c/group__calib3d_1ga69f2545a8b62a6b0fc2ee060dc30559d}),\n * [initUndistortRectifyMap](#d9/d0c/group__calib3d_1ga7dfb72c9cf9780a347fbe3d1c47e5d5a})\n * \n * @param map1 The first input map of type CV_16SC2, CV_32FC1, or CV_32FC2 .\n * \n * @param map2 The second input map of type CV_16UC1, CV_32FC1, or none (empty matrix), respectively.\n * \n * @param dstmap1 The first output map that has the type dstmap1type and the same size as src .\n * \n * @param dstmap2 The second output map.\n * \n * @param dstmap1type Type of the first output map that should be CV_16SC2, CV_32FC1, or CV_32FC2 .\n * \n * @param nninterpolation Flag indicating whether the fixed-point maps are used for the\n * nearest-neighbor or for a more complex interpolation.\n */\nexport declare function convertMaps(map1: InputArray, map2: InputArray, dstmap1: OutputArray, dstmap2: OutputArray, dstmap1type: int, nninterpolation?: bool): void\n\n/**\n * The function calculates the `$2 \\\\times 3$` matrix of an affine transform so that:\n * \n * `\\\\[\\\\begin{bmatrix} x'_i \\\\\\\\ y'_i \\\\end{bmatrix} = \\\\texttt{map_matrix} \\\\cdot \\\\begin{bmatrix}\n * x_i \\\\\\\\ y_i \\\\\\\\ 1 \\\\end{bmatrix}\\\\]`\n * \n * where\n * \n * `\\\\[dst(i)=(x'_i,y'_i), src(i)=(x_i, y_i), i=0,1,2\\\\]`\n * \n * [warpAffine](#da/d54/group__imgproc__transform_1ga0203d9ee5fcd28d40dbc4a1ea4451983}),\n * [transform](#d2/de8/group__core__array_1ga393164aa54bb9169ce0a8cc44e08ff22})\n * \n * @param src Coordinates of triangle vertices in the source image.\n * \n * @param dst Coordinates of the corresponding triangle vertices in the destination image.\n */\nexport declare function getAffineTransform(src: any, dst: any): Mat\n\nexport declare function getAffineTransform(src: InputArray, dst: InputArray): Mat\n\n/**\n * The function calculates the `$3 \\\\times 3$` matrix of a perspective transform so that:\n * \n * `\\\\[\\\\begin{bmatrix} t_i x'_i \\\\\\\\ t_i y'_i \\\\\\\\ t_i \\\\end{bmatrix} = \\\\texttt{map_matrix} \\\\cdot\n * \\\\begin{bmatrix} x_i \\\\\\\\ y_i \\\\\\\\ 1 \\\\end{bmatrix}\\\\]`\n * \n * where\n * \n * `\\\\[dst(i)=(x'_i,y'_i), src(i)=(x_i, y_i), i=0,1,2,3\\\\]`\n * \n * [findHomography](#d9/d0c/group__calib3d_1ga4abc2ece9fab9398f2e560d53c8c9780}),\n * [warpPerspective](#da/d54/group__imgproc__transform_1gaf73673a7e8e18ec6963e3774e6a94b87}),\n * [perspectiveTransform](#d2/de8/group__core__array_1gad327659ac03e5fd6894b90025e6900a7})\n * \n * @param src Coordinates of quadrangle vertices in the source image.\n * \n * @param dst Coordinates of the corresponding quadrangle vertices in the destination image.\n * \n * @param solveMethod method passed to cv::solve (DecompTypes)\n */\nexport declare function getPerspectiveTransform(src: InputArray, dst: InputArray, solveMethod?: int): Mat\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function getPerspectiveTransform(src: any, dst: any, solveMethod?: int): Mat\n\n/**\n * The function getRectSubPix extracts pixels from src:\n * \n * `\\\\[patch(x, y) = src(x + \\\\texttt{center.x} - ( \\\\texttt{dst.cols} -1)*0.5, y + \\\\texttt{center.y}\n * - ( \\\\texttt{dst.rows} -1)*0.5)\\\\]`\n * \n * where the values of the pixels at non-integer coordinates are retrieved using bilinear\n * interpolation. Every channel of multi-channel images is processed independently. Also the image\n * should be a single channel or three channel image. While the center of the rectangle must be inside\n * the image, parts of the rectangle may be outside.\n * \n * [warpAffine](#da/d54/group__imgproc__transform_1ga0203d9ee5fcd28d40dbc4a1ea4451983}),\n * [warpPerspective](#da/d54/group__imgproc__transform_1gaf73673a7e8e18ec6963e3774e6a94b87})\n * \n * @param image Source image.\n * \n * @param patchSize Size of the extracted patch.\n * \n * @param center Floating point coordinates of the center of the extracted rectangle within the source\n * image. The center must be inside the image.\n * \n * @param patch Extracted patch that has the size patchSize and the same number of channels as src .\n * \n * @param patchType Depth of the extracted pixels. By default, they have the same depth as src .\n */\nexport declare function getRectSubPix(image: InputArray, patchSize: Size, center: Point2f, patch: OutputArray, patchType?: int): void\n\n/**\n * The function calculates the following matrix:\n * \n * `\\\\[\\\\begin{bmatrix} \\\\alpha & \\\\beta & (1- \\\\alpha ) \\\\cdot \\\\texttt{center.x} - \\\\beta \\\\cdot\n * \\\\texttt{center.y} \\\\\\\\ - \\\\beta & \\\\alpha & \\\\beta \\\\cdot \\\\texttt{center.x} + (1- \\\\alpha ) \\\\cdot\n * \\\\texttt{center.y} \\\\end{bmatrix}\\\\]`\n * \n * where\n * \n * `\\\\[\\\\begin{array}{l} \\\\alpha = \\\\texttt{scale} \\\\cdot \\\\cos \\\\texttt{angle} , \\\\\\\\ \\\\beta =\n * \\\\texttt{scale} \\\\cdot \\\\sin \\\\texttt{angle} \\\\end{array}\\\\]`\n * \n * The transformation maps the rotation center to itself. If this is not the target, adjust the shift.\n * \n * [getAffineTransform](#da/d54/group__imgproc__transform_1ga8f6d378f9f8eebb5cb55cd3ae295a999}),\n * [warpAffine](#da/d54/group__imgproc__transform_1ga0203d9ee5fcd28d40dbc4a1ea4451983}),\n * [transform](#d2/de8/group__core__array_1ga393164aa54bb9169ce0a8cc44e08ff22})\n * \n * @param center Center of the rotation in the source image.\n * \n * @param angle Rotation angle in degrees. Positive values mean counter-clockwise rotation (the\n * coordinate origin is assumed to be the top-left corner).\n * \n * @param scale Isotropic scale factor.\n */\nexport declare function getRotationMatrix2D(center: Point2f, angle: double, scale: double): Mat\n\n/**\n * The function computes an inverse affine transformation represented by `$2 \\\\times 3$` matrix M:\n * \n * `\\\\[\\\\begin{bmatrix} a_{11} & a_{12} & b_1 \\\\\\\\ a_{21} & a_{22} & b_2 \\\\end{bmatrix}\\\\]`\n * \n * The result is also a `$2 \\\\times 3$` matrix of the same type as M.\n * \n * @param M Original affine transformation.\n * \n * @param iM Output reverse affine transformation.\n */\nexport declare function invertAffineTransform(M: InputArray, iM: OutputArray): void\n\nexport declare function linearPolar(src: InputArray, dst: OutputArray, center: Point2f, maxRadius: double, flags: int): void\n\nexport declare function logPolar(src: InputArray, dst: OutputArray, center: Point2f, M: double, flags: int): void\n\n/**\n * The function remap transforms the source image using the specified map:\n * \n * `\\\\[\\\\texttt{dst} (x,y) = \\\\texttt{src} (map_x(x,y),map_y(x,y))\\\\]`\n * \n * where values of pixels with non-integer coordinates are computed using one of available\n * interpolation methods. `$map_x$` and `$map_y$` can be encoded as separate floating-point maps in\n * `$map_1$` and `$map_2$` respectively, or interleaved floating-point maps of `$(x,y)$` in `$map_1$`,\n * or fixed-point maps created by using convertMaps. The reason you might want to convert from floating\n * to fixed-point representations of a map is that they can yield much faster (2x) remapping\n * operations. In the converted case, `$map_1$` contains pairs (cvFloor(x), cvFloor(y)) and `$map_2$`\n * contains indices in a table of interpolation coefficients.\n * \n * This function cannot operate in-place.\n * \n * Due to current implementation limitations the size of an input and output images should be less than\n * 32767x32767.\n * \n * @param src Source image.\n * \n * @param dst Destination image. It has the same size as map1 and the same type as src .\n * \n * @param map1 The first map of either (x,y) points or just x values having the type CV_16SC2 ,\n * CV_32FC1, or CV_32FC2. See convertMaps for details on converting a floating point representation to\n * fixed-point for speed.\n * \n * @param map2 The second map of y values having the type CV_16UC1, CV_32FC1, or none (empty map if\n * map1 is (x,y) points), respectively.\n * \n * @param interpolation Interpolation method (see InterpolationFlags). The method INTER_AREA is not\n * supported by this function.\n * \n * @param borderMode Pixel extrapolation method (see BorderTypes). When borderMode=BORDER_TRANSPARENT,\n * it means that the pixels in the destination image that corresponds to the \"outliers\" in the source\n * image are not modified by the function.\n * \n * @param borderValue Value used in case of a constant border. By default, it is 0.\n */\nexport declare function remap(src: InputArray, dst: OutputArray, map1: InputArray, map2: InputArray, interpolation: int, borderMode?: int, borderValue?: any): void\n\n/**\n * The function resize resizes the image src down to or up to the specified size. Note that the initial\n * dst type or size are not taken into account. Instead, the size and type are derived from the\n * `src`,`dsize`,`fx`, and `fy`. If you want to resize src so that it fits the pre-created dst, you may\n * call the function as follows: \n * \n * ```cpp\n * // explicitly specify dsize=dst.size(); fx and fy will be computed from that.\n * resize(src, dst, dst.size(), 0, 0, interpolation);\n * ```\n * \n *  If you want to decimate the image by factor of 2 in each direction, you can call the function this\n * way: \n * \n * ```cpp\n * // specify fx and fy and let the function compute the destination image size.\n * resize(src, dst, Size(), 0.5, 0.5, interpolation);\n * ```\n * \n *  To shrink an image, it will generally look best with\n * [INTER_AREA](#da/d54/group__imgproc__transform_1gga5bb5a1fea74ea38e1a5445ca803ff121acf959dca2480cc694ca016b81b442ceb})\n * interpolation, whereas to enlarge an image, it will generally look best with c::INTER_CUBIC (slow)\n * or\n * [INTER_LINEAR](#da/d54/group__imgproc__transform_1gga5bb5a1fea74ea38e1a5445ca803ff121ac97d8e4880d8b5d509e96825c7522deb})\n * (faster but still looks OK).\n * \n * [warpAffine](#da/d54/group__imgproc__transform_1ga0203d9ee5fcd28d40dbc4a1ea4451983}),\n * [warpPerspective](#da/d54/group__imgproc__transform_1gaf73673a7e8e18ec6963e3774e6a94b87}),\n * [remap](#da/d54/group__imgproc__transform_1gab75ef31ce5cdfb5c44b6da5f3b908ea4})\n * \n * @param src input image.\n * \n * @param dst output image; it has the size dsize (when it is non-zero) or the size computed from\n * src.size(), fx, and fy; the type of dst is the same as of src.\n * \n * @param dsize output image size; if it equals zero, it is computed as: \\[\\texttt{dsize =\n * Size(round(fx*src.cols), round(fy*src.rows))}\\] Either dsize or both fx and fy must be non-zero.\n * \n * @param fx scale factor along the horizontal axis; when it equals 0, it is computed as\n * \\[\\texttt{(double)dsize.width/src.cols}\\]\n * \n * @param fy scale factor along the vertical axis; when it equals 0, it is computed as\n * \\[\\texttt{(double)dsize.height/src.rows}\\]\n * \n * @param interpolation interpolation method, see InterpolationFlags\n */\nexport declare function resize(src: InputArray, dst: OutputArray, dsize: Size, fx?: double, fy?: double, interpolation?: int): void\n\n/**\n * The function warpAffine transforms the source image using the specified matrix:\n * \n * `\\\\[\\\\texttt{dst} (x,y) = \\\\texttt{src} ( \\\\texttt{M} _{11} x + \\\\texttt{M} _{12} y + \\\\texttt{M}\n * _{13}, \\\\texttt{M} _{21} x + \\\\texttt{M} _{22} y + \\\\texttt{M} _{23})\\\\]`\n * \n * when the flag\n * [WARP_INVERSE_MAP](#da/d54/group__imgproc__transform_1gga5bb5a1fea74ea38e1a5445ca803ff121aa48be1c433186c4eae1ea86aa0ca75ba})\n * is set. Otherwise, the transformation is first inverted with\n * [invertAffineTransform](#da/d54/group__imgproc__transform_1ga57d3505a878a7e1a636645727ca08f51}) and\n * then put in the formula above instead of M. The function cannot operate in-place.\n * \n * [warpPerspective](#da/d54/group__imgproc__transform_1gaf73673a7e8e18ec6963e3774e6a94b87}),\n * [resize](#da/d54/group__imgproc__transform_1ga47a974309e9102f5f08231edc7e7529d}),\n * [remap](#da/d54/group__imgproc__transform_1gab75ef31ce5cdfb5c44b6da5f3b908ea4}),\n * [getRectSubPix](#da/d54/group__imgproc__transform_1ga77576d06075c1a4b6ba1a608850cd614}),\n * [transform](#d2/de8/group__core__array_1ga393164aa54bb9169ce0a8cc44e08ff22})\n * \n * @param src input image.\n * \n * @param dst output image that has the size dsize and the same type as src .\n * \n * @param M $2\\times 3$ transformation matrix.\n * \n * @param dsize size of the output image.\n * \n * @param flags combination of interpolation methods (see InterpolationFlags) and the optional flag\n * WARP_INVERSE_MAP that means that M is the inverse transformation (\n * $\\texttt{dst}\\rightarrow\\texttt{src}$ ).\n * \n * @param borderMode pixel extrapolation method (see BorderTypes); when borderMode=BORDER_TRANSPARENT,\n * it means that the pixels in the destination image corresponding to the \"outliers\" in the source\n * image are not modified by the function.\n * \n * @param borderValue value used in case of a constant border; by default, it is 0.\n */\nexport declare function warpAffine(src: InputArray, dst: OutputArray, M: InputArray, dsize: Size, flags?: int, borderMode?: int, borderValue?: any): void\n\n/**\n * The function warpPerspective transforms the source image using the specified matrix:\n * \n * `\\\\[\\\\texttt{dst} (x,y) = \\\\texttt{src} \\\\left ( \\\\frac{M_{11} x + M_{12} y + M_{13}}{M_{31} x +\n * M_{32} y + M_{33}} , \\\\frac{M_{21} x + M_{22} y + M_{23}}{M_{31} x + M_{32} y + M_{33}} \\\\right\n * )\\\\]`\n * \n * when the flag\n * [WARP_INVERSE_MAP](#da/d54/group__imgproc__transform_1gga5bb5a1fea74ea38e1a5445ca803ff121aa48be1c433186c4eae1ea86aa0ca75ba})\n * is set. Otherwise, the transformation is first inverted with invert and then put in the formula\n * above instead of M. The function cannot operate in-place.\n * \n * [warpAffine](#da/d54/group__imgproc__transform_1ga0203d9ee5fcd28d40dbc4a1ea4451983}),\n * [resize](#da/d54/group__imgproc__transform_1ga47a974309e9102f5f08231edc7e7529d}),\n * [remap](#da/d54/group__imgproc__transform_1gab75ef31ce5cdfb5c44b6da5f3b908ea4}),\n * [getRectSubPix](#da/d54/group__imgproc__transform_1ga77576d06075c1a4b6ba1a608850cd614}),\n * [perspectiveTransform](#d2/de8/group__core__array_1gad327659ac03e5fd6894b90025e6900a7})\n * \n * @param src input image.\n * \n * @param dst output image that has the size dsize and the same type as src .\n * \n * @param M $3\\times 3$ transformation matrix.\n * \n * @param dsize size of the output image.\n * \n * @param flags combination of interpolation methods (INTER_LINEAR or INTER_NEAREST) and the optional\n * flag WARP_INVERSE_MAP, that sets M as the inverse transformation (\n * $\\texttt{dst}\\rightarrow\\texttt{src}$ ).\n * \n * @param borderMode pixel extrapolation method (BORDER_CONSTANT or BORDER_REPLICATE).\n * \n * @param borderValue value used in case of a constant border; by default, it equals 0.\n */\nexport declare function warpPerspective(src: InputArray, dst: OutputArray, M: InputArray, dsize: Size, flags?: int, borderMode?: int, borderValue?: any): void\n\n/**\n * <a name=\"da/d54/group__imgproc__transform_1polar_remaps_reference_image\"></a>\n *  Transform the source image using the following transformation: `\\\\[ dst(\\\\rho , \\\\phi ) = src(x,y)\n * \\\\]`\n * \n * where `\\\\[ \\\\begin{array}{l} \\\\vec{I} = (x - center.x, \\\\;y - center.y) \\\\\\\\ \\\\phi = Kangle \\\\cdot\n * \\\\texttt{angle} (\\\\vec{I}) \\\\\\\\ \\\\rho = \\\\left\\\\{\\\\begin{matrix} Klin \\\\cdot \\\\texttt{magnitude}\n * (\\\\vec{I}) & default \\\\\\\\ Klog \\\\cdot log_e(\\\\texttt{magnitude} (\\\\vec{I})) & if \\\\; semilog \\\\\\\\\n * \\\\end{matrix}\\\\right. \\\\end{array} \\\\]`\n * \n * and `\\\\[ \\\\begin{array}{l} Kangle = dsize.height / 2\\\\Pi \\\\\\\\ Klin = dsize.width / maxRadius \\\\\\\\\n * Klog = dsize.width / log_e(maxRadius) \\\\\\\\ \\\\end{array} \\\\]`\n * \n * Polar mapping can be linear or semi-log. Add one of\n * [WarpPolarMode](#da/d54/group__imgproc__transform_1ga066c91770d0dea54cc1a018ce0344485}) to `flags`\n * to specify the polar mapping mode.\n * \n * Linear is the default mode.\n * \n * The semilog mapping emulates the human \"foveal\" vision that permit very high acuity on the line of\n * sight (central vision) in contrast to peripheral vision where acuity is minor.\n * \n * if both values in `dsize <=0` (default), the destination image will have (almost) same area of\n * source bounding circle: `\\\\[\\\\begin{array}{l} dsize.area \\\\leftarrow (maxRadius^2 \\\\cdot \\\\Pi) \\\\\\\\\n * dsize.width = \\\\texttt{cvRound}(maxRadius) \\\\\\\\ dsize.height = \\\\texttt{cvRound}(maxRadius \\\\cdot\n * \\\\Pi) \\\\\\\\ \\\\end{array}\\\\]`\n * if only `dsize.height <= 0`, the destination image area will be proportional to the bounding circle\n * area but scaled by `Kx * Kx`: `\\\\[\\\\begin{array}{l} dsize.height = \\\\texttt{cvRound}(dsize.width\n * \\\\cdot \\\\Pi) \\\\\\\\ \\\\end{array} \\\\]`\n * if both values in `dsize > 0`, the destination image will have the given size therefore the area of\n * the bounding circle will be scaled to `dsize`.\n * \n * You can get reverse mapping adding\n * [WARP_INVERSE_MAP](#da/d54/group__imgproc__transform_1gga5bb5a1fea74ea38e1a5445ca803ff121aa48be1c433186c4eae1ea86aa0ca75ba})\n * to `flags` \n * \n * ```cpp\n *         // direct transform\n *         warpPolar(src, lin_polar_img, Size(),center, maxRadius, flags);                     //\n * linear Polar\n *         warpPolar(src, log_polar_img, Size(),center, maxRadius, flags + WARP_POLAR_LOG);    //\n * semilog Polar\n *         // inverse transform\n *         warpPolar(lin_polar_img, recovered_lin_polar_img, src.size(), center, maxRadius, flags +\n * WARP_INVERSE_MAP);\n *         warpPolar(log_polar_img, recovered_log_polar, src.size(), center, maxRadius, flags +\n * WARP_POLAR_LOG + WARP_INVERSE_MAP);\n * ```\n * \n *  In addiction, to calculate the original coordinate from a polar mapped coordinate `$(rho, phi)->(x,\n * y)$`: \n * \n * ```cpp\n *         double angleRad, magnitude;\n *         double Kangle = dst.rows / CV_2PI;\n *         angleRad = phi / Kangle;\n *         if (flags & WARP_POLAR_LOG)\n *         {\n *             double Klog = dst.cols / std::log(maxRadius);\n *             magnitude = std::exp(rho / Klog);\n *         }\n *         else\n *         {\n *             double Klin = dst.cols / maxRadius;\n *             magnitude = rho / Klin;\n *         }\n *         int x = cvRound(center.x + magnitude * cos(angleRad));\n *         int y = cvRound(center.y + magnitude * sin(angleRad));\n * ```\n * \n * The function can not operate in-place.\n * To calculate magnitude and angle in degrees\n * [cartToPolar](#d2/de8/group__core__array_1gac5f92f48ec32cacf5275969c33ee837d}) is used internally\n * thus angles are measured from 0 to 360 with accuracy about 0.3 degrees.\n * This function uses [remap](#da/d54/group__imgproc__transform_1gab75ef31ce5cdfb5c44b6da5f3b908ea4}).\n * Due to current implementation limitations the size of an input and output images should be less than\n * 32767x32767.\n * \n * [cv::remap](#da/d54/group__imgproc__transform_1gab75ef31ce5cdfb5c44b6da5f3b908ea4})\n * \n * @param src Source image.\n * \n * @param dst Destination image. It will have same type as src.\n * \n * @param dsize The destination image size (see description for valid options).\n * \n * @param center The transformation center.\n * \n * @param maxRadius The radius of the bounding circle to transform. It determines the inverse magnitude\n * scale parameter too.\n * \n * @param flags A combination of interpolation methods, InterpolationFlags + WarpPolarMode.\n * Add WARP_POLAR_LINEAR to select linear polar mapping (default)Add WARP_POLAR_LOG to select semilog\n * polar mappingAdd WARP_INVERSE_MAP for reverse mapping.\n */\nexport declare function warpPolar(src: InputArray, dst: OutputArray, dsize: Size, center: Point2f, maxRadius: double, flags: int): void\n\n/**\n * nearest neighbor interpolation\n * \n */\nexport declare const INTER_NEAREST: InterpolationFlags // initializer: = 0\n\n/**\n * bilinear interpolation\n * \n */\nexport declare const INTER_LINEAR: InterpolationFlags // initializer: = 1\n\n/**\n * bicubic interpolation\n * \n */\nexport declare const INTER_CUBIC: InterpolationFlags // initializer: = 2\n\n/**\n * resampling using pixel area relation. It may be a preferred method for image decimation, as it gives\n * moire'-free results. But when the image is zoomed, it is similar to the INTER_NEAREST method.\n * \n */\nexport declare const INTER_AREA: InterpolationFlags // initializer: = 3\n\n/**\n * Lanczos interpolation over 8x8 neighborhood\n * \n */\nexport declare const INTER_LANCZOS4: InterpolationFlags // initializer: = 4\n\n/**\n * Bit exact bilinear interpolation\n * \n */\nexport declare const INTER_LINEAR_EXACT: InterpolationFlags // initializer: = 5\n\n/**\n * mask for interpolation codes\n * \n */\nexport declare const INTER_MAX: InterpolationFlags // initializer: = 7\n\n/**\n * flag, fills all of the destination image pixels. If some of them correspond to outliers in the\n * source image, they are set to zero\n * \n */\nexport declare const WARP_FILL_OUTLIERS: InterpolationFlags // initializer: = 8\n\n/**\n * flag, inverse transformation\n * \n * For example, [linearPolar](#da/d54/group__imgproc__transform_1gaa38a6884ac8b6e0b9bed47939b5362f3})\n * or [logPolar](#da/d54/group__imgproc__transform_1gaec3a0b126a85b5ca2c667b16e0ae022d}) transforms:\n * \n * flag is **not** set: `$dst( \\\\rho , \\\\phi ) = src(x,y)$`\n * flag is set: `$dst(x,y) = src( \\\\rho , \\\\phi )$`\n * \n */\nexport declare const WARP_INVERSE_MAP: InterpolationFlags // initializer: = 16\n\nexport declare const INTER_BITS: InterpolationMasks // initializer: = 5\n\nexport declare const INTER_BITS2: InterpolationMasks // initializer: = INTER_BITS * 2\n\nexport declare const INTER_TAB_SIZE: InterpolationMasks // initializer: = 1 << INTER_BITS\n\nexport declare const INTER_TAB_SIZE2: InterpolationMasks // initializer: = INTER_TAB_SIZE * INTER_TAB_SIZE\n\nexport declare const WARP_POLAR_LINEAR: WarpPolarMode // initializer: = 0\n\nexport declare const WARP_POLAR_LOG: WarpPolarMode // initializer: = 256\n\nexport type InterpolationFlags = any\n\nexport type InterpolationMasks = any\n\nexport type WarpPolarMode = any\n\n"},"node_modules_mirada_dist_src_types_opencv_MatExpr_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_MatExpr_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/MatExpr.d.ts","content":"\nimport { double, int, Mat, MatOp, Scalar } from './_types'\n\n/**\n * <a name=\"d1/d10/classcv_1_1MatExpr_1MatrixExpressions\"></a>This is a list of implemented matrix\n * operations that can be combined in arbitrary complex expressions (here A, B stand for matrices (\n * [Mat](#d3/d63/classcv_1_1Mat}) ), s for a scalar ( Scalar ), alpha for a real-valued scalar ( double\n * )):\n * \n * Addition, subtraction, negation: `A+B`, `A-B`, `A+s`, `A-s`, `s+A`, `s-A`, `-A`\n * Scaling: `A*alpha`\n * Per-element multiplication and division: `A.mul(B)`, `A/B`, `alpha/A`\n * Matrix multiplication: `A*B`\n * Transposition: `A.t()` (means A)\n * Matrix inversion and pseudo-inversion, solving linear systems and least-squares problems:\n * `A.inv([method]) (~ A<sup>-1</sup>)`, `A.inv([method])*B (~ X: AX=B)`\n * Comparison: `A cmpop B`, `A cmpop alpha`, `alpha cmpop A`, where *cmpop* is one of `>`, `>=`, `==`,\n * `!=`, `<=`, `<`. The result of comparison is an 8-bit single channel mask whose elements are set to\n * 255 (if the particular element or pair of elements satisfy the condition) or 0.\n * Bitwise logical operations: `A logicop B`, `A logicop s`, `s logicop A`, `~A`, where *logicop* is\n * one of `&`, `|`, `^`.\n * Element-wise minimum and maximum: `min(A, B)`, `min(A, alpha)`, `max(A, B)`, `max(A, alpha)`\n * Element-wise absolute value: `abs(A)`\n * Cross-product, dot-product: `A.cross(B)`, `A.dot(B)`\n * Any function of matrix or matrices and scalars that returns a matrix or a scalar, such as norm,\n * mean, sum, countNonZero, trace, determinant, repeat, and others.\n * Matrix initializers ( [Mat::eye()](#d3/d63/classcv_1_1Mat_1a2cf9b9acde7a9852542bbc20ef851ed2}),\n * [Mat::zeros()](#d3/d63/classcv_1_1Mat_1a0b57b6a326c8876d944d188a46e0f556}),\n * [Mat::ones()](#d3/d63/classcv_1_1Mat_1a69ae0402d116fc9c71908d8508dc2f09}) ), matrix comma-separated\n * initializers, matrix constructors and operators that extract sub-matrices (see\n * [Mat](#d3/d63/classcv_1_1Mat}) description).\n * Mat_<destination_type>() constructors to cast the result to the proper type. \n * \n * Comma-separated initializers and probably some other operations may require additional explicit\n * Mat() or Mat_<T>() constructor calls to resolve a possible ambiguity.\n * Here are examples of matrix expressions: \n * \n * ```cpp\n * // compute pseudo-inverse of A, equivalent to A.inv(DECOMP_SVD)\n * SVD svd(A);\n * Mat pinvA = svd.vt.t()*Mat::diag(1./svd.w)*svd.u.t();\n * \n * // compute the new vector of parameters in the Levenberg-Marquardt algorithm\n * x -= (A.t()*A + lambda*Mat::eye(A.cols,A.cols,A.type())).inv(DECOMP_CHOLESKY)*(A.t()*err);\n * \n * // sharpen image using \"unsharp mask\" algorithm\n * Mat blurred; double sigma = 1, threshold = 5, amount = 1;\n * GaussianBlur(img, blurred, Size(), sigma, sigma);\n * Mat lowContrastMask = abs(img - blurred) < threshold;\n * Mat sharpened = img*(1+amount) + blurred*(-amount);\n * img.copyTo(sharpened, lowContrastMask);\n * ```\n * \n * Source:\n * [opencv2/core/mat.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/core/mat.hpp#L3557).\n * \n */\nexport declare class MatExpr extends Mat {\n\n  public a: Mat\n\n  public alpha: double\n\n  public b: Mat\n\n  public beta: double\n\n  public c: Mat\n\n  public flags: int\n\n  public op: MatOp\n\n  public s: Scalar\n\n  public constructor()\n\n  public constructor(m: Mat)\n\n  public constructor(_op: MatOp, _flags: int, _a?: Mat, _b?: Mat, _c?: Mat, _alpha?: double, _beta?: double, _s?: Scalar)\n\n  public col(x: int): MatExpr\n\n  public cross(m: Mat): Mat\n\n  public diag(d?: int): MatExpr\n\n  public dot(m: Mat): Mat\n\n  public inv(method?: int): MatExpr\n\n  public mul(e: MatExpr, scale?: double): MatExpr\n\n  public mul(m: Mat, scale?: double): MatExpr\n\n  public row(y: int): MatExpr\n  public t(): MatExpr\n\n  public type(): int\n}\n\n"},"node_modules_mirada_dist_src_types_opencv_MatOp_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_MatOp_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/MatOp.d.ts","content":"\nimport { double, int, Mat, MatExpr, Scalar, Size } from './_types'\n\nexport declare class MatOp {\n\n  public constructor()\n\n  public abs(expr: MatExpr, res: MatExpr): MatExpr\n\n  public add(expr1: MatExpr, expr2: MatExpr, res: MatExpr): MatExpr\n\n  public add(expr1: MatExpr, s: Scalar, res: MatExpr): MatExpr\n\n  public assign(expr: MatExpr, m: Mat, type?: int): MatExpr\n\n  public augAssignAdd(expr: MatExpr, m: Mat): MatExpr\n\n  public augAssignAnd(expr: MatExpr, m: Mat): MatExpr\n\n  public augAssignDivide(expr: MatExpr, m: Mat): MatExpr\n\n  public augAssignMultiply(expr: MatExpr, m: Mat): MatExpr\n\n  public augAssignOr(expr: MatExpr, m: Mat): MatExpr\n\n  public augAssignSubtract(expr: MatExpr, m: Mat): MatExpr\n\n  public augAssignXor(expr: MatExpr, m: Mat): MatExpr\n\n  public diag(expr: MatExpr, d: int, res: MatExpr): MatExpr\n\n  public divide(expr1: MatExpr, expr2: MatExpr, res: MatExpr, scale?: double): MatExpr\n\n  public divide(s: double, expr: MatExpr, res: MatExpr): MatExpr\n\n  public elementWise(expr: MatExpr): MatExpr\n\n  public invert(expr: MatExpr, method: int, res: MatExpr): MatExpr\n\n  public matmul(expr1: MatExpr, expr2: MatExpr, res: MatExpr): MatExpr\n\n  public multiply(expr1: MatExpr, expr2: MatExpr, res: MatExpr, scale?: double): MatExpr\n\n  public multiply(expr1: MatExpr, s: double, res: MatExpr): MatExpr\n\n  public roi(expr: MatExpr, rowRange: Range, colRange: Range, res: MatExpr): MatExpr\n\n  public size(expr: MatExpr): Size\n\n  public subtract(expr1: MatExpr, expr2: MatExpr, res: MatExpr): MatExpr\n\n  public subtract(s: Scalar, expr: MatExpr, res: MatExpr): Scalar\n\n  public transpose(expr: MatExpr, res: MatExpr): MatExpr\n\n  public type(expr: MatExpr): MatExpr\n}\n\n"},"node_modules_mirada_dist_src_types_opencv_Matx_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_Matx_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/Matx.d.ts","content":"\nimport { diag_type, int, Matx_AddOp, Matx_DivOp, Matx_MatMulOp, Matx_MulOp, Matx_ScaleOp, Matx_SubOp, Matx_TOp, Vec, _T2, _Tp } from './_types'\n\n/**\n * If you need a more flexible type, use [Mat](#d3/d63/classcv_1_1Mat}) . The elements of the matrix M\n * are accessible using the M(i,j) notation. Most of the common matrix operations (see also\n * [MatrixExpressions](#d1/d10/classcv_1_1MatExpr_1MatrixExpressions}) ) are available. To do an\n * operation on [Matx](#de/de1/classcv_1_1Matx}) that is not implemented, you can easily convert the\n * matrix to [Mat](#d3/d63/classcv_1_1Mat}) and backwards: \n * \n * ```cpp\n * Matx33f m(1, 2, 3,\n *           4, 5, 6,\n *           7, 8, 9);\n * cout << sum(Mat(m*m.t())) << endl;\n * ```\n * \n *  Except of the plain constructor which takes a list of elements, [Matx](#de/de1/classcv_1_1Matx})\n * can be initialized from a C-array: \n * \n * ```cpp\n * float values[] = { 1, 2, 3};\n * Matx31f m(values);\n * ```\n * \n *  In case if C++11 features are available, std::initializer_list can be also used to initialize\n * [Matx](#de/de1/classcv_1_1Matx}): \n * \n * ```cpp\n * Matx31f m = { 1, 2, 3};\n * ```\n * \n * Source:\n * [opencv2/core/matx.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/core/matx.hpp#L1185).\n * \n */\nexport declare class Matx {\n\n  public val: _Tp\n\n  public constructor()\n\n  public constructor(v0: _Tp)\n\n  public constructor(v0: _Tp, v1: _Tp)\n\n  public constructor(v0: _Tp, v1: _Tp, v2: _Tp)\n\n  public constructor(v0: _Tp, v1: _Tp, v2: _Tp, v3: _Tp)\n\n  public constructor(v0: _Tp, v1: _Tp, v2: _Tp, v3: _Tp, v4: _Tp)\n\n  public constructor(v0: _Tp, v1: _Tp, v2: _Tp, v3: _Tp, v4: _Tp, v5: _Tp)\n\n  public constructor(v0: _Tp, v1: _Tp, v2: _Tp, v3: _Tp, v4: _Tp, v5: _Tp, v6: _Tp)\n\n  public constructor(v0: _Tp, v1: _Tp, v2: _Tp, v3: _Tp, v4: _Tp, v5: _Tp, v6: _Tp, v7: _Tp)\n\n  public constructor(v0: _Tp, v1: _Tp, v2: _Tp, v3: _Tp, v4: _Tp, v5: _Tp, v6: _Tp, v7: _Tp, v8: _Tp)\n\n  public constructor(v0: _Tp, v1: _Tp, v2: _Tp, v3: _Tp, v4: _Tp, v5: _Tp, v6: _Tp, v7: _Tp, v8: _Tp, v9: _Tp)\n\n  public constructor(v0: _Tp, v1: _Tp, v2: _Tp, v3: _Tp, v4: _Tp, v5: _Tp, v6: _Tp, v7: _Tp, v8: _Tp, v9: _Tp, v10: _Tp, v11: _Tp)\n\n  public constructor(v0: _Tp, v1: _Tp, v2: _Tp, v3: _Tp, v4: _Tp, v5: _Tp, v6: _Tp, v7: _Tp, v8: _Tp, v9: _Tp, v10: _Tp, v11: _Tp, v12: _Tp, v13: _Tp)\n\n  public constructor(v0: _Tp, v1: _Tp, v2: _Tp, v3: _Tp, v4: _Tp, v5: _Tp, v6: _Tp, v7: _Tp, v8: _Tp, v9: _Tp, v10: _Tp, v11: _Tp, v12: _Tp, v13: _Tp, v14: _Tp, v15: _Tp)\n\n  public constructor(vals: any)\n\n  public constructor(arg334: any)\n\n  public constructor(a: Matx, b: Matx, arg335: Matx_AddOp)\n\n  public constructor(a: Matx, b: Matx, arg336: Matx_SubOp)\n\n  public constructor(arg337: any, a: Matx, alpha: _T2, arg338: Matx_ScaleOp)\n\n  public constructor(a: Matx, b: Matx, arg339: Matx_MulOp)\n\n  public constructor(a: Matx, b: Matx, arg340: Matx_DivOp)\n\n  public constructor(l: int, a: Matx, b: Matx, arg341: Matx_MatMulOp)\n\n  public constructor(a: Matx, arg342: Matx_TOp)\n\n  public col(i: int): Matx\n\n  public ddot(v: Matx): Matx\n\n  public diag(): diag_type\n\n  public div(a: Matx): Matx\n\n  public dot(v: Matx): Matx\n\n  public get_minor(m1: int, n1: int, base_row: int, base_col: int): Matx\n\n  public inv(method?: int, p_is_ok?: any): Matx\n\n  public mul(a: Matx): Matx\n\n  public reshape(m1: int, n1: int): Matx\n\n  public row(i: int): Matx\n\n  public solve(l: int, rhs: Matx, flags?: int): Matx\n\n  public solve(rhs: Vec, method: int): Vec\n\n  public t(): Matx\n\n  public static all(alpha: _Tp): Matx\n\n  public static diag(d: diag_type): Matx\n\n  public static eye(): Matx\n\n  public static ones(): Matx\n\n  public static randn(a: _Tp, b: _Tp): Matx\n\n  public static randu(a: _Tp, b: _Tp): Matx\n\n  public static zeros(): Matx\n}\n\nexport declare const rows: any // initializer: = m\n\nexport declare const cols: any // initializer: = n\n\nexport declare const channels: any // initializer: = rows*cols\n\nexport declare const shortdim: any // initializer: = (m < n ? m : n)\n\n"},"node_modules_mirada_dist_src_types_opencv_Mat_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_Mat_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/Mat.d.ts","content":"\nimport { AccessFlag, bool, double, InputArray, int, MatAllocator, MatCommaInitializer_, MatConstIterator_, MatExpr, MatIterator_, MatSize, MatStep, Matx, Mat_, OutputArray, Point, Point3_, Point_, Rect, Scalar, Size, size_t, typename, uchar, UMat, UMatData, UMatUsageFlags, Vec } from './_types'\n\n/**\n * <a name=\"d3/d63/classcv_1_1Mat_1CVMat_Details\"></a> The class [Mat](#d3/d63/classcv_1_1Mat})\n * represents an n-dimensional dense numerical single-channel or multi-channel array. It can be used to\n * store real or complex-valued vectors and matrices, grayscale or color images, voxel volumes, vector\n * fields, point clouds, tensors, histograms (though, very high-dimensional histograms may be better\n * stored in a [SparseMat](#dd/da9/classcv_1_1SparseMat}) ). The data layout of the array `M` is\n * defined by the array `M.step[]`, so that the address of element `$(i_0,...,i_{M.dims-1})$`, where\n * `$0\\\\leq i_k<M.size[k]$`, is computed as: `\\\\[addr(M_{i_0,...,i_{M.dims-1}}) = M.data +\n * M.step[0]*i_0 + M.step[1]*i_1 + ... + M.step[M.dims-1]*i_{M.dims-1}\\\\]` In case of a 2-dimensional\n * array, the above formula is reduced to: `\\\\[addr(M_{i,j}) = M.data + M.step[0]*i + M.step[1]*j\\\\]`\n * Note that `M.step[i] >= M.step[i+1]` (in fact, `M.step[i] >= M.step[i+1]*M.size[i+1]` ). This means\n * that 2-dimensional matrices are stored row-by-row, 3-dimensional matrices are stored plane-by-plane,\n * and so on. M.step[M.dims-1] is minimal and always equal to the element size M.elemSize() .\n * \n * So, the data layout in [Mat](#d3/d63/classcv_1_1Mat}) is compatible with the majority of dense array\n * types from the standard toolkits and SDKs, such as Numpy (ndarray), Win32 (independent device\n * bitmaps), and others, that is, with any array that uses *steps* (or *strides*) to compute the\n * position of a pixel. Due to this compatibility, it is possible to make a\n * [Mat](#d3/d63/classcv_1_1Mat}) header for user-allocated data and process it in-place using OpenCV\n * functions.\n * \n * There are many different ways to create a [Mat](#d3/d63/classcv_1_1Mat}) object. The most popular\n * options are listed below:\n * \n * Use the create(nrows, ncols, type) method or the similar Mat(nrows, ncols, type[, fillValue])\n * constructor. A new array of the specified size and type is allocated. type has the same meaning as\n * in the cvCreateMat method. For example, CV_8UC1 means a 8-bit single-channel array, CV_32FC2 means a\n * 2-channel (complex) floating-point array, and so on. \n * \n * ```cpp\n * // make a 7x7 complex matrix filled with 1+3j.\n * Mat M(7,7,CV_32FC2,Scalar(1,3));\n * // and now turn M to a 100x60 15-channel 8-bit matrix.\n * // The old content will be deallocated\n * M.create(100,60,CV_8UC(15));\n * ```\n * \n *  As noted in the introduction to this chapter,\n * [create()](#d3/d63/classcv_1_1Mat_1a55ced2c8d844d683ea9a725c60037ad0}) allocates only a new array\n * when the shape or type of the current array are different from the specified ones.\n * Create a multi-dimensional array: \n * \n * ```cpp\n * // create a 100x100x100 8-bit array\n * int sz[] = {100, 100, 100};\n * Mat bigCube(3, sz, CV_8U, Scalar::all(0));\n * ```\n * \n *  It passes the number of dimensions =1 to the [Mat](#d3/d63/classcv_1_1Mat}) constructor but the\n * created array will be 2-dimensional with the number of columns set to 1. So,\n * [Mat::dims](#d3/d63/classcv_1_1Mat_1a39cf614aa52567e9a945cd2609bd767b}) is always >= 2 (can also be\n * 0 when the array is empty).\n * Use a copy constructor or assignment operator where there can be an array or expression on the right\n * side (see below). As noted in the introduction, the array assignment is an O(1) operation because it\n * only copies the header and increases the reference counter. The\n * [Mat::clone()](#d3/d63/classcv_1_1Mat_1adff2ea98da45eae0833e73582dd4a660}) method can be used to get\n * a full (deep) copy of the array when you need it.\n * Construct a header for a part of another array. It can be a single row, single column, several rows,\n * several columns, rectangular region in the array (called a *minor* in algebra) or a diagonal. Such\n * operations are also O(1) because the new header references the same data. You can actually modify a\n * part of the array using this feature, for example: \n * \n * ```cpp\n * // add the 5-th row, multiplied by 3 to the 3rd row\n * M.row(3) = M.row(3) + M.row(5)*3;\n * // now copy the 7-th column to the 1-st column\n * // M.col(1) = M.col(7); // this will not work\n * Mat M1 = M.col(1);\n * M.col(7).copyTo(M1);\n * // create a new 320x240 image\n * Mat img(Size(320,240),CV_8UC3);\n * // select a ROI\n * Mat roi(img, Rect(10,10,100,100));\n * // fill the ROI with (0,255,0) (which is green in RGB space);\n * // the original 320x240 image will be modified\n * roi = Scalar(0,255,0);\n * ```\n * \n *  Due to the additional datastart and dataend members, it is possible to compute a relative sub-array\n * position in the main *container* array using\n * [locateROI()](#d3/d63/classcv_1_1Mat_1a40b5b3371a9c2a4b2b8ce0c8068d7c96}): \n * \n * ```cpp\n * Mat A = Mat::eye(10, 10, CV_32S);\n * // extracts A columns, 1 (inclusive) to 3 (exclusive).\n * Mat B = A(Range::all(), Range(1, 3));\n * // extracts B rows, 5 (inclusive) to 9 (exclusive).\n * // that is, C \\\\~ A(Range(5, 9), Range(1, 3))\n * Mat C = B(Range(5, 9), Range::all());\n * Size size; Point ofs;\n * C.locateROI(size, ofs);\n * // size will be (width=10,height=10) and the ofs will be (x=1, y=5)\n * ```\n * \n *  As in case of whole matrices, if you need a deep copy, use the\n * `[clone()](#d3/d63/classcv_1_1Mat_1adff2ea98da45eae0833e73582dd4a660})` method of the extracted\n * sub-matrices.\n * Make a header for user-allocated data. It can be useful to do the following:\n * \n * Process \"foreign\" data using OpenCV (for example, when you implement a DirectShow* filter or a\n * processing module for gstreamer, and so on). For example: \n * \n * ```cpp\n * void process_video_frame(const unsigned char* pixels,\n *                          int width, int height, int step)\n * {\n *     Mat img(height, width, CV_8UC3, pixels, step);\n *     GaussianBlur(img, img, Size(7,7), 1.5, 1.5);\n * }\n * ```\n * \n * Quickly initialize small matrices and/or get a super-fast element access. \n * \n * ```cpp\n * double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}};\n * Mat M = Mat(3, 3, CV_64F, m).inv();\n * ```\n * \n * Use MATLAB-style array initializers,\n * [zeros()](#d3/d63/classcv_1_1Mat_1a0b57b6a326c8876d944d188a46e0f556}),\n * [ones()](#d3/d63/classcv_1_1Mat_1a69ae0402d116fc9c71908d8508dc2f09}),\n * [eye()](#d3/d63/classcv_1_1Mat_1a2cf9b9acde7a9852542bbc20ef851ed2}), for example: \n * \n * ```cpp\n * // create a double-precision identity matrix and add it to M.\n * M += Mat::eye(M.rows, M.cols, CV_64F);\n * ```\n * \n * Use a comma-separated initializer: \n * \n * ```cpp\n * // create a 3x3 double-precision identity matrix\n * Mat M = (Mat_<double>(3,3) << 1, 0, 0, 0, 1, 0, 0, 0, 1);\n * ```\n * \n *  With this approach, you first call a constructor of the [Mat](#d3/d63/classcv_1_1Mat}) class with\n * the proper parameters, and then you just put `<< operator` followed by comma-separated values that\n * can be constants, variables, expressions, and so on. Also, note the extra parentheses required to\n * avoid compilation errors.\n * \n * Once the array is created, it is automatically managed via a reference-counting mechanism. If the\n * array header is built on top of user-allocated data, you should handle the data by yourself. The\n * array data is deallocated when no one points to it. If you want to release the data pointed by a\n * array header before the array destructor is called, use\n * [Mat::release()](#d3/d63/classcv_1_1Mat_1ae48d4913285518e2c21a3457017e716e}).\n * \n * The next important thing to learn about the array class is element access. This manual already\n * described how to compute an address of each array element. Normally, you are not required to use the\n * formula directly in the code. If you know the array element type (which can be retrieved using the\n * method [Mat::type()](#d3/d63/classcv_1_1Mat_1af2d2652e552d7de635988f18a84b53e5}) ), you can access\n * the element `$M_{ij}$` of a 2-dimensional array as: \n * \n * ```cpp\n * M.at<double>(i,j) += 1.f;\n * ```\n * \n *  assuming that `M` is a double-precision floating-point array. There are several variants of the\n * method at for a different number of dimensions.\n * \n * If you need to process a whole row of a 2D array, the most efficient way is to get the pointer to\n * the row first, and then just use the plain C operator [] : \n * \n * ```cpp\n * // compute sum of positive matrix elements\n * // (assuming that M is a double-precision matrix)\n * double sum=0;\n * for(int i = 0; i < M.rows; i++)\n * {\n *     const double* Mi = M.ptr<double>(i);\n *     for(int j = 0; j < M.cols; j++)\n *         sum += std::max(Mi[j], 0.);\n * }\n * ```\n * \n *  Some operations, like the one above, do not actually depend on the array shape. They just process\n * elements of an array one by one (or elements from multiple arrays that have the same coordinates,\n * for example, array addition). Such operations are called *element-wise*. It makes sense to check\n * whether all the input/output arrays are continuous, namely, have no gaps at the end of each row. If\n * yes, process them as a long single row: \n * \n * ```cpp\n * // compute the sum of positive matrix elements, optimized variant\n * double sum=0;\n * int cols = M.cols, rows = M.rows;\n * if(M.isContinuous())\n * {\n *     cols *= rows;\n *     rows = 1;\n * }\n * for(int i = 0; i < rows; i++)\n * {\n *     const double* Mi = M.ptr<double>(i);\n *     for(int j = 0; j < cols; j++)\n *         sum += std::max(Mi[j], 0.);\n * }\n * ```\n * \n *  In case of the continuous matrix, the outer loop body is executed just once. So, the overhead is\n * smaller, which is especially noticeable in case of small matrices.\n * \n * Finally, there are STL-style iterators that are smart enough to skip gaps between successive rows: \n * \n * ```cpp\n * // compute sum of positive matrix elements, iterator-based variant\n * double sum=0;\n * MatConstIterator_<double> it = M.begin<double>(), it_end = M.end<double>();\n * for(; it != it_end; ++it)\n *     sum += std::max(*it, 0.);\n * ```\n * \n *  The matrix iterators are random-access iterators, so they can be passed to any STL algorithm,\n * including [std::sort()](#d2/de8/group__core__array_1ga45dd56da289494ce874be2324856898f}).\n * \n * Matrix Expressions and arithmetic see [MatExpr](#d1/d10/classcv_1_1MatExpr})\n * \n * Source:\n * [opencv2/core/mat.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/core/mat.hpp#L2073).\n * \n */\nexport declare class Mat extends Mat_ {\n\n  public allocator: MatAllocator\n\n  public cols: int\n\n  public data: uchar\n\n  public dataend: uchar\n\n  public datalimit: uchar\n\n  public datastart: uchar\n\n  public dims: int\n\n  /**\n   *   includes several bit-fields:\n   *   \n   * the magic signature\n   * continuity flag\n   * depth\n   * number of channels\n   *   \n   */\n  public flags: int\n\n  public rows: int\n\n  public size: MatSize\n\n  public step: MatStep\n\n  public u: UMatData\n\n  /**\n   *   These are various constructors that form a matrix. As noted in the AutomaticAllocation, often the\n   * default constructor is enough, and the proper matrix will be allocated by an OpenCV function. The\n   * constructed matrix can further be assigned to another matrix or matrix expression or can be\n   * allocated with [Mat::create](#d3/d63/classcv_1_1Mat_1a55ced2c8d844d683ea9a725c60037ad0}) . In the\n   * former case, the old content is de-referenced.\n   */\n  public constructor()\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param rows Number of rows in a 2D array.\n   *   \n   *   @param cols Number of columns in a 2D array.\n   *   \n   *   @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),\n   * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.\n   */\n  public constructor(rows: int, cols: int, type: int)\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param size 2D array size: Size(cols, rows) . In the Size() constructor, the number of rows and\n   * the number of columns go in the reverse order.\n   *   \n   *   @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),\n   * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.\n   */\n  public constructor(size: Size, type: int)\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param rows Number of rows in a 2D array.\n   *   \n   *   @param cols Number of columns in a 2D array.\n   *   \n   *   @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),\n   * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.\n   *   \n   *   @param s An optional value to initialize each matrix element with. To set all the matrix elements\n   * to the particular value after the construction, use the assignment operator Mat::operator=(const\n   * Scalar& value) .\n   */\n  public constructor(rows: int, cols: int, type: int, s: Scalar)\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param size 2D array size: Size(cols, rows) . In the Size() constructor, the number of rows and\n   * the number of columns go in the reverse order.\n   *   \n   *   @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),\n   * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.\n   *   \n   *   @param s An optional value to initialize each matrix element with. To set all the matrix elements\n   * to the particular value after the construction, use the assignment operator Mat::operator=(const\n   * Scalar& value) .\n   */\n  public constructor(size: Size, type: int, s: Scalar)\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param ndims Array dimensionality.\n   *   \n   *   @param sizes Array of integers specifying an n-dimensional array shape.\n   *   \n   *   @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),\n   * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.\n   */\n  public constructor(ndims: int, sizes: any, type: int)\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param sizes Array of integers specifying an n-dimensional array shape.\n   *   \n   *   @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),\n   * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.\n   */\n  public constructor(sizes: any, type: int)\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param ndims Array dimensionality.\n   *   \n   *   @param sizes Array of integers specifying an n-dimensional array shape.\n   *   \n   *   @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),\n   * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.\n   *   \n   *   @param s An optional value to initialize each matrix element with. To set all the matrix elements\n   * to the particular value after the construction, use the assignment operator Mat::operator=(const\n   * Scalar& value) .\n   */\n  public constructor(ndims: int, sizes: any, type: int, s: Scalar)\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param sizes Array of integers specifying an n-dimensional array shape.\n   *   \n   *   @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),\n   * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.\n   *   \n   *   @param s An optional value to initialize each matrix element with. To set all the matrix elements\n   * to the particular value after the construction, use the assignment operator Mat::operator=(const\n   * Scalar& value) .\n   */\n  public constructor(sizes: any, type: int, s: Scalar)\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param m Array that (as a whole or partly) is assigned to the constructed matrix. No data is\n   * copied by these constructors. Instead, the header pointing to m data or its sub-array is constructed\n   * and associated with it. The reference counter, if any, is incremented. So, when you modify the\n   * matrix formed using such a constructor, you also modify the corresponding elements of m . If you\n   * want to have an independent copy of the sub-array, use Mat::clone() .\n   */\n  public constructor(m: Mat)\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param rows Number of rows in a 2D array.\n   *   \n   *   @param cols Number of columns in a 2D array.\n   *   \n   *   @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),\n   * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.\n   *   \n   *   @param data Pointer to the user data. Matrix constructors that take data and step parameters do\n   * not allocate matrix data. Instead, they just initialize the matrix header that points to the\n   * specified data, which means that no data is copied. This operation is very efficient and can be used\n   * to process external data using OpenCV functions. The external data is not automatically deallocated,\n   * so you should take care of it.\n   *   \n   *   @param step Number of bytes each matrix row occupies. The value should include the padding bytes\n   * at the end of each row, if any. If the parameter is missing (set to AUTO_STEP ), no padding is\n   * assumed and the actual step is calculated as cols*elemSize(). See Mat::elemSize.\n   */\n  public constructor(rows: int, cols: int, type: int, data: any, step?: size_t)\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param size 2D array size: Size(cols, rows) . In the Size() constructor, the number of rows and\n   * the number of columns go in the reverse order.\n   *   \n   *   @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),\n   * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.\n   *   \n   *   @param data Pointer to the user data. Matrix constructors that take data and step parameters do\n   * not allocate matrix data. Instead, they just initialize the matrix header that points to the\n   * specified data, which means that no data is copied. This operation is very efficient and can be used\n   * to process external data using OpenCV functions. The external data is not automatically deallocated,\n   * so you should take care of it.\n   *   \n   *   @param step Number of bytes each matrix row occupies. The value should include the padding bytes\n   * at the end of each row, if any. If the parameter is missing (set to AUTO_STEP ), no padding is\n   * assumed and the actual step is calculated as cols*elemSize(). See Mat::elemSize.\n   */\n  public constructor(size: Size, type: int, data: any, step?: size_t)\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param ndims Array dimensionality.\n   *   \n   *   @param sizes Array of integers specifying an n-dimensional array shape.\n   *   \n   *   @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),\n   * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.\n   *   \n   *   @param data Pointer to the user data. Matrix constructors that take data and step parameters do\n   * not allocate matrix data. Instead, they just initialize the matrix header that points to the\n   * specified data, which means that no data is copied. This operation is very efficient and can be used\n   * to process external data using OpenCV functions. The external data is not automatically deallocated,\n   * so you should take care of it.\n   *   \n   *   @param steps Array of ndims-1 steps in case of a multi-dimensional array (the last step is always\n   * set to the element size). If not specified, the matrix is assumed to be continuous.\n   */\n  public constructor(ndims: int, sizes: any, type: int, data: any, steps?: any)\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param sizes Array of integers specifying an n-dimensional array shape.\n   *   \n   *   @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),\n   * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.\n   *   \n   *   @param data Pointer to the user data. Matrix constructors that take data and step parameters do\n   * not allocate matrix data. Instead, they just initialize the matrix header that points to the\n   * specified data, which means that no data is copied. This operation is very efficient and can be used\n   * to process external data using OpenCV functions. The external data is not automatically deallocated,\n   * so you should take care of it.\n   *   \n   *   @param steps Array of ndims-1 steps in case of a multi-dimensional array (the last step is always\n   * set to the element size). If not specified, the matrix is assumed to be continuous.\n   */\n  public constructor(sizes: any, type: int, data: any, steps?: any)\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param m Array that (as a whole or partly) is assigned to the constructed matrix. No data is\n   * copied by these constructors. Instead, the header pointing to m data or its sub-array is constructed\n   * and associated with it. The reference counter, if any, is incremented. So, when you modify the\n   * matrix formed using such a constructor, you also modify the corresponding elements of m . If you\n   * want to have an independent copy of the sub-array, use Mat::clone() .\n   *   \n   *   @param rowRange Range of the m rows to take. As usual, the range start is inclusive and the range\n   * end is exclusive. Use Range::all() to take all the rows.\n   *   \n   *   @param colRange Range of the m columns to take. Use Range::all() to take all the columns.\n   */\n  public constructor(m: Mat, rowRange: Range, colRange?: Range)\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param m Array that (as a whole or partly) is assigned to the constructed matrix. No data is\n   * copied by these constructors. Instead, the header pointing to m data or its sub-array is constructed\n   * and associated with it. The reference counter, if any, is incremented. So, when you modify the\n   * matrix formed using such a constructor, you also modify the corresponding elements of m . If you\n   * want to have an independent copy of the sub-array, use Mat::clone() .\n   *   \n   *   @param roi Region of interest.\n   */\n  public constructor(m: Mat, roi: Rect)\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param m Array that (as a whole or partly) is assigned to the constructed matrix. No data is\n   * copied by these constructors. Instead, the header pointing to m data or its sub-array is constructed\n   * and associated with it. The reference counter, if any, is incremented. So, when you modify the\n   * matrix formed using such a constructor, you also modify the corresponding elements of m . If you\n   * want to have an independent copy of the sub-array, use Mat::clone() .\n   *   \n   *   @param ranges Array of selected ranges of m along each dimensionality.\n   */\n  public constructor(m: Mat, ranges: Range)\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param m Array that (as a whole or partly) is assigned to the constructed matrix. No data is\n   * copied by these constructors. Instead, the header pointing to m data or its sub-array is constructed\n   * and associated with it. The reference counter, if any, is incremented. So, when you modify the\n   * matrix formed using such a constructor, you also modify the corresponding elements of m . If you\n   * want to have an independent copy of the sub-array, use Mat::clone() .\n   *   \n   *   @param ranges Array of selected ranges of m along each dimensionality.\n   */\n  public constructor(m: Mat, ranges: Range)\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param vec STL vector whose elements form the matrix. The matrix has a single column and the\n   * number of rows equal to the number of vector elements. Type of the matrix matches the type of vector\n   * elements. The constructor can handle arbitrary types, for which there is a properly declared\n   * DataType . This means that the vector elements must be primitive numbers or uni-type numerical\n   * tuples of numbers. Mixed-type structures are not supported. The corresponding constructor is\n   * explicit. Since STL vectors are not automatically converted to Mat instances, you should write\n   * Mat(vec) explicitly. Unless you copy the data into the matrix ( copyData=true ), no new elements\n   * will be added to the vector because it can potentially yield vector data reallocation, and, thus,\n   * the matrix data pointer will be invalid.\n   *   \n   *   @param copyData Flag to specify whether the underlying data of the STL vector should be copied to\n   * (true) or shared with (false) the newly constructed matrix. When the data is copied, the allocated\n   * buffer is managed using Mat reference counting mechanism. While the data is shared, the reference\n   * counter is NULL, and you should not deallocate the data until the matrix is not destructed.\n   */\n  public constructor(arg3: any, vec: any, copyData?: bool)\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   */\n  public constructor(arg4: any, arg5?: typename, list?: any)\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   */\n  public constructor(arg6: any, sizes: any, list: any)\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   */\n  public constructor(arg7: any, _Nm: size_t, arr: any, copyData?: bool)\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   */\n  public constructor(arg8: any, n: int, vec: Vec, copyData?: bool)\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   */\n  public constructor(arg9: any, m: int, n: int, mtx: Matx, copyData?: bool)\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   */\n  public constructor(arg10: any, pt: Point_, copyData?: bool)\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   */\n  public constructor(arg11: any, pt: Point3_, copyData?: bool)\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   */\n  public constructor(arg12: any, commaInitializer: MatCommaInitializer_)\n\n  public constructor(m: any)\n\n  public constructor(m: Mat)\n\n  /**\n   *   The method increments the reference counter associated with the matrix data. If the matrix header\n   * points to an external data set (see\n   * [Mat::Mat](#d3/d63/classcv_1_1Mat_1af1d014cecd1510cdf580bf2ed7e5aafc}) ), the reference counter is\n   * NULL, and the method has no effect in this case. Normally, to avoid memory leaks, the method should\n   * not be called explicitly. It is called implicitly by the matrix assignment operator. The reference\n   * counter increment is an atomic operation on the platforms that support it. Thus, it is safe to\n   * operate on the same matrices asynchronously in different threads.\n   */\n  public addref(): void\n\n  /**\n   *   The method is complimentary to\n   * [Mat::locateROI](#d3/d63/classcv_1_1Mat_1a40b5b3371a9c2a4b2b8ce0c8068d7c96}) . The typical use of\n   * these functions is to determine the submatrix position within the parent matrix and then shift the\n   * position somehow. Typically, it can be required for filtering operations when pixels outside of the\n   * ROI should be taken into account. When all the method parameters are positive, the ROI needs to grow\n   * in all directions by the specified amount, for example: \n   *   \n   *   ```cpp\n   *   A.adjustROI(2, 2, 2, 2);\n   *   ```\n   *   \n   *    In this example, the matrix size is increased by 4 elements in each direction. The matrix is\n   * shifted by 2 elements to the left and 2 elements up, which brings in all the necessary pixels for\n   * the filtering with the 5x5 kernel.\n   *   \n   *   adjustROI forces the adjusted ROI to be inside of the parent matrix that is boundaries of the\n   * adjusted ROI are constrained by boundaries of the parent matrix. For example, if the submatrix A is\n   * located in the first row of a parent matrix and you called A.adjustROI(2, 2, 2, 2) then A will not\n   * be increased in the upward direction.\n   *   \n   *   The function is used internally by the OpenCV filtering functions, like filter2D , morphological\n   * operations, and so on. \n   *   \n   *   [copyMakeBorder](#d2/de8/group__core__array_1ga2ac1049c2c3dd25c2b41bffe17658a36})\n   *   \n   *   @param dtop Shift of the top submatrix boundary upwards.\n   *   \n   *   @param dbottom Shift of the bottom submatrix boundary downwards.\n   *   \n   *   @param dleft Shift of the left submatrix boundary to the left.\n   *   \n   *   @param dright Shift of the right submatrix boundary to the right.\n   */\n  public adjustROI(dtop: int, dbottom: int, dleft: int, dright: int): Mat\n\n  /**\n   *   This is an internally used method called by the\n   * [MatrixExpressions](#d1/d10/classcv_1_1MatExpr_1MatrixExpressions}) engine.\n   *   \n   *   @param m Destination array.\n   *   \n   *   @param type Desired destination array depth (or -1 if it should be the same as the source type).\n   */\n  public assignTo(m: Mat, type?: int): Mat\n\n  /**\n   *   The template methods return a reference to the specified array element. For the sake of higher\n   * performance, the index range checks are only performed in the Debug configuration.\n   *   \n   *   Note that the variants with a single index (i) can be used to access elements of single-row or\n   * single-column 2-dimensional arrays. That is, if, for example, A is a 1 x N floating-point matrix and\n   * B is an M x 1 integer matrix, you can simply write `A.at<float>(k+4)` and `B.at<int>(2*i+1)` instead\n   * of `A.at<float>(0,k+4)` and `B.at<int>(2*i+1,0)`, respectively.\n   *   \n   *   The example below initializes a Hilbert matrix: \n   *   \n   *   ```cpp\n   *   Mat H(100, 100, CV_64F);\n   *   for(int i = 0; i < H.rows; i++)\n   *       for(int j = 0; j < H.cols; j++)\n   *           H.at<double>(i,j)=1./(i+j+1);\n   *   ```\n   *   \n   *   Keep in mind that the size identifier used in the at operator cannot be chosen at random. It\n   * depends on the image from which you are trying to retrieve the data. The table below gives a better\n   * insight in this:\n   *   \n   * If matrix is of type `CV_8U` then use\n   * `[Mat.at](#d3/d63/classcv_1_1Mat_1aa5d20fc86d41d59e4d71ae93daee9726})<uchar>(y,x)`.\n   * If matrix is of type `CV_8S` then use\n   * `[Mat.at](#d3/d63/classcv_1_1Mat_1aa5d20fc86d41d59e4d71ae93daee9726})<schar>(y,x)`.\n   * If matrix is of type `CV_16U` then use\n   * `[Mat.at](#d3/d63/classcv_1_1Mat_1aa5d20fc86d41d59e4d71ae93daee9726})<ushort>(y,x)`.\n   * If matrix is of type `CV_16S` then use\n   * `[Mat.at](#d3/d63/classcv_1_1Mat_1aa5d20fc86d41d59e4d71ae93daee9726})<short>(y,x)`.\n   * If matrix is of type `CV_32S` then use\n   * `[Mat.at](#d3/d63/classcv_1_1Mat_1aa5d20fc86d41d59e4d71ae93daee9726})<int>(y,x)`.\n   * If matrix is of type `CV_32F` then use\n   * `[Mat.at](#d3/d63/classcv_1_1Mat_1aa5d20fc86d41d59e4d71ae93daee9726})<float>(y,x)`.\n   * If matrix is of type `CV_64F` then use\n   * `[Mat.at](#d3/d63/classcv_1_1Mat_1aa5d20fc86d41d59e4d71ae93daee9726})<double>(y,x)`.\n   *   \n   *   @param i0 Index along the dimension 0\n   */\n  public at(arg13: any, i0?: int): any\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param i0 Index along the dimension 0\n   */\n  public at(arg14: any, i0?: int): any\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param row Index along the dimension 0\n   *   \n   *   @param col Index along the dimension 1\n   */\n  public at(arg15: any, row: int, col: int): any\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param row Index along the dimension 0\n   *   \n   *   @param col Index along the dimension 1\n   */\n  public at(arg16: any, row: int, col: int): any\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param i0 Index along the dimension 0\n   *   \n   *   @param i1 Index along the dimension 1\n   *   \n   *   @param i2 Index along the dimension 2\n   */\n  public at(arg17: any, i0: int, i1: int, i2: int): any\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param i0 Index along the dimension 0\n   *   \n   *   @param i1 Index along the dimension 1\n   *   \n   *   @param i2 Index along the dimension 2\n   */\n  public at(arg18: any, i0: int, i1: int, i2: int): any\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param idx Array of Mat::dims indices.\n   */\n  public at(arg19: any, idx: any): any\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param idx Array of Mat::dims indices.\n   */\n  public at(arg20: any, idx: any): any\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   */\n  public at(arg21: any, n: int, idx: Vec): Vec\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   */\n  public at(arg22: any, n: int, idx: Vec): Vec\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts. special versions for 2D arrays (especially convenient\n   * for referencing image pixels)\n   *   \n   *   @param pt Element position specified as Point(j,i) .\n   */\n  public at(arg23: any, pt: Point): Point\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts. special versions for 2D arrays (especially convenient\n   * for referencing image pixels)\n   *   \n   *   @param pt Element position specified as Point(j,i) .\n   */\n  public at(arg24: any, pt: Point): Point\n\n  /**\n   *   The methods return the matrix read-only or read-write iterators. The use of matrix iterators is\n   * very similar to the use of bi-directional STL iterators. In the example below, the alpha blending\n   * function is rewritten using the matrix iterators: \n   *   \n   *   ```cpp\n   *   template<typename T>\n   *   void alphaBlendRGBA(const Mat& src1, const Mat& src2, Mat& dst)\n   *   {\n   *       typedef Vec<T, 4> VT;\n   *   \n   *       const float alpha_scale = (float)std::numeric_limits<T>::max(),\n   *                   inv_scale = 1.f/alpha_scale;\n   *   \n   *       CV_Assert( src1.type() == src2.type() &&\n   *                  src1.type() == traits::Type<VT>::value &&\n   *                  src1.size() == src2.size());\n   *       Size size = src1.size();\n   *       dst.create(size, src1.type());\n   *   \n   *       MatConstIterator_<VT> it1 = src1.begin<VT>(), it1_end = src1.end<VT>();\n   *       MatConstIterator_<VT> it2 = src2.begin<VT>();\n   *       MatIterator_<VT> dst_it = dst.begin<VT>();\n   *   \n   *       for( ; it1 != it1_end; ++it1, ++it2, ++dst_it )\n   *       {\n   *           VT pix1 = *it1, pix2 = *it2;\n   *           float alpha = pix1[3]*inv_scale, beta = pix2[3]*inv_scale;\n   * dst_it = VT(saturate_cast<T>(pix1[0]*alpha + pix2[0]*beta),\n   *                        saturate_cast<T>(pix1[1]*alpha + pix2[1]*beta),\n   *                        saturate_cast<T>(pix1[2]*alpha + pix2[2]*beta),\n   *                        saturate_cast<T>((1 - (1-alpha)*(1-beta))*alpha_scale));\n   *       }\n   *   }\n   *   ```\n   */\n  public begin(arg25: any): MatIterator_\n\n  public begin(arg26: any): MatConstIterator_\n\n  /**\n   *   The method returns the number of matrix channels.\n   */\n  public channels(): int\n\n  /**\n   *   -1 if the requirement is not satisfied. Otherwise, it returns the number of elements in the\n   * matrix. Note that an element may have multiple channels.\n   *   The following code demonstrates its usage for a 2-d matrix: \n   *   \n   *   ```cpp\n   *       cv::Mat mat(20, 1, CV_32FC2);\n   *       int n = mat.checkVector(2);\n   *       CV_Assert(n == 20); // mat has 20 elements\n   *   \n   *       mat.create(20, 2, CV_32FC1);\n   *       n = mat.checkVector(1);\n   *       CV_Assert(n == -1); // mat is neither a column nor a row vector\n   *   \n   *       n = mat.checkVector(2);\n   *       CV_Assert(n == 20); // the 2 columns are considered as 1 element\n   *   ```\n   *   \n   *    The following code demonstrates its usage for a 3-d matrix: \n   *   \n   *   ```cpp\n   *       int dims[] = {1, 3, 5}; // 1 plane, every plane has 3 rows and 5 columns\n   *       mat.create(3, dims, CV_32FC1); // for 3-d mat, it MUST have only 1 channel\n   *       n = mat.checkVector(5); // the 5 columns are considered as 1 element\n   *       CV_Assert(n == 3);\n   *   \n   *       int dims2[] = {3, 1, 5}; // 3 planes, every plane has 1 row and 5 columns\n   *       mat.create(3, dims2, CV_32FC1);\n   *       n = mat.checkVector(5); // the 5 columns are considered as 1 element\n   *       CV_Assert(n == 3);\n   *   ```\n   *   \n   *   @param elemChannels Number of channels or number of columns the matrix should have. For a 2-D\n   * matrix, when the matrix has only 1 column, then it should have elemChannels channels; When the\n   * matrix has only 1 channel, then it should have elemChannels columns. For a 3-D matrix, it should\n   * have only one channel. Furthermore, if the number of planes is not one, then the number of rows\n   * within every plane has to be 1; if the number of rows within every plane is not 1, then the number\n   * of planes has to be 1.\n   *   \n   *   @param depth The depth the matrix should have. Set it to -1 when any depth is fine.\n   *   \n   *   @param requireContinuous Set it to true to require the matrix to be continuous\n   */\n  public checkVector(elemChannels: int, depth?: int, requireContinuous?: bool): int\n\n  /**\n   *   The method creates a full copy of the array. The original step[] is not taken into account. So,\n   * the array copy is a continuous array occupying\n   * [total()](#d3/d63/classcv_1_1Mat_1aa4d317d43fb0cba9c2503f3c61b866c8})*elemSize() bytes.\n   */\n  public clone(): Mat\n\n  /**\n   *   The method makes a new header for the specified matrix column and returns it. This is an O(1)\n   * operation, regardless of the matrix size. The underlying data of the new matrix is shared with the\n   * original matrix. See also the [Mat::row](#d3/d63/classcv_1_1Mat_1a4b22e1c23af7a7f2eef8fa478cfa7434})\n   * description.\n   *   \n   *   @param x A 0-based column index.\n   */\n  public col(x: int): Mat\n\n  /**\n   *   The method makes a new header for the specified column span of the matrix. Similarly to\n   * [Mat::row](#d3/d63/classcv_1_1Mat_1a4b22e1c23af7a7f2eef8fa478cfa7434}) and\n   * [Mat::col](#d3/d63/classcv_1_1Mat_1a23df02a07ffbfa4aa59c19bc003919fe}) , this is an O(1) operation.\n   *   \n   *   @param startcol An inclusive 0-based start index of the column span.\n   *   \n   *   @param endcol An exclusive 0-based ending index of the column span.\n   */\n  public colRange(startcol: int, endcol: int): Mat\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param r Range structure containing both the start and the end indices.\n   */\n  public colRange(r: Range): Mat\n\n  /**\n   *   The method converts source pixel values to the target data type. saturate_cast<> is applied at the\n   * end to avoid possible overflows:\n   *   \n   *   `\\\\[m(x,y) = saturate \\\\_ cast<rType>( \\\\alpha (*this)(x,y) + \\\\beta )\\\\]`\n   *   \n   *   @param m output matrix; if it does not have a proper size or type before the operation, it is\n   * reallocated.\n   *   \n   *   @param rtype desired output matrix type or, rather, the depth since the number of channels are the\n   * same as the input has; if rtype is negative, the output matrix will have the same type as the input.\n   *   \n   *   @param alpha optional scale factor.\n   *   \n   *   @param beta optional delta added to the scaled values.\n   */\n  public convertTo(m: OutputArray, rtype: int, alpha?: double, beta?: double): OutputArray\n\n  public copySize(m: Mat): Mat\n\n  /**\n   *   The method copies the matrix data to another matrix. Before copying the data, the method invokes :\n   * \n   *   \n   *   ```cpp\n   *   m.create(this->size(), this->type());\n   *   ```\n   *   \n   *    so that the destination matrix is reallocated if needed. While m.copyTo(m); works flawlessly, the\n   * function does not handle the case of a partial overlap between the source and the destination\n   * matrices.\n   *   \n   *   When the operation mask is specified, if the\n   * [Mat::create](#d3/d63/classcv_1_1Mat_1a55ced2c8d844d683ea9a725c60037ad0}) call shown above\n   * reallocates the matrix, the newly allocated matrix is initialized with all zeros before copying the\n   * data.\n   *   \n   *   @param m Destination matrix. If it does not have a proper size or type before the operation, it is\n   * reallocated.\n   */\n  public copyTo(m: OutputArray): OutputArray\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param m Destination matrix. If it does not have a proper size or type before the operation, it is\n   * reallocated.\n   *   \n   *   @param mask Operation mask of the same size as *this. Its non-zero elements indicate which matrix\n   * elements need to be copied. The mask has to be of type CV_8U and can have 1 or multiple channels.\n   */\n  public copyTo(m: OutputArray, mask: InputArray): OutputArray\n\n  /**\n   *   This is one of the key [Mat](#d3/d63/classcv_1_1Mat}) methods. Most new-style OpenCV functions and\n   * methods that produce arrays call this method for each output array. The method uses the following\n   * algorithm:\n   *   \n   * If the current array shape and the type match the new ones, return immediately. Otherwise,\n   * de-reference the previous data by calling\n   * [Mat::release](#d3/d63/classcv_1_1Mat_1ae48d4913285518e2c21a3457017e716e}).\n   * Initialize the new header.\n   * Allocate the new data of\n   * [total()](#d3/d63/classcv_1_1Mat_1aa4d317d43fb0cba9c2503f3c61b866c8})*elemSize() bytes.\n   * Allocate the new, associated with the data, reference counter and set it to 1.\n   *   \n   *   Such a scheme makes the memory management robust and efficient at the same time and helps avoid\n   * extra typing for you. This means that usually there is no need to explicitly allocate output arrays.\n   * That is, instead of writing: \n   *   \n   *   ```cpp\n   *   Mat color;\n   *   ...\n   *   Mat gray(color.rows, color.cols, color.depth());\n   *   cvtColor(color, gray, COLOR_BGR2GRAY);\n   *   ```\n   *   \n   *    you can simply write: \n   *   \n   *   ```cpp\n   *   Mat color;\n   *   ...\n   *   Mat gray;\n   *   cvtColor(color, gray, COLOR_BGR2GRAY);\n   *   ```\n   *   \n   *    because cvtColor, as well as the most of OpenCV functions, calls\n   * [Mat::create()](#d3/d63/classcv_1_1Mat_1a55ced2c8d844d683ea9a725c60037ad0}) for the output array\n   * internally.\n   *   \n   *   @param rows New number of rows.\n   *   \n   *   @param cols New number of columns.\n   *   \n   *   @param type New matrix type.\n   */\n  public create(rows: int, cols: int, type: int): void\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param size Alternative new matrix size specification: Size(cols, rows)\n   *   \n   *   @param type New matrix type.\n   */\n  public create(size: Size, type: int): Size\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param ndims New array dimensionality.\n   *   \n   *   @param sizes Array of integers specifying a new array shape.\n   *   \n   *   @param type New matrix type.\n   */\n  public create(ndims: int, sizes: any, type: int): void\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param sizes Array of integers specifying a new array shape.\n   *   \n   *   @param type New matrix type.\n   */\n  public create(sizes: any, type: int): void\n\n  /**\n   *   The method computes a cross-product of two 3-element vectors. The vectors must be 3-element\n   * floating-point vectors of the same shape and size. The result is another 3-element vector of the\n   * same shape and type as operands.\n   *   \n   *   @param m Another cross-product operand.\n   */\n  public cross(m: InputArray): Mat\n\n  public deallocate(): void\n\n  /**\n   *   The method returns the identifier of the matrix element depth (the type of each individual\n   * channel). For example, for a 16-bit signed element array, the method returns CV_16S . A complete\n   * list of matrix types contains the following values:\n   *   \n   * CV_8U - 8-bit unsigned integers ( 0..255 )\n   * CV_8S - 8-bit signed integers ( -128..127 )\n   * CV_16U - 16-bit unsigned integers ( 0..65535 )\n   * CV_16S - 16-bit signed integers ( -32768..32767 )\n   * CV_32S - 32-bit signed integers ( -2147483648..2147483647 )\n   * CV_32F - 32-bit floating-point numbers ( -FLT_MAX..FLT_MAX, INF, NAN )\n   * CV_64F - 64-bit floating-point numbers ( -DBL_MAX..DBL_MAX, INF, NAN )\n   */\n  public depth(): int\n\n  /**\n   *   The method makes a new header for the specified matrix diagonal. The new matrix is represented as\n   * a single-column matrix. Similarly to\n   * [Mat::row](#d3/d63/classcv_1_1Mat_1a4b22e1c23af7a7f2eef8fa478cfa7434}) and\n   * [Mat::col](#d3/d63/classcv_1_1Mat_1a23df02a07ffbfa4aa59c19bc003919fe}), this is an O(1) operation.\n   *   \n   *   @param d index of the diagonal, with the following values:\n   *   d=0 is the main diagonal.d<0 is a diagonal from the lower half. For example, d=-1 means the\n   * diagonal is set immediately below the main one.d>0 is a diagonal from the upper half. For example,\n   * d=1 means the diagonal is set immediately above the main one. For example: Matm=(Mat_<int>(3,3)<<\n   *   1,2,3,\n   *   4,5,6,\n   *   7,8,9);\n   *   Matd0=m.diag(0);\n   *   Matd1=m.diag(1);\n   *   Matd_1=m.diag(-1);\n   *    The resulting matrices are d0=\n   *   [1;\n   *   5;\n   *   9]\n   *   d1=\n   *   [2;\n   *   6]\n   *   d_1=\n   *   [4;\n   *   8]\n   */\n  public diag(d?: int): Mat\n\n  /**\n   *   The method computes a dot-product of two matrices. If the matrices are not single-column or\n   * single-row vectors, the top-to-bottom left-to-right scan ordering is used to treat them as 1D\n   * vectors. The vectors must have the same size and type. If the matrices have more than one channel,\n   * the dot products from all the channels are summed together.\n   *   \n   *   @param m another dot-product operand.\n   */\n  public dot(m: InputArray): InputArray\n\n  /**\n   *   The method returns the matrix element size in bytes. For example, if the matrix type is CV_16SC3 ,\n   * the method returns 3*sizeof(short) or 6.\n   */\n  public elemSize(): size_t\n\n  /**\n   *   The method returns the matrix element channel size in bytes, that is, it ignores the number of\n   * channels. For example, if the matrix type is CV_16SC3 , the method returns sizeof(short) or 2.\n   */\n  public elemSize1(): size_t\n\n  /**\n   *   The method returns true if\n   * [Mat::total()](#d3/d63/classcv_1_1Mat_1aa4d317d43fb0cba9c2503f3c61b866c8}) is 0 or if\n   * [Mat::data](#d3/d63/classcv_1_1Mat_1a4d33bed1c850265370d2af0ff02e1564}) is NULL. Because of\n   * [pop_back()](#d3/d63/classcv_1_1Mat_1a88bbb01901fdfe3f1d0592c592e8757c}) and\n   * [resize()](#d3/d63/classcv_1_1Mat_1ad0127b138acfcc2dcd5dafc51175b309}) methods\n   * `[M.total()](#df/d57/namespacecv_1_1dnn_1a65ad6cf1b64a572bf78d696d2014b0e6}) == 0` does not imply\n   * that `M.data == NULL`.\n   */\n  public empty(): bool\n\n  /**\n   *   The methods return the matrix read-only or read-write iterators, set to the point following the\n   * last matrix element.\n   */\n  public end(arg27: any): MatIterator_\n\n  public end(arg28: any): MatConstIterator_\n\n  /**\n   *   The operation passed as argument has to be a function pointer, a function object or a\n   * lambda(C++11).\n   *   \n   *   Example 1. All of the operations below put 0xFF the first channel of all matrix elements: \n   *   \n   *   ```cpp\n   *   Mat image(1920, 1080, CV_8UC3);\n   *   typedef cv::Point3_<uint8_t> Pixel;\n   *   \n   *   // first. raw pointer access.\n   *   for (int r = 0; r < image.rows; ++r) {\n   *       Pixel* ptr = image.ptr<Pixel>(r, 0);\n   *       const Pixel* ptr_end = ptr + image.cols;\n   *       for (; ptr != ptr_end; ++ptr) {\n   *           ptr->x = 255;\n   *       }\n   *   }\n   *   \n   *   // Using MatIterator. (Simple but there are a Iterator's overhead)\n   *   for (Pixel &p : cv::Mat_<Pixel>(image)) {\n   *       p.x = 255;\n   *   }\n   *   \n   *   // Parallel execution with function object.\n   *   struct Operator {\n   *       void operator ()(Pixel &pixel, const int * position) {\n   *           pixel.x = 255;\n   *       }\n   *   };\n   *   image.forEach<Pixel>(Operator());\n   *   \n   *   // Parallel execution using C++11 lambda.\n   *   image.forEach<Pixel>([](Pixel &p, const int * position) -> void {\n   *       p.x = 255;\n   *   });\n   *   ```\n   *   \n   *    Example 2. Using the pixel's position: \n   *   \n   *   ```cpp\n   *   // Creating 3D matrix (255 x 255 x 255) typed uint8_t\n   *   // and initialize all elements by the value which equals elements position.\n   *   // i.e. pixels (x,y,z) = (1,2,3) is (b,g,r) = (1,2,3).\n   *   \n   *   int sizes[] = { 255, 255, 255 };\n   *   typedef cv::Point3_<uint8_t> Pixel;\n   *   \n   *   Mat_<Pixel> image = Mat::zeros(3, sizes, CV_8UC3);\n   *   \n   *   image.forEach<Pixel>([&](Pixel& pixel, const int position[]) -> void {\n   *       pixel.x = position[0];\n   *       pixel.y = position[1];\n   *       pixel.z = position[2];\n   *   });\n   *   ```\n   */\n  public forEach(arg29: any, arg30: any, operation: any): any\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   */\n  public forEach(arg31: any, arg32: any, operation: any): any\n\n  public getUMat(accessFlags: AccessFlag, usageFlags?: UMatUsageFlags): UMat\n\n  /**\n   *   The method performs a matrix inversion by means of matrix expressions. This means that a temporary\n   * matrix inversion object is returned by the method and can be used further as a part of more complex\n   * matrix expressions or can be assigned to a matrix.\n   *   \n   *   @param method Matrix inversion method. One of cv::DecompTypes\n   */\n  public inv(method?: int): MatExpr\n\n  /**\n   *   The method returns true if the matrix elements are stored continuously without gaps at the end of\n   * each row. Otherwise, it returns false. Obviously, 1x1 or 1xN matrices are always continuous.\n   * Matrices created with [Mat::create](#d3/d63/classcv_1_1Mat_1a55ced2c8d844d683ea9a725c60037ad0}) are\n   * always continuous. But if you extract a part of the matrix using\n   * [Mat::col](#d3/d63/classcv_1_1Mat_1a23df02a07ffbfa4aa59c19bc003919fe}),\n   * [Mat::diag](#d3/d63/classcv_1_1Mat_1a024cc0510a4c61c7f266d4ab9fe13d7a}), and so on, or constructed a\n   * matrix header for externally allocated data, such matrices may no longer have this property.\n   *   \n   *   The continuity flag is stored as a bit in the\n   * [Mat::flags](#d3/d63/classcv_1_1Mat_1af9333f06c84f115fda4cdf3af18c2ad0}) field and is computed\n   * automatically when you construct a matrix header. Thus, the continuity check is a very fast\n   * operation, though theoretically it could be done as follows: \n   *   \n   *   ```cpp\n   *   // alternative implementation of Mat::isContinuous()\n   *   bool myCheckMatContinuity(const Mat& m)\n   *   {\n   *       //return (m.flags & Mat::CONTINUOUS_FLAG) != 0;\n   *       return m.rows == 1 || m.step == m.cols*m.elemSize();\n   *   }\n   *   ```\n   *   \n   *    The method is used in quite a few of OpenCV functions. The point is that element-wise operations\n   * (such as arithmetic and logical operations, math functions, alpha blending, color space\n   * transformations, and others) do not depend on the image geometry. Thus, if all the input and output\n   * arrays are continuous, the functions can process them as very long single-row vectors. The example\n   * below illustrates how an alpha-blending function can be implemented: \n   *   \n   *   ```cpp\n   *   template<typename T>\n   *   void alphaBlendRGBA(const Mat& src1, const Mat& src2, Mat& dst)\n   *   {\n   *       const float alpha_scale = (float)std::numeric_limits<T>::max(),\n   *                   inv_scale = 1.f/alpha_scale;\n   *   \n   *       CV_Assert( src1.type() == src2.type() &&\n   *                  src1.type() == CV_MAKETYPE(traits::Depth<T>::value, 4) &&\n   *                  src1.size() == src2.size());\n   *       Size size = src1.size();\n   *       dst.create(size, src1.type());\n   *   \n   *       // here is the idiom: check the arrays for continuity and,\n   *       // if this is the case,\n   *       // treat the arrays as 1D vectors\n   *       if( src1.isContinuous() && src2.isContinuous() && dst.isContinuous() )\n   *       {\n   *           size.width *= size.height;\n   *           size.height = 1;\n   *       }\n   *       size.width *= 4;\n   *   \n   *       for( int i = 0; i < size.height; i++ )\n   *       {\n   *           // when the arrays are continuous,\n   *           // the outer loop is executed only once\n   *           const T* ptr1 = src1.ptr<T>(i);\n   *           const T* ptr2 = src2.ptr<T>(i);\n   *           T* dptr = dst.ptr<T>(i);\n   *   \n   *           for( int j = 0; j < size.width; j += 4 )\n   *           {\n   *               float alpha = ptr1[j+3]*inv_scale, beta = ptr2[j+3]*inv_scale;\n   *               dptr[j] = saturate_cast<T>(ptr1[j]*alpha + ptr2[j]*beta);\n   *               dptr[j+1] = saturate_cast<T>(ptr1[j+1]*alpha + ptr2[j+1]*beta);\n   *               dptr[j+2] = saturate_cast<T>(ptr1[j+2]*alpha + ptr2[j+2]*beta);\n   *               dptr[j+3] = saturate_cast<T>((1 - (1-alpha)*(1-beta))*alpha_scale);\n   *           }\n   *       }\n   *   }\n   *   ```\n   *   \n   *    This approach, while being very simple, can boost the performance of a simple element-operation\n   * by 10-20 percents, especially if the image is rather small and the operation is quite simple.\n   *   \n   *   Another OpenCV idiom in this function, a call of\n   * [Mat::create](#d3/d63/classcv_1_1Mat_1a55ced2c8d844d683ea9a725c60037ad0}) for the destination array,\n   * that allocates the destination array unless it already has the proper size and type. And while the\n   * newly allocated arrays are always continuous, you still need to check the destination array because\n   * [Mat::create](#d3/d63/classcv_1_1Mat_1a55ced2c8d844d683ea9a725c60037ad0}) does not always allocate a\n   * new matrix.\n   */\n  public isContinuous(): bool\n\n  public isSubmatrix(): bool\n\n  /**\n   *   After you extracted a submatrix from a matrix using\n   * [Mat::row](#d3/d63/classcv_1_1Mat_1a4b22e1c23af7a7f2eef8fa478cfa7434}),\n   * [Mat::col](#d3/d63/classcv_1_1Mat_1a23df02a07ffbfa4aa59c19bc003919fe}),\n   * [Mat::rowRange](#d3/d63/classcv_1_1Mat_1aa6542193430356ad631a9beabc624107}),\n   * [Mat::colRange](#d3/d63/classcv_1_1Mat_1aadc8f9210fe4dec50513746c246fa8d9}), and others, the\n   * resultant submatrix points just to the part of the original big matrix. However, each submatrix\n   * contains information (represented by datastart and dataend fields) that helps reconstruct the\n   * original matrix size and the position of the extracted submatrix within the original matrix. The\n   * method locateROI does exactly that.\n   *   \n   *   @param wholeSize Output parameter that contains the size of the whole matrix containing this as a\n   * part.\n   *   \n   *   @param ofs Output parameter that contains an offset of this inside the whole matrix.\n   */\n  public locateROI(wholeSize: Size, ofs: Point): Size\n\n  /**\n   *   The method returns a temporary object encoding per-element array multiplication, with optional\n   * scale. Note that this is not a matrix multiplication that corresponds to a simpler \"\\\\*\" operator.\n   *   \n   *   Example: \n   *   \n   *   ```cpp\n   *   Mat C = A.mul(5/B); // equivalent to divide(A, B, C, 5)\n   *   ```\n   *   \n   *   @param m Another array of the same type and the same size as *this, or a matrix expression.\n   *   \n   *   @param scale Optional scale factor.\n   */\n  public mul(m: InputArray, scale?: double): MatExpr\n\n  /**\n   *   The method removes one or more rows from the bottom of the matrix.\n   *   \n   *   @param nelems Number of removed rows. If it is greater than the total number of rows, an exception\n   * is thrown.\n   */\n  public pop_back(nelems?: size_t): void\n\n  /**\n   *   The methods return `uchar*` or typed pointer to the specified matrix row. See the sample in\n   * [Mat::isContinuous](#d3/d63/classcv_1_1Mat_1aa90cea495029c7d1ee0a41361ccecdf3}) to know how to use\n   * these methods.\n   *   \n   *   @param i0 A 0-based row index.\n   */\n  public ptr(i0?: int): uchar\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   */\n  public ptr(i0?: int): uchar\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param row Index along the dimension 0\n   *   \n   *   @param col Index along the dimension 1\n   */\n  public ptr(row: int, col: int): uchar\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param row Index along the dimension 0\n   *   \n   *   @param col Index along the dimension 1\n   */\n  public ptr(row: int, col: int): uchar\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   */\n  public ptr(i0: int, i1: int, i2: int): uchar\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   */\n  public ptr(i0: int, i1: int, i2: int): uchar\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   */\n  public ptr(idx: any): uchar\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   */\n  public ptr(idx: any): uchar\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   */\n  public ptr(n: int, idx: Vec): uchar\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   */\n  public ptr(n: int, idx: Vec): uchar\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   */\n  public ptr(arg37: any, i0?: int): any\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   */\n  public ptr(arg38: any, i0?: int): any\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param row Index along the dimension 0\n   *   \n   *   @param col Index along the dimension 1\n   */\n  public ptr(arg39: any, row: int, col: int): any\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param row Index along the dimension 0\n   *   \n   *   @param col Index along the dimension 1\n   */\n  public ptr(arg40: any, row: int, col: int): any\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   */\n  public ptr(arg41: any, i0: int, i1: int, i2: int): any\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   */\n  public ptr(arg42: any, i0: int, i1: int, i2: int): any\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   */\n  public ptr(arg43: any, idx: any): any\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   */\n  public ptr(arg44: any, idx: any): any\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   */\n  public ptr(arg45: any, n: int, idx: Vec): Vec\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   */\n  public ptr(arg46: any, n: int, idx: Vec): Vec\n\n  /**\n   *   The methods add one or more elements to the bottom of the matrix. They emulate the corresponding\n   * method of the STL vector class. When elem is [Mat](#d3/d63/classcv_1_1Mat}) , its type and the\n   * number of columns must be the same as in the container matrix.\n   *   \n   *   @param elem Added element(s).\n   */\n  public push_back(arg47: any, elem: any): any\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param elem Added element(s).\n   */\n  public push_back(arg48: any, elem: Mat_): Mat_\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param elem Added element(s).\n   */\n  public push_back(arg49: any, elem: any): any\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param m Added line(s).\n   */\n  public push_back(m: Mat): Mat\n\n  public push_back_(elem: any): void\n\n  /**\n   *   The method decrements the reference counter associated with the matrix data. When the reference\n   * counter reaches 0, the matrix data is deallocated and the data and the reference counter pointers\n   * are set to NULL's. If the matrix header points to an external data set (see\n   * [Mat::Mat](#d3/d63/classcv_1_1Mat_1af1d014cecd1510cdf580bf2ed7e5aafc}) ), the reference counter is\n   * NULL, and the method has no effect in this case.\n   *   \n   *   This method can be called manually to force the matrix data deallocation. But since this method is\n   * automatically called in the destructor, or by any other method that changes the data pointer, it is\n   * usually not needed. The reference counter decrement and check for 0 is an atomic operation on the\n   * platforms that support it. Thus, it is safe to operate on the same matrices asynchronously in\n   * different threads.\n   */\n  public release(): void\n\n  /**\n   *   The method reserves space for sz rows. If the matrix already has enough space to store sz rows,\n   * nothing happens. If the matrix is reallocated, the first\n   * [Mat::rows](#d3/d63/classcv_1_1Mat_1abed816466c45234254d25bc59c31245e}) rows are preserved. The\n   * method emulates the corresponding method of the STL vector class.\n   *   \n   *   @param sz Number of rows.\n   */\n  public reserve(sz: size_t): void\n\n  /**\n   *   The method reserves space for sz bytes. If the matrix already has enough space to store sz bytes,\n   * nothing happens. If matrix has to be reallocated its previous content could be lost.\n   *   \n   *   @param sz Number of bytes.\n   */\n  public reserveBuffer(sz: size_t): void\n\n  /**\n   *   The method makes a new matrix header for *this elements. The new matrix may have a different size\n   * and/or different number of channels. Any combination is possible if:\n   *   \n   * No extra elements are included into the new matrix and no elements are excluded. Consequently, the\n   * product rows*cols*channels() must stay the same after the transformation.\n   * No data is copied. That is, this is an O(1) operation. Consequently, if you change the number of\n   * rows, or the operation changes the indices of elements row in some other way, the matrix must be\n   * continuous. See [Mat::isContinuous](#d3/d63/classcv_1_1Mat_1aa90cea495029c7d1ee0a41361ccecdf3}) .\n   *   \n   *   For example, if there is a set of 3D points stored as an STL vector, and you want to represent the\n   * points as a 3xN matrix, do the following: \n   *   \n   *   ```cpp\n   *   std::vector<Point3f> vec;\n   *   ...\n   *   Mat pointMat = Mat(vec). // convert vector to Mat, O(1) operation\n   *                     reshape(1). // make Nx3 1-channel matrix out of Nx1 3-channel.\n   *                                 // Also, an O(1) operation\n   *                        t(); // finally, transpose the Nx3 matrix.\n   *                             // This involves copying all the elements\n   *   ```\n   *   \n   *   @param cn New number of channels. If the parameter is 0, the number of channels remains the same.\n   *   \n   *   @param rows New number of rows. If the parameter is 0, the number of rows remains the same.\n   */\n  public reshape(cn: int, rows?: int): Mat\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   */\n  public reshape(cn: int, newndims: int, newsz: any): Mat\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   */\n  public reshape(cn: int, newshape: any): Mat\n\n  /**\n   *   The methods change the number of matrix rows. If the matrix is reallocated, the first\n   * min(Mat::rows, sz) rows are preserved. The methods emulate the corresponding methods of the STL\n   * vector class.\n   *   \n   *   @param sz New number of rows.\n   */\n  public resize(sz: size_t): void\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param sz New number of rows.\n   *   \n   *   @param s Value assigned to the newly added elements.\n   */\n  public resize(sz: size_t, s: Scalar): Scalar\n\n  /**\n   *   The method makes a new header for the specified matrix row and returns it. This is an O(1)\n   * operation, regardless of the matrix size. The underlying data of the new matrix is shared with the\n   * original matrix. Here is the example of one of the classical basic matrix processing operations,\n   * axpy, used by LU and many other algorithms: \n   *   \n   *   ```cpp\n   *   inline void matrix_axpy(Mat& A, int i, int j, double alpha)\n   *   {\n   *       A.row(i) += A.row(j)*alpha;\n   *   }\n   *   ```\n   *   \n   *   In the current implementation, the following code does not work as expected: \n   *   \n   *   ```cpp\n   *   Mat A;\n   *   ...\n   *   A.row(i) = A.row(j); // will not work\n   *   ```\n   *   \n   *    This happens because A.row(i) forms a temporary header that is further assigned to another\n   * header. Remember that each of these operations is O(1), that is, no data is copied. Thus, the above\n   * assignment is not true if you may have expected the j-th row to be copied to the i-th row. To\n   * achieve that, you should either turn this simple assignment into an expression or use the\n   * [Mat::copyTo](#d3/d63/classcv_1_1Mat_1a33fd5d125b4c302b0c9aa86980791a77}) method: \n   *   \n   *   ```cpp\n   *   Mat A;\n   *   ...\n   *   // works, but looks a bit obscure.\n   *   A.row(i) = A.row(j) + 0;\n   *   // this is a bit longer, but the recommended method.\n   *   A.row(j).copyTo(A.row(i));\n   *   ```\n   *   \n   *   @param y A 0-based row index.\n   */\n  public row(y: int): Mat\n\n  /**\n   *   The method makes a new header for the specified row span of the matrix. Similarly to\n   * [Mat::row](#d3/d63/classcv_1_1Mat_1a4b22e1c23af7a7f2eef8fa478cfa7434}) and\n   * [Mat::col](#d3/d63/classcv_1_1Mat_1a23df02a07ffbfa4aa59c19bc003919fe}) , this is an O(1) operation.\n   *   \n   *   @param startrow An inclusive 0-based start index of the row span.\n   *   \n   *   @param endrow An exclusive 0-based ending index of the row span.\n   */\n  public rowRange(startrow: int, endrow: int): Mat\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param r Range structure containing both the start and the end indices.\n   */\n  public rowRange(r: Range): Mat\n\n  /**\n   *   This is an advanced variant of the [Mat::operator=(const Scalar&\n   * s)](#d3/d63/classcv_1_1Mat_1aa5c947f7e449a4d856a4f3a87fcebd50}) operator.\n   *   \n   *   @param value Assigned scalar converted to the actual array type.\n   *   \n   *   @param mask Operation mask of the same size as *this. Its non-zero elements indicate which matrix\n   * elements need to be copied. The mask has to be of type CV_8U and can have 1 or multiple channels\n   */\n  public setTo(value: InputArray, mask?: InputArray): Mat\n\n  /**\n   *   The method returns a matrix step divided by\n   * [Mat::elemSize1()](#d3/d63/classcv_1_1Mat_1a9acde8f32d4b294558fb406bc05171bc}) . It can be useful to\n   * quickly access an arbitrary matrix element.\n   */\n  public step1(i?: int): size_t\n\n  /**\n   *   The method performs matrix transposition by means of matrix expressions. It does not perform the\n   * actual transposition but returns a temporary matrix transposition object that can be further used as\n   * a part of more complex matrix expressions or can be assigned to a matrix: \n   *   \n   *   ```cpp\n   *   Mat A1 = A + Mat::eye(A.size(), A.type())*lambda;\n   *   Mat C = A1.t()*A1; // compute (A + lambda*I)^t * (A + lamda*I)\n   *   ```\n   */\n  public t(): MatExpr\n\n  /**\n   *   The method returns the number of array elements (a number of pixels if the array represents an\n   * image).\n   */\n  public total(): size_t\n\n  /**\n   *   The method returns the number of elements within a certain sub-array slice with startDim <= dim <\n   * endDim\n   */\n  public total(startDim: int, endDim?: int): size_t\n\n  /**\n   *   The method returns a matrix element type. This is an identifier compatible with the CvMat type\n   * system, like CV_16SC3 or 16-bit signed 3-channel array, and so on.\n   */\n  public type(): int\n\n  public updateContinuityFlag(): void\n\n  /**\n   *   The method creates a square diagonal matrix from specified main diagonal.\n   *   \n   *   @param d One-dimensional matrix that represents the main diagonal.\n   */\n  public static diag(d: Mat): Mat\n\n  /**\n   *   The method returns a Matlab-style identity matrix initializer, similarly to\n   * [Mat::zeros](#d3/d63/classcv_1_1Mat_1a0b57b6a326c8876d944d188a46e0f556}). Similarly to\n   * [Mat::ones](#d3/d63/classcv_1_1Mat_1a69ae0402d116fc9c71908d8508dc2f09}), you can use a scale\n   * operation to create a scaled identity matrix efficiently: \n   *   \n   *   ```cpp\n   *   // make a 4x4 diagonal matrix with 0.1's on the diagonal.\n   *   Mat A = Mat::eye(4, 4, CV_32F)*0.1;\n   *   ```\n   *   \n   *   In case of multi-channels type, identity matrix will be initialized only for the first channel,\n   * the others will be set to 0's\n   *   \n   *   @param rows Number of rows.\n   *   \n   *   @param cols Number of columns.\n   *   \n   *   @param type Created matrix type.\n   */\n  public static eye(rows: int, cols: int, type: int): MatExpr\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param size Alternative matrix size specification as Size(cols, rows) .\n   *   \n   *   @param type Created matrix type.\n   */\n  public static eye(size: Size, type: int): MatExpr\n\n  public static getDefaultAllocator(): MatAllocator\n\n  public static getStdAllocator(): MatAllocator\n\n  /**\n   *   The method returns a Matlab-style 1's array initializer, similarly to\n   * [Mat::zeros](#d3/d63/classcv_1_1Mat_1a0b57b6a326c8876d944d188a46e0f556}). Note that using this\n   * method you can initialize an array with an arbitrary value, using the following Matlab idiom: \n   *   \n   *   ```cpp\n   *   Mat A = Mat::ones(100, 100, CV_8U)*3; // make 100x100 matrix filled with 3.\n   *   ```\n   *   \n   *    The above operation does not form a 100x100 matrix of 1's and then multiply it by 3. Instead, it\n   * just remembers the scale factor (3 in this case) and use it when actually invoking the matrix\n   * initializer. \n   *   \n   *   In case of multi-channels type, only the first channel will be initialized with 1's, the others\n   * will be set to 0's.\n   *   \n   *   @param rows Number of rows.\n   *   \n   *   @param cols Number of columns.\n   *   \n   *   @param type Created matrix type.\n   */\n  public static ones(rows: int, cols: int, type: int): MatExpr\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param size Alternative to the matrix size specification Size(cols, rows) .\n   *   \n   *   @param type Created matrix type.\n   */\n  public static ones(size: Size, type: int): MatExpr\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param ndims Array dimensionality.\n   *   \n   *   @param sz Array of integers specifying the array shape.\n   *   \n   *   @param type Created matrix type.\n   */\n  public static ones(ndims: int, sz: any, type: int): MatExpr\n\n  public static setDefaultAllocator(allocator: MatAllocator): MatAllocator\n\n  /**\n   *   The method returns a Matlab-style zero array initializer. It can be used to quickly form a\n   * constant array as a function parameter, part of a matrix expression, or as a matrix initializer: \n   *   \n   *   ```cpp\n   *   Mat A;\n   *   A = Mat::zeros(3, 3, CV_32F);\n   *   ```\n   *   \n   *    In the example above, a new matrix is allocated only if A is not a 3x3 floating-point matrix.\n   * Otherwise, the existing matrix A is filled with zeros.\n   *   \n   *   @param rows Number of rows.\n   *   \n   *   @param cols Number of columns.\n   *   \n   *   @param type Created matrix type.\n   */\n  public static zeros(rows: int, cols: int, type: int): MatExpr\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param size Alternative to the matrix size specification Size(cols, rows) .\n   *   \n   *   @param type Created matrix type.\n   */\n  public static zeros(size: Size, type: int): MatExpr\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param ndims Array dimensionality.\n   *   \n   *   @param sz Array of integers specifying the array shape.\n   *   \n   *   @param type Created matrix type.\n   */\n  public static zeros(ndims: int, sz: any, type: int): MatExpr\n}\n\nexport declare const MAGIC_VAL: any // initializer: = 0x42FF0000\n\nexport declare const AUTO_STEP: any // initializer: = 0\n\nexport declare const CONTINUOUS_FLAG: any // initializer: = CV_MAT_CONT_FLAG\n\nexport declare const SUBMATRIX_FLAG: any // initializer: = CV_SUBMAT_FLAG\n\nexport declare const MAGIC_MASK: any // initializer: = 0xFFFF0000\n\nexport declare const TYPE_MASK: any // initializer: = 0x00000FFF\n\nexport declare const DEPTH_MASK: any // initializer: = 7\n\n"},"node_modules_mirada_dist_src_types_opencv_objdetect_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_objdetect_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/objdetect.d.ts","content":"\nimport { double, int, Size } from './_types'\n/*\n * # Object Detection\n * ## Haar Feature-based Cascade Classifier for Object Detection \n * \n * \n * The object detector described below has been initially proposed by Paul Viola Viola01 and improved by Rainer Lienhart Lienhart02 .\n * \n * First, a classifier (namely a *cascade of boosted classifiers working with haar-like features*) is trained with a few hundred sample views of a particular object (i.e., a face or a car), called positive examples, that are scaled to the same size (say, 20x20), and negative examples - arbitrary images of the same size.\n * \n * After a classifier is trained, it can be applied to a region of interest (of the same size as used during the training) in an input image. The classifier outputs a \"1\" if the region is likely to show the object (i.e., face/car), and \"0\" otherwise. To search for the object in the whole image one can move the search window across the image and check every location using the classifier. The classifier is designed so that it can be easily \"resized\" in order to be able to find the objects of interest at different sizes, which is more efficient than resizing the image itself. So, to find an object of an unknown size in the image the scan procedure should be done several times at different scales.\n * \n * The word \"cascade\" in the classifier name means that the resultant classifier consists of several simpler classifiers (*stages*) that are applied subsequently to a region of interest until at some stage the candidate is rejected or all the stages are passed. The word \"boosted\" means that the classifiers at every stage of the cascade are complex themselves and they are built out of basic classifiers using one of four different boosting techniques (weighted voting). Currently Discrete Adaboost, Real Adaboost, Gentle Adaboost and Logitboost are supported. The basic classifiers are decision-tree classifiers with at least 2 leaves. Haar-like features are the input to the basic classifiers, and are calculated as described below. The current algorithm uses the following Haar-like features:\n * \n * \n *  The feature used in a particular classifier is specified by its shape (1a, 2b etc.), position within the region of interest and the scale (this scale is not the same as the scale used at the detection stage, though these two scales are multiplied). For example, in the case of the third line feature (2c) the response is calculated as the difference between the sum of image pixels under the rectangle covering the whole feature (including the two white stripes and the black stripe in the middle) and the sum of the image pixels under the black stripe multiplied by 3 in order to compensate for the differences in the size of areas. The sums of pixel values over a rectangular regions are calculated rapidly using integral images (see below and the integral description).\n * \n * To see the object detector at work, have a look at the facedetect demo: \n * \n * The following reference is for the detection part only. There is a separate application called opencv_traincascade that can train a cascade of boosted classifiers from a set of samples.\n * \n * \n * \n * In the new C++ interface it is also possible to use LBP (local binary pattern) features in addition to Haar-like features. .. [Viola01] Paul Viola and Michael J. Jones. Rapid Object Detection using a Boosted Cascade of Simple Features. IEEE CVPR, 2001. The paper is available online at\n */\nexport declare function createFaceDetectionMaskGenerator(): any\n\n/**\n * The function is a wrapper for the generic function partition . It clusters all the input rectangles\n * using the rectangle equivalence criteria that combines rectangles with similar sizes and similar\n * locations. The similarity is defined by eps. When eps=0 , no clustering is done at all. If\n * `$\\\\texttt{eps}\\\\rightarrow +\\\\inf$` , all the rectangles are put in one cluster. Then, the small\n * clusters containing less than or equal to groupThreshold rectangles are rejected. In each other\n * cluster, the average rectangle is computed and put into the output rectangle list.\n * \n * @param rectList Input/output vector of rectangles. Output vector includes retained and grouped\n * rectangles. (The Python list is not modified in place.)\n * \n * @param groupThreshold Minimum possible number of rectangles minus 1. The threshold is used in a\n * group of rectangles to retain it.\n * \n * @param eps Relative difference between sides of the rectangles to merge them into a group.\n */\nexport declare function groupRectangles(rectList: any, groupThreshold: int, eps?: double): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function groupRectangles(rectList: any, weights: any, groupThreshold: int, eps?: double): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function groupRectangles(rectList: any, groupThreshold: int, eps: double, weights: any, levelWeights: any): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function groupRectangles(rectList: any, rejectLevels: any, levelWeights: any, groupThreshold: int, eps?: double): void\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function groupRectangles_meanshift(rectList: any, foundWeights: any, foundScales: any, detectThreshold?: double, winDetSize?: Size): void\n\nexport declare const CASCADE_DO_CANNY_PRUNING: any // initializer: = 1\n\nexport declare const CASCADE_SCALE_IMAGE: any // initializer: = 2\n\nexport declare const CASCADE_FIND_BIGGEST_OBJECT: any // initializer: = 4\n\nexport declare const CASCADE_DO_ROUGH_SEARCH: any // initializer: = 8\n\n"},"node_modules_mirada_dist_src_types_opencv_PCA_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_PCA_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/PCA.d.ts","content":"\nimport { double, FileNode, FileStorage, InputArray, int, Mat, OutputArray } from './_types'\n\n/**\n * The class is used to calculate a special basis for a set of vectors. The basis will consist of\n * eigenvectors of the covariance matrix calculated from the input set of vectors. The class PCA can\n * also transform vectors to/from the new coordinate space defined by the basis. Usually, in this new\n * coordinate system, each vector from the original set (and any linear combination of such vectors)\n * can be quite accurately approximated by taking its first few components, corresponding to the\n * eigenvectors of the largest eigenvalues of the covariance matrix. Geometrically it means that you\n * calculate a projection of the vector to a subspace formed by a few eigenvectors corresponding to the\n * dominant eigenvalues of the covariance matrix. And usually such a projection is very close to the\n * original vector. So, you can represent the original vector from a high-dimensional space with a much\n * shorter vector consisting of the projected vector's coordinates in the subspace. Such a\n * transformation is also known as Karhunen-Loeve Transform, or KLT. See \n * \n * The sample below is the function that takes two matrices. The first function stores a set of vectors\n * (a row per vector) that is used to calculate [PCA](#d3/d8d/classcv_1_1PCA}). The second function\n * stores another \"test\" set of vectors (a row per vector). First, these vectors are compressed with\n * [PCA](#d3/d8d/classcv_1_1PCA}), then reconstructed back, and then the reconstruction error norm is\n * computed and printed for each vector. :\n * \n * ```cpp\n * using namespace cv;\n * \n * PCA compressPCA(const Mat& pcaset, int maxComponents,\n *                 const Mat& testset, Mat& compressed)\n * {\n *     PCA pca(pcaset, // pass the data\n *             Mat(), // we do not have a pre-computed mean vector,\n *                    // so let the PCA engine to compute it\n *             PCA::DATA_AS_ROW, // indicate that the vectors\n *                                 // are stored as matrix rows\n *                                 // (use PCA::DATA_AS_COL if the vectors are\n *                                 // the matrix columns)\n *             maxComponents // specify, how many principal components to retain\n *             );\n *     // if there is no test data, just return the computed basis, ready-to-use\n *     if( !testset.data )\n *         return pca;\n *     CV_Assert( testset.cols == pcaset.cols );\n * \n *     compressed.create(testset.rows, maxComponents, testset.type());\n * \n *     Mat reconstructed;\n *     for( int i = 0; i < testset.rows; i++ )\n *     {\n *         Mat vec = testset.row(i), coeffs = compressed.row(i), reconstructed;\n *         // compress the vector, the result will be stored\n *         // in the i-th row of the output matrix\n *         pca.project(vec, coeffs);\n *         // and then reconstruct it\n *         pca.backProject(coeffs, reconstructed);\n *         // and measure the error\n *         printf(\"%d. diff = %g\\\\n\", i, norm(vec, reconstructed, NORM_L2));\n *     }\n *     return pca;\n * }\n * ```\n * \n * [calcCovarMatrix](#d2/de8/group__core__array_1gae6ffa9354633f984246945d52823165d}),\n * [mulTransposed](#d2/de8/group__core__array_1gadc4e49f8f7a155044e3be1b9e3b270ab}),\n * [SVD](#df/df7/classcv_1_1SVD}),\n * [dft](#d2/de8/group__core__array_1gadd6cf9baf2b8b704a11b5f04aaf4f39d}),\n * [dct](#d2/de8/group__core__array_1ga85aad4d668c01fbd64825f589e3696d4})\n * \n * Source:\n * [opencv2/core.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/core.hpp#L2393).\n * \n */\nexport declare class PCA {\n\n  public eigenvalues: Mat\n\n  public eigenvectors: Mat\n\n  public mean: Mat\n\n  /**\n   *   The default constructor initializes an empty PCA structure. The other constructors initialize the\n   * structure and call [PCA::operator()()](#d3/d8d/classcv_1_1PCA_1a26e76331a68988144a403649c6b8af5c}).\n   */\n  public constructor()\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param data input samples stored as matrix rows or matrix columns.\n   *   \n   *   @param mean optional mean value; if the matrix is empty (noArray()), the mean is computed from the\n   * data.\n   *   \n   *   @param flags operation flags; currently the parameter is only used to specify the data layout\n   * (PCA::Flags)\n   *   \n   *   @param maxComponents maximum number of components that PCA should retain; by default, all the\n   * components are retained.\n   */\n  public constructor(data: InputArray, mean: InputArray, flags: int, maxComponents?: int)\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param data input samples stored as matrix rows or matrix columns.\n   *   \n   *   @param mean optional mean value; if the matrix is empty (noArray()), the mean is computed from the\n   * data.\n   *   \n   *   @param flags operation flags; currently the parameter is only used to specify the data layout\n   * (PCA::Flags)\n   *   \n   *   @param retainedVariance Percentage of variance that PCA should retain. Using this parameter will\n   * let the PCA decided how many components to retain but it will always keep at least 2.\n   */\n  public constructor(data: InputArray, mean: InputArray, flags: int, retainedVariance: double)\n\n  /**\n   *   The methods are inverse operations to\n   * [PCA::project](#d3/d8d/classcv_1_1PCA_1a67c9a3f8fe804f40be58c88a3ae73f41}). They take PC coordinates\n   * of projected vectors and reconstruct the original vectors. Unless all the principal components have\n   * been retained, the reconstructed vectors are different from the originals. But typically, the\n   * difference is small if the number of components is large enough (but still much smaller than the\n   * original vector dimensionality). As a result, [PCA](#d3/d8d/classcv_1_1PCA}) is used.\n   *   \n   *   @param vec coordinates of the vectors in the principal component subspace, the layout and size are\n   * the same as of PCA::project output vectors.\n   */\n  public backProject(vec: InputArray): Mat\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param vec coordinates of the vectors in the principal component subspace, the layout and size are\n   * the same as of PCA::project output vectors.\n   *   \n   *   @param result reconstructed vectors; the layout and size are the same as of PCA::project input\n   * vectors.\n   */\n  public backProject(vec: InputArray, result: OutputArray): InputArray\n\n  /**\n   *   The methods project one or more vectors to the principal component subspace, where each vector\n   * projection is represented by coefficients in the principal component basis. The first form of the\n   * method returns the matrix that the second form writes to the result. So the first form can be used\n   * as a part of expression while the second form can be more efficient in a processing loop.\n   *   \n   *   @param vec input vector(s); must have the same dimensionality and the same layout as the input\n   * data used at PCA phase, that is, if DATA_AS_ROW are specified, then vec.cols==data.cols (vector\n   * dimensionality) and vec.rows is the number of vectors to project, and the same is true for the\n   * PCA::DATA_AS_COL case.\n   */\n  public project(vec: InputArray): Mat\n\n  /**\n   *   This is an overloaded member function, provided for convenience. It differs from the above\n   * function only in what argument(s) it accepts.\n   *   \n   *   @param vec input vector(s); must have the same dimensionality and the same layout as the input\n   * data used at PCA phase, that is, if DATA_AS_ROW are specified, then vec.cols==data.cols (vector\n   * dimensionality) and vec.rows is the number of vectors to project, and the same is true for the\n   * PCA::DATA_AS_COL case.\n   *   \n   *   @param result output vectors; in case of PCA::DATA_AS_COL, the output matrix has as many columns\n   * as the number of input vectors, this means that result.cols==vec.cols and the number of rows match\n   * the number of principal components (for example, maxComponents parameter passed to the constructor).\n   */\n  public project(vec: InputArray, result: OutputArray): InputArray\n\n  /**\n   *   Loads [eigenvalues](#d3/d8d/classcv_1_1PCA_1a1c9d34c02df49120474a4a366b971303})\n   * [eigenvectors](#d3/d8d/classcv_1_1PCA_1a8fed85cf5f9d8bb9b17f031398cb74a0}) and\n   * [mean](#d3/d8d/classcv_1_1PCA_1a1bca9d1cc7808b7d08b2a046ee92cd11}) from specified\n   * [FileNode](#de/dd9/classcv_1_1FileNode})\n   */\n  public read(fn: FileNode): FileNode\n\n  /**\n   *   Writes [eigenvalues](#d3/d8d/classcv_1_1PCA_1a1c9d34c02df49120474a4a366b971303})\n   * [eigenvectors](#d3/d8d/classcv_1_1PCA_1a8fed85cf5f9d8bb9b17f031398cb74a0}) and\n   * [mean](#d3/d8d/classcv_1_1PCA_1a1bca9d1cc7808b7d08b2a046ee92cd11}) to specified\n   * [FileStorage](#da/d56/classcv_1_1FileStorage})\n   */\n  public write(fs: FileStorage): FileStorage\n}\n\nexport declare const DATA_AS_ROW: Flags // initializer: = 0\n\nexport declare const DATA_AS_COL: Flags // initializer: = 1\n\nexport declare const USE_AVG: Flags // initializer: = 2\n\nexport type Flags = any\n\n"},"node_modules_mirada_dist_src_types_opencv_Node_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_Node_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/Node.d.ts","content":"\nimport { double, int } from './_types'\n\nexport declare class Node {\n\n  /**\n   *   Class index normalized to 0..class_count-1 range and assigned to the node. It is used internally\n   * in classification trees and tree ensembles.\n   *   \n   */\n  public classIdx: int\n\n  /**\n   *   Default direction where to go (-1: left or +1: right). It helps in the case of missing values.\n   *   \n   */\n  public defaultDir: int\n\n  public left: int\n\n  public parent: int\n\n  public right: int\n\n  public split: int\n\n  /**\n   *   Value at the node: a class label in case of classification or estimated function value in case of\n   * regression.\n   *   \n   */\n  public value: double\n\n  public constructor()\n}\n\n"},"node_modules_mirada_dist_src_types_opencv_photo_inpaint_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_photo_inpaint_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/photo_inpaint.d.ts","content":"\nimport { double, InputArray, int, OutputArray } from './_types'\n/*\n * # Inpainting\n * the inpainting algorithm\n */\n/**\n * The function reconstructs the selected image area from the pixel near the area boundary. The\n * function may be used to remove dust and scratches from a scanned photo, or to remove undesirable\n * objects from still images or video. See  for more details.\n * \n * An example using the inpainting technique can be found at opencv_source_code/samples/cpp/inpaint.cpp\n * (Python) An example using the inpainting technique can be found at\n * opencv_source_code/samples/python/inpaint.py\n * \n * @param src Input 8-bit, 16-bit unsigned or 32-bit float 1-channel or 8-bit 3-channel image.\n * \n * @param inpaintMask Inpainting mask, 8-bit 1-channel image. Non-zero pixels indicate the area that\n * needs to be inpainted.\n * \n * @param dst Output image with the same size and type as src .\n * \n * @param inpaintRadius Radius of a circular neighborhood of each point inpainted that is considered by\n * the algorithm.\n * \n * @param flags Inpainting method that could be cv::INPAINT_NS or cv::INPAINT_TELEA\n */\nexport declare function inpaint(src: InputArray, inpaintMask: InputArray, dst: OutputArray, inpaintRadius: double, flags: int): void\n\nexport declare const INPAINT_NS: any // initializer: = 0\n\nexport declare const INPAINT_TELEA: any // initializer: = 1\n\n"},"node_modules_mirada_dist_src_types_opencv_RotatedRect_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_RotatedRect_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/RotatedRect.d.ts","content":"\nimport { float, Point2f, Rect, Rect_, Size2f } from './_types'\n\n/**\n * Each rectangle is specified by the center point (mass center), length of each side (represented by\n * [Size2f](#dc/d84/group__core__basic_1gab34496d2466b5f69930ab74c70f117d4}) structure) and the\n * rotation angle in degrees.\n * \n * The sample below demonstrates how to use [RotatedRect](#db/dd6/classcv_1_1RotatedRect}): \n * \n * ```cpp\n *     Mat test_image(200, 200, CV_8UC3, Scalar(0));\n *     RotatedRect rRect = RotatedRect(Point2f(100,100), Size2f(100,50), 30);\n * \n *     Point2f vertices[4];\n *     rRect.points(vertices);\n *     for (int i = 0; i < 4; i++)\n *         line(test_image, vertices[i], vertices[(i+1)%4], Scalar(0,255,0), 2);\n * \n *     Rect brect = rRect.boundingRect();\n *     rectangle(test_image, brect, Scalar(255,0,0), 2);\n * \n *     imshow(\"rectangles\", test_image);\n *     waitKey(0);\n * ```\n * \n * [CamShift](#dc/d6b/group__video__track_1gaef2bd39c8356f423124f1fe7c44d54a1}),\n * [fitEllipse](#d3/dc0/group__imgproc__shape_1gaf259efaad93098103d6c27b9e4900ffa}),\n * [minAreaRect](#d3/dc0/group__imgproc__shape_1ga3d476a3417130ae5154aea421ca7ead9}), CvBox2D\n * \n * Source:\n * [opencv2/core/types.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/core/types.hpp#L534).\n * \n */\nexport declare class RotatedRect {\n\n  public angle: float\n\n  public center: Point2f\n\n  public size: Size2f\n\n  public constructor()\n\n  /**\n   *   full constructor\n   *   \n   *   @param center The rectangle mass center.\n   *   \n   *   @param size Width and height of the rectangle.\n   *   \n   *   @param angle The rotation angle in a clockwise direction. When the angle is 0, 90, 180, 270 etc.,\n   * the rectangle becomes an up-right rectangle.\n   */\n  public constructor(center: Point2f, size: Size2f, angle: float)\n\n  /**\n   *   Any 3 end points of the [RotatedRect](#db/dd6/classcv_1_1RotatedRect}). They must be given in\n   * order (either clockwise or anticlockwise).\n   */\n  public constructor(point1: Point2f, point2: Point2f, point3: Point2f)\n\n  public boundingRect(): Rect\n\n  public boundingRect2f(): Rect_\n\n  /**\n   *   returns 4 vertices of the rectangle\n   *   \n   *   @param pts The points array for storing rectangle vertices. The order is bottomLeft, topLeft,\n   * topRight, bottomRight.\n   */\n  public points(pts: Point2f): Point2f\n}\n\n"},"node_modules_mirada_dist_src_types_opencv_softdouble_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_softdouble_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/softdouble.d.ts","content":"\nimport { bool, int, int32_t, int64_t, uint32_t, uint64_t } from './_types'\n\nexport declare class softdouble {\n\n  public v: uint64_t\n\n  public constructor()\n\n  public constructor(c: softdouble)\n\n  public constructor(arg159: uint32_t)\n\n  public constructor(arg160: uint64_t)\n\n  public constructor(arg161: int32_t)\n\n  public constructor(arg162: int64_t)\n\n  public constructor(a: any)\n\n  public getExp(): int\n\n  /**\n   *   Returns a number 1 <= x < 2 with the same significand\n   */\n  public getFrac(): softdouble\n\n  public getSign(): bool\n\n  public isInf(): bool\n\n  public isNaN(): bool\n\n  public isSubnormal(): bool\n\n  public setExp(e: int): softdouble\n\n  /**\n   *   Constructs a copy of a number with significand taken from parameter\n   */\n  public setFrac(s: softdouble): softdouble\n\n  public setSign(sign: bool): softdouble\n\n  public static eps(): softdouble\n\n  /**\n   *   Builds new value from raw binary representation\n   */\n  public static fromRaw(a: uint64_t): softdouble\n\n  public static inf(): softdouble\n\n  public static max(): softdouble\n\n  public static min(): softdouble\n\n  public static nan(): softdouble\n\n  public static one(): softdouble\n\n  public static pi(): softdouble\n\n  public static zero(): softdouble\n}\n\n"},"node_modules_mirada_dist_src_types_opencv_softfloat_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_softfloat_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/softfloat.d.ts","content":"\nimport { bool, int, int32_t, int64_t, uint32_t, uint64_t } from './_types'\n\nexport declare class softfloat {\n\n  public v: uint32_t\n\n  public constructor()\n\n  public constructor(c: softfloat)\n\n  public constructor(arg174: uint32_t)\n\n  public constructor(arg175: uint64_t)\n\n  public constructor(arg176: int32_t)\n\n  public constructor(arg177: int64_t)\n\n  public constructor(a: any)\n\n  public getExp(): int\n\n  /**\n   *   Returns a number 1 <= x < 2 with the same significand\n   */\n  public getFrac(): softfloat\n\n  public getSign(): bool\n\n  public isInf(): bool\n\n  public isNaN(): bool\n\n  public isSubnormal(): bool\n\n  public setExp(e: int): softfloat\n\n  /**\n   *   Constructs a copy of a number with significand taken from parameter\n   */\n  public setFrac(s: softfloat): softfloat\n\n  public setSign(sign: bool): softfloat\n\n  public static eps(): softfloat\n\n  /**\n   *   Builds new value from raw binary representation\n   */\n  public static fromRaw(a: uint32_t): softfloat\n\n  public static inf(): softfloat\n\n  public static max(): softfloat\n\n  public static min(): softfloat\n\n  public static nan(): softfloat\n\n  public static one(): softfloat\n\n  public static pi(): softfloat\n\n  public static zero(): softfloat\n}\n\n"},"node_modules_mirada_dist_src_types_opencv_video_track_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_types_opencv_video_track_d_ts","originalFileName":"node_modules/mirada/dist/src/types/opencv/video_track.d.ts","content":"\nimport { bool, double, InputArray, InputOutputArray, int, Mat, OutputArray, OutputArrayOfArrays, RotatedRect, Size, TermCriteria } from './_types'\n/*\n * # Object Tracking\n * \n */\n/**\n * number of levels in constructed pyramid. Can be less than maxLevel.\n * \n * @param img 8-bit input image.\n * \n * @param pyramid output pyramid.\n * \n * @param winSize window size of optical flow algorithm. Must be not less than winSize argument of\n * calcOpticalFlowPyrLK. It is needed to calculate required padding for pyramid levels.\n * \n * @param maxLevel 0-based maximal pyramid level number.\n * \n * @param withDerivatives set to precompute gradients for the every pyramid level. If pyramid is\n * constructed without the gradients then calcOpticalFlowPyrLK will calculate them internally.\n * \n * @param pyrBorder the border mode for pyramid layers.\n * \n * @param derivBorder the border mode for gradients.\n * \n * @param tryReuseInputImage put ROI of input image into the pyramid if possible. You can pass false to\n * force data copying.\n */\nexport declare function buildOpticalFlowPyramid(img: InputArray, pyramid: OutputArrayOfArrays, winSize: Size, maxLevel: int, withDerivatives?: bool, pyrBorder?: int, derivBorder?: int, tryReuseInputImage?: bool): int\n\n/**\n * The function finds an optical flow for each prev pixel using the Farneback2003 algorithm so that\n * \n * `\\\\[\\\\texttt{prev} (y,x) \\\\sim \\\\texttt{next} ( y + \\\\texttt{flow} (y,x)[1], x + \\\\texttt{flow}\n * (y,x)[0])\\\\]`\n * \n * An example using the optical flow algorithm described by Gunnar Farneback can be found at\n * opencv_source_code/samples/cpp/fback.cpp\n * (Python) An example using the optical flow algorithm described by Gunnar Farneback can be found at\n * opencv_source_code/samples/python/opt_flow.py\n * \n * @param prev first 8-bit single-channel input image.\n * \n * @param next second input image of the same size and the same type as prev.\n * \n * @param flow computed flow image that has the same size as prev and type CV_32FC2.\n * \n * @param pyr_scale parameter, specifying the image scale (<1) to build pyramids for each image;\n * pyr_scale=0.5 means a classical pyramid, where each next layer is twice smaller than the previous\n * one.\n * \n * @param levels number of pyramid layers including the initial image; levels=1 means that no extra\n * layers are created and only the original images are used.\n * \n * @param winsize averaging window size; larger values increase the algorithm robustness to image noise\n * and give more chances for fast motion detection, but yield more blurred motion field.\n * \n * @param iterations number of iterations the algorithm does at each pyramid level.\n * \n * @param poly_n size of the pixel neighborhood used to find polynomial expansion in each pixel; larger\n * values mean that the image will be approximated with smoother surfaces, yielding more robust\n * algorithm and more blurred motion field, typically poly_n =5 or 7.\n * \n * @param poly_sigma standard deviation of the Gaussian that is used to smooth derivatives used as a\n * basis for the polynomial expansion; for poly_n=5, you can set poly_sigma=1.1, for poly_n=7, a good\n * value would be poly_sigma=1.5.\n * \n * @param flags operation flags that can be a combination of the following:\n * OPTFLOW_USE_INITIAL_FLOW uses the input flow as an initial flow\n * approximation.OPTFLOW_FARNEBACK_GAUSSIAN uses the Gaussian $\\texttt{winsize}\\times\\texttt{winsize}$\n * filter instead of a box filter of the same size for optical flow estimation; usually, this option\n * gives z more accurate flow than with a box filter, at the cost of lower speed; normally, winsize for\n * a Gaussian window should be set to a larger value to achieve the same level of robustness.\n */\nexport declare function calcOpticalFlowFarneback(prev: InputArray, next: InputArray, flow: InputOutputArray, pyr_scale: double, levels: int, winsize: int, iterations: int, poly_n: int, poly_sigma: double, flags: int): void\n\n/**\n * The function implements a sparse iterative version of the Lucas-Kanade optical flow in pyramids. See\n * Bouguet00 . The function is parallelized with the TBB library.\n * \n * An example using the Lucas-Kanade optical flow algorithm can be found at\n * opencv_source_code/samples/cpp/lkdemo.cpp\n * (Python) An example using the Lucas-Kanade optical flow algorithm can be found at\n * opencv_source_code/samples/python/lk_track.py\n * (Python) An example using the Lucas-Kanade tracker for homography matching can be found at\n * opencv_source_code/samples/python/lk_homography.py\n * \n * @param prevImg first 8-bit input image or pyramid constructed by buildOpticalFlowPyramid.\n * \n * @param nextImg second input image or pyramid of the same size and the same type as prevImg.\n * \n * @param prevPts vector of 2D points for which the flow needs to be found; point coordinates must be\n * single-precision floating-point numbers.\n * \n * @param nextPts output vector of 2D points (with single-precision floating-point coordinates)\n * containing the calculated new positions of input features in the second image; when\n * OPTFLOW_USE_INITIAL_FLOW flag is passed, the vector must have the same size as in the input.\n * \n * @param status output status vector (of unsigned chars); each element of the vector is set to 1 if\n * the flow for the corresponding features has been found, otherwise, it is set to 0.\n * \n * @param err output vector of errors; each element of the vector is set to an error for the\n * corresponding feature, type of the error measure can be set in flags parameter; if the flow wasn't\n * found then the error is not defined (use the status parameter to find such cases).\n * \n * @param winSize size of the search window at each pyramid level.\n * \n * @param maxLevel 0-based maximal pyramid level number; if set to 0, pyramids are not used (single\n * level), if set to 1, two levels are used, and so on; if pyramids are passed to input then algorithm\n * will use as many levels as pyramids have but no more than maxLevel.\n * \n * @param criteria parameter, specifying the termination criteria of the iterative search algorithm\n * (after the specified maximum number of iterations criteria.maxCount or when the search window moves\n * by less than criteria.epsilon.\n * \n * @param flags operation flags:\n * OPTFLOW_USE_INITIAL_FLOW uses initial estimations, stored in nextPts; if the flag is not set, then\n * prevPts is copied to nextPts and is considered the initial estimate.OPTFLOW_LK_GET_MIN_EIGENVALS use\n * minimum eigen values as an error measure (see minEigThreshold description); if the flag is not set,\n * then L1 distance between patches around the original and a moved point, divided by number of pixels\n * in a window, is used as a error measure.\n * \n * @param minEigThreshold the algorithm calculates the minimum eigen value of a 2x2 normal matrix of\n * optical flow equations (this matrix is called a spatial gradient matrix in Bouguet00), divided by\n * number of pixels in a window; if this value is less than minEigThreshold, then a corresponding\n * feature is filtered out and its flow is not processed, so it allows to remove bad points and get a\n * performance boost.\n */\nexport declare function calcOpticalFlowPyrLK(prevImg: InputArray, nextImg: InputArray, prevPts: InputArray, nextPts: InputOutputArray, status: OutputArray, err: OutputArray, winSize?: Size, maxLevel?: int, criteria?: TermCriteria, flags?: int, minEigThreshold?: double): void\n\n/**\n * See the OpenCV sample camshiftdemo.c that tracks colored objects.\n * \n * (Python) A sample explaining the camshift tracking algorithm can be found at\n * opencv_source_code/samples/python/camshift.py\n * \n * @param probImage Back projection of the object histogram. See calcBackProject.\n * \n * @param window Initial search window.\n * \n * @param criteria Stop criteria for the underlying meanShift. returns (in old interfaces) Number of\n * iterations CAMSHIFT took to converge The function implements the CAMSHIFT object tracking algorithm\n * Bradski98 . First, it finds an object center using meanShift and then adjusts the window size and\n * finds the optimal rotation. The function returns the rotated rectangle structure that includes the\n * object position, size, and orientation. The next position of the search window can be obtained with\n * RotatedRect::boundingRect()\n */\nexport declare function CamShift(probImage: InputArray, window: any, criteria: TermCriteria): RotatedRect\n\n/**\n * [findTransformECC](#dc/d6b/group__video__track_1ga1aa357007eaec11e9ed03500ecbcbe47})\n * \n * @param templateImage single-channel template image; CV_8U or CV_32F array.\n * \n * @param inputImage single-channel input image to be warped to provide an image similar to\n * templateImage, same type as templateImage.\n * \n * @param inputMask An optional mask to indicate valid values of inputImage.\n */\nexport declare function computeECC(templateImage: InputArray, inputImage: InputArray, inputMask?: InputArray): double\n\n/**\n * The function finds an optimal affine transform *[A|b]* (a 2 x 3 floating-point matrix) that\n * approximates best the affine transformation between:  In case of point sets, the problem is\n * formulated as follows: you need to find a 2x2 matrix *A* and 2x1 vector *b* so that:\n * \n * `\\\\[[A^*|b^*] = arg \\\\min _{[A|b]} \\\\sum _i \\\\| \\\\texttt{dst}[i] - A { \\\\texttt{src}[i]}^T - b \\\\|\n * ^2\\\\]` where src[i] and dst[i] are the i-th points in src and dst, respectively `$[A|b]$` can be\n * either arbitrary (when fullAffine=true ) or have a form of `\\\\[\\\\begin{bmatrix} a_{11} & a_{12} &\n * b_1 \\\\\\\\ -a_{12} & a_{11} & b_2 \\\\end{bmatrix}\\\\]` when fullAffine=false.\n * \n * [estimateAffine2D](#d9/d0c/group__calib3d_1ga27865b1d26bac9ce91efaee83e94d4dd}),\n * [estimateAffinePartial2D](#d9/d0c/group__calib3d_1gad767faff73e9cbd8b9d92b955b50062d}),\n * [getAffineTransform](#da/d54/group__imgproc__transform_1ga8f6d378f9f8eebb5cb55cd3ae295a999}),\n * [getPerspectiveTransform](#da/d54/group__imgproc__transform_1ga20f62aa3235d869c9956436c870893ae}),\n * [findHomography](#d9/d0c/group__calib3d_1ga4abc2ece9fab9398f2e560d53c8c9780})\n * \n * @param src First input 2D point set stored in std::vector or Mat, or an image stored in Mat.\n * \n * @param dst Second input 2D point set of the same size and the same type as A, or another image.\n * \n * @param fullAffine If true, the function finds an optimal affine transformation with no additional\n * restrictions (6 degrees of freedom). Otherwise, the class of transformations to choose from is\n * limited to combinations of translation, rotation, and uniform scaling (4 degrees of freedom).\n */\nexport declare function estimateRigidTransform(src: InputArray, dst: InputArray, fullAffine: bool): Mat\n\n/**\n * The function estimates the optimum transformation (warpMatrix) with respect to ECC criterion (EP08),\n * that is\n * \n * `\\\\[\\\\texttt{warpMatrix} = \\\\texttt{warpMatrix} = \\\\arg\\\\max_{W}\n * \\\\texttt{ECC}(\\\\texttt{templateImage}(x,y),\\\\texttt{inputImage}(x',y'))\\\\]`\n * \n * where\n * \n * `\\\\[\\\\begin{bmatrix} x' \\\\\\\\ y' \\\\end{bmatrix} = W \\\\cdot \\\\begin{bmatrix} x \\\\\\\\ y \\\\\\\\ 1\n * \\\\end{bmatrix}\\\\]`\n * \n * (the equation holds with homogeneous coordinates for homography). It returns the final enhanced\n * correlation coefficient, that is the correlation coefficient between the template image and the\n * final warped input image. When a `$3\\\\times 3$` matrix is given with motionType =0, 1 or 2, the\n * third row is ignored.\n * \n * Unlike findHomography and estimateRigidTransform, the function findTransformECC implements an\n * area-based alignment that builds on intensity similarities. In essence, the function updates the\n * initial transformation that roughly aligns the images. If this information is missing, the identity\n * warp (unity matrix) is used as an initialization. Note that if images undergo strong\n * displacements/rotations, an initial transformation that roughly aligns the images is necessary\n * (e.g., a simple euclidean/similarity transform that allows for the images showing the same image\n * content approximately). Use inverse warping in the second image to take an image close to the first\n * one, i.e. use the flag WARP_INVERSE_MAP with warpAffine or warpPerspective. See also the OpenCV\n * sample image_alignment.cpp that demonstrates the use of the function. Note that the function throws\n * an exception if algorithm does not converges.\n * \n * [computeECC](#dc/d6b/group__video__track_1gae94247c0014ff6497a3f85e60eab0a66}),\n * [estimateAffine2D](#d9/d0c/group__calib3d_1ga27865b1d26bac9ce91efaee83e94d4dd}),\n * [estimateAffinePartial2D](#d9/d0c/group__calib3d_1gad767faff73e9cbd8b9d92b955b50062d}),\n * [findHomography](#d9/d0c/group__calib3d_1ga4abc2ece9fab9398f2e560d53c8c9780})\n * \n * @param templateImage single-channel template image; CV_8U or CV_32F array.\n * \n * @param inputImage single-channel input image which should be warped with the final warpMatrix in\n * order to provide an image similar to templateImage, same type as templateImage.\n * \n * @param warpMatrix floating-point $2\\times 3$ or $3\\times 3$ mapping matrix (warp).\n * \n * @param motionType parameter, specifying the type of motion:\n * MOTION_TRANSLATION sets a translational motion model; warpMatrix is $2\\times 3$ with the first\n * $2\\times 2$ part being the unity matrix and the rest two parameters being estimated.MOTION_EUCLIDEAN\n * sets a Euclidean (rigid) transformation as motion model; three parameters are estimated; warpMatrix\n * is $2\\times 3$.MOTION_AFFINE sets an affine motion model (DEFAULT); six parameters are estimated;\n * warpMatrix is $2\\times 3$.MOTION_HOMOGRAPHY sets a homography as a motion model; eight parameters\n * are estimated;`warpMatrix` is $3\\times 3$.\n * \n * @param criteria parameter, specifying the termination criteria of the ECC algorithm;\n * criteria.epsilon defines the threshold of the increment in the correlation coefficient between two\n * iterations (a negative criteria.epsilon makes criteria.maxcount the only termination criterion).\n * Default values are shown in the declaration above.\n * \n * @param inputMask An optional mask to indicate valid values of inputImage.\n * \n * @param gaussFiltSize An optional value indicating size of gaussian blur filter; (DEFAULT: 5)\n */\nexport declare function findTransformECC(templateImage: InputArray, inputImage: InputArray, warpMatrix: InputOutputArray, motionType: int, criteria: TermCriteria, inputMask: InputArray, gaussFiltSize: int): double\n\n/**\n * This is an overloaded member function, provided for convenience. It differs from the above function\n * only in what argument(s) it accepts.\n */\nexport declare function findTransformECC(templateImage: InputArray, inputImage: InputArray, warpMatrix: InputOutputArray, motionType?: int, criteria?: TermCriteria, inputMask?: InputArray): double\n\n/**\n * @param probImage Back projection of the object histogram. See calcBackProject for details.\n * \n * @param window Initial search window.\n * \n * @param criteria Stop criteria for the iterative search algorithm. returns : Number of iterations\n * CAMSHIFT took to converge. The function implements the iterative object search algorithm. It takes\n * the input back projection of an object and the initial position. The mass center in window of the\n * back projection image is computed and the search window center shifts to the mass center. The\n * procedure is repeated until the specified number of iterations criteria.maxCount is done or until\n * the window center shifts by less than criteria.epsilon. The algorithm is used inside CamShift and,\n * unlike CamShift , the search window size or orientation do not change during the search. You can\n * simply pass the output of calcBackProject to this function. But better results can be obtained if\n * you pre-filter the back projection and remove the noise. For example, you can do this by retrieving\n * connected components with findContours , throwing away contours with small area ( contourArea ), and\n * rendering the remaining contours with drawContours.\n */\nexport declare function meanShift(probImage: InputArray, window: any, criteria: TermCriteria): int\n\n/**\n * The function readOpticalFlow loads a flow field from a file and returns it as a single matrix.\n * Resulting [Mat](#d3/d63/classcv_1_1Mat}) has a type CV_32FC2 - floating-point, 2-channel. First\n * channel corresponds to the flow in the horizontal direction (u), second - vertical (v).\n * \n * @param path Path to the file to be loaded\n */\nexport declare function readOpticalFlow(path: any): Mat\n\n/**\n * The function stores a flow field in a file, returns true on success, false otherwise. The flow field\n * must be a 2-channel, floating-point matrix (CV_32FC2). First channel corresponds to the flow in the\n * horizontal direction (u), second - vertical (v).\n * \n * @param path Path to the file to be written\n * \n * @param flow Flow field to be stored\n */\nexport declare function writeOpticalFlow(path: any, flow: InputArray): bool\n\nexport declare const OPTFLOW_USE_INITIAL_FLOW: any // initializer: = 4\n\nexport declare const OPTFLOW_LK_GET_MIN_EIGENVALS: any // initializer: = 8\n\nexport declare const OPTFLOW_FARNEBACK_GAUSSIAN: any // initializer: = 256\n\nexport declare const MOTION_TRANSLATION: any // initializer: = 0\n\nexport declare const MOTION_EUCLIDEAN: any // initializer: = 1\n\nexport declare const MOTION_AFFINE: any // initializer: = 2\n\nexport declare const MOTION_HOMOGRAPHY: any // initializer: = 3\n\n"},"node_modules_mirada_dist_src_util_base64_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_util_base64_d_ts","originalFileName":"node_modules/mirada/dist/src/util/base64.d.ts","content":"export declare function dataToUrl(data: string, mimeType: string, fileName?: string): string;\nexport declare function dataToBase64(data: string): string;\n/**\n * Creates a DataUrl like `data:image/jpeg;name=hindenburg.jpg;base64,` using given base64 content, mimeType and fileName.\n */\nexport declare function base64ToUrl(base64: string, mimeType: string, fileName?: string): string;\nexport declare function urlToBase64(s: string): string;\nexport declare function urlToData(s: string): string;\nexport declare function isBase64(str: string): boolean;\n/**\n * Extracts the name of a data url like `data:image/jpeg;name=hindenburg.jpg;base64,`..., if any.\n */\nexport declare function getDataUrlFileName(url: string): string;\nexport declare function arrayBufferToBase64(buffer: ArrayBuffer): string;\nexport declare function arrayBufferToString(buffer: ArrayBuffer): string;\n"},"node_modules_mirada_dist_src_util_browserImageUtil_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_util_browserImageUtil_d_ts","originalFileName":"node_modules/mirada/dist/src/util/browserImageUtil.d.ts","content":"import { Mat } from '../types/opencv';\nexport declare function fetchImageData(url: string): Promise<ImageData>;\n/**\n * A subptimal method to load a image array buffer (encoded in jpg, png) wihtout knowing its format or size.\n  * 1) creates a blob and a url object\n  * * loads the url in a HTML Image (to know its dimentions )\n  * * draw the image in a canvas ().\n  *\n  * This method is useful as a decoder for the browser without libraries\n */\nexport declare function renderArrayBufferInCanvas(a: ArrayBuffer, canvas?: HTMLCanvasElement, appendToBody?: boolean): Promise<{\n    canvas: HTMLCanvasElement;\n    width: number;\n    height: number;\n}>;\nexport declare function renderInCanvas(mat: Mat, canvas?: HTMLCanvasElement, appendToBody?: boolean): HTMLCanvasElement;\nexport declare function getHtmlImageData(img: Mat): ImageData;\n"},"node_modules_mirada_dist_src_util_fileUtil_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_util_fileUtil_d_ts","originalFileName":"node_modules/mirada/dist/src/util/fileUtil.d.ts","content":"/**\n * if given a file it ignores its contents and alwasys read again from FS\n */\nexport declare function readFile(f: string, FS?: import(\"..\").FS): ArrayBufferView;\n/**\n * Returns file name / path of given file relative to emscripten FS root  (in the context of emscripten FS)\n */\nexport declare function getFileName(path: string): string;\n/**\n * Returns absolute path of given file (in the context of emscripten FS)\n */\nexport declare function getFilePath(path: string): string;\nexport declare function writeFile(name: string, f: ArrayBufferView, FS?: import(\"..\").FS): void;\nexport declare function removeFile(f: string, FS?: import(\"..\").FS): void;\nexport declare function isDir(f: string, FS?: import(\"..\").FS): boolean;\nexport declare function isFile(f: string, FS?: import(\"..\").FS): boolean;\n"},"node_modules_mirada_dist_src_util_imageUtil_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_util_imageUtil_d_ts","originalFileName":"node_modules/mirada/dist/src/util/imageUtil.d.ts","content":"import { Mat } from '../types/opencv';\n/**\n * Creates an CV ImageData object from given image.\n */\nexport declare function toImageData(img: Mat): {\n    data: Uint8ClampedArray;\n    width: any;\n    height: any;\n};\n/**\n * Returns a new image that is identical to given (1, 3 or 4 channels)\n * but has 4 RGBA channels.\n */\nexport declare function toRgba(mat: Mat): Mat;\nexport declare function fromFile(f: string): Promise<Mat>;\nexport declare function fromArrayBuffer(a: ArrayBuffer): Promise<Mat>;\nexport declare function fromInputFile(a: HTMLInputElement): Promise<Mat[]>;\nexport declare function fromUrl(f: string): Promise<Mat>;\n"},"node_modules_mirada_dist_src_util_index_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_util_index_d_ts","originalFileName":"node_modules/mirada/dist/src/util/index.d.ts","content":"export * from './browserImageUtil';\nexport * from './imageUtil';\n"},"node_modules_mirada_dist_src_util_misc_d_ts":{"isBinary":false,"fileName":"node_modules_mirada_dist_src_util_misc_d_ts","originalFileName":"node_modules/mirada/dist/src/util/misc.d.ts","content":"export declare function buildError(e: any): Error;\nexport declare function resolveNodeModule(p: string): string;\n"}};
