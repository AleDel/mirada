
export const faceDetectionCamera_ts: string = "import * as Mirada from 'mirada'\ndeclare var cv: Mirada.CV;\n\n(async () => {\n  // heads up - Mirada supports CameraHelper that simplify this code - but in this example \n  // we want to do it all manually\n  async function test() {\n    const height = parseInt(videoInput.getAttribute('height')!)\n    const width = parseInt(videoInput.getAttribute('width')!)\n    let src = new cv.Mat(height, width, cv.CV_8UC4)\n    let dst = new cv.Mat(height, width, cv.CV_8UC4)\n    let gray = new cv.Mat()\n    let cap = new cv.VideoCapture(videoInput)\n    let faces = new cv.RectVector()\n    let classifier = new cv.CascadeClassifier()\n    // load pre-trained classifiers\n    classifier.load(await Mirada.loadDataFile('haarcascade_frontalface_default.xml'))\n    const FPS = 30\n    function processVideo() {\n      try {\n        if (!streaming) {\n          // clean and stop.\n          src.delete()\n          dst.delete()\n          gray.delete()\n          faces.delete()\n          classifier.delete()\n          return\n        }\n        let begin = Date.now()\n        // start processing.\n        cap.read(src)\n        src.copyTo(dst)\n        cv.cvtColor(dst, gray, cv.COLOR_RGBA2GRAY, 0)\n        // detect faces.\n        classifier.detectMultiScale(gray, faces, 1.1, 3, 0)\n        // draw faces.\n        for (let i = 0; i < faces.size(); ++i) {\n          let face = faces.get(i)\n          let point1 = new cv.Point(face.x, face.y)\n          let point2 = new cv.Point(face.x + face.width, face.y + face.height)\n          cv.rectangle(dst, point1, point2, [255, 0, 0, 255])\n        }\n        cv.imshow(outputCanvas, dst)\n        // schedule the next one.\n        let delay = 1000 / FPS - (Date.now() - begin)\n        setTimeout(processVideo, delay)\n      } catch (err) {\n        console.error(err)\n      }\n    }\n    setTimeout(processVideo, 0)\n  }\n  // schedule the first one.\n  setTimeout(start, 0)\n  setTimeout(onVideoStopped, 10000)\n\n  let streaming = false\n  let videoInput = document.getElementById('videoInput')! as HTMLVideoElement\n  let outputCanvas = document.getElementById('outputCanvas')! as HTMLCanvasElement\n  let canvasContext = outputCanvas.getContext('2d')!\n  let stream: MediaStream\n  function start() {\n    if (!streaming) {\n      startCamera('qvga', onVideoStarted, videoInput)\n    } else {\n      stopCamera()\n      onVideoStopped()\n    }\n  }\n  function onVideoStarted() {\n    streaming = true\n    outputCanvas.width = videoInput.videoWidth\n    outputCanvas.height = videoInput.videoHeight\n    test()\n  }\n  function onVideoStopped() {\n    streaming = false\n    canvasContext.clearRect(0, 0, outputCanvas.width, outputCanvas.height)\n    stopCamera()\n  }\n  let onCameraStartedCallback: (...args: any[]) => any\n  function onVideoCanPlay() {\n    if (onCameraStartedCallback) {\n      onCameraStartedCallback(stream, videoInput)\n    }\n  }\n  function startCamera(resolution: 'qvga' | 'vga', callback: (...args: any[]) => any, video: HTMLVideoElement) {\n    const constraints: any = {\n      qvga: { width: { exact: 320 }, height: { exact: 240 } },\n      vga: { width: { exact: 640 }, height: { exact: 480 } }\n    }\n    let videoConstraint = constraints[resolution]\n    if (!videoConstraint) {\n      videoConstraint = true\n    }\n    navigator.mediaDevices\n      .getUserMedia({ video, audio: false })\n      .then(function(s) {\n        video.srcObject = s\n        video.play()\n        videoInput = video\n        stream = s\n        onCameraStartedCallback = callback\n        video.addEventListener('canplay', onVideoCanPlay, false)\n      })\n      .catch(function(err) {\n        console.error(err)\n      })\n  }\n  function stopCamera() {\n    if (videoInput) {\n      videoInput.pause()\n      videoInput.srcObject = null\n      videoInput.removeEventListener('canplay', onVideoCanPlay)\n    }\n    if (stream) {\n      stream.getVideoTracks()[0].stop()\n    }\n  }\n})()\n";
